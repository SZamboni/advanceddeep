{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 loaded\n",
      "Teacher loaded from./pretrained_models/wrn_16_2.h5\n",
      "Wide Residual Network-16-1 created.\n",
      "Simple student loaded\n",
      "Generator loaded\n",
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 32, 32, 3)    1051791     input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 10)           693498      sequential_10[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "model_28 (Model)                [(None, 10), (None,  176250      sequential_10[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 20)           0           model_1[1][0]                    \n",
      "                                                                 model_28[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,921,539\n",
      "Trainable params: 1,226,467\n",
      "Non-trainable params: 695,072\n",
      "__________________________________________________________________________________________________\n",
      "Teacher predictions shape: (4, 10)\n",
      "Teacher LAYER1 predictions shape: (4, 1024)\n",
      "Teacher LAYER2 predictions shape: (4, 256)\n",
      "Teacher LAYER3 predictions shape: (4, 64)\n",
      "Student Predictions 0 shape: (4, 10)\n",
      "Student Predictions 1 shape: (4, 1024)\n",
      "Student Predictions 2 shape: (4, 256)\n",
      "Student Predictions 3 shape: (4, 64)\n",
      "teacher pred:\n",
      "[[4.76076873e-03 1.87054597e-04 1.02246270e-01 8.16853881e-01\n",
      "  5.14954366e-02 1.98314944e-03 7.24808173e-03 1.35721895e-03\n",
      "  1.28046535e-02 1.06354884e-03]\n",
      " [3.19511106e-04 7.22314053e-06 1.16125755e-02 9.79710340e-01\n",
      "  6.75475085e-03 4.45891434e-04 6.54547184e-04 5.33825805e-05\n",
      "  2.96126032e-04 1.45603772e-04]]\n",
      "t_pred_l3: \n",
      "[[0.09251235 0.11516428 0.11051296 0.1450984  0.12300624 0.10224582\n",
      "  0.09604268 0.07786237 0.12227617 0.14848964 0.14344738 0.17753047\n",
      "  0.15660402 0.14745376 0.12887305 0.10663622 0.09892891 0.13148077\n",
      "  0.12284884 0.15492709 0.16682082 0.14865509 0.1291847  0.07222414\n",
      "  0.09771384 0.10776518 0.1434054  0.12573563 0.15355441 0.15853436\n",
      "  0.15903282 0.08569924 0.09484289 0.0944421  0.13099487 0.11446723\n",
      "  0.12566596 0.11843221 0.11638538 0.09043395 0.09630976 0.1418605\n",
      "  0.15442875 0.13839552 0.15021831 0.12529516 0.11849242 0.09921522\n",
      "  0.11296548 0.14658955 0.14088656 0.13111581 0.12488626 0.14280117\n",
      "  0.1218216  0.09328945 0.09373342 0.12342508 0.11733642 0.11878053\n",
      "  0.1178434  0.12932025 0.09587168 0.08381126]\n",
      " [0.09423899 0.13268776 0.13807832 0.12352355 0.11859139 0.11202792\n",
      "  0.11321859 0.08327282 0.10336411 0.16081533 0.18183964 0.14753085\n",
      "  0.11592624 0.11636005 0.1539485  0.09781906 0.08956733 0.11393394\n",
      "  0.15043369 0.12595467 0.12079422 0.10593159 0.13804552 0.0964403\n",
      "  0.09340587 0.11944556 0.15237784 0.14938965 0.15691346 0.14130026\n",
      "  0.15827015 0.1114862  0.09992097 0.13086967 0.14796218 0.16355194\n",
      "  0.16990086 0.15076233 0.15610792 0.11131002 0.07862148 0.10999123\n",
      "  0.15974046 0.16098897 0.15893933 0.146363   0.13744399 0.09874616\n",
      "  0.08576765 0.08784227 0.12297511 0.1309747  0.14463559 0.14940014\n",
      "  0.1253572  0.11912677 0.05958914 0.05848952 0.07084818 0.06934646\n",
      "  0.0966304  0.08119786 0.08052784 0.08987474]]\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d4359b2062a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Student test loss: '\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-d4359b2062a1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_pred_l3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mt_pred_l3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mt_pred_l3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't_pred_l3 modified: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_pred_l3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ourwrnet import create_wide_residual_network\n",
    "from student_wrnet import create_wide_residual_network_student\n",
    "from cifar10utils import getCIFAR10, getCIFAR10InputShape\n",
    "\n",
    "\n",
    "'''\n",
    "Function that loads from a file the teacher\n",
    "'''\n",
    "def getTeacher(file_name):\n",
    "    with open(file_name + '.json', 'r') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(file_name + '.h5')\n",
    "    \n",
    "    with open(file_name + '_layer1.json', 'r') as f:\n",
    "        m1 = model_from_json(f.read())\n",
    "    m1.load_weights(file_name + '_layer1.h5')\n",
    "    \n",
    "    with open(file_name + '_layer2.json', 'r') as f:\n",
    "        m2 = model_from_json(f.read())\n",
    "    m2.load_weights(file_name + '_layer2.h5')\n",
    "    \n",
    "    with open(file_name + '_layer3.json', 'r') as f:\n",
    "        m3 = model_from_json(f.read())\n",
    "    m3.load_weights(file_name + '_layer3.h5')    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Teacher loaded from' + file_name + '.h5')\n",
    "    return model,m1,m2,m3\n",
    "    \n",
    "'''\n",
    "Function that loads from a file the teacher and test it on the CIRAF10 dataset\n",
    "'''\n",
    "def testTeacher(file_name):\n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    model = getTeacher(file_name)\n",
    "    opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt_rms, metrics=['accuracy'])\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Teacher test loss:', score[0])\n",
    "    print('Teacher test accuracy:', score[1])\n",
    "    \n",
    "'''\n",
    "Function that returns a simple student done by 2 convolutions, a maxpool and a final two fully connected layers\n",
    "'''\n",
    "def getStudent(input_shape):\n",
    "    num_classes = 10\n",
    "    \n",
    "    model_train,model_test, = create_wide_residual_network_student(input_shape, nb_classes=10, N=2,k=1)\n",
    "    \n",
    "    print('Simple student loaded')\n",
    "    return model_train, model_test\n",
    "\n",
    "'''\n",
    "Function that returns a simple generator\n",
    "'''\n",
    "def getGenerator():\n",
    "\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    img_shape = getCIFAR10InputShape()\n",
    "\n",
    "    model.add(Dense(128*8**2, input_shape=noise_shape))\n",
    "    model.add(Reshape((8, 8, 128)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2)) \n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(3, kernel_size=(3,3), strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization())   \n",
    "\n",
    "    #model.summary()\n",
    "    print('Generator loaded')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getGAN(teacher,student,generator):\n",
    "    z = Input(shape=(100,))\n",
    "    img = generator(z)\n",
    "    #student.trainable = False # it works if it is not true\n",
    "    teacher.trainable = False\n",
    "    \n",
    "    out_t = teacher(img)\n",
    "    out_s = student(img)\n",
    "    \n",
    "    joinedOutput = Concatenate()([out_t,out_s[0]])\n",
    "    \n",
    "    gan = Model(z,joinedOutput)\n",
    "    \n",
    "    return gan\n",
    "\n",
    "def gan_loss(y_true, y_pred):\n",
    "    t_out = y_pred[:,0:10]\n",
    "    s_out = y_pred[:,10:21]\n",
    "    \n",
    "    loss = keras.losses.kullback_leibler_divergence(t_out,s_out)\n",
    "    min_loss = (-(loss))\n",
    "    \n",
    "    return min_loss\n",
    "\n",
    "def student_loss(y_true,y_pred):\n",
    "    \n",
    "    subtracted = (y_true - y_pred)\n",
    "    \n",
    "    power2 = K.pow(subtracted,2)\n",
    "    \n",
    "    avg = K.mean(power2,-1)\n",
    "    \n",
    "    beta = 250\n",
    "    \n",
    "    to_return = avg*beta\n",
    "    \n",
    "    return to_return\n",
    "        \n",
    "def main():\n",
    "    \n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    input_shape = getCIFAR10InputShape()\n",
    "    \n",
    "    teacher, t_layer1, t_layer2,t_layer3 = getTeacher('./pretrained_models/wrn_16_2')\n",
    "    teacher.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    t_layer1.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    t_layer2.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    t_layer3.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    optim_stud = Adam(lr=2e-3, clipnorm=5.0)\n",
    "    optim_gen = Adam(lr=1e-3, clipnorm=5.0)\n",
    "    \n",
    "    student_train, student_test = getStudent(input_shape)\n",
    "    \n",
    "    student_test.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    student_train.compile(loss=[keras.losses.kullback_leibler_divergence,student_loss,student_loss,student_loss], \n",
    "                          optimizer=optim_stud)\n",
    "    \n",
    "    generator = getGenerator()\n",
    "    '''\n",
    "    print('TEACHER SUMMARY:')\n",
    "    teacher.summary()\n",
    "    print('TEACHER L1 SUMMARY:')\n",
    "    t_layer1.summary()\n",
    "    print('TEACHER L2 SUMMARY:')\n",
    "    t_layer2.summary()\n",
    "    print('TEACHER L3 SUMMARY:')\n",
    "    t_layer3.summary()\n",
    "    \n",
    "    print('STUDENT SUMMARY:')\n",
    "    student_train.summary()\n",
    "    '''\n",
    "    \n",
    "    gan = getGAN(teacher,student_train,generator)\n",
    "    gan.summary()\n",
    "    \n",
    "    gan.compile(loss=gan_loss, optimizer=optim_gen)\n",
    "    \n",
    "    noise = np.random.normal(0, 1, (4, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    t_predictions = teacher.predict(gen_imgs)\n",
    "    t_pred_l1 = t_layer1.predict(gen_imgs)\n",
    "    t_pred_l2 = t_layer2.predict(gen_imgs)\n",
    "    t_pred_l3 = t_layer3.predict(gen_imgs)\n",
    "    \n",
    "    print('Teacher predictions shape: ' + str(t_predictions.shape))\n",
    "    print('Teacher LAYER1 predictions shape: ' + str(t_pred_l1[0].shape))\n",
    "    print('Teacher LAYER2 predictions shape: ' + str(t_pred_l2[0].shape))\n",
    "    print('Teacher LAYER3 predictions shape: ' + str(t_pred_l3[0].shape))\n",
    "    \n",
    "    s_predictions = student_train.predict(gen_imgs)\n",
    "    for i in range(len(s_predictions)):\n",
    "        print('Student Predictions ' + str(i) + ' shape: ' + str(s_predictions[i].shape))    \n",
    "    \n",
    "    n_batches = 1000\n",
    "    batch_size = 2\n",
    "    log_freq = 10\n",
    "    ns = 10\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        t_predictions = teacher.predict(gen_imgs)\n",
    "        t_pred_l1 = t_layer1.predict(gen_imgs)[0]\n",
    "        t_pred_l2 = t_layer2.predict(gen_imgs)[0]\n",
    "        t_pred_l3 = t_layer3.predict(gen_imgs)[0]\n",
    "        \n",
    "        fake_lbl = K.zeros((batch_size,20))\n",
    "        g_loss = gan.train_on_batch(noise,fake_lbl)\n",
    "        s_loss = 0\n",
    "        \n",
    "        fake1 = K.zeros((batch_size,1024))\n",
    "        fake2 = K.zeros((batch_size,256))\n",
    "        fake3 = K.zeros((batch_size,64))\n",
    "        \n",
    "        print('teacher pred:')\n",
    "        print(str(t_predictions))\n",
    "        print('t_pred_l3: ')\n",
    "        print(str(t_pred_l3))\n",
    "        \n",
    "        #t_pred_l3 = np.concatenate( t_pred_l3, axis=1 )\n",
    "        print('t_pred_l3 modified: ')\n",
    "        print(str(t_pred_l3))\n",
    "        \n",
    "        \n",
    "        for j in range(ns):\n",
    "            s_loss = student_train.train_on_batch(gen_imgs,[t_predictions, fake1,fake2,t_pred_l3])\n",
    "        \n",
    "        print('batch ' + str(i) + '/' + str(n_batches) + ' G loss: ' + str(g_loss) + ' S loss: ' + str(s_loss))\n",
    "        \n",
    "        if (i % log_freq) == 0:\n",
    "            score = student_test.evaluate(x_test, y_test, verbose=0)\n",
    "            print('Student test loss: '  + str(score))\n",
    "        \n",
    "        \n",
    "    score = student_test.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Student test loss: '  + str(score))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n",
      "tensor([[0.0281, 0.0180, 0.0306,  ..., 0.0311, 0.0323, 0.0307],\n",
      "        [0.0376, 0.0267, 0.0293,  ..., 0.0250, 0.0251, 0.0296],\n",
      "        [0.0344, 0.0360, 0.0278,  ..., 0.0272, 0.0277, 0.0373],\n",
      "        [0.0358, 0.0309, 0.0307,  ..., 0.0341, 0.0362, 0.0355]])\n",
      "torch.Size([4, 1024])\n",
      "tensor([[0.0275, 0.0311, 0.0335,  ..., 0.0392, 0.0358, 0.0284],\n",
      "        [0.0417, 0.0398, 0.0382,  ..., 0.0369, 0.0415, 0.0331],\n",
      "        [0.0202, 0.0169, 0.0289,  ..., 0.0372, 0.0257, 0.0403],\n",
      "        [0.0257, 0.0245, 0.0465,  ..., 0.0305, 0.0282, 0.0244]])\n",
      "tensor(6.9508e-05)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def attention(x):\n",
    "\n",
    "    return F.normalize(x.pow(2).mean(1).view(x.size(0), -1))\n",
    "\n",
    "def attention_diff(x, y):\n",
    "\n",
    "    return (attention(x) - attention(y)).pow(2).mean()\n",
    "\n",
    "x1 = torch.rand(4,32,32,32)\n",
    "y1 = attention(x1)\n",
    "print(str(y1.shape))\n",
    "print(str(y1))\n",
    "\n",
    "x2 = torch.rand(4,16,32,32)\n",
    "y2 = attention(x2)\n",
    "print(str(y2.shape))\n",
    "print(str(y2))\n",
    "\n",
    "y3 = attention_diff(x1,x2)\n",
    "print(str(y3))\n",
    "print(str(y3.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
