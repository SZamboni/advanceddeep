{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 loaded\n",
      "CIFAR10 shape: (32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/advanceddeep/our_code/ourwrnet.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(input)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:61: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:61: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:104: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:112: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:61: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:135: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:207: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n",
      "  x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide Residual Network-16-2 created.\n",
      "Epoch 1/100\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 2.6344 - acc: 0.3312 - val_loss: 2.4125 - val_acc: 0.4068\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 39s 99ms/step - loss: 1.9057 - acc: 0.4913 - val_loss: 1.6556 - val_acc: 0.5557\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 1.5682 - acc: 0.5647 - val_loss: 1.4513 - val_acc: 0.6050\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 1.3714 - acc: 0.6142 - val_loss: 1.5334 - val_acc: 0.6015\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 1.2475 - acc: 0.6494 - val_loss: 1.2005 - val_acc: 0.6855\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 1.1641 - acc: 0.6744 - val_loss: 1.2367 - val_acc: 0.6822\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 1.1052 - acc: 0.6933 - val_loss: 1.2150 - val_acc: 0.6822\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 1.0682 - acc: 0.7047 - val_loss: 1.1559 - val_acc: 0.7063\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 1.0354 - acc: 0.7165 - val_loss: 1.0565 - val_acc: 0.7257\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9956 - acc: 0.7303 - val_loss: 1.0041 - val_acc: 0.7417\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9670 - acc: 0.7395 - val_loss: 1.0747 - val_acc: 0.7206\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9484 - acc: 0.7456 - val_loss: 1.0351 - val_acc: 0.7323\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9248 - acc: 0.7521 - val_loss: 1.0951 - val_acc: 0.7249\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9063 - acc: 0.7613 - val_loss: 1.0673 - val_acc: 0.7335\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.8942 - acc: 0.7656 - val_loss: 1.0640 - val_acc: 0.7262\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.8810 - acc: 0.7684 - val_loss: 1.0098 - val_acc: 0.7405\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.8614 - acc: 0.7736 - val_loss: 0.9402 - val_acc: 0.7597\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.8548 - acc: 0.7756 - val_loss: 0.9112 - val_acc: 0.7689\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.8350 - acc: 0.7813 - val_loss: 0.9927 - val_acc: 0.7529\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.8229 - acc: 0.7867 - val_loss: 0.9892 - val_acc: 0.7585\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.8177 - acc: 0.7893 - val_loss: 1.0723 - val_acc: 0.7430\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.8023 - acc: 0.7927 - val_loss: 0.8931 - val_acc: 0.7768\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7998 - acc: 0.7941 - val_loss: 0.8136 - val_acc: 0.7970\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7820 - acc: 0.8004 - val_loss: 0.9488 - val_acc: 0.7724\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7734 - acc: 0.8041 - val_loss: 0.8138 - val_acc: 0.7985\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7680 - acc: 0.8042 - val_loss: 0.8788 - val_acc: 0.7825\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7526 - acc: 0.8096 - val_loss: 0.9914 - val_acc: 0.7740\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7487 - acc: 0.8097 - val_loss: 0.7784 - val_acc: 0.8045\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7413 - acc: 0.8112 - val_loss: 0.7856 - val_acc: 0.8071\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7332 - acc: 0.8146 - val_loss: 0.8591 - val_acc: 0.7929\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.7224 - acc: 0.8185 - val_loss: 0.7938 - val_acc: 0.8080\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.6407 - acc: 0.8467 - val_loss: 0.7203 - val_acc: 0.8320\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.6096 - acc: 0.8564 - val_loss: 0.6641 - val_acc: 0.8438\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5926 - acc: 0.8615 - val_loss: 0.6827 - val_acc: 0.8382\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5842 - acc: 0.8635 - val_loss: 0.6732 - val_acc: 0.8417\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5754 - acc: 0.8649 - val_loss: 0.6580 - val_acc: 0.8423\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5683 - acc: 0.8646 - val_loss: 0.6367 - val_acc: 0.8510\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5613 - acc: 0.8669 - val_loss: 0.6842 - val_acc: 0.8383\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5562 - acc: 0.8686 - val_loss: 0.6790 - val_acc: 0.8433\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5524 - acc: 0.8693 - val_loss: 0.6391 - val_acc: 0.8495\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5461 - acc: 0.8715 - val_loss: 0.6407 - val_acc: 0.8477\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5413 - acc: 0.8728 - val_loss: 0.6213 - val_acc: 0.8526\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5379 - acc: 0.8711 - val_loss: 0.6138 - val_acc: 0.8519\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5370 - acc: 0.8717 - val_loss: 0.6242 - val_acc: 0.8516\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5288 - acc: 0.8752 - val_loss: 0.6466 - val_acc: 0.8446\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5213 - acc: 0.8761 - val_loss: 0.6181 - val_acc: 0.8519\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5179 - acc: 0.8773 - val_loss: 0.6565 - val_acc: 0.8425\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5127 - acc: 0.8783 - val_loss: 0.6271 - val_acc: 0.8508\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5112 - acc: 0.8770 - val_loss: 0.6762 - val_acc: 0.8379\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5103 - acc: 0.8777 - val_loss: 0.6365 - val_acc: 0.8514\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5100 - acc: 0.8768 - val_loss: 0.6155 - val_acc: 0.8540\n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4972 - acc: 0.8806 - val_loss: 0.6000 - val_acc: 0.8544\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5004 - acc: 0.8792 - val_loss: 0.5957 - val_acc: 0.8563\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4953 - acc: 0.8819 - val_loss: 0.6288 - val_acc: 0.8491\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4926 - acc: 0.8817 - val_loss: 0.5770 - val_acc: 0.8568\n",
      "Epoch 56/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4866 - acc: 0.8828 - val_loss: 0.5911 - val_acc: 0.8560\n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4868 - acc: 0.8813 - val_loss: 0.6111 - val_acc: 0.8493\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4843 - acc: 0.8840 - val_loss: 0.6050 - val_acc: 0.8558\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4819 - acc: 0.8829 - val_loss: 0.5831 - val_acc: 0.8586\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4761 - acc: 0.8856 - val_loss: 0.6160 - val_acc: 0.8524\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 40s 101ms/step - loss: 0.4746 - acc: 0.8857 - val_loss: 0.6071 - val_acc: 0.8523\n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4609 - acc: 0.8888 - val_loss: 0.5683 - val_acc: 0.8617\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4523 - acc: 0.8940 - val_loss: 0.6066 - val_acc: 0.8544\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4502 - acc: 0.8937 - val_loss: 0.6602 - val_acc: 0.8398\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4498 - acc: 0.8945 - val_loss: 0.5929 - val_acc: 0.8579\n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4444 - acc: 0.8956 - val_loss: 0.5758 - val_acc: 0.8623\n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4489 - acc: 0.8937 - val_loss: 0.5419 - val_acc: 0.8697\n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4459 - acc: 0.8954 - val_loss: 0.5684 - val_acc: 0.8664\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4403 - acc: 0.8970 - val_loss: 0.5824 - val_acc: 0.8617\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4420 - acc: 0.8969 - val_loss: 0.5379 - val_acc: 0.8727\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4424 - acc: 0.8970 - val_loss: 0.5513 - val_acc: 0.8692\n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4395 - acc: 0.8960 - val_loss: 0.5892 - val_acc: 0.8598\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4378 - acc: 0.8981 - val_loss: 0.5632 - val_acc: 0.8648\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4374 - acc: 0.8979 - val_loss: 0.5775 - val_acc: 0.8607\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4332 - acc: 0.8987 - val_loss: 0.5652 - val_acc: 0.8640\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4332 - acc: 0.9003 - val_loss: 0.5649 - val_acc: 0.8644\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4347 - acc: 0.8988 - val_loss: 0.5991 - val_acc: 0.8568\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4333 - acc: 0.8972 - val_loss: 0.5661 - val_acc: 0.8645\n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4320 - acc: 0.8978 - val_loss: 0.5641 - val_acc: 0.8673\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4330 - acc: 0.8977 - val_loss: 0.5876 - val_acc: 0.8636\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4265 - acc: 0.9008 - val_loss: 0.6023 - val_acc: 0.8582\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4325 - acc: 0.8992 - val_loss: 0.5427 - val_acc: 0.8719\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4261 - acc: 0.9014 - val_loss: 0.5538 - val_acc: 0.8693\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4256 - acc: 0.9007 - val_loss: 0.5353 - val_acc: 0.8751\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4279 - acc: 0.8996 - val_loss: 0.5649 - val_acc: 0.8642\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4231 - acc: 0.9009 - val_loss: 0.5819 - val_acc: 0.8637\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4249 - acc: 0.9021 - val_loss: 0.5726 - val_acc: 0.8628\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4257 - acc: 0.9012 - val_loss: 0.5473 - val_acc: 0.8684\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4216 - acc: 0.9038 - val_loss: 0.5644 - val_acc: 0.8642\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4277 - acc: 0.9011 - val_loss: 0.5721 - val_acc: 0.8644\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4261 - acc: 0.9010 - val_loss: 0.5621 - val_acc: 0.8682\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4254 - acc: 0.9005 - val_loss: 0.6235 - val_acc: 0.8514\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4252 - acc: 0.9015 - val_loss: 0.5245 - val_acc: 0.8762\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4238 - acc: 0.9024 - val_loss: 0.6034 - val_acc: 0.8606\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4271 - acc: 0.8993 - val_loss: 0.5903 - val_acc: 0.8572\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4234 - acc: 0.9034 - val_loss: 0.6045 - val_acc: 0.8611\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4258 - acc: 0.9011 - val_loss: 0.5966 - val_acc: 0.8591\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4286 - acc: 0.9004 - val_loss: 0.5768 - val_acc: 0.8595\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4244 - acc: 0.9022 - val_loss: 0.5775 - val_acc: 0.8658\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4233 - acc: 0.9020 - val_loss: 0.5449 - val_acc: 0.8720\n",
      "Full model predictions:\n",
      "Model 1 predictions:\n",
      "Model 2 predictions:\n",
      "Model 3 predictions:\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from ourwrnet import *\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.losses import KLDivergence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import keras\n",
    "from keract import get_activations\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "#import wide_residual_network as wrn\n",
    "from keras.datasets import cifar10\n",
    "import keras.callbacks as callbacks\n",
    "import keras.utils.np_utils as kutils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Concatenate\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "Function that returns the trainand test data of the CIFAR10 already preprocessed\n",
    "'''\n",
    "def getCIFAR10():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 32, 32\n",
    "    num_classes = 10\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # format of the tensor\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "        input_shape = (3, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "    # convert in to float the images\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # new normalization with z-score\n",
    "    mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "    std = np.std(x_train,axis=(0,1,2,3))\n",
    "    x_train = (x_train-mean)/(std+1e-7)\n",
    "    x_test = (x_test-mean)/(std+1e-7)\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    print('CIFAR10 loaded')\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "\n",
    "'''\n",
    "Function that returns the shape of the CIFAR10 images\n",
    "'''\n",
    "def getCIFAR10InputShape():\n",
    "    img_rows, img_cols = 32, 32\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "        \n",
    "    return input_shape\n",
    "\n",
    "'''\n",
    "Function that returns the network to train\n",
    "'''\n",
    "def getNetwork(input_shape):\n",
    "    model, m1, m2, m3=create_wide_residual_network(input_shape, 10, N=2, k=2, dropout=0.)\n",
    "    return model,m1,m2,m3;\n",
    "\n",
    "'''\n",
    "Function that saves a model on the disk\n",
    "'''\n",
    "def saveModel(model,filename):\n",
    "    model_json = model.to_json()\n",
    "    with open(filename + '.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(filename + '.h5') \n",
    "\n",
    "'''\n",
    "Learning rate scheduler\n",
    "'''\n",
    "def lr_schedule(epoch):\n",
    "    total_epochs = 100\n",
    "    lrate = 0.1\n",
    "    if epoch > total_epochs*0.3:\n",
    "        lrate = 0.02\n",
    "    if epoch > total_epochs*0.6:\n",
    "        lrate = 0.004\n",
    "    if epoch > total_epochs*0.8:\n",
    "        lrate = 0.0008\n",
    "    return lrate\n",
    "\n",
    "'''\n",
    "Placeholder loss\n",
    "'''\n",
    "def useless_loss(y_true,y_pred):\n",
    "    zer = K.zeros(1)\n",
    "    \n",
    "    return zer\n",
    "    \n",
    "'''\n",
    "Function to try to train the network on CIFAR10\n",
    "'''\n",
    "def main():\n",
    "    epochs = 100\n",
    "    batch_size = 128\n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    \n",
    "    input_shape = getCIFAR10InputShape()\n",
    "    print('CIFAR10 shape: ' + str(input_shape))\n",
    "    \n",
    "    model,m1,m2,m3 = getNetwork(input_shape)\n",
    "    \n",
    "    # define optimizer\n",
    "    optim_sgd = optimizers.SGD(lr=0.1, decay=5e-4, momentum=0.9)\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optim_sgd, metrics=[\"acc\"])\n",
    "    \n",
    "    m1.compile(loss=[useless_loss,useless_loss],optimizer='sgd')\n",
    "    m2.compile(loss=[useless_loss,useless_loss],optimizer='sgd')\n",
    "    m3.compile(loss=[useless_loss,useless_loss],optimizer='sgd')\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=45,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "    )\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        workers=4,\n",
    "        shuffle=True,\n",
    "        callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "    \n",
    "    model_predictions = model.predict(x_train[0:1])\n",
    "    m1_predictions = m1.predict(x_train[0:1])\n",
    "    m2_predictions = m2.predict(x_train[0:1])\n",
    "    m3_predictions = m3.predict(x_train[0:1])\n",
    "    \n",
    "    print('Full model predictions:')\n",
    "    #print(str(model_predictions))\n",
    "    print(\"Model 1 predictions:\")\n",
    "    #print(str(m1_predictions))\n",
    "    print(\"Model 2 predictions:\")\n",
    "    #print(str(m2_predictions))\n",
    "    print(\"Model 3 predictions:\")\n",
    "    #print(str(m3_predictions))\n",
    "    \n",
    "    saveModel(model,'wrn_16_2')\n",
    "    saveModel(m1,'wrn_16_2_layer1')\n",
    "    saveModel(m2,'wrn_16_2_layer2')\n",
    "    saveModel(m3,'wrn_16_2_layer3')\n",
    "    \n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
