{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 loaded\n",
      "CIFAR10 shape: (32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/advanceddeep/our_code/ourwrnet.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(input)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:64: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), padding=\"same\", strides=(1, 1), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:80: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:88: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:64: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:111: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:64: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), padding=\"same\", strides=(2, 2), kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(init)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:134: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False, kernel_regularizer=<keras.reg...)`\n",
      "  use_bias=False)(x)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1011 11:12:21.414078 139876948231552 deprecation_wrapper.py:119] From /home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "/home/test/advanceddeep/our_code/ourwrnet.py:206: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n",
      "  x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide Residual Network-16-2 created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1011 11:12:24.193294 139876948231552 deprecation_wrapper.py:119] From /home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 2.3035 - acc: 0.3873 - val_loss: 1.7251 - val_acc: 0.5155\n",
      "Epoch 2/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 1.5332 - acc: 0.5671 - val_loss: 1.4966 - val_acc: 0.5849\n",
      "Epoch 3/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 1.2835 - acc: 0.6315 - val_loss: 1.6026 - val_acc: 0.5976\n",
      "Epoch 4/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 1.1430 - acc: 0.6758 - val_loss: 1.3580 - val_acc: 0.6575\n",
      "Epoch 5/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 1.0496 - acc: 0.7089 - val_loss: 1.2072 - val_acc: 0.6898\n",
      "Epoch 6/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.9925 - acc: 0.7321 - val_loss: 1.0431 - val_acc: 0.7219\n",
      "Epoch 7/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.9500 - acc: 0.7455 - val_loss: 1.0337 - val_acc: 0.7348\n",
      "Epoch 8/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.9090 - acc: 0.7596 - val_loss: 1.0845 - val_acc: 0.7166\n",
      "Epoch 9/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.8758 - acc: 0.7693 - val_loss: 0.9005 - val_acc: 0.7664\n",
      "Epoch 10/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.8477 - acc: 0.7797 - val_loss: 0.8215 - val_acc: 0.7933\n",
      "Epoch 11/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.8213 - acc: 0.7904 - val_loss: 0.7968 - val_acc: 0.8079\n",
      "Epoch 12/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.8072 - acc: 0.7930 - val_loss: 0.8759 - val_acc: 0.7816\n",
      "Epoch 13/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.7902 - acc: 0.7986 - val_loss: 0.9094 - val_acc: 0.7714\n",
      "Epoch 14/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.7671 - acc: 0.8043 - val_loss: 0.8126 - val_acc: 0.8061\n",
      "Epoch 15/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.7528 - acc: 0.8115 - val_loss: 0.9022 - val_acc: 0.7835\n",
      "Epoch 16/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.7412 - acc: 0.8148 - val_loss: 0.7910 - val_acc: 0.8057\n",
      "Epoch 17/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.7239 - acc: 0.8194 - val_loss: 0.7746 - val_acc: 0.8092\n",
      "Epoch 18/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.7160 - acc: 0.8224 - val_loss: 0.7669 - val_acc: 0.8136\n",
      "Epoch 19/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.7023 - acc: 0.8269 - val_loss: 0.7190 - val_acc: 0.8277\n",
      "Epoch 20/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.6920 - acc: 0.8298 - val_loss: 0.7299 - val_acc: 0.8189\n",
      "Epoch 21/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.6823 - acc: 0.8332 - val_loss: 0.7893 - val_acc: 0.8110\n",
      "Epoch 22/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.6706 - acc: 0.8378 - val_loss: 0.7598 - val_acc: 0.8142\n",
      "Epoch 23/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.6619 - acc: 0.8387 - val_loss: 0.6898 - val_acc: 0.8328\n",
      "Epoch 24/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.6512 - acc: 0.8400 - val_loss: 0.7418 - val_acc: 0.8174\n",
      "Epoch 25/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.6440 - acc: 0.8435 - val_loss: 0.7071 - val_acc: 0.8308\n",
      "Epoch 26/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.6347 - acc: 0.8444 - val_loss: 0.6664 - val_acc: 0.8411\n",
      "Epoch 27/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.6240 - acc: 0.8484 - val_loss: 0.6529 - val_acc: 0.8494\n",
      "Epoch 28/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.6190 - acc: 0.8521 - val_loss: 0.6212 - val_acc: 0.8552\n",
      "Epoch 29/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.6123 - acc: 0.8540 - val_loss: 0.6302 - val_acc: 0.8521\n",
      "Epoch 30/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.6039 - acc: 0.8556 - val_loss: 0.6645 - val_acc: 0.8390\n",
      "Epoch 31/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.5989 - acc: 0.8582 - val_loss: 0.6315 - val_acc: 0.8540\n",
      "Epoch 32/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.5925 - acc: 0.8575 - val_loss: 0.6693 - val_acc: 0.8374\n",
      "Epoch 33/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.5854 - acc: 0.8622 - val_loss: 0.6841 - val_acc: 0.8327\n",
      "Epoch 34/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.5801 - acc: 0.8634 - val_loss: 0.5885 - val_acc: 0.8633\n",
      "Epoch 35/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.5747 - acc: 0.8645 - val_loss: 0.6507 - val_acc: 0.8426\n",
      "Epoch 36/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.5658 - acc: 0.8680 - val_loss: 0.6349 - val_acc: 0.8480\n",
      "Epoch 37/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.5667 - acc: 0.8649 - val_loss: 0.6259 - val_acc: 0.8544\n",
      "Epoch 38/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.5534 - acc: 0.8699 - val_loss: 0.6095 - val_acc: 0.8596\n",
      "Epoch 39/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.5547 - acc: 0.8697 - val_loss: 0.6081 - val_acc: 0.8574\n",
      "Epoch 40/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.5443 - acc: 0.8732 - val_loss: 0.6617 - val_acc: 0.8409\n",
      "Epoch 41/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.4799 - acc: 0.8961 - val_loss: 0.5228 - val_acc: 0.8827\n",
      "Epoch 42/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.4504 - acc: 0.9047 - val_loss: 0.5210 - val_acc: 0.8822\n",
      "Epoch 43/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.4358 - acc: 0.9095 - val_loss: 0.5142 - val_acc: 0.8846\n",
      "Epoch 44/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.4300 - acc: 0.9115 - val_loss: 0.5178 - val_acc: 0.8843\n",
      "Epoch 45/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.4199 - acc: 0.9135 - val_loss: 0.4991 - val_acc: 0.8891\n",
      "Epoch 46/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.4192 - acc: 0.9120 - val_loss: 0.5734 - val_acc: 0.8665\n",
      "Epoch 47/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.4123 - acc: 0.9147 - val_loss: 0.5434 - val_acc: 0.8769\n",
      "Epoch 48/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.4089 - acc: 0.9138 - val_loss: 0.5275 - val_acc: 0.8816\n",
      "Epoch 49/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.4011 - acc: 0.9172 - val_loss: 0.4864 - val_acc: 0.8894\n",
      "Epoch 50/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3989 - acc: 0.9175 - val_loss: 0.5398 - val_acc: 0.8778\n",
      "Epoch 51/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3964 - acc: 0.9170 - val_loss: 0.5200 - val_acc: 0.8805\n",
      "Epoch 52/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3907 - acc: 0.9191 - val_loss: 0.5306 - val_acc: 0.8769\n",
      "Epoch 53/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3888 - acc: 0.9192 - val_loss: 0.5056 - val_acc: 0.8816\n",
      "Epoch 54/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3836 - acc: 0.9200 - val_loss: 0.4915 - val_acc: 0.8917\n",
      "Epoch 55/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3826 - acc: 0.9195 - val_loss: 0.4810 - val_acc: 0.8909\n",
      "Epoch 56/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3780 - acc: 0.9209 - val_loss: 0.4954 - val_acc: 0.8891\n",
      "Epoch 57/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3751 - acc: 0.9216 - val_loss: 0.5477 - val_acc: 0.8734\n",
      "Epoch 58/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3726 - acc: 0.9227 - val_loss: 0.5658 - val_acc: 0.8701\n",
      "Epoch 59/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3675 - acc: 0.9224 - val_loss: 0.5422 - val_acc: 0.8745\n",
      "Epoch 60/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3661 - acc: 0.9229 - val_loss: 0.5024 - val_acc: 0.8843\n",
      "Epoch 61/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3628 - acc: 0.9240 - val_loss: 0.4860 - val_acc: 0.8886\n",
      "Epoch 62/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3589 - acc: 0.9267 - val_loss: 0.4863 - val_acc: 0.8892\n",
      "Epoch 63/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3573 - acc: 0.9263 - val_loss: 0.4609 - val_acc: 0.8922\n",
      "Epoch 64/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3538 - acc: 0.9269 - val_loss: 0.5176 - val_acc: 0.8786\n",
      "Epoch 65/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3522 - acc: 0.9266 - val_loss: 0.5217 - val_acc: 0.8795\n",
      "Epoch 66/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3487 - acc: 0.9277 - val_loss: 0.4946 - val_acc: 0.8853\n",
      "Epoch 67/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3447 - acc: 0.9279 - val_loss: 0.5015 - val_acc: 0.8826\n",
      "Epoch 68/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3439 - acc: 0.9285 - val_loss: 0.5060 - val_acc: 0.8833\n",
      "Epoch 69/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3448 - acc: 0.9277 - val_loss: 0.4816 - val_acc: 0.8896\n",
      "Epoch 70/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3403 - acc: 0.9298 - val_loss: 0.4520 - val_acc: 0.8933\n",
      "Epoch 71/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3421 - acc: 0.9281 - val_loss: 0.4739 - val_acc: 0.8868\n",
      "Epoch 72/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3421 - acc: 0.9282 - val_loss: 0.4604 - val_acc: 0.8956\n",
      "Epoch 73/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3348 - acc: 0.9297 - val_loss: 0.4605 - val_acc: 0.8907\n",
      "Epoch 74/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3298 - acc: 0.9303 - val_loss: 0.4801 - val_acc: 0.8866\n",
      "Epoch 75/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3315 - acc: 0.9301 - val_loss: 0.4468 - val_acc: 0.8946\n",
      "Epoch 76/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3255 - acc: 0.9320 - val_loss: 0.4550 - val_acc: 0.8883\n",
      "Epoch 77/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3283 - acc: 0.9312 - val_loss: 0.4716 - val_acc: 0.8912\n",
      "Epoch 78/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3275 - acc: 0.9308 - val_loss: 0.4984 - val_acc: 0.8818\n",
      "Epoch 79/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3210 - acc: 0.9326 - val_loss: 0.4599 - val_acc: 0.8901\n",
      "Epoch 80/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.3110 - acc: 0.9366 - val_loss: 0.4502 - val_acc: 0.8943\n",
      "Epoch 81/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3060 - acc: 0.9387 - val_loss: 0.4547 - val_acc: 0.8926\n",
      "Epoch 82/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3020 - acc: 0.9407 - val_loss: 0.4594 - val_acc: 0.8926\n",
      "Epoch 83/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3009 - acc: 0.9392 - val_loss: 0.4832 - val_acc: 0.8876\n",
      "Epoch 84/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2996 - acc: 0.9416 - val_loss: 0.4812 - val_acc: 0.8833\n",
      "Epoch 85/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3024 - acc: 0.9392 - val_loss: 0.4342 - val_acc: 0.8974\n",
      "Epoch 86/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2978 - acc: 0.9411 - val_loss: 0.5004 - val_acc: 0.8816\n",
      "Epoch 87/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2978 - acc: 0.9415 - val_loss: 0.4661 - val_acc: 0.8875\n",
      "Epoch 88/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2976 - acc: 0.9407 - val_loss: 0.5289 - val_acc: 0.8718\n",
      "Epoch 89/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2934 - acc: 0.9425 - val_loss: 0.4642 - val_acc: 0.8887\n",
      "Epoch 90/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2982 - acc: 0.9408 - val_loss: 0.4565 - val_acc: 0.8916\n",
      "Epoch 91/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2944 - acc: 0.9412 - val_loss: 0.4261 - val_acc: 0.9026\n",
      "Epoch 92/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2921 - acc: 0.9423 - val_loss: 0.4794 - val_acc: 0.8862\n",
      "Epoch 93/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2942 - acc: 0.9429 - val_loss: 0.4980 - val_acc: 0.8818\n",
      "Epoch 94/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2925 - acc: 0.9442 - val_loss: 0.5086 - val_acc: 0.8794\n",
      "Epoch 95/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2916 - acc: 0.9423 - val_loss: 0.4619 - val_acc: 0.8922\n",
      "Epoch 96/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2892 - acc: 0.9448 - val_loss: 0.4833 - val_acc: 0.8862\n",
      "Epoch 97/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2928 - acc: 0.9436 - val_loss: 0.5142 - val_acc: 0.8757\n",
      "Epoch 98/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2894 - acc: 0.9441 - val_loss: 0.5205 - val_acc: 0.8782\n",
      "Epoch 99/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2939 - acc: 0.9421 - val_loss: 0.4601 - val_acc: 0.8893\n",
      "Epoch 100/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2852 - acc: 0.9453 - val_loss: 0.4140 - val_acc: 0.9011\n",
      "Epoch 101/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2882 - acc: 0.9437 - val_loss: 0.4831 - val_acc: 0.8890\n",
      "Epoch 102/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2898 - acc: 0.9445 - val_loss: 0.4527 - val_acc: 0.8930\n",
      "Epoch 103/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2859 - acc: 0.9446 - val_loss: 0.4502 - val_acc: 0.8930\n",
      "Epoch 104/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2848 - acc: 0.9442 - val_loss: 0.4827 - val_acc: 0.8847\n",
      "Epoch 105/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2855 - acc: 0.9450 - val_loss: 0.4556 - val_acc: 0.8917\n",
      "Epoch 106/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2846 - acc: 0.9446 - val_loss: 0.5147 - val_acc: 0.8805\n",
      "Epoch 107/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2843 - acc: 0.9449 - val_loss: 0.4296 - val_acc: 0.8975\n",
      "Epoch 108/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2851 - acc: 0.9448 - val_loss: 0.4925 - val_acc: 0.8842\n",
      "Epoch 109/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2851 - acc: 0.9447 - val_loss: 0.5217 - val_acc: 0.8761\n",
      "Epoch 110/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2831 - acc: 0.9455 - val_loss: 0.5064 - val_acc: 0.8830\n",
      "Epoch 111/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2799 - acc: 0.9469 - val_loss: 0.4880 - val_acc: 0.8854\n",
      "Epoch 112/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2826 - acc: 0.9465 - val_loss: 0.4673 - val_acc: 0.8916\n",
      "Epoch 113/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2810 - acc: 0.9465 - val_loss: 0.4706 - val_acc: 0.8882\n",
      "Epoch 114/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2812 - acc: 0.9448 - val_loss: 0.5027 - val_acc: 0.8830\n",
      "Epoch 115/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2857 - acc: 0.9456 - val_loss: 0.4515 - val_acc: 0.8928\n",
      "Epoch 116/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2856 - acc: 0.9449 - val_loss: 0.4667 - val_acc: 0.8887\n",
      "Epoch 117/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2824 - acc: 0.9459 - val_loss: 0.4335 - val_acc: 0.8956\n",
      "Epoch 118/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2823 - acc: 0.9461 - val_loss: 0.4406 - val_acc: 0.8977\n",
      "Epoch 119/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2806 - acc: 0.9459 - val_loss: 0.4554 - val_acc: 0.8919\n",
      "Epoch 120/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2824 - acc: 0.9459 - val_loss: 0.4690 - val_acc: 0.8921\n",
      "Epoch 121/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2818 - acc: 0.9464 - val_loss: 0.4665 - val_acc: 0.8879\n",
      "Epoch 122/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2804 - acc: 0.9455 - val_loss: 0.4534 - val_acc: 0.8928\n",
      "Epoch 123/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2833 - acc: 0.9452 - val_loss: 0.4284 - val_acc: 0.8999\n",
      "Epoch 124/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2826 - acc: 0.9454 - val_loss: 0.4552 - val_acc: 0.8936\n",
      "Epoch 125/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2829 - acc: 0.9454 - val_loss: 0.4526 - val_acc: 0.8963\n",
      "Epoch 126/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2814 - acc: 0.9461 - val_loss: 0.4735 - val_acc: 0.8885\n",
      "Epoch 127/130\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.2825 - acc: 0.9450 - val_loss: 0.4642 - val_acc: 0.8903\n",
      "Epoch 128/130\n",
      "782/782 [==============================] - 47s 59ms/step - loss: 0.2842 - acc: 0.9448 - val_loss: 0.4577 - val_acc: 0.8922\n",
      "Epoch 129/130\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.2830 - acc: 0.9457 - val_loss: 0.5003 - val_acc: 0.8808\n",
      "Epoch 130/130\n",
      "782/782 [==============================] - 47s 61ms/step - loss: 0.2819 - acc: 0.9460 - val_loss: 0.4401 - val_acc: 0.8957\n",
      "Full model predictions:\n",
      "[[1.1337215e-08 3.5368473e-07 9.3441406e-07 2.7240490e-04 3.2695600e-06\n",
      "  4.5064429e-05 9.9963617e-01 3.7000824e-05 4.4504573e-06 2.8306141e-07]]\n",
      "Model 1 predictions:\n",
      "[array([[0.04614264, 0.04883225, 0.03891909, ..., 0.06816944, 0.05165309,\n",
      "        0.01846801]], dtype=float32), array([[1.1337215e-08, 3.5368473e-07, 9.3441406e-07, 2.7240490e-04,\n",
      "        3.2695600e-06, 4.5064429e-05, 9.9963617e-01, 3.7000824e-05,\n",
      "        4.4504573e-06, 2.8306141e-07]], dtype=float32)]\n",
      "Model 2 predictions:\n",
      "[array([[0.04863744, 0.06954986, 0.05041125, 0.03682549, 0.04070268,\n",
      "        0.04868624, 0.05525834, 0.05039717, 0.03770172, 0.02706121,\n",
      "        0.02963895, 0.03686907, 0.04692338, 0.05392923, 0.04576487,\n",
      "        0.02739978, 0.0552136 , 0.10070647, 0.07988935, 0.05876635,\n",
      "        0.05992587, 0.06575115, 0.06211096, 0.06634955, 0.05855341,\n",
      "        0.04649568, 0.04549085, 0.06586329, 0.05744946, 0.05308207,\n",
      "        0.06783658, 0.05469132, 0.02679625, 0.05689673, 0.04169701,\n",
      "        0.04114164, 0.04372011, 0.04917281, 0.06238669, 0.05778835,\n",
      "        0.07045375, 0.04779329, 0.04838935, 0.05689338, 0.04527168,\n",
      "        0.02769626, 0.04421341, 0.05311766, 0.02163196, 0.03607035,\n",
      "        0.03430247, 0.04394363, 0.05354032, 0.07296325, 0.07449111,\n",
      "        0.07660565, 0.09667133, 0.06483368, 0.04301174, 0.06193938,\n",
      "        0.05945008, 0.04427904, 0.04223152, 0.04379889, 0.02546759,\n",
      "        0.04124195, 0.03690332, 0.05129141, 0.06957279, 0.09052479,\n",
      "        0.06520987, 0.04716628, 0.10930679, 0.0618029 , 0.03053249,\n",
      "        0.04067747, 0.05225147, 0.03472391, 0.03733281, 0.03242303,\n",
      "        0.01920788, 0.0340906 , 0.04830563, 0.05308434, 0.06311195,\n",
      "        0.06680399, 0.05082745, 0.05797374, 0.06136559, 0.05453825,\n",
      "        0.05516791, 0.05657529, 0.05006154, 0.03045203, 0.03080126,\n",
      "        0.02531705, 0.02034036, 0.0336935 , 0.04910052, 0.05675533,\n",
      "        0.05272449, 0.06520284, 0.07792611, 0.08993887, 0.06193428,\n",
      "        0.04617794, 0.04279447, 0.03981322, 0.04429611, 0.03617124,\n",
      "        0.0336003 , 0.01857569, 0.02609506, 0.03633273, 0.04571064,\n",
      "        0.05826086, 0.06352516, 0.10598803, 0.11121266, 0.08584731,\n",
      "        0.073448  , 0.04895093, 0.03807664, 0.05836722, 0.07580106,\n",
      "        0.0698963 , 0.0427996 , 0.01920472, 0.03329092, 0.03978773,\n",
      "        0.04843675, 0.05510129, 0.09178238, 0.12347035, 0.09179424,\n",
      "        0.08793607, 0.08225992, 0.08434945, 0.04967597, 0.05755106,\n",
      "        0.09387615, 0.07200921, 0.05154787, 0.0292983 , 0.03696232,\n",
      "        0.03912884, 0.0487707 , 0.10706976, 0.12585898, 0.09783059,\n",
      "        0.07540053, 0.10151951, 0.08328898, 0.07540596, 0.0461763 ,\n",
      "        0.08823934, 0.11241625, 0.05587509, 0.05798522, 0.04285336,\n",
      "        0.02842913, 0.03632225, 0.04646423, 0.1399933 , 0.13159034,\n",
      "        0.05524552, 0.05060052, 0.06773327, 0.06051014, 0.06864574,\n",
      "        0.04663445, 0.09842281, 0.10621972, 0.09503441, 0.09180119,\n",
      "        0.05987955, 0.03097546, 0.04121967, 0.03963856, 0.06714832,\n",
      "        0.05503441, 0.04389197, 0.04992873, 0.04959895, 0.04710173,\n",
      "        0.03920408, 0.04421512, 0.08298555, 0.13313784, 0.10906072,\n",
      "        0.16770858, 0.08123218, 0.03356598, 0.05558543, 0.0295303 ,\n",
      "        0.02603438, 0.03467208, 0.05381522, 0.06921844, 0.07322309,\n",
      "        0.04903849, 0.04362651, 0.05965551, 0.08236358, 0.08358382,\n",
      "        0.12524095, 0.12281431, 0.09208863, 0.03067458, 0.06442858,\n",
      "        0.03245289, 0.02752011, 0.03073818, 0.03331219, 0.04520695,\n",
      "        0.05068976, 0.03617632, 0.0329758 , 0.03571027, 0.04286533,\n",
      "        0.05084922, 0.0685201 , 0.09438978, 0.09103744, 0.04424012,\n",
      "        0.08346433, 0.04191775, 0.04632393, 0.04517226, 0.04808248,\n",
      "        0.0499364 , 0.05822723, 0.05562432, 0.05089648, 0.04307739,\n",
      "        0.04731854, 0.05737912, 0.07797479, 0.09791597, 0.08354134,\n",
      "        0.03281445, 0.0619438 , 0.03586866, 0.03303877, 0.03753066,\n",
      "        0.04598614, 0.04724125, 0.04083135, 0.03904915, 0.02814206,\n",
      "        0.03946331, 0.03451403, 0.04750654, 0.08498245, 0.09038963,\n",
      "        0.05152659]], dtype=float32), array([[1.1337215e-08, 3.5368473e-07, 9.3441406e-07, 2.7240490e-04,\n",
      "        3.2695600e-06, 4.5064429e-05, 9.9963617e-01, 3.7000824e-05,\n",
      "        4.4504573e-06, 2.8306141e-07]], dtype=float32)]\n",
      "Model 3 predictions:\n",
      "[array([[0.0477697 , 0.06993371, 0.08053167, 0.08227068, 0.09148529,\n",
      "        0.08737265, 0.07651399, 0.04418597, 0.0470265 , 0.10480588,\n",
      "        0.14457627, 0.1852772 , 0.21565276, 0.15863295, 0.11365537,\n",
      "        0.05686123, 0.03303677, 0.0780974 , 0.1453834 , 0.2686693 ,\n",
      "        0.32220247, 0.1838678 , 0.1432918 , 0.07294639, 0.03954766,\n",
      "        0.09321037, 0.15557547, 0.23742253, 0.28797728, 0.20206432,\n",
      "        0.15119597, 0.06431422, 0.04213367, 0.08219709, 0.13017309,\n",
      "        0.16915058, 0.19289432, 0.13402148, 0.12505156, 0.04595035,\n",
      "        0.05048444, 0.08286978, 0.10792572, 0.10058293, 0.12077916,\n",
      "        0.08525621, 0.12561274, 0.06694037, 0.04402964, 0.05817908,\n",
      "        0.06167331, 0.08755547, 0.09035408, 0.08583776, 0.13238543,\n",
      "        0.10748755, 0.02913481, 0.04788752, 0.0445747 , 0.05167644,\n",
      "        0.061061  , 0.06929033, 0.10245492, 0.07357039]], dtype=float32), array([[1.1337215e-08, 3.5368473e-07, 9.3441406e-07, 2.7240490e-04,\n",
      "        3.2695600e-06, 4.5064429e-05, 9.9963617e-01, 3.7000824e-05,\n",
      "        4.4504573e-06, 2.8306141e-07]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from ourwrnet import *\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.losses import KLDivergence\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "#import wide_residual_network as wrn\n",
    "from keras.datasets import cifar10\n",
    "import keras.callbacks as callbacks\n",
    "import keras.utils.np_utils as kutils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Concatenate\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from cifar10utils import getCIFAR10, getCIFAR10InputShape\n",
    "\n",
    "'''\n",
    "Function that returns the network to train\n",
    "'''\n",
    "def getNetwork(input_shape):\n",
    "    model, m1, m2, m3=create_wide_residual_network(input_shape, 10, N=2, k=2, dropout=0.)\n",
    "    return model,m1,m2,m3;\n",
    "\n",
    "'''\n",
    "Function that saves a model on the disk\n",
    "'''\n",
    "def saveModel(model,filename):\n",
    "    model_json = model.to_json()\n",
    "    with open(filename + '.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(filename + '.h5') \n",
    "\n",
    "'''\n",
    "Learning rate scheduler\n",
    "'''\n",
    "def lr_schedule(epoch):\n",
    "    total_epochs = 130\n",
    "    lrate = 0.1\n",
    "    if epoch > total_epochs*0.3:\n",
    "        lrate = 0.02\n",
    "    if epoch > total_epochs*0.6:\n",
    "        lrate = 0.004\n",
    "    if epoch > total_epochs*0.8:\n",
    "        lrate = 0.0008\n",
    "    return lrate\n",
    "\n",
    "'''\n",
    "Placeholder loss\n",
    "'''\n",
    "def useless_loss(y_true,y_pred):\n",
    "    zer = K.zeros(1)\n",
    "    \n",
    "    return zer\n",
    "    \n",
    "'''\n",
    "Function to try to train the network on CIFAR10\n",
    "'''\n",
    "def main():\n",
    "    epochs = 130\n",
    "    batch_size = 64\n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    \n",
    "    input_shape = getCIFAR10InputShape()\n",
    "    print('CIFAR10 shape: ' + str(input_shape))\n",
    "    \n",
    "    model,m1,m2,m3 = getNetwork(input_shape)\n",
    "    \n",
    "    # define optimizer\n",
    "    optim_sgd = optimizers.SGD(lr=0.1, decay=5e-4, momentum=0.9)\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optim_sgd, metrics=[\"acc\"])\n",
    "    \n",
    "    m1.compile(loss=[useless_loss,useless_loss],optimizer='sgd')\n",
    "    m2.compile(loss=[useless_loss,useless_loss],optimizer='sgd')\n",
    "    m3.compile(loss=[useless_loss,useless_loss],optimizer='sgd')\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "    )\n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    model.fit_generator(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        workers=4,\n",
    "        shuffle=True,\n",
    "        callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "    \n",
    "    model_predictions = model.predict(x_train[0:1])\n",
    "    m1_predictions = m1.predict(x_train[0:1])\n",
    "    m2_predictions = m2.predict(x_train[0:1])\n",
    "    m3_predictions = m3.predict(x_train[0:1])\n",
    "    \n",
    "    print('Full model predictions:')\n",
    "    print(str(model_predictions))\n",
    "    print(\"Model 1 predictions:\")\n",
    "    print(str(m1_predictions))\n",
    "    print(\"Model 2 predictions:\")\n",
    "    print(str(m2_predictions))\n",
    "    print(\"Model 3 predictions:\")\n",
    "    print(str(m3_predictions))\n",
    "    \n",
    "    saveModel(model,'./pretrained_models/wrn_16_2')\n",
    "    saveModel(m1,'./pretrained_models/wrn_16_2_layer1')\n",
    "    saveModel(m2,'./pretrained_models/wrn_16_2_layer2')\n",
    "    saveModel(m3,'./pretrained_models/wrn_16_2_layer3')\n",
    "    \n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
