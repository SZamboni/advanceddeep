{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "layer names:\n",
      "conv2d_3\n",
      "conv2d_4\n",
      "max_pooling2d_2\n",
      "dropout_3\n",
      "flatten_2\n",
      "dense_3\n",
      "dropout_4\n",
      "dense_4\n",
      "8\n",
      "epoch: 0 batch 0 loss: 2.3175635 acc: 7.8125\n",
      "training time 1.1076486110687256 all layer time: 0.05936932563781738 shape: (128, 10)\n",
      "epoch: 0 batch 1 loss: 2.2415533 acc: 22.65625\n",
      "training time 0.0225222110748291 all layer time: 0.005574941635131836 shape: (128, 10)\n",
      "epoch: 0 batch 2 loss: 2.1374862 acc: 28.90625\n",
      "training time 0.022604942321777344 all layer time: 0.0058023929595947266 shape: (128, 10)\n",
      "epoch: 0 batch 3 loss: 2.0582056 acc: 35.9375\n",
      "training time 0.022075653076171875 all layer time: 0.005417823791503906 shape: (128, 10)\n",
      "epoch: 0 batch 4 loss: 1.9877467 acc: 35.15625\n",
      "training time 0.02189016342163086 all layer time: 0.005617618560791016 shape: (128, 10)\n",
      "epoch: 0 batch 5 loss: 1.8938826 acc: 39.84375\n",
      "training time 0.022000789642333984 all layer time: 0.00596165657043457 shape: (128, 10)\n",
      "epoch: 0 batch 6 loss: 1.8588821 acc: 39.84375\n",
      "training time 0.021926403045654297 all layer time: 0.005137920379638672 shape: (128, 10)\n",
      "epoch: 0 batch 7 loss: 1.7134904 acc: 43.75\n",
      "training time 0.02081441879272461 all layer time: 0.005152702331542969 shape: (128, 10)\n",
      "epoch: 0 batch 8 loss: 1.8065739 acc: 36.71875\n",
      "training time 0.020282268524169922 all layer time: 0.005129575729370117 shape: (128, 10)\n",
      "epoch: 0 batch 9 loss: 1.6735058 acc: 45.3125\n",
      "training time 0.020647287368774414 all layer time: 0.0051610469818115234 shape: (128, 10)\n",
      "epoch: 0 batch 10 loss: 1.6130893 acc: 44.53125\n",
      "training time 0.02023792266845703 all layer time: 0.005056142807006836 shape: (128, 10)\n",
      "epoch: 0 batch 11 loss: 1.5820253 acc: 51.5625\n",
      "training time 0.019641876220703125 all layer time: 0.00465846061706543 shape: (128, 10)\n",
      "epoch: 0 batch 12 loss: 1.1264138 acc: 68.75\n",
      "training time 0.01904153823852539 all layer time: 0.004925251007080078 shape: (128, 10)\n",
      "epoch: 0 batch 13 loss: 0.90988064 acc: 71.875\n",
      "training time 0.019269227981567383 all layer time: 0.004822492599487305 shape: (128, 10)\n",
      "epoch: 0 batch 14 loss: 1.4236654 acc: 51.5625\n",
      "training time 0.019321680068969727 all layer time: 0.004813194274902344 shape: (128, 10)\n",
      "epoch: 0 batch 15 loss: 1.5900068 acc: 57.03125\n",
      "training time 0.019305706024169922 all layer time: 0.00479888916015625 shape: (128, 10)\n",
      "epoch: 0 batch 16 loss: 1.1902 acc: 69.53125\n",
      "training time 0.018805503845214844 all layer time: 0.004733085632324219 shape: (128, 10)\n",
      "epoch: 0 batch 17 loss: 0.8224031 acc: 78.125\n",
      "training time 0.0189971923828125 all layer time: 0.004910945892333984 shape: (128, 10)\n",
      "epoch: 0 batch 18 loss: 0.82176304 acc: 76.5625\n",
      "training time 0.018447399139404297 all layer time: 0.004552125930786133 shape: (128, 10)\n",
      "epoch: 0 batch 19 loss: 0.8682251 acc: 68.75\n",
      "training time 0.018451213836669922 all layer time: 0.004633903503417969 shape: (128, 10)\n",
      "epoch: 0 batch 20 loss: 1.1349285 acc: 73.4375\n",
      "training time 0.01857471466064453 all layer time: 0.004798173904418945 shape: (128, 10)\n",
      "epoch: 0 batch 21 loss: 1.1330364 acc: 66.40625\n",
      "training time 0.018832921981811523 all layer time: 0.004492998123168945 shape: (128, 10)\n",
      "epoch: 0 batch 22 loss: 0.7739564 acc: 76.5625\n",
      "training time 0.01872539520263672 all layer time: 0.00448918342590332 shape: (128, 10)\n",
      "epoch: 0 batch 23 loss: 0.9135355 acc: 70.3125\n",
      "training time 0.01817011833190918 all layer time: 0.004649162292480469 shape: (128, 10)\n",
      "epoch: 0 batch 24 loss: 0.6983707 acc: 80.46875\n",
      "training time 0.017390727996826172 all layer time: 0.004612445831298828 shape: (128, 10)\n",
      "epoch: 0 batch 25 loss: 0.6670189 acc: 82.03125\n",
      "training time 0.0183565616607666 all layer time: 0.004730224609375 shape: (128, 10)\n",
      "epoch: 0 batch 26 loss: 0.58721936 acc: 82.8125\n",
      "training time 0.01899266242980957 all layer time: 0.004481077194213867 shape: (128, 10)\n",
      "epoch: 0 batch 27 loss: 0.65937895 acc: 76.5625\n",
      "training time 0.017933130264282227 all layer time: 0.004583597183227539 shape: (128, 10)\n",
      "epoch: 0 batch 28 loss: 0.59637356 acc: 84.375\n",
      "training time 0.0183103084564209 all layer time: 0.004582881927490234 shape: (128, 10)\n",
      "epoch: 0 batch 29 loss: 0.61844325 acc: 82.03125\n",
      "training time 0.018450498580932617 all layer time: 0.004629850387573242 shape: (128, 10)\n",
      "epoch: 0 batch 30 loss: 0.49874783 acc: 85.15625\n",
      "training time 0.018355846405029297 all layer time: 0.0045642852783203125 shape: (128, 10)\n",
      "epoch: 0 batch 31 loss: 0.5190003 acc: 85.9375\n",
      "training time 0.01821279525756836 all layer time: 0.004623889923095703 shape: (128, 10)\n",
      "epoch: 0 batch 32 loss: 0.5916039 acc: 78.125\n",
      "training time 0.018185138702392578 all layer time: 0.004602193832397461 shape: (128, 10)\n",
      "epoch: 0 batch 33 loss: 0.6817923 acc: 80.46875\n",
      "training time 0.01802682876586914 all layer time: 0.004825115203857422 shape: (128, 10)\n",
      "epoch: 0 batch 34 loss: 0.563216 acc: 82.8125\n",
      "training time 0.018190860748291016 all layer time: 0.004476070404052734 shape: (128, 10)\n",
      "epoch: 0 batch 35 loss: 0.45276946 acc: 82.8125\n",
      "training time 0.01803874969482422 all layer time: 0.004512310028076172 shape: (128, 10)\n",
      "epoch: 0 batch 36 loss: 0.575981 acc: 79.6875\n",
      "training time 0.017821788787841797 all layer time: 0.00447535514831543 shape: (128, 10)\n",
      "epoch: 0 batch 37 loss: 0.65177655 acc: 78.125\n",
      "training time 0.01781463623046875 all layer time: 0.004491329193115234 shape: (128, 10)\n",
      "epoch: 0 batch 38 loss: 0.51633465 acc: 83.59375\n",
      "training time 0.01811838150024414 all layer time: 0.0044155120849609375 shape: (128, 10)\n",
      "epoch: 0 batch 39 loss: 0.48065656 acc: 84.375\n",
      "training time 0.017524242401123047 all layer time: 0.004446983337402344 shape: (128, 10)\n",
      "epoch: 0 batch 40 loss: 0.623134 acc: 84.375\n",
      "training time 0.01808786392211914 all layer time: 0.004416704177856445 shape: (128, 10)\n",
      "epoch: 0 batch 41 loss: 0.50908244 acc: 85.9375\n",
      "training time 0.01818537712097168 all layer time: 0.00458216667175293 shape: (128, 10)\n",
      "epoch: 0 batch 42 loss: 0.35536748 acc: 90.625\n",
      "training time 0.018040180206298828 all layer time: 0.0043811798095703125 shape: (128, 10)\n",
      "epoch: 0 batch 43 loss: 0.47741008 acc: 86.71875\n",
      "training time 0.017822980880737305 all layer time: 0.004485130310058594 shape: (128, 10)\n",
      "epoch: 0 batch 44 loss: 0.5135746 acc: 83.59375\n",
      "training time 0.017271041870117188 all layer time: 0.004368305206298828 shape: (128, 10)\n",
      "epoch: 0 batch 45 loss: 0.4329364 acc: 86.71875\n",
      "training time 0.01756906509399414 all layer time: 0.0043718814849853516 shape: (128, 10)\n",
      "epoch: 0 batch 46 loss: 0.34447372 acc: 93.75\n",
      "training time 0.020715951919555664 all layer time: 0.004530191421508789 shape: (128, 10)\n",
      "epoch: 0 batch 47 loss: 0.30545342 acc: 93.75\n",
      "training time 0.018138408660888672 all layer time: 0.004339694976806641 shape: (128, 10)\n",
      "epoch: 0 batch 48 loss: 0.41695198 acc: 85.9375\n",
      "training time 0.017050504684448242 all layer time: 0.0042645931243896484 shape: (128, 10)\n",
      "epoch: 0 batch 49 loss: 0.30867216 acc: 89.84375\n",
      "training time 0.017417192459106445 all layer time: 0.004246950149536133 shape: (128, 10)\n",
      "epoch: 0 batch 50 loss: 0.4833319 acc: 87.5\n",
      "training time 0.017269611358642578 all layer time: 0.004532814025878906 shape: (128, 10)\n",
      "epoch: 0 batch 51 loss: 0.32268694 acc: 90.625\n",
      "training time 0.01769399642944336 all layer time: 0.00437617301940918 shape: (128, 10)\n",
      "epoch: 0 batch 52 loss: 0.3094381 acc: 89.84375\n",
      "training time 0.017320871353149414 all layer time: 0.0044651031494140625 shape: (128, 10)\n",
      "epoch: 0 batch 53 loss: 0.49493772 acc: 85.9375\n",
      "training time 0.017342090606689453 all layer time: 0.004513263702392578 shape: (128, 10)\n",
      "epoch: 0 batch 54 loss: 0.5060237 acc: 82.8125\n",
      "training time 0.01691126823425293 all layer time: 0.004356861114501953 shape: (128, 10)\n",
      "epoch: 0 batch 55 loss: 0.41205013 acc: 89.0625\n",
      "training time 0.017372846603393555 all layer time: 0.004312753677368164 shape: (128, 10)\n",
      "epoch: 0 batch 56 loss: 0.60135245 acc: 82.03125\n",
      "training time 0.017047643661499023 all layer time: 0.004347324371337891 shape: (128, 10)\n",
      "epoch: 0 batch 57 loss: 0.3352136 acc: 92.96875\n",
      "training time 0.01707291603088379 all layer time: 0.004492282867431641 shape: (128, 10)\n",
      "epoch: 0 batch 58 loss: 0.3826602 acc: 89.84375\n",
      "training time 0.017472505569458008 all layer time: 0.004348039627075195 shape: (128, 10)\n",
      "epoch: 0 batch 59 loss: 0.36650285 acc: 89.0625\n",
      "training time 0.017428159713745117 all layer time: 0.004354715347290039 shape: (128, 10)\n",
      "epoch: 0 batch 60 loss: 0.4565939 acc: 81.25\n",
      "training time 0.017488718032836914 all layer time: 0.0042455196380615234 shape: (128, 10)\n",
      "epoch: 0 batch 61 loss: 0.5328392 acc: 81.25\n",
      "training time 0.017560482025146484 all layer time: 0.004389047622680664 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch 62 loss: 0.48905122 acc: 84.375\n",
      "training time 0.017287015914916992 all layer time: 0.004300594329833984 shape: (128, 10)\n",
      "epoch: 0 batch 63 loss: 0.3325061 acc: 90.625\n",
      "training time 0.018769264221191406 all layer time: 0.004352569580078125 shape: (128, 10)\n",
      "epoch: 0 batch 64 loss: 0.48752308 acc: 86.71875\n",
      "training time 0.01722097396850586 all layer time: 0.0043256282806396484 shape: (128, 10)\n",
      "epoch: 0 batch 65 loss: 0.3791962 acc: 92.1875\n",
      "training time 0.017220020294189453 all layer time: 0.004446983337402344 shape: (128, 10)\n",
      "epoch: 0 batch 66 loss: 0.42252165 acc: 89.84375\n",
      "training time 0.016841888427734375 all layer time: 0.004199504852294922 shape: (128, 10)\n",
      "epoch: 0 batch 67 loss: 0.6574448 acc: 77.34375\n",
      "training time 0.016928911209106445 all layer time: 0.0043642520904541016 shape: (128, 10)\n",
      "epoch: 0 batch 68 loss: 0.6191781 acc: 82.8125\n",
      "training time 0.01737689971923828 all layer time: 0.004367828369140625 shape: (128, 10)\n",
      "epoch: 0 batch 69 loss: 0.493394 acc: 84.375\n",
      "training time 0.017274141311645508 all layer time: 0.004290580749511719 shape: (128, 10)\n",
      "epoch: 0 batch 70 loss: 0.2444049 acc: 92.96875\n",
      "training time 0.01676321029663086 all layer time: 0.004312992095947266 shape: (128, 10)\n",
      "epoch: 0 batch 71 loss: 0.33942056 acc: 90.625\n",
      "training time 0.016888856887817383 all layer time: 0.004205942153930664 shape: (128, 10)\n",
      "epoch: 0 batch 72 loss: 0.34571195 acc: 90.625\n",
      "training time 0.016809940338134766 all layer time: 0.004280567169189453 shape: (128, 10)\n",
      "epoch: 0 batch 73 loss: 0.4457722 acc: 85.9375\n",
      "training time 0.01759934425354004 all layer time: 0.0043048858642578125 shape: (128, 10)\n",
      "epoch: 0 batch 74 loss: 0.30738118 acc: 90.625\n",
      "training time 0.017286300659179688 all layer time: 0.004252910614013672 shape: (128, 10)\n",
      "epoch: 0 batch 75 loss: 0.17250991 acc: 95.3125\n",
      "training time 0.017676353454589844 all layer time: 0.0042934417724609375 shape: (128, 10)\n",
      "epoch: 0 batch 76 loss: 0.3295011 acc: 90.625\n",
      "training time 0.017034292221069336 all layer time: 0.004414796829223633 shape: (128, 10)\n",
      "epoch: 0 batch 77 loss: 0.17645153 acc: 94.53125\n",
      "training time 0.017517805099487305 all layer time: 0.004277944564819336 shape: (128, 10)\n",
      "epoch: 0 batch 78 loss: 0.3686513 acc: 90.625\n",
      "training time 0.0171506404876709 all layer time: 0.004079103469848633 shape: (128, 10)\n",
      "epoch: 0 batch 79 loss: 0.3085453 acc: 88.28125\n",
      "training time 0.017282724380493164 all layer time: 0.004240989685058594 shape: (128, 10)\n",
      "epoch: 0 batch 80 loss: 0.31487727 acc: 89.84375\n",
      "training time 0.0166623592376709 all layer time: 0.004292726516723633 shape: (128, 10)\n",
      "epoch: 0 batch 81 loss: 0.31390956 acc: 88.28125\n",
      "training time 0.016735553741455078 all layer time: 0.004248619079589844 shape: (128, 10)\n",
      "epoch: 0 batch 82 loss: 0.15701206 acc: 94.53125\n",
      "training time 0.016817808151245117 all layer time: 0.0042057037353515625 shape: (128, 10)\n",
      "epoch: 0 batch 83 loss: 0.32285684 acc: 89.84375\n",
      "training time 0.017255306243896484 all layer time: 0.0043392181396484375 shape: (128, 10)\n",
      "epoch: 0 batch 84 loss: 0.24615818 acc: 92.1875\n",
      "training time 0.0172271728515625 all layer time: 0.004413127899169922 shape: (128, 10)\n",
      "epoch: 0 batch 85 loss: 0.31772226 acc: 93.75\n",
      "training time 0.017412662506103516 all layer time: 0.0042231082916259766 shape: (128, 10)\n",
      "epoch: 0 batch 86 loss: 0.2869081 acc: 92.1875\n",
      "training time 0.017136096954345703 all layer time: 0.004208803176879883 shape: (128, 10)\n",
      "epoch: 0 batch 87 loss: 0.28065953 acc: 91.40625\n",
      "training time 0.0169222354888916 all layer time: 0.004455089569091797 shape: (128, 10)\n",
      "epoch: 0 batch 88 loss: 0.20949095 acc: 95.3125\n",
      "training time 0.016660213470458984 all layer time: 0.004328727722167969 shape: (128, 10)\n",
      "epoch: 0 batch 89 loss: 0.23914875 acc: 90.625\n",
      "training time 0.016898393630981445 all layer time: 0.004427194595336914 shape: (128, 10)\n",
      "epoch: 0 batch 90 loss: 0.49143583 acc: 85.15625\n",
      "training time 0.017037153244018555 all layer time: 0.0042002201080322266 shape: (128, 10)\n",
      "epoch: 0 batch 91 loss: 0.3697413 acc: 90.625\n",
      "training time 0.016782045364379883 all layer time: 0.004331350326538086 shape: (128, 10)\n",
      "epoch: 0 batch 92 loss: 0.26356202 acc: 93.75\n",
      "training time 0.016842126846313477 all layer time: 0.00420069694519043 shape: (128, 10)\n",
      "epoch: 0 batch 93 loss: 0.30512333 acc: 88.28125\n",
      "training time 0.016579866409301758 all layer time: 0.0042858123779296875 shape: (128, 10)\n",
      "epoch: 0 batch 94 loss: 0.18802524 acc: 96.09375\n",
      "training time 0.017184019088745117 all layer time: 0.004347562789916992 shape: (128, 10)\n",
      "epoch: 0 batch 95 loss: 0.34098107 acc: 89.84375\n",
      "training time 0.01676011085510254 all layer time: 0.004160881042480469 shape: (128, 10)\n",
      "epoch: 0 batch 96 loss: 0.26284754 acc: 90.625\n",
      "training time 0.016766786575317383 all layer time: 0.0042188167572021484 shape: (128, 10)\n",
      "epoch: 0 batch 97 loss: 0.25033584 acc: 90.625\n",
      "training time 0.016955852508544922 all layer time: 0.004294157028198242 shape: (128, 10)\n",
      "epoch: 0 batch 98 loss: 0.4778669 acc: 86.71875\n",
      "training time 0.016840219497680664 all layer time: 0.004145383834838867 shape: (128, 10)\n",
      "epoch: 0 batch 99 loss: 0.3464792 acc: 90.625\n",
      "training time 0.01743459701538086 all layer time: 0.004277467727661133 shape: (128, 10)\n",
      "epoch: 0 batch 100 loss: 0.26282084 acc: 90.625\n",
      "training time 0.0174105167388916 all layer time: 0.004252433776855469 shape: (128, 10)\n",
      "epoch: 0 batch 101 loss: 0.43664685 acc: 89.0625\n",
      "training time 0.01727581024169922 all layer time: 0.004282236099243164 shape: (128, 10)\n",
      "epoch: 0 batch 102 loss: 0.46034467 acc: 88.28125\n",
      "training time 0.016935348510742188 all layer time: 0.004292726516723633 shape: (128, 10)\n",
      "epoch: 0 batch 103 loss: 0.3100897 acc: 92.1875\n",
      "training time 0.017603158950805664 all layer time: 0.004153013229370117 shape: (128, 10)\n",
      "epoch: 0 batch 104 loss: 0.2766401 acc: 92.1875\n",
      "training time 0.01717209815979004 all layer time: 0.004313468933105469 shape: (128, 10)\n",
      "epoch: 0 batch 105 loss: 0.28956318 acc: 93.75\n",
      "training time 0.01648426055908203 all layer time: 0.004389286041259766 shape: (128, 10)\n",
      "epoch: 0 batch 106 loss: 0.2184821 acc: 93.75\n",
      "training time 0.01696038246154785 all layer time: 0.0042438507080078125 shape: (128, 10)\n",
      "epoch: 0 batch 107 loss: 0.32048893 acc: 89.84375\n",
      "training time 0.01723337173461914 all layer time: 0.004387378692626953 shape: (128, 10)\n",
      "epoch: 0 batch 108 loss: 0.25702098 acc: 89.0625\n",
      "training time 0.017620325088500977 all layer time: 0.004239320755004883 shape: (128, 10)\n",
      "epoch: 0 batch 109 loss: 0.34763277 acc: 90.625\n",
      "training time 0.016992568969726562 all layer time: 0.004123210906982422 shape: (128, 10)\n",
      "epoch: 0 batch 110 loss: 0.29796827 acc: 89.84375\n",
      "training time 0.017201662063598633 all layer time: 0.004210948944091797 shape: (128, 10)\n",
      "epoch: 0 batch 111 loss: 0.32900494 acc: 89.84375\n",
      "training time 0.01692795753479004 all layer time: 0.0043108463287353516 shape: (128, 10)\n",
      "epoch: 0 batch 112 loss: 0.3093061 acc: 91.40625\n",
      "training time 0.0176541805267334 all layer time: 0.004290103912353516 shape: (128, 10)\n",
      "epoch: 0 batch 113 loss: 0.29101926 acc: 91.40625\n",
      "training time 0.01705169677734375 all layer time: 0.004296302795410156 shape: (128, 10)\n",
      "epoch: 0 batch 114 loss: 0.3294325 acc: 89.0625\n",
      "training time 0.017136096954345703 all layer time: 0.004492044448852539 shape: (128, 10)\n",
      "epoch: 0 batch 115 loss: 0.48014733 acc: 85.15625\n",
      "training time 0.01733851432800293 all layer time: 0.0042760372161865234 shape: (128, 10)\n",
      "epoch: 0 batch 116 loss: 0.21676609 acc: 92.1875\n",
      "training time 0.01772141456604004 all layer time: 0.004243135452270508 shape: (128, 10)\n",
      "epoch: 0 batch 117 loss: 0.16580713 acc: 95.3125\n",
      "training time 0.017070770263671875 all layer time: 0.00430607795715332 shape: (128, 10)\n",
      "epoch: 0 batch 118 loss: 0.3383348 acc: 90.625\n",
      "training time 0.017391681671142578 all layer time: 0.004282951354980469 shape: (128, 10)\n",
      "epoch: 0 batch 119 loss: 0.23246026 acc: 92.1875\n",
      "training time 0.01691746711730957 all layer time: 0.0042383670806884766 shape: (128, 10)\n",
      "epoch: 0 batch 120 loss: 0.25988862 acc: 92.1875\n",
      "training time 0.017370939254760742 all layer time: 0.004309654235839844 shape: (128, 10)\n",
      "epoch: 0 batch 121 loss: 0.26918358 acc: 91.40625\n",
      "training time 0.017479658126831055 all layer time: 0.004147768020629883 shape: (128, 10)\n",
      "epoch: 0 batch 122 loss: 0.2163117 acc: 92.96875\n",
      "training time 0.01653456687927246 all layer time: 0.004278421401977539 shape: (128, 10)\n",
      "epoch: 0 batch 123 loss: 0.3319816 acc: 86.71875\n",
      "training time 0.016843318939208984 all layer time: 0.004282474517822266 shape: (128, 10)\n",
      "epoch: 0 batch 124 loss: 0.37213886 acc: 90.625\n",
      "training time 0.016828298568725586 all layer time: 0.0042688846588134766 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch 125 loss: 0.24473669 acc: 94.53125\n",
      "training time 0.017943620681762695 all layer time: 0.004197835922241211 shape: (128, 10)\n",
      "epoch: 0 batch 126 loss: 0.12191232 acc: 96.09375\n",
      "training time 0.01726078987121582 all layer time: 0.004031181335449219 shape: (128, 10)\n",
      "epoch: 0 batch 127 loss: 0.2148695 acc: 94.53125\n",
      "training time 0.016941070556640625 all layer time: 0.004265546798706055 shape: (128, 10)\n",
      "epoch: 0 batch 128 loss: 0.16346046 acc: 96.875\n",
      "training time 0.017228364944458008 all layer time: 0.004332542419433594 shape: (128, 10)\n",
      "epoch: 0 batch 129 loss: 0.21067408 acc: 92.96875\n",
      "training time 0.01812267303466797 all layer time: 0.0044252872467041016 shape: (128, 10)\n",
      "epoch: 0 batch 130 loss: 0.2718116 acc: 89.84375\n",
      "training time 0.017023801803588867 all layer time: 0.004177570343017578 shape: (128, 10)\n",
      "epoch: 0 batch 131 loss: 0.23609957 acc: 93.75\n",
      "training time 0.016955137252807617 all layer time: 0.004306793212890625 shape: (128, 10)\n",
      "epoch: 0 batch 132 loss: 0.28586596 acc: 89.84375\n",
      "training time 0.0173494815826416 all layer time: 0.0042362213134765625 shape: (128, 10)\n",
      "epoch: 0 batch 133 loss: 0.2571951 acc: 89.84375\n",
      "training time 0.017226219177246094 all layer time: 0.004280805587768555 shape: (128, 10)\n",
      "epoch: 0 batch 134 loss: 0.2850224 acc: 91.40625\n",
      "training time 0.017181873321533203 all layer time: 0.004168272018432617 shape: (128, 10)\n",
      "epoch: 0 batch 135 loss: 0.15569656 acc: 95.3125\n",
      "training time 0.016941308975219727 all layer time: 0.004171848297119141 shape: (128, 10)\n",
      "epoch: 0 batch 136 loss: 0.18745577 acc: 92.1875\n",
      "training time 0.017018795013427734 all layer time: 0.004247903823852539 shape: (128, 10)\n",
      "epoch: 0 batch 137 loss: 0.31655926 acc: 90.625\n",
      "training time 0.017746925354003906 all layer time: 0.004264354705810547 shape: (128, 10)\n",
      "epoch: 0 batch 138 loss: 0.31121445 acc: 90.625\n",
      "training time 0.017203807830810547 all layer time: 0.004194974899291992 shape: (128, 10)\n",
      "epoch: 0 batch 139 loss: 0.40268993 acc: 89.0625\n",
      "training time 0.017189741134643555 all layer time: 0.004194974899291992 shape: (128, 10)\n",
      "epoch: 0 batch 140 loss: 0.24879599 acc: 93.75\n",
      "training time 0.017055988311767578 all layer time: 0.004415273666381836 shape: (128, 10)\n",
      "epoch: 0 batch 141 loss: 0.18162006 acc: 93.75\n",
      "training time 0.01752948760986328 all layer time: 0.004453182220458984 shape: (128, 10)\n",
      "epoch: 0 batch 142 loss: 0.13629395 acc: 96.875\n",
      "training time 0.016880035400390625 all layer time: 0.004431486129760742 shape: (128, 10)\n",
      "epoch: 0 batch 143 loss: 0.16178177 acc: 97.65625\n",
      "training time 0.017217636108398438 all layer time: 0.004153728485107422 shape: (128, 10)\n",
      "epoch: 0 batch 144 loss: 0.1271365 acc: 96.09375\n",
      "training time 0.01712799072265625 all layer time: 0.004336833953857422 shape: (128, 10)\n",
      "epoch: 0 batch 145 loss: 0.19547167 acc: 92.96875\n",
      "training time 0.016871213912963867 all layer time: 0.004215717315673828 shape: (128, 10)\n",
      "epoch: 0 batch 146 loss: 0.16591619 acc: 93.75\n",
      "training time 0.017255306243896484 all layer time: 0.0043354034423828125 shape: (128, 10)\n",
      "epoch: 0 batch 147 loss: 0.08212021 acc: 97.65625\n",
      "training time 0.017701148986816406 all layer time: 0.004139423370361328 shape: (128, 10)\n",
      "epoch: 0 batch 148 loss: 0.2547815 acc: 91.40625\n",
      "training time 0.01729106903076172 all layer time: 0.004061698913574219 shape: (128, 10)\n",
      "epoch: 0 batch 149 loss: 0.27342725 acc: 92.96875\n",
      "training time 0.017327547073364258 all layer time: 0.00434422492980957 shape: (128, 10)\n",
      "epoch: 0 batch 150 loss: 0.17473428 acc: 96.875\n",
      "training time 0.017214059829711914 all layer time: 0.0043675899505615234 shape: (128, 10)\n",
      "epoch: 0 batch 151 loss: 0.22783211 acc: 94.53125\n",
      "training time 0.01771688461303711 all layer time: 0.0041027069091796875 shape: (128, 10)\n",
      "epoch: 0 batch 152 loss: 0.1816128 acc: 96.09375\n",
      "training time 0.017171859741210938 all layer time: 0.004300117492675781 shape: (128, 10)\n",
      "epoch: 0 batch 153 loss: 0.091697924 acc: 98.4375\n",
      "training time 0.01686859130859375 all layer time: 0.004314899444580078 shape: (128, 10)\n",
      "epoch: 0 batch 154 loss: 0.11349149 acc: 96.09375\n",
      "training time 0.016762495040893555 all layer time: 0.0042307376861572266 shape: (128, 10)\n",
      "epoch: 0 batch 155 loss: 0.21746308 acc: 92.1875\n",
      "training time 0.017740964889526367 all layer time: 0.004327297210693359 shape: (128, 10)\n",
      "epoch: 0 batch 156 loss: 0.3835152 acc: 88.28125\n",
      "training time 0.01645827293395996 all layer time: 0.004164695739746094 shape: (128, 10)\n",
      "epoch: 0 batch 157 loss: 0.36937356 acc: 86.71875\n",
      "training time 0.017735719680786133 all layer time: 0.004186868667602539 shape: (128, 10)\n",
      "epoch: 0 batch 158 loss: 0.23923454 acc: 93.75\n",
      "training time 0.016999483108520508 all layer time: 0.004218578338623047 shape: (128, 10)\n",
      "epoch: 0 batch 159 loss: 0.07161829 acc: 98.4375\n",
      "training time 0.017146825790405273 all layer time: 0.004443168640136719 shape: (128, 10)\n",
      "epoch: 0 batch 160 loss: 0.19357644 acc: 94.53125\n",
      "training time 0.016817331314086914 all layer time: 0.0041255950927734375 shape: (128, 10)\n",
      "epoch: 0 batch 161 loss: 0.32782215 acc: 92.96875\n",
      "training time 0.0166018009185791 all layer time: 0.004208803176879883 shape: (128, 10)\n",
      "epoch: 0 batch 162 loss: 0.23265156 acc: 92.96875\n",
      "training time 0.017232179641723633 all layer time: 0.004183530807495117 shape: (128, 10)\n",
      "epoch: 0 batch 163 loss: 0.35152894 acc: 89.84375\n",
      "training time 0.016628503799438477 all layer time: 0.004467487335205078 shape: (128, 10)\n",
      "epoch: 0 batch 164 loss: 0.193141 acc: 92.96875\n",
      "training time 0.016862154006958008 all layer time: 0.004216432571411133 shape: (128, 10)\n",
      "epoch: 0 batch 165 loss: 0.13530774 acc: 95.3125\n",
      "training time 0.017107248306274414 all layer time: 0.004293680191040039 shape: (128, 10)\n",
      "epoch: 0 batch 166 loss: 0.12639138 acc: 96.09375\n",
      "training time 0.01699519157409668 all layer time: 0.00417327880859375 shape: (128, 10)\n",
      "epoch: 0 batch 167 loss: 0.1521007 acc: 96.875\n",
      "training time 0.016867637634277344 all layer time: 0.004182577133178711 shape: (128, 10)\n",
      "epoch: 0 batch 168 loss: 0.12512659 acc: 96.09375\n",
      "training time 0.016621112823486328 all layer time: 0.004460811614990234 shape: (128, 10)\n",
      "epoch: 0 batch 169 loss: 0.16177653 acc: 95.3125\n",
      "training time 0.017229795455932617 all layer time: 0.004193782806396484 shape: (128, 10)\n",
      "epoch: 0 batch 170 loss: 0.10929993 acc: 96.875\n",
      "training time 0.016811609268188477 all layer time: 0.004269599914550781 shape: (128, 10)\n",
      "epoch: 0 batch 171 loss: 0.203724 acc: 92.1875\n",
      "training time 0.016292333602905273 all layer time: 0.004354238510131836 shape: (128, 10)\n",
      "epoch: 0 batch 172 loss: 0.14008108 acc: 95.3125\n",
      "training time 0.017412900924682617 all layer time: 0.0043489933013916016 shape: (128, 10)\n",
      "epoch: 0 batch 173 loss: 0.23448949 acc: 92.96875\n",
      "training time 0.01698470115661621 all layer time: 0.004258155822753906 shape: (128, 10)\n",
      "epoch: 0 batch 174 loss: 0.16802146 acc: 94.53125\n",
      "training time 0.017543792724609375 all layer time: 0.004133939743041992 shape: (128, 10)\n",
      "epoch: 0 batch 175 loss: 0.26772603 acc: 92.1875\n",
      "training time 0.016743898391723633 all layer time: 0.004305601119995117 shape: (128, 10)\n",
      "epoch: 0 batch 176 loss: 0.33363456 acc: 90.625\n",
      "training time 0.017025470733642578 all layer time: 0.004256486892700195 shape: (128, 10)\n",
      "epoch: 0 batch 177 loss: 0.3622739 acc: 87.5\n",
      "training time 0.016919851303100586 all layer time: 0.0050144195556640625 shape: (128, 10)\n",
      "epoch: 0 batch 178 loss: 0.202626 acc: 93.75\n",
      "training time 0.016875743865966797 all layer time: 0.004201650619506836 shape: (128, 10)\n",
      "epoch: 0 batch 179 loss: 0.0855553 acc: 96.875\n",
      "training time 0.017056703567504883 all layer time: 0.004335641860961914 shape: (128, 10)\n",
      "epoch: 0 batch 180 loss: 0.16995394 acc: 93.75\n",
      "training time 0.01731109619140625 all layer time: 0.004239559173583984 shape: (128, 10)\n",
      "epoch: 0 batch 181 loss: 0.083705835 acc: 98.4375\n",
      "training time 0.018291234970092773 all layer time: 0.004254817962646484 shape: (128, 10)\n",
      "epoch: 0 batch 182 loss: 0.16429946 acc: 95.3125\n",
      "training time 0.019365549087524414 all layer time: 0.0041429996490478516 shape: (128, 10)\n",
      "epoch: 0 batch 183 loss: 0.10357812 acc: 96.875\n",
      "training time 0.016928911209106445 all layer time: 0.0043103694915771484 shape: (128, 10)\n",
      "epoch: 0 batch 184 loss: 0.1527847 acc: 95.3125\n",
      "training time 0.017245769500732422 all layer time: 0.00428318977355957 shape: (128, 10)\n",
      "epoch: 0 batch 185 loss: 0.18549463 acc: 93.75\n",
      "training time 0.01658177375793457 all layer time: 0.0042688846588134766 shape: (128, 10)\n",
      "epoch: 0 batch 186 loss: 0.22201185 acc: 92.96875\n",
      "training time 0.017234086990356445 all layer time: 0.0041539669036865234 shape: (128, 10)\n",
      "epoch: 0 batch 187 loss: 0.18193954 acc: 95.3125\n",
      "training time 0.016801118850708008 all layer time: 0.004157304763793945 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch 188 loss: 0.123314895 acc: 96.875\n",
      "training time 0.01717829704284668 all layer time: 0.0044209957122802734 shape: (128, 10)\n",
      "epoch: 0 batch 189 loss: 0.15101744 acc: 94.53125\n",
      "training time 0.01681351661682129 all layer time: 0.004233360290527344 shape: (128, 10)\n",
      "epoch: 0 batch 190 loss: 0.11005266 acc: 95.3125\n",
      "training time 0.017132282257080078 all layer time: 0.004393577575683594 shape: (128, 10)\n",
      "epoch: 0 batch 191 loss: 0.17555098 acc: 92.96875\n",
      "training time 0.01703047752380371 all layer time: 0.004194736480712891 shape: (128, 10)\n",
      "epoch: 0 batch 192 loss: 0.13202779 acc: 93.75\n",
      "training time 0.017070531845092773 all layer time: 0.004325151443481445 shape: (128, 10)\n",
      "epoch: 0 batch 193 loss: 0.29315534 acc: 92.1875\n",
      "training time 0.016951322555541992 all layer time: 0.004182100296020508 shape: (128, 10)\n",
      "epoch: 0 batch 194 loss: 0.2582217 acc: 90.625\n",
      "training time 0.017529726028442383 all layer time: 0.004147768020629883 shape: (128, 10)\n",
      "epoch: 0 batch 195 loss: 0.106562 acc: 97.65625\n",
      "training time 0.016996383666992188 all layer time: 0.004192352294921875 shape: (128, 10)\n",
      "epoch: 0 batch 196 loss: 0.14180385 acc: 95.3125\n",
      "training time 0.017056941986083984 all layer time: 0.00433659553527832 shape: (128, 10)\n",
      "epoch: 0 batch 197 loss: 0.2031422 acc: 93.75\n",
      "training time 0.016777515411376953 all layer time: 0.0042078495025634766 shape: (128, 10)\n",
      "epoch: 0 batch 198 loss: 0.07282829 acc: 99.21875\n",
      "training time 0.017674922943115234 all layer time: 0.00426793098449707 shape: (128, 10)\n",
      "epoch: 0 batch 199 loss: 0.20504011 acc: 96.09375\n",
      "training time 0.016746044158935547 all layer time: 0.004201412200927734 shape: (128, 10)\n",
      "epoch: 0 batch 200 loss: 0.1335917 acc: 96.09375\n",
      "training time 0.0169219970703125 all layer time: 0.004235744476318359 shape: (128, 10)\n",
      "epoch: 0 batch 201 loss: 0.21535061 acc: 94.53125\n",
      "training time 0.016988277435302734 all layer time: 0.0043315887451171875 shape: (128, 10)\n",
      "epoch: 0 batch 202 loss: 0.1532628 acc: 94.53125\n",
      "training time 0.017308473587036133 all layer time: 0.00424957275390625 shape: (128, 10)\n",
      "epoch: 0 batch 203 loss: 0.13789114 acc: 95.3125\n",
      "training time 0.016901493072509766 all layer time: 0.00444793701171875 shape: (128, 10)\n",
      "epoch: 0 batch 204 loss: 0.098975174 acc: 97.65625\n",
      "training time 0.01717066764831543 all layer time: 0.004233360290527344 shape: (128, 10)\n",
      "epoch: 0 batch 205 loss: 0.14767228 acc: 96.09375\n",
      "training time 0.017360210418701172 all layer time: 0.004237174987792969 shape: (128, 10)\n",
      "epoch: 0 batch 206 loss: 0.28418025 acc: 92.96875\n",
      "training time 0.016457557678222656 all layer time: 0.0041201114654541016 shape: (128, 10)\n",
      "epoch: 0 batch 207 loss: 0.25111076 acc: 92.96875\n",
      "training time 0.017607688903808594 all layer time: 0.00449061393737793 shape: (128, 10)\n",
      "epoch: 0 batch 208 loss: 0.40780705 acc: 86.71875\n",
      "training time 0.01730942726135254 all layer time: 0.0043218135833740234 shape: (128, 10)\n",
      "epoch: 0 batch 209 loss: 0.15255788 acc: 96.09375\n",
      "training time 0.017149686813354492 all layer time: 0.0042803287506103516 shape: (128, 10)\n",
      "epoch: 0 batch 210 loss: 0.13126141 acc: 98.4375\n",
      "training time 0.01726245880126953 all layer time: 0.004143714904785156 shape: (128, 10)\n",
      "epoch: 0 batch 211 loss: 0.08974643 acc: 97.65625\n",
      "training time 0.016993045806884766 all layer time: 0.004229068756103516 shape: (128, 10)\n",
      "epoch: 0 batch 212 loss: 0.35023198 acc: 89.84375\n",
      "training time 0.01745891571044922 all layer time: 0.004216194152832031 shape: (128, 10)\n",
      "epoch: 0 batch 213 loss: 0.135806 acc: 96.09375\n",
      "training time 0.01709723472595215 all layer time: 0.00414729118347168 shape: (128, 10)\n",
      "epoch: 0 batch 214 loss: 0.13473919 acc: 95.3125\n",
      "training time 0.017135143280029297 all layer time: 0.004228115081787109 shape: (128, 10)\n",
      "epoch: 0 batch 215 loss: 0.15300874 acc: 94.53125\n",
      "training time 0.01707768440246582 all layer time: 0.004454135894775391 shape: (128, 10)\n",
      "epoch: 0 batch 216 loss: 0.12985249 acc: 95.3125\n",
      "training time 0.016938447952270508 all layer time: 0.004042863845825195 shape: (128, 10)\n",
      "epoch: 0 batch 217 loss: 0.1851173 acc: 93.75\n",
      "training time 0.0174105167388916 all layer time: 0.0040700435638427734 shape: (128, 10)\n",
      "epoch: 0 batch 218 loss: 0.08944771 acc: 97.65625\n",
      "training time 0.017212390899658203 all layer time: 0.004182577133178711 shape: (128, 10)\n",
      "epoch: 0 batch 219 loss: 0.1806876 acc: 94.53125\n",
      "training time 0.01694321632385254 all layer time: 0.0042629241943359375 shape: (128, 10)\n",
      "epoch: 0 batch 220 loss: 0.13158947 acc: 96.09375\n",
      "training time 0.016971588134765625 all layer time: 0.004422664642333984 shape: (128, 10)\n",
      "epoch: 0 batch 221 loss: 0.20327207 acc: 92.96875\n",
      "training time 0.017658472061157227 all layer time: 0.004424571990966797 shape: (128, 10)\n",
      "epoch: 0 batch 222 loss: 0.10874314 acc: 96.875\n",
      "training time 0.01700735092163086 all layer time: 0.0042951107025146484 shape: (128, 10)\n",
      "epoch: 0 batch 223 loss: 0.2848882 acc: 93.75\n",
      "training time 0.017154693603515625 all layer time: 0.004196643829345703 shape: (128, 10)\n",
      "epoch: 0 batch 224 loss: 0.11482031 acc: 96.09375\n",
      "training time 0.016828298568725586 all layer time: 0.0044057369232177734 shape: (128, 10)\n",
      "epoch: 0 batch 225 loss: 0.11980271 acc: 96.875\n",
      "training time 0.01702713966369629 all layer time: 0.004373311996459961 shape: (128, 10)\n",
      "epoch: 0 batch 226 loss: 0.1061381 acc: 96.875\n",
      "training time 0.01701188087463379 all layer time: 0.0043239593505859375 shape: (128, 10)\n",
      "epoch: 0 batch 227 loss: 0.20738263 acc: 92.1875\n",
      "training time 0.01749587059020996 all layer time: 0.004312992095947266 shape: (128, 10)\n",
      "epoch: 0 batch 228 loss: 0.2069701 acc: 92.1875\n",
      "training time 0.016845226287841797 all layer time: 0.004166364669799805 shape: (128, 10)\n",
      "epoch: 0 batch 229 loss: 0.17611204 acc: 95.3125\n",
      "training time 0.017494678497314453 all layer time: 0.004216194152832031 shape: (128, 10)\n",
      "epoch: 0 batch 230 loss: 0.100944296 acc: 96.875\n",
      "training time 0.017181396484375 all layer time: 0.004525184631347656 shape: (128, 10)\n",
      "epoch: 0 batch 231 loss: 0.14175752 acc: 96.09375\n",
      "training time 0.01697850227355957 all layer time: 0.004145622253417969 shape: (128, 10)\n",
      "epoch: 0 batch 232 loss: 0.15976259 acc: 96.09375\n",
      "training time 0.017037630081176758 all layer time: 0.004357337951660156 shape: (128, 10)\n",
      "epoch: 0 batch 233 loss: 0.30255443 acc: 91.40625\n",
      "training time 0.017182350158691406 all layer time: 0.004175901412963867 shape: (128, 10)\n",
      "epoch: 0 batch 234 loss: 0.23478928 acc: 89.84375\n",
      "training time 0.017276763916015625 all layer time: 0.004297494888305664 shape: (128, 10)\n",
      "epoch: 0 batch 235 loss: 0.2162396 acc: 92.1875\n",
      "training time 0.017204999923706055 all layer time: 0.004216670989990234 shape: (128, 10)\n",
      "epoch: 0 batch 236 loss: 0.11691141 acc: 94.53125\n",
      "training time 0.016724824905395508 all layer time: 0.004263162612915039 shape: (128, 10)\n",
      "epoch: 0 batch 237 loss: 0.14669192 acc: 95.3125\n",
      "training time 0.017056941986083984 all layer time: 0.004298686981201172 shape: (128, 10)\n",
      "epoch: 0 batch 238 loss: 0.1117709 acc: 97.65625\n",
      "training time 0.01822972297668457 all layer time: 0.004475593566894531 shape: (128, 10)\n",
      "epoch: 0 batch 239 loss: 0.21097127 acc: 93.75\n",
      "training time 0.017358064651489258 all layer time: 0.004338741302490234 shape: (128, 10)\n",
      "epoch: 0 batch 240 loss: 0.12648693 acc: 94.53125\n",
      "training time 0.01755499839782715 all layer time: 0.004399776458740234 shape: (128, 10)\n",
      "epoch: 0 batch 241 loss: 0.1563707 acc: 96.875\n",
      "training time 0.016945362091064453 all layer time: 0.004133701324462891 shape: (128, 10)\n",
      "epoch: 0 batch 242 loss: 0.07490432 acc: 97.65625\n",
      "training time 0.017130374908447266 all layer time: 0.004333019256591797 shape: (128, 10)\n",
      "epoch: 0 batch 243 loss: 0.3121805 acc: 95.3125\n",
      "training time 0.017088890075683594 all layer time: 0.004204750061035156 shape: (128, 10)\n",
      "epoch: 0 batch 244 loss: 0.24437796 acc: 92.96875\n",
      "training time 0.01877880096435547 all layer time: 0.004445075988769531 shape: (128, 10)\n",
      "epoch: 0 batch 245 loss: 0.20962045 acc: 90.625\n",
      "training time 0.01755690574645996 all layer time: 0.004384517669677734 shape: (128, 10)\n",
      "epoch: 0 batch 246 loss: 0.16790204 acc: 92.96875\n",
      "training time 0.017404556274414062 all layer time: 0.004243373870849609 shape: (128, 10)\n",
      "epoch: 0 batch 247 loss: 0.23991491 acc: 92.96875\n",
      "training time 0.017771244049072266 all layer time: 0.004178762435913086 shape: (128, 10)\n",
      "epoch: 0 batch 248 loss: 0.21942466 acc: 92.96875\n",
      "training time 0.016974687576293945 all layer time: 0.004172563552856445 shape: (128, 10)\n",
      "epoch: 0 batch 249 loss: 0.18490444 acc: 96.09375\n",
      "training time 0.01710987091064453 all layer time: 0.0042607784271240234 shape: (128, 10)\n",
      "epoch: 0 batch 250 loss: 0.18410432 acc: 95.3125\n",
      "training time 0.01735854148864746 all layer time: 0.004242420196533203 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch 251 loss: 0.108872026 acc: 98.4375\n",
      "training time 0.01708221435546875 all layer time: 0.004308938980102539 shape: (128, 10)\n",
      "epoch: 0 batch 252 loss: 0.22440246 acc: 93.75\n",
      "training time 0.016959428787231445 all layer time: 0.004243135452270508 shape: (128, 10)\n",
      "epoch: 0 batch 253 loss: 0.21212912 acc: 92.1875\n",
      "training time 0.016895294189453125 all layer time: 0.004206180572509766 shape: (128, 10)\n",
      "epoch: 0 batch 254 loss: 0.09133582 acc: 96.09375\n",
      "training time 0.016765594482421875 all layer time: 0.004285097122192383 shape: (128, 10)\n",
      "epoch: 0 batch 255 loss: 0.15171187 acc: 94.53125\n",
      "training time 0.01710796356201172 all layer time: 0.004395961761474609 shape: (128, 10)\n",
      "epoch: 0 batch 256 loss: 0.28416127 acc: 93.75\n",
      "training time 0.017151832580566406 all layer time: 0.004220724105834961 shape: (128, 10)\n",
      "epoch: 0 batch 257 loss: 0.06252152 acc: 98.4375\n",
      "training time 0.01677989959716797 all layer time: 0.004183292388916016 shape: (128, 10)\n",
      "epoch: 0 batch 258 loss: 0.15118511 acc: 96.09375\n",
      "training time 0.017019033432006836 all layer time: 0.004169940948486328 shape: (128, 10)\n",
      "epoch: 0 batch 259 loss: 0.10257494 acc: 98.4375\n",
      "training time 0.01757025718688965 all layer time: 0.0044171810150146484 shape: (128, 10)\n",
      "epoch: 0 batch 260 loss: 0.1336231 acc: 96.875\n",
      "training time 0.017612218856811523 all layer time: 0.004252910614013672 shape: (128, 10)\n",
      "epoch: 0 batch 261 loss: 0.10514869 acc: 95.3125\n",
      "training time 0.016890287399291992 all layer time: 0.004427194595336914 shape: (128, 10)\n",
      "epoch: 0 batch 262 loss: 0.09966615 acc: 96.875\n",
      "training time 0.017220497131347656 all layer time: 0.004324913024902344 shape: (128, 10)\n",
      "epoch: 0 batch 263 loss: 0.14556511 acc: 96.09375\n",
      "training time 0.016989707946777344 all layer time: 0.004319429397583008 shape: (128, 10)\n",
      "epoch: 0 batch 264 loss: 0.05138584 acc: 99.21875\n",
      "training time 0.017399072647094727 all layer time: 0.004446983337402344 shape: (128, 10)\n",
      "epoch: 0 batch 265 loss: 0.056782056 acc: 98.4375\n",
      "training time 0.017124176025390625 all layer time: 0.004285097122192383 shape: (128, 10)\n",
      "epoch: 0 batch 266 loss: 0.124167874 acc: 95.3125\n",
      "training time 0.016520023345947266 all layer time: 0.0042989253997802734 shape: (128, 10)\n",
      "epoch: 0 batch 267 loss: 0.08763656 acc: 95.3125\n",
      "training time 0.017141103744506836 all layer time: 0.00422358512878418 shape: (128, 10)\n",
      "epoch: 0 batch 268 loss: 0.13934962 acc: 95.3125\n",
      "training time 0.01714491844177246 all layer time: 0.00440669059753418 shape: (128, 10)\n",
      "epoch: 0 batch 269 loss: 0.13786894 acc: 95.3125\n",
      "training time 0.017611265182495117 all layer time: 0.0043294429779052734 shape: (128, 10)\n",
      "epoch: 0 batch 270 loss: 0.16191974 acc: 93.75\n",
      "training time 0.017093658447265625 all layer time: 0.004415035247802734 shape: (128, 10)\n",
      "epoch: 0 batch 271 loss: 0.17256035 acc: 93.75\n",
      "training time 0.01709151268005371 all layer time: 0.004318952560424805 shape: (128, 10)\n",
      "epoch: 0 batch 272 loss: 0.27528253 acc: 92.96875\n",
      "training time 0.017318248748779297 all layer time: 0.0043315887451171875 shape: (128, 10)\n",
      "epoch: 0 batch 273 loss: 0.08868377 acc: 97.65625\n",
      "training time 0.018508195877075195 all layer time: 0.004423856735229492 shape: (128, 10)\n",
      "epoch: 0 batch 274 loss: 0.14056514 acc: 94.53125\n",
      "training time 0.01703619956970215 all layer time: 0.004259824752807617 shape: (128, 10)\n",
      "epoch: 0 batch 275 loss: 0.1357697 acc: 95.3125\n",
      "training time 0.017566442489624023 all layer time: 0.004198789596557617 shape: (128, 10)\n",
      "epoch: 0 batch 276 loss: 0.06849099 acc: 96.875\n",
      "training time 0.017354488372802734 all layer time: 0.0042264461517333984 shape: (128, 10)\n",
      "epoch: 0 batch 277 loss: 0.10977715 acc: 97.65625\n",
      "training time 0.017313480377197266 all layer time: 0.00423121452331543 shape: (128, 10)\n",
      "epoch: 0 batch 278 loss: 0.11394527 acc: 95.3125\n",
      "training time 0.01711583137512207 all layer time: 0.004290580749511719 shape: (128, 10)\n",
      "epoch: 0 batch 279 loss: 0.095055215 acc: 96.875\n",
      "training time 0.016885042190551758 all layer time: 0.004340648651123047 shape: (128, 10)\n",
      "epoch: 0 batch 280 loss: 0.1306791 acc: 95.3125\n",
      "training time 0.017012596130371094 all layer time: 0.004312276840209961 shape: (128, 10)\n",
      "epoch: 0 batch 281 loss: 0.17166835 acc: 93.75\n",
      "training time 0.017337322235107422 all layer time: 0.0045623779296875 shape: (128, 10)\n",
      "epoch: 0 batch 282 loss: 0.1831246 acc: 94.53125\n",
      "training time 0.017127275466918945 all layer time: 0.004263877868652344 shape: (128, 10)\n",
      "epoch: 0 batch 283 loss: 0.121633366 acc: 97.65625\n",
      "training time 0.017008066177368164 all layer time: 0.004146575927734375 shape: (128, 10)\n",
      "epoch: 0 batch 284 loss: 0.1392225 acc: 92.96875\n",
      "training time 0.01702260971069336 all layer time: 0.004282712936401367 shape: (128, 10)\n",
      "epoch: 0 batch 285 loss: 0.14246908 acc: 96.875\n",
      "training time 0.01774287223815918 all layer time: 0.004284381866455078 shape: (128, 10)\n",
      "epoch: 0 batch 286 loss: 0.061519258 acc: 98.4375\n",
      "training time 0.017571210861206055 all layer time: 0.00439143180847168 shape: (128, 10)\n",
      "epoch: 0 batch 287 loss: 0.16640517 acc: 95.3125\n",
      "training time 0.01708817481994629 all layer time: 0.004252433776855469 shape: (128, 10)\n",
      "epoch: 0 batch 288 loss: 0.10668175 acc: 95.3125\n",
      "training time 0.016999244689941406 all layer time: 0.00431060791015625 shape: (128, 10)\n",
      "epoch: 0 batch 289 loss: 0.23509835 acc: 96.09375\n",
      "training time 0.01718878746032715 all layer time: 0.0045735836029052734 shape: (128, 10)\n",
      "epoch: 0 batch 290 loss: 0.15415703 acc: 93.75\n",
      "training time 0.017586469650268555 all layer time: 0.004277229309082031 shape: (128, 10)\n",
      "epoch: 0 batch 291 loss: 0.1750418 acc: 96.875\n",
      "training time 0.0172119140625 all layer time: 0.0040967464447021484 shape: (128, 10)\n",
      "epoch: 0 batch 292 loss: 0.20670456 acc: 90.625\n",
      "training time 0.01745462417602539 all layer time: 0.004357099533081055 shape: (128, 10)\n",
      "epoch: 0 batch 293 loss: 0.20047677 acc: 93.75\n",
      "training time 0.017657041549682617 all layer time: 0.004205226898193359 shape: (128, 10)\n",
      "epoch: 0 batch 294 loss: 0.17466983 acc: 95.3125\n",
      "training time 0.01709151268005371 all layer time: 0.004423618316650391 shape: (128, 10)\n",
      "epoch: 0 batch 295 loss: 0.16259648 acc: 96.09375\n",
      "training time 0.017700672149658203 all layer time: 0.004295825958251953 shape: (128, 10)\n",
      "epoch: 0 batch 296 loss: 0.08407327 acc: 98.4375\n",
      "training time 0.017055273056030273 all layer time: 0.0043354034423828125 shape: (128, 10)\n",
      "epoch: 0 batch 297 loss: 0.046881415 acc: 99.21875\n",
      "training time 0.017067909240722656 all layer time: 0.00419926643371582 shape: (128, 10)\n",
      "epoch: 0 batch 298 loss: 0.080408566 acc: 97.65625\n",
      "training time 0.0168759822845459 all layer time: 0.0043222904205322266 shape: (128, 10)\n",
      "epoch: 0 batch 299 loss: 0.20355314 acc: 92.1875\n",
      "training time 0.01784682273864746 all layer time: 0.004289388656616211 shape: (128, 10)\n",
      "epoch: 0 batch 300 loss: 0.11946714 acc: 96.875\n",
      "training time 0.01682424545288086 all layer time: 0.004239797592163086 shape: (128, 10)\n",
      "epoch: 0 batch 301 loss: 0.20016314 acc: 93.75\n",
      "training time 0.017439603805541992 all layer time: 0.004135847091674805 shape: (128, 10)\n",
      "epoch: 0 batch 302 loss: 0.124737 acc: 96.09375\n",
      "training time 0.017209768295288086 all layer time: 0.004283428192138672 shape: (128, 10)\n",
      "epoch: 0 batch 303 loss: 0.057269942 acc: 98.4375\n",
      "training time 0.01782965660095215 all layer time: 0.004266500473022461 shape: (128, 10)\n",
      "epoch: 0 batch 304 loss: 0.1330868 acc: 96.09375\n",
      "training time 0.017170429229736328 all layer time: 0.0043184757232666016 shape: (128, 10)\n",
      "epoch: 0 batch 305 loss: 0.05995102 acc: 98.4375\n",
      "training time 0.017155170440673828 all layer time: 0.004312753677368164 shape: (128, 10)\n",
      "epoch: 0 batch 306 loss: 0.15717377 acc: 97.65625\n",
      "training time 0.01699376106262207 all layer time: 0.004085540771484375 shape: (128, 10)\n",
      "epoch: 0 batch 307 loss: 0.26607996 acc: 91.40625\n",
      "training time 0.01706981658935547 all layer time: 0.004312753677368164 shape: (128, 10)\n",
      "epoch: 0 batch 308 loss: 0.12150127 acc: 95.3125\n",
      "training time 0.017202138900756836 all layer time: 0.004307985305786133 shape: (128, 10)\n",
      "epoch: 0 batch 309 loss: 0.18009466 acc: 95.3125\n",
      "training time 0.01690530776977539 all layer time: 0.004265308380126953 shape: (128, 10)\n",
      "epoch: 0 batch 310 loss: 0.13984883 acc: 92.96875\n",
      "training time 0.017000913619995117 all layer time: 0.004106998443603516 shape: (128, 10)\n",
      "epoch: 0 batch 311 loss: 0.061420392 acc: 98.4375\n",
      "training time 0.01636815071105957 all layer time: 0.0043070316314697266 shape: (128, 10)\n",
      "epoch: 0 batch 312 loss: 0.17833707 acc: 92.1875\n",
      "training time 0.016791582107543945 all layer time: 0.004379749298095703 shape: (128, 10)\n",
      "epoch: 0 batch 313 loss: 0.061113268 acc: 99.21875\n",
      "training time 0.01725029945373535 all layer time: 0.0042874813079833984 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch 314 loss: 0.12814246 acc: 96.875\n",
      "training time 0.017448902130126953 all layer time: 0.004283905029296875 shape: (128, 10)\n",
      "epoch: 0 batch 315 loss: 0.08340928 acc: 96.875\n",
      "training time 0.017075777053833008 all layer time: 0.004298210144042969 shape: (128, 10)\n",
      "epoch: 0 batch 316 loss: 0.15603365 acc: 92.96875\n",
      "training time 0.01769089698791504 all layer time: 0.0044324398040771484 shape: (128, 10)\n",
      "epoch: 0 batch 317 loss: 0.07725185 acc: 98.4375\n",
      "training time 0.01714348793029785 all layer time: 0.004354238510131836 shape: (128, 10)\n",
      "epoch: 0 batch 318 loss: 0.17815328 acc: 95.3125\n",
      "training time 0.017121553421020508 all layer time: 0.004209995269775391 shape: (128, 10)\n",
      "epoch: 0 batch 319 loss: 0.10907248 acc: 96.875\n",
      "training time 0.016962528228759766 all layer time: 0.004240274429321289 shape: (128, 10)\n",
      "epoch: 0 batch 320 loss: 0.13819697 acc: 96.09375\n",
      "training time 0.01689887046813965 all layer time: 0.004359245300292969 shape: (128, 10)\n",
      "epoch: 0 batch 321 loss: 0.12318627 acc: 96.875\n",
      "training time 0.017581701278686523 all layer time: 0.0042531490325927734 shape: (128, 10)\n",
      "epoch: 0 batch 322 loss: 0.27378044 acc: 92.96875\n",
      "training time 0.017045259475708008 all layer time: 0.004225015640258789 shape: (128, 10)\n",
      "epoch: 0 batch 323 loss: 0.17915154 acc: 96.09375\n",
      "training time 0.01685333251953125 all layer time: 0.004196882247924805 shape: (128, 10)\n",
      "epoch: 0 batch 324 loss: 0.1263848 acc: 96.09375\n",
      "training time 0.01733255386352539 all layer time: 0.004235029220581055 shape: (128, 10)\n",
      "epoch: 0 batch 325 loss: 0.08132553 acc: 98.4375\n",
      "training time 0.01725459098815918 all layer time: 0.004353523254394531 shape: (128, 10)\n",
      "epoch: 0 batch 326 loss: 0.061126444 acc: 99.21875\n",
      "training time 0.016848325729370117 all layer time: 0.004271745681762695 shape: (128, 10)\n",
      "epoch: 0 batch 327 loss: 0.1265392 acc: 95.3125\n",
      "training time 0.01702594757080078 all layer time: 0.004160165786743164 shape: (128, 10)\n",
      "epoch: 0 batch 328 loss: 0.16404778 acc: 95.3125\n",
      "training time 0.016971111297607422 all layer time: 0.004248380661010742 shape: (128, 10)\n",
      "epoch: 0 batch 329 loss: 0.11371641 acc: 96.09375\n",
      "training time 0.017518281936645508 all layer time: 0.0042650699615478516 shape: (128, 10)\n",
      "epoch: 0 batch 330 loss: 0.12026079 acc: 96.09375\n",
      "training time 0.017131805419921875 all layer time: 0.0042841434478759766 shape: (128, 10)\n",
      "epoch: 0 batch 331 loss: 0.17067847 acc: 95.3125\n",
      "training time 0.017112255096435547 all layer time: 0.004206180572509766 shape: (128, 10)\n",
      "epoch: 0 batch 332 loss: 0.15519723 acc: 95.3125\n",
      "training time 0.016968250274658203 all layer time: 0.004244565963745117 shape: (128, 10)\n",
      "epoch: 0 batch 333 loss: 0.1704886 acc: 96.09375\n",
      "training time 0.016588687896728516 all layer time: 0.004530668258666992 shape: (128, 10)\n",
      "epoch: 0 batch 334 loss: 0.20366018 acc: 96.875\n",
      "training time 0.016495943069458008 all layer time: 0.004309654235839844 shape: (128, 10)\n",
      "epoch: 0 batch 335 loss: 0.23322022 acc: 92.1875\n",
      "training time 0.017304182052612305 all layer time: 0.004212141036987305 shape: (128, 10)\n",
      "epoch: 0 batch 336 loss: 0.114969596 acc: 98.4375\n",
      "training time 0.017139196395874023 all layer time: 0.004208803176879883 shape: (128, 10)\n",
      "epoch: 0 batch 337 loss: 0.16588917 acc: 95.3125\n",
      "training time 0.017145156860351562 all layer time: 0.004309177398681641 shape: (128, 10)\n",
      "epoch: 0 batch 338 loss: 0.095738664 acc: 96.875\n",
      "training time 0.017101287841796875 all layer time: 0.004100322723388672 shape: (128, 10)\n",
      "epoch: 0 batch 339 loss: 0.111138016 acc: 97.65625\n",
      "training time 0.017363548278808594 all layer time: 0.00432276725769043 shape: (128, 10)\n",
      "epoch: 0 batch 340 loss: 0.11940648 acc: 96.09375\n",
      "training time 0.016982316970825195 all layer time: 0.004358530044555664 shape: (128, 10)\n",
      "epoch: 0 batch 341 loss: 0.14947464 acc: 96.875\n",
      "training time 0.017195463180541992 all layer time: 0.004407167434692383 shape: (128, 10)\n",
      "epoch: 0 batch 342 loss: 0.14436008 acc: 95.3125\n",
      "training time 0.01689434051513672 all layer time: 0.0042209625244140625 shape: (128, 10)\n",
      "epoch: 0 batch 343 loss: 0.18849003 acc: 96.09375\n",
      "training time 0.017971277236938477 all layer time: 0.004193544387817383 shape: (128, 10)\n",
      "epoch: 0 batch 344 loss: 0.17825282 acc: 92.1875\n",
      "training time 0.01719808578491211 all layer time: 0.004090070724487305 shape: (128, 10)\n",
      "epoch: 0 batch 345 loss: 0.16399647 acc: 96.09375\n",
      "training time 0.016452789306640625 all layer time: 0.004312276840209961 shape: (128, 10)\n",
      "epoch: 0 batch 346 loss: 0.10418156 acc: 96.875\n",
      "training time 0.017007112503051758 all layer time: 0.004515886306762695 shape: (128, 10)\n",
      "epoch: 0 batch 347 loss: 0.20779982 acc: 96.09375\n",
      "training time 0.017360448837280273 all layer time: 0.005543708801269531 shape: (128, 10)\n",
      "epoch: 0 batch 348 loss: 0.11217677 acc: 96.875\n",
      "training time 0.01698136329650879 all layer time: 0.004096269607543945 shape: (128, 10)\n",
      "epoch: 0 batch 349 loss: 0.09704931 acc: 96.09375\n",
      "training time 0.01706719398498535 all layer time: 0.0041124820709228516 shape: (128, 10)\n",
      "epoch: 0 batch 350 loss: 0.10133106 acc: 96.09375\n",
      "training time 0.017012834548950195 all layer time: 0.004259586334228516 shape: (128, 10)\n",
      "epoch: 0 batch 351 loss: 0.14597058 acc: 95.3125\n",
      "training time 0.017305374145507812 all layer time: 0.004131317138671875 shape: (128, 10)\n",
      "epoch: 0 batch 352 loss: 0.19109556 acc: 96.09375\n",
      "training time 0.017548322677612305 all layer time: 0.004172086715698242 shape: (128, 10)\n",
      "epoch: 0 batch 353 loss: 0.06491735 acc: 98.4375\n",
      "training time 0.017078161239624023 all layer time: 0.004125118255615234 shape: (128, 10)\n",
      "epoch: 0 batch 354 loss: 0.08492917 acc: 96.09375\n",
      "training time 0.016913652420043945 all layer time: 0.004321575164794922 shape: (128, 10)\n",
      "epoch: 0 batch 355 loss: 0.22905149 acc: 92.96875\n",
      "training time 0.01729559898376465 all layer time: 0.004189729690551758 shape: (128, 10)\n",
      "epoch: 0 batch 356 loss: 0.15121065 acc: 92.96875\n",
      "training time 0.01733851432800293 all layer time: 0.004235506057739258 shape: (128, 10)\n",
      "epoch: 0 batch 357 loss: 0.1776135 acc: 95.3125\n",
      "training time 0.017036914825439453 all layer time: 0.0041081905364990234 shape: (128, 10)\n",
      "epoch: 0 batch 358 loss: 0.16517127 acc: 94.53125\n",
      "training time 0.01694035530090332 all layer time: 0.0042417049407958984 shape: (128, 10)\n",
      "epoch: 0 batch 359 loss: 0.13368645 acc: 94.53125\n",
      "training time 0.016940832138061523 all layer time: 0.004195213317871094 shape: (128, 10)\n",
      "epoch: 0 batch 360 loss: 0.09091237 acc: 96.09375\n",
      "training time 0.017795562744140625 all layer time: 0.004410743713378906 shape: (128, 10)\n",
      "epoch: 0 batch 361 loss: 0.21812831 acc: 92.96875\n",
      "training time 0.01714015007019043 all layer time: 0.004150390625 shape: (128, 10)\n",
      "epoch: 0 batch 362 loss: 0.2957314 acc: 91.40625\n",
      "training time 0.016858577728271484 all layer time: 0.004113435745239258 shape: (128, 10)\n",
      "epoch: 0 batch 363 loss: 0.073876455 acc: 96.875\n",
      "training time 0.017564058303833008 all layer time: 0.004202604293823242 shape: (128, 10)\n",
      "epoch: 0 batch 364 loss: 0.07981888 acc: 96.875\n",
      "training time 0.016896724700927734 all layer time: 0.0044286251068115234 shape: (128, 10)\n",
      "epoch: 0 batch 365 loss: 0.1363104 acc: 96.09375\n",
      "training time 0.017081737518310547 all layer time: 0.004119157791137695 shape: (128, 10)\n",
      "epoch: 0 batch 366 loss: 0.110773005 acc: 97.65625\n",
      "training time 0.016655445098876953 all layer time: 0.004464387893676758 shape: (128, 10)\n",
      "epoch: 0 batch 367 loss: 0.1196194 acc: 94.53125\n",
      "training time 0.017027616500854492 all layer time: 0.004109382629394531 shape: (128, 10)\n",
      "epoch: 0 batch 368 loss: 0.08338523 acc: 96.875\n",
      "training time 0.017078161239624023 all layer time: 0.00439143180847168 shape: (128, 10)\n",
      "epoch: 0 batch 369 loss: 0.2034777 acc: 95.3125\n",
      "training time 0.017281293869018555 all layer time: 0.006051778793334961 shape: (128, 10)\n",
      "epoch: 0 batch 370 loss: 0.23075566 acc: 93.75\n",
      "training time 0.017078876495361328 all layer time: 0.0042400360107421875 shape: (128, 10)\n",
      "epoch: 0 batch 371 loss: 0.12869292 acc: 97.65625\n",
      "training time 0.01727128028869629 all layer time: 0.004272937774658203 shape: (128, 10)\n",
      "epoch: 0 batch 372 loss: 0.060251966 acc: 98.4375\n",
      "training time 0.017169713973999023 all layer time: 0.004308938980102539 shape: (128, 10)\n",
      "epoch: 0 batch 373 loss: 0.0969234 acc: 96.875\n",
      "training time 0.01799488067626953 all layer time: 0.004197597503662109 shape: (128, 10)\n",
      "epoch: 0 batch 374 loss: 0.14573352 acc: 94.53125\n",
      "training time 0.017133474349975586 all layer time: 0.004296779632568359 shape: (128, 10)\n",
      "epoch: 0 batch 375 loss: 0.12685172 acc: 96.09375\n",
      "training time 0.016889095306396484 all layer time: 0.004360198974609375 shape: (128, 10)\n",
      "epoch: 0 batch 376 loss: 0.07453276 acc: 98.4375\n",
      "training time 0.016758203506469727 all layer time: 0.004196643829345703 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch 377 loss: 0.12772004 acc: 94.53125\n",
      "training time 0.016961097717285156 all layer time: 0.004347801208496094 shape: (128, 10)\n",
      "epoch: 0 batch 378 loss: 0.06961817 acc: 97.65625\n",
      "training time 0.017129182815551758 all layer time: 0.004230499267578125 shape: (128, 10)\n",
      "epoch: 0 batch 379 loss: 0.11177169 acc: 96.09375\n",
      "training time 0.017514705657958984 all layer time: 0.004149436950683594 shape: (128, 10)\n",
      "epoch: 0 batch 380 loss: 0.07263971 acc: 96.875\n",
      "training time 0.017255544662475586 all layer time: 0.004525423049926758 shape: (128, 10)\n",
      "epoch: 0 batch 381 loss: 0.05000017 acc: 98.4375\n",
      "training time 0.017183542251586914 all layer time: 0.004278421401977539 shape: (128, 10)\n",
      "epoch: 0 batch 382 loss: 0.26665607 acc: 92.96875\n",
      "training time 0.017950057983398438 all layer time: 0.004134654998779297 shape: (128, 10)\n",
      "epoch: 0 batch 383 loss: 0.13953044 acc: 96.09375\n",
      "training time 0.017640113830566406 all layer time: 0.004225492477416992 shape: (128, 10)\n",
      "epoch: 0 batch 384 loss: 0.22154102 acc: 94.53125\n",
      "training time 0.017061948776245117 all layer time: 0.004228353500366211 shape: (128, 10)\n",
      "epoch: 0 batch 385 loss: 0.0574437 acc: 97.65625\n",
      "training time 0.0171968936920166 all layer time: 0.004156351089477539 shape: (128, 10)\n",
      "epoch: 0 batch 386 loss: 0.21186668 acc: 92.96875\n",
      "training time 0.017679452896118164 all layer time: 0.0042340755462646484 shape: (128, 10)\n",
      "epoch: 0 batch 387 loss: 0.28151798 acc: 92.96875\n",
      "training time 0.01706695556640625 all layer time: 0.004095315933227539 shape: (128, 10)\n",
      "epoch: 0 batch 388 loss: 0.08645178 acc: 95.3125\n",
      "training time 0.017091035842895508 all layer time: 0.0043756961822509766 shape: (128, 10)\n",
      "epoch: 0 batch 389 loss: 0.19360188 acc: 93.75\n",
      "training time 0.016973495483398438 all layer time: 0.004334926605224609 shape: (128, 10)\n",
      "epoch: 0 batch 390 loss: 0.043184653 acc: 100.0\n",
      "training time 0.01749706268310547 all layer time: 0.004444599151611328 shape: (128, 10)\n",
      "epoch: 0 batch 391 loss: 0.077309124 acc: 97.65625\n",
      "training time 0.017181396484375 all layer time: 0.0043528079986572266 shape: (128, 10)\n",
      "epoch: 0 batch 392 loss: 0.13415737 acc: 96.875\n",
      "training time 0.01695537567138672 all layer time: 0.0043184757232666016 shape: (128, 10)\n",
      "epoch: 0 batch 393 loss: 0.2774434 acc: 90.625\n",
      "training time 0.016691207885742188 all layer time: 0.004269123077392578 shape: (128, 10)\n",
      "epoch: 0 batch 394 loss: 0.20044492 acc: 96.875\n",
      "training time 0.01743459701538086 all layer time: 0.004641532897949219 shape: (128, 10)\n",
      "epoch: 0 batch 395 loss: 0.10781064 acc: 97.65625\n",
      "training time 0.016999483108520508 all layer time: 0.00423884391784668 shape: (128, 10)\n",
      "epoch: 0 batch 396 loss: 0.19273776 acc: 94.53125\n",
      "training time 0.0172579288482666 all layer time: 0.004262208938598633 shape: (128, 10)\n",
      "epoch: 0 batch 397 loss: 0.16318548 acc: 94.53125\n",
      "training time 0.017107725143432617 all layer time: 0.0044040679931640625 shape: (128, 10)\n",
      "epoch: 0 batch 398 loss: 0.10691063 acc: 97.65625\n",
      "training time 0.016845226287841797 all layer time: 0.004306316375732422 shape: (128, 10)\n",
      "epoch: 0 batch 399 loss: 0.061773755 acc: 98.4375\n",
      "training time 0.017874956130981445 all layer time: 0.0042934417724609375 shape: (128, 10)\n",
      "epoch: 0 batch 400 loss: 0.15896513 acc: 94.53125\n",
      "training time 0.017026662826538086 all layer time: 0.004176616668701172 shape: (128, 10)\n",
      "epoch: 0 batch 401 loss: 0.06630865 acc: 96.09375\n",
      "training time 0.01685357093811035 all layer time: 0.004113435745239258 shape: (128, 10)\n",
      "epoch: 0 batch 402 loss: 0.09670029 acc: 96.875\n",
      "training time 0.017081737518310547 all layer time: 0.004193782806396484 shape: (128, 10)\n",
      "epoch: 0 batch 403 loss: 0.09347728 acc: 97.65625\n",
      "training time 0.01780414581298828 all layer time: 0.0044138431549072266 shape: (128, 10)\n",
      "epoch: 0 batch 404 loss: 0.15497963 acc: 97.65625\n",
      "training time 0.01893329620361328 all layer time: 0.004228830337524414 shape: (128, 10)\n",
      "epoch: 0 batch 405 loss: 0.10439449 acc: 98.4375\n",
      "training time 0.017138242721557617 all layer time: 0.004479408264160156 shape: (128, 10)\n",
      "epoch: 0 batch 406 loss: 0.1494927 acc: 93.75\n",
      "training time 0.017032861709594727 all layer time: 0.0042340755462646484 shape: (128, 10)\n",
      "epoch: 0 batch 407 loss: 0.21053323 acc: 95.3125\n",
      "training time 0.017778873443603516 all layer time: 0.00432276725769043 shape: (128, 10)\n",
      "epoch: 0 batch 408 loss: 0.09809965 acc: 96.875\n",
      "training time 0.0177457332611084 all layer time: 0.004094600677490234 shape: (128, 10)\n",
      "epoch: 0 batch 409 loss: 0.11594058 acc: 96.875\n",
      "training time 0.017090797424316406 all layer time: 0.004168033599853516 shape: (128, 10)\n",
      "epoch: 0 batch 410 loss: 0.02820119 acc: 99.21875\n",
      "training time 0.0169222354888916 all layer time: 0.004179716110229492 shape: (128, 10)\n",
      "epoch: 0 batch 411 loss: 0.11665761 acc: 96.09375\n",
      "training time 0.0173494815826416 all layer time: 0.004488468170166016 shape: (128, 10)\n",
      "epoch: 0 batch 412 loss: 0.08648665 acc: 97.65625\n",
      "training time 0.017830848693847656 all layer time: 0.005290508270263672 shape: (128, 10)\n",
      "epoch: 0 batch 413 loss: 0.33065307 acc: 93.75\n",
      "training time 0.016945838928222656 all layer time: 0.004270792007446289 shape: (128, 10)\n",
      "epoch: 0 batch 414 loss: 0.04426607 acc: 100.0\n",
      "training time 0.016937255859375 all layer time: 0.0041980743408203125 shape: (128, 10)\n",
      "epoch: 0 batch 415 loss: 0.18161304 acc: 95.3125\n",
      "training time 0.016979217529296875 all layer time: 0.004164457321166992 shape: (128, 10)\n",
      "epoch: 0 batch 416 loss: 0.022894612 acc: 99.21875\n",
      "training time 0.017319917678833008 all layer time: 0.004130840301513672 shape: (128, 10)\n",
      "epoch: 0 batch 417 loss: 0.12311288 acc: 96.09375\n",
      "training time 0.017049074172973633 all layer time: 0.0041043758392333984 shape: (128, 10)\n",
      "epoch: 0 batch 418 loss: 0.09559333 acc: 96.875\n",
      "training time 0.017121076583862305 all layer time: 0.00422215461730957 shape: (128, 10)\n",
      "epoch: 0 batch 419 loss: 0.14364073 acc: 93.75\n",
      "training time 0.017505884170532227 all layer time: 0.004321575164794922 shape: (128, 10)\n",
      "epoch: 0 batch 420 loss: 0.19662294 acc: 96.09375\n",
      "training time 0.017566919326782227 all layer time: 0.004362821578979492 shape: (128, 10)\n",
      "epoch: 0 batch 421 loss: 0.18822926 acc: 94.53125\n",
      "training time 0.01689887046813965 all layer time: 0.004254579544067383 shape: (128, 10)\n",
      "epoch: 0 batch 422 loss: 0.1008882 acc: 96.875\n",
      "training time 0.01712179183959961 all layer time: 0.004308938980102539 shape: (128, 10)\n",
      "epoch: 0 batch 423 loss: 0.08974003 acc: 97.65625\n",
      "training time 0.017831802368164062 all layer time: 0.004219770431518555 shape: (128, 10)\n",
      "epoch: 0 batch 424 loss: 0.1084961 acc: 98.4375\n",
      "training time 0.016999006271362305 all layer time: 0.0043146610260009766 shape: (128, 10)\n",
      "epoch: 0 batch 425 loss: 0.09547326 acc: 98.4375\n",
      "training time 0.016806602478027344 all layer time: 0.004498481750488281 shape: (128, 10)\n",
      "epoch: 0 batch 426 loss: 0.09724296 acc: 96.875\n",
      "training time 0.01718592643737793 all layer time: 0.00417780876159668 shape: (128, 10)\n",
      "epoch: 0 batch 427 loss: 0.061893076 acc: 98.4375\n",
      "training time 0.016939640045166016 all layer time: 0.0042607784271240234 shape: (128, 10)\n",
      "epoch: 0 batch 428 loss: 0.118634626 acc: 96.875\n",
      "training time 0.016892433166503906 all layer time: 0.004259824752807617 shape: (128, 10)\n",
      "epoch: 0 batch 429 loss: 0.07954748 acc: 97.65625\n",
      "training time 0.0176241397857666 all layer time: 0.004273414611816406 shape: (128, 10)\n",
      "epoch: 0 batch 430 loss: 0.055059478 acc: 97.65625\n",
      "training time 0.017388105392456055 all layer time: 0.004251956939697266 shape: (128, 10)\n",
      "epoch: 0 batch 431 loss: 0.10384441 acc: 96.875\n",
      "training time 0.01699233055114746 all layer time: 0.004205226898193359 shape: (128, 10)\n",
      "epoch: 0 batch 432 loss: 0.1648378 acc: 97.65625\n",
      "training time 0.016879558563232422 all layer time: 0.004199504852294922 shape: (128, 10)\n",
      "epoch: 0 batch 433 loss: 0.14416906 acc: 94.53125\n",
      "training time 0.01737499237060547 all layer time: 0.004160881042480469 shape: (128, 10)\n",
      "epoch: 0 batch 434 loss: 0.063748546 acc: 98.4375\n",
      "training time 0.017015457153320312 all layer time: 0.004193305969238281 shape: (128, 10)\n",
      "epoch: 0 batch 435 loss: 0.16044144 acc: 94.53125\n",
      "training time 0.017452001571655273 all layer time: 0.0043489933013916016 shape: (128, 10)\n",
      "epoch: 0 batch 436 loss: 0.07883004 acc: 96.09375\n",
      "training time 0.016993045806884766 all layer time: 0.004266977310180664 shape: (128, 10)\n",
      "epoch: 0 batch 437 loss: 0.025564928 acc: 100.0\n",
      "training time 0.017140865325927734 all layer time: 0.00421452522277832 shape: (128, 10)\n",
      "epoch: 0 batch 438 loss: 0.06046568 acc: 97.65625\n",
      "training time 0.017149925231933594 all layer time: 0.004221916198730469 shape: (128, 10)\n",
      "epoch: 0 batch 439 loss: 0.09348511 acc: 97.65625\n",
      "training time 0.017995595932006836 all layer time: 0.004178524017333984 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch 440 loss: 0.06798394 acc: 97.65625\n",
      "training time 0.01686882972717285 all layer time: 0.0043714046478271484 shape: (128, 10)\n",
      "epoch: 0 batch 441 loss: 0.1298178 acc: 94.53125\n",
      "training time 0.017316818237304688 all layer time: 0.004143476486206055 shape: (128, 10)\n",
      "epoch: 0 batch 442 loss: 0.12841469 acc: 95.3125\n",
      "training time 0.017351388931274414 all layer time: 0.004461526870727539 shape: (128, 10)\n",
      "epoch: 0 batch 443 loss: 0.090267666 acc: 96.875\n",
      "training time 0.017720460891723633 all layer time: 0.00424957275390625 shape: (128, 10)\n",
      "epoch: 0 batch 444 loss: 0.072420254 acc: 96.875\n",
      "training time 0.016813278198242188 all layer time: 0.004233121871948242 shape: (128, 10)\n",
      "epoch: 0 batch 445 loss: 0.09822374 acc: 96.875\n",
      "training time 0.01674175262451172 all layer time: 0.004163265228271484 shape: (128, 10)\n",
      "epoch: 0 batch 446 loss: 0.055919304 acc: 98.4375\n",
      "training time 0.01691746711730957 all layer time: 0.004227161407470703 shape: (128, 10)\n",
      "epoch: 0 batch 447 loss: 0.089199916 acc: 96.875\n",
      "training time 0.017689228057861328 all layer time: 0.004202842712402344 shape: (128, 10)\n",
      "epoch: 0 batch 448 loss: 0.06268267 acc: 97.65625\n",
      "training time 0.016973018646240234 all layer time: 0.0042803287506103516 shape: (128, 10)\n",
      "epoch: 0 batch 449 loss: 0.11597213 acc: 96.09375\n",
      "training time 0.01674675941467285 all layer time: 0.004214763641357422 shape: (128, 10)\n",
      "epoch: 0 batch 450 loss: 0.12392363 acc: 95.3125\n",
      "training time 0.016934871673583984 all layer time: 0.00507807731628418 shape: (128, 10)\n",
      "epoch: 0 batch 451 loss: 0.15716688 acc: 94.53125\n",
      "training time 0.017719268798828125 all layer time: 0.004307746887207031 shape: (128, 10)\n",
      "epoch: 0 batch 452 loss: 0.0708693 acc: 97.65625\n",
      "training time 0.017185688018798828 all layer time: 0.0041391849517822266 shape: (128, 10)\n",
      "epoch: 0 batch 453 loss: 0.080547616 acc: 96.875\n",
      "training time 0.016906023025512695 all layer time: 0.004234790802001953 shape: (128, 10)\n",
      "epoch: 0 batch 454 loss: 0.025610063 acc: 100.0\n",
      "training time 0.017097949981689453 all layer time: 0.004240751266479492 shape: (128, 10)\n",
      "epoch: 0 batch 455 loss: 0.025012441 acc: 98.4375\n",
      "training time 0.018326997756958008 all layer time: 0.004447221755981445 shape: (128, 10)\n",
      "epoch: 0 batch 456 loss: 0.08651334 acc: 98.4375\n",
      "training time 0.017357826232910156 all layer time: 0.00421595573425293 shape: (128, 10)\n",
      "epoch: 0 batch 457 loss: 0.03671587 acc: 99.21875\n",
      "training time 0.017005443572998047 all layer time: 0.004419088363647461 shape: (128, 10)\n",
      "epoch: 0 batch 458 loss: 0.030991923 acc: 98.4375\n",
      "training time 0.017024993896484375 all layer time: 0.004284381866455078 shape: (128, 10)\n",
      "epoch: 0 batch 459 loss: 0.052969843 acc: 97.65625\n",
      "training time 0.016950130462646484 all layer time: 0.004273414611816406 shape: (128, 10)\n",
      "epoch: 0 batch 460 loss: 0.011373468 acc: 100.0\n",
      "training time 0.016953229904174805 all layer time: 0.004218339920043945 shape: (128, 10)\n",
      "epoch: 0 batch 461 loss: 0.005851901 acc: 100.0\n",
      "training time 0.017238855361938477 all layer time: 0.004229307174682617 shape: (128, 10)\n",
      "epoch: 0 batch 462 loss: 0.006526429 acc: 100.0\n",
      "training time 0.01688361167907715 all layer time: 0.004385471343994141 shape: (128, 10)\n",
      "epoch: 0 batch 463 loss: 0.1413931 acc: 96.09375\n",
      "training time 0.017094135284423828 all layer time: 0.004477262496948242 shape: (128, 10)\n",
      "epoch: 0 batch 464 loss: 0.102643125 acc: 96.875\n",
      "training time 0.01728367805480957 all layer time: 0.004450798034667969 shape: (128, 10)\n",
      "epoch: 0 batch 465 loss: 0.006067063 acc: 100.0\n",
      "training time 0.016865253448486328 all layer time: 0.004110097885131836 shape: (128, 10)\n",
      "epoch: 0 batch 466 loss: 0.54137295 acc: 89.84375\n",
      "training time 0.016664505004882812 all layer time: 0.004408836364746094 shape: (128, 10)\n",
      "epoch: 0 batch 467 loss: 0.016910475 acc: 100.0\n",
      "training time 0.01696181297302246 all layer time: 0.004258394241333008 shape: (128, 10)\n",
      "before\n",
      "[0.0296854  0.         0.         0.07860769 0.02956086 0.00098033\n",
      " 0.         0.         0.         0.01309082 0.         0.03796437\n",
      " 0.         0.0007236  0.0278454  0.02397558 0.         0.\n",
      " 0.04431852 0.06322791 0.00572213 0.         0.         0.0727847\n",
      " 0.         0.05096438 0.         0.03992214 0.05394752 0.\n",
      " 0.04855422 0.0103492 ]\n",
      "after\n",
      "[0.02962416 0.         0.         0.07830986 0.02978463 0.00090728\n",
      " 0.         0.         0.         0.013043   0.         0.03822162\n",
      " 0.         0.00080553 0.02784008 0.02415862 0.         0.\n",
      " 0.04431806 0.06353398 0.00582617 0.         0.         0.07280303\n",
      " 0.         0.05112815 0.         0.03999132 0.05403705 0.\n",
      " 0.04874928 0.01036435]\n",
      "epoch: 1 batch 0 loss: 0.09132023 acc: 97.65625\n",
      "training time 0.017432689666748047 all layer time: 0.004409074783325195 shape: (128, 10)\n",
      "epoch: 1 batch 1 loss: 0.18661502 acc: 94.53125\n",
      "training time 0.01940131187438965 all layer time: 0.004148244857788086 shape: (128, 10)\n",
      "epoch: 1 batch 2 loss: 0.06713328 acc: 96.09375\n",
      "training time 0.01675105094909668 all layer time: 0.004336833953857422 shape: (128, 10)\n",
      "epoch: 1 batch 3 loss: 0.17288002 acc: 96.09375\n",
      "training time 0.01751852035522461 all layer time: 0.004384756088256836 shape: (128, 10)\n",
      "epoch: 1 batch 4 loss: 0.10485246 acc: 97.65625\n",
      "training time 0.01724076271057129 all layer time: 0.004203081130981445 shape: (128, 10)\n",
      "epoch: 1 batch 5 loss: 0.08147921 acc: 98.4375\n",
      "training time 0.017326831817626953 all layer time: 0.004281759262084961 shape: (128, 10)\n",
      "epoch: 1 batch 6 loss: 0.14648145 acc: 95.3125\n",
      "training time 0.016755342483520508 all layer time: 0.004295825958251953 shape: (128, 10)\n",
      "epoch: 1 batch 7 loss: 0.14884056 acc: 95.3125\n",
      "training time 0.01707172393798828 all layer time: 0.004111289978027344 shape: (128, 10)\n",
      "epoch: 1 batch 8 loss: 0.16249922 acc: 93.75\n",
      "training time 0.01715707778930664 all layer time: 0.004470109939575195 shape: (128, 10)\n",
      "epoch: 1 batch 9 loss: 0.12744424 acc: 96.875\n",
      "training time 0.017194747924804688 all layer time: 0.004487752914428711 shape: (128, 10)\n",
      "epoch: 1 batch 10 loss: 0.20213462 acc: 90.625\n",
      "training time 0.017023324966430664 all layer time: 0.004212379455566406 shape: (128, 10)\n",
      "epoch: 1 batch 11 loss: 0.07942177 acc: 96.875\n",
      "training time 0.017171859741210938 all layer time: 0.004277229309082031 shape: (128, 10)\n",
      "epoch: 1 batch 12 loss: 0.16303441 acc: 98.4375\n",
      "training time 0.01681041717529297 all layer time: 0.0041391849517822266 shape: (128, 10)\n",
      "epoch: 1 batch 13 loss: 0.078214586 acc: 96.875\n",
      "training time 0.017481565475463867 all layer time: 0.004337310791015625 shape: (128, 10)\n",
      "epoch: 1 batch 14 loss: 0.055480495 acc: 98.4375\n",
      "training time 0.01937556266784668 all layer time: 0.004180192947387695 shape: (128, 10)\n",
      "epoch: 1 batch 15 loss: 0.17566076 acc: 94.53125\n",
      "training time 0.01642012596130371 all layer time: 0.004190206527709961 shape: (128, 10)\n",
      "epoch: 1 batch 16 loss: 0.05646219 acc: 97.65625\n",
      "training time 0.017110109329223633 all layer time: 0.004252910614013672 shape: (128, 10)\n",
      "epoch: 1 batch 17 loss: 0.06937298 acc: 97.65625\n",
      "training time 0.0176846981048584 all layer time: 0.004267692565917969 shape: (128, 10)\n",
      "epoch: 1 batch 18 loss: 0.09367805 acc: 97.65625\n",
      "training time 0.017318248748779297 all layer time: 0.00441431999206543 shape: (128, 10)\n",
      "epoch: 1 batch 19 loss: 0.06257647 acc: 98.4375\n",
      "training time 0.017249107360839844 all layer time: 0.004209756851196289 shape: (128, 10)\n",
      "epoch: 1 batch 20 loss: 0.19586277 acc: 95.3125\n",
      "training time 0.016646146774291992 all layer time: 0.004229068756103516 shape: (128, 10)\n",
      "epoch: 1 batch 21 loss: 0.1268692 acc: 98.4375\n",
      "training time 0.016524076461791992 all layer time: 0.004163503646850586 shape: (128, 10)\n",
      "epoch: 1 batch 22 loss: 0.077875674 acc: 98.4375\n",
      "training time 0.01736903190612793 all layer time: 0.0064280033111572266 shape: (128, 10)\n",
      "epoch: 1 batch 23 loss: 0.06652123 acc: 96.875\n",
      "training time 0.01725316047668457 all layer time: 0.004171848297119141 shape: (128, 10)\n",
      "epoch: 1 batch 24 loss: 0.05271422 acc: 98.4375\n",
      "training time 0.016829967498779297 all layer time: 0.004171133041381836 shape: (128, 10)\n",
      "epoch: 1 batch 25 loss: 0.105074316 acc: 97.65625\n",
      "training time 0.01699995994567871 all layer time: 0.004381895065307617 shape: (128, 10)\n",
      "epoch: 1 batch 26 loss: 0.032123275 acc: 100.0\n",
      "training time 0.01729130744934082 all layer time: 0.0043752193450927734 shape: (128, 10)\n",
      "epoch: 1 batch 27 loss: 0.11782084 acc: 96.875\n",
      "training time 0.018134117126464844 all layer time: 0.004160165786743164 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 28 loss: 0.09504135 acc: 96.875\n",
      "training time 0.017005443572998047 all layer time: 0.004210948944091797 shape: (128, 10)\n",
      "epoch: 1 batch 29 loss: 0.13174115 acc: 96.09375\n",
      "training time 0.017186641693115234 all layer time: 0.004304170608520508 shape: (128, 10)\n",
      "epoch: 1 batch 30 loss: 0.026262129 acc: 99.21875\n",
      "training time 0.017649173736572266 all layer time: 0.0041887760162353516 shape: (128, 10)\n",
      "epoch: 1 batch 31 loss: 0.0845215 acc: 97.65625\n",
      "training time 0.016631364822387695 all layer time: 0.004178524017333984 shape: (128, 10)\n",
      "epoch: 1 batch 32 loss: 0.067428604 acc: 97.65625\n",
      "training time 0.016954898834228516 all layer time: 0.004159688949584961 shape: (128, 10)\n",
      "epoch: 1 batch 33 loss: 0.06674706 acc: 96.09375\n",
      "training time 0.017308950424194336 all layer time: 0.004354953765869141 shape: (128, 10)\n",
      "epoch: 1 batch 34 loss: 0.13951631 acc: 96.09375\n",
      "training time 0.016817092895507812 all layer time: 0.004202365875244141 shape: (128, 10)\n",
      "epoch: 1 batch 35 loss: 0.06949593 acc: 97.65625\n",
      "training time 0.017393112182617188 all layer time: 0.004345417022705078 shape: (128, 10)\n",
      "epoch: 1 batch 36 loss: 0.1178736 acc: 95.3125\n",
      "training time 0.01718306541442871 all layer time: 0.004184246063232422 shape: (128, 10)\n",
      "epoch: 1 batch 37 loss: 0.05785413 acc: 98.4375\n",
      "training time 0.01734614372253418 all layer time: 0.004187345504760742 shape: (128, 10)\n",
      "epoch: 1 batch 38 loss: 0.1289171 acc: 96.875\n",
      "training time 0.016905546188354492 all layer time: 0.004232883453369141 shape: (128, 10)\n",
      "epoch: 1 batch 39 loss: 0.0770852 acc: 96.875\n",
      "training time 0.01779794692993164 all layer time: 0.004250764846801758 shape: (128, 10)\n",
      "epoch: 1 batch 40 loss: 0.09991469 acc: 96.09375\n",
      "training time 0.01753401756286621 all layer time: 0.0042073726654052734 shape: (128, 10)\n",
      "epoch: 1 batch 41 loss: 0.18527941 acc: 95.3125\n",
      "training time 0.01697063446044922 all layer time: 0.004370927810668945 shape: (128, 10)\n",
      "epoch: 1 batch 42 loss: 0.07772652 acc: 98.4375\n",
      "training time 0.017181873321533203 all layer time: 0.004202604293823242 shape: (128, 10)\n",
      "epoch: 1 batch 43 loss: 0.13029876 acc: 96.09375\n",
      "training time 0.017098426818847656 all layer time: 0.004261970520019531 shape: (128, 10)\n",
      "epoch: 1 batch 44 loss: 0.20385198 acc: 92.1875\n",
      "training time 0.017728567123413086 all layer time: 0.004507303237915039 shape: (128, 10)\n",
      "epoch: 1 batch 45 loss: 0.06952935 acc: 98.4375\n",
      "training time 0.017453432083129883 all layer time: 0.0042765140533447266 shape: (128, 10)\n",
      "epoch: 1 batch 46 loss: 0.053651772 acc: 96.875\n",
      "training time 0.016707181930541992 all layer time: 0.0042572021484375 shape: (128, 10)\n",
      "epoch: 1 batch 47 loss: 0.10565014 acc: 97.65625\n",
      "training time 0.016556262969970703 all layer time: 0.00419306755065918 shape: (128, 10)\n",
      "epoch: 1 batch 48 loss: 0.0837038 acc: 98.4375\n",
      "training time 0.018245697021484375 all layer time: 0.004362583160400391 shape: (128, 10)\n",
      "epoch: 1 batch 49 loss: 0.06431276 acc: 98.4375\n",
      "training time 0.017520427703857422 all layer time: 0.004224061965942383 shape: (128, 10)\n",
      "epoch: 1 batch 50 loss: 0.13841467 acc: 96.09375\n",
      "training time 0.016946077346801758 all layer time: 0.004323244094848633 shape: (128, 10)\n",
      "epoch: 1 batch 51 loss: 0.03766232 acc: 99.21875\n",
      "training time 0.01669907569885254 all layer time: 0.004315853118896484 shape: (128, 10)\n",
      "epoch: 1 batch 52 loss: 0.13477069 acc: 96.09375\n",
      "training time 0.01737666130065918 all layer time: 0.004383563995361328 shape: (128, 10)\n",
      "epoch: 1 batch 53 loss: 0.20246921 acc: 95.3125\n",
      "training time 0.017112016677856445 all layer time: 0.004215240478515625 shape: (128, 10)\n",
      "epoch: 1 batch 54 loss: 0.16842887 acc: 96.875\n",
      "training time 0.017029762268066406 all layer time: 0.004207611083984375 shape: (128, 10)\n",
      "epoch: 1 batch 55 loss: 0.16680862 acc: 95.3125\n",
      "training time 0.0171053409576416 all layer time: 0.0043811798095703125 shape: (128, 10)\n",
      "epoch: 1 batch 56 loss: 0.12792796 acc: 96.09375\n",
      "training time 0.016666412353515625 all layer time: 0.004190206527709961 shape: (128, 10)\n",
      "epoch: 1 batch 57 loss: 0.09333575 acc: 97.65625\n",
      "training time 0.016997337341308594 all layer time: 0.004353523254394531 shape: (128, 10)\n",
      "epoch: 1 batch 58 loss: 0.09003367 acc: 96.875\n",
      "training time 0.017215251922607422 all layer time: 0.004225730895996094 shape: (128, 10)\n",
      "epoch: 1 batch 59 loss: 0.049478177 acc: 98.4375\n",
      "training time 0.020491600036621094 all layer time: 0.004359722137451172 shape: (128, 10)\n",
      "epoch: 1 batch 60 loss: 0.10653878 acc: 94.53125\n",
      "training time 0.017394304275512695 all layer time: 0.0043294429779052734 shape: (128, 10)\n",
      "epoch: 1 batch 61 loss: 0.10942977 acc: 96.09375\n",
      "training time 0.01771259307861328 all layer time: 0.004439115524291992 shape: (128, 10)\n",
      "epoch: 1 batch 62 loss: 0.091319755 acc: 96.09375\n",
      "training time 0.017413616180419922 all layer time: 0.004494190216064453 shape: (128, 10)\n",
      "epoch: 1 batch 63 loss: 0.14652498 acc: 96.09375\n",
      "training time 0.017201662063598633 all layer time: 0.00442194938659668 shape: (128, 10)\n",
      "epoch: 1 batch 64 loss: 0.17162262 acc: 95.3125\n",
      "training time 0.017208099365234375 all layer time: 0.004324197769165039 shape: (128, 10)\n",
      "epoch: 1 batch 65 loss: 0.04905783 acc: 98.4375\n",
      "training time 0.0171051025390625 all layer time: 0.004502058029174805 shape: (128, 10)\n",
      "epoch: 1 batch 66 loss: 0.08589737 acc: 95.3125\n",
      "training time 0.01779961585998535 all layer time: 0.004213809967041016 shape: (128, 10)\n",
      "epoch: 1 batch 67 loss: 0.16993618 acc: 92.96875\n",
      "training time 0.017140865325927734 all layer time: 0.00417637825012207 shape: (128, 10)\n",
      "epoch: 1 batch 68 loss: 0.2891981 acc: 93.75\n",
      "training time 0.01734304428100586 all layer time: 0.004353046417236328 shape: (128, 10)\n",
      "epoch: 1 batch 69 loss: 0.23465283 acc: 92.96875\n",
      "training time 0.01731729507446289 all layer time: 0.0042154788970947266 shape: (128, 10)\n",
      "epoch: 1 batch 70 loss: 0.07711126 acc: 97.65625\n",
      "training time 0.018200159072875977 all layer time: 0.004194021224975586 shape: (128, 10)\n",
      "epoch: 1 batch 71 loss: 0.111691326 acc: 96.875\n",
      "training time 0.017045021057128906 all layer time: 0.00428318977355957 shape: (128, 10)\n",
      "epoch: 1 batch 72 loss: 0.1312921 acc: 96.875\n",
      "training time 0.01715874671936035 all layer time: 0.004195451736450195 shape: (128, 10)\n",
      "epoch: 1 batch 73 loss: 0.3063448 acc: 92.96875\n",
      "training time 0.016915082931518555 all layer time: 0.004243135452270508 shape: (128, 10)\n",
      "epoch: 1 batch 74 loss: 0.12079608 acc: 96.09375\n",
      "training time 0.017071008682250977 all layer time: 0.004273414611816406 shape: (128, 10)\n",
      "epoch: 1 batch 75 loss: 0.036914315 acc: 99.21875\n",
      "training time 0.01688528060913086 all layer time: 0.004190206527709961 shape: (128, 10)\n",
      "epoch: 1 batch 76 loss: 0.059024714 acc: 97.65625\n",
      "training time 0.01688981056213379 all layer time: 0.004305601119995117 shape: (128, 10)\n",
      "epoch: 1 batch 77 loss: 0.05179864 acc: 97.65625\n",
      "training time 0.01674485206604004 all layer time: 0.004390716552734375 shape: (128, 10)\n",
      "epoch: 1 batch 78 loss: 0.08340103 acc: 97.65625\n",
      "training time 0.017180919647216797 all layer time: 0.0043218135833740234 shape: (128, 10)\n",
      "epoch: 1 batch 79 loss: 0.16452706 acc: 95.3125\n",
      "training time 0.017327547073364258 all layer time: 0.0042133331298828125 shape: (128, 10)\n",
      "epoch: 1 batch 80 loss: 0.13087052 acc: 95.3125\n",
      "training time 0.016941547393798828 all layer time: 0.0044400691986083984 shape: (128, 10)\n",
      "epoch: 1 batch 81 loss: 0.04908269 acc: 98.4375\n",
      "training time 0.017020225524902344 all layer time: 0.004156351089477539 shape: (128, 10)\n",
      "epoch: 1 batch 82 loss: 0.021364097 acc: 99.21875\n",
      "training time 0.01665782928466797 all layer time: 0.004233360290527344 shape: (128, 10)\n",
      "epoch: 1 batch 83 loss: 0.055701695 acc: 99.21875\n",
      "training time 0.01728034019470215 all layer time: 0.0043218135833740234 shape: (128, 10)\n",
      "epoch: 1 batch 84 loss: 0.096883796 acc: 97.65625\n",
      "training time 0.017293453216552734 all layer time: 0.004383563995361328 shape: (128, 10)\n",
      "epoch: 1 batch 85 loss: 0.15365386 acc: 97.65625\n",
      "training time 0.017130374908447266 all layer time: 0.00438380241394043 shape: (128, 10)\n",
      "epoch: 1 batch 86 loss: 0.050759904 acc: 98.4375\n",
      "training time 0.016809701919555664 all layer time: 0.0043065547943115234 shape: (128, 10)\n",
      "epoch: 1 batch 87 loss: 0.09578329 acc: 96.875\n",
      "training time 0.017908573150634766 all layer time: 0.004321098327636719 shape: (128, 10)\n",
      "epoch: 1 batch 88 loss: 0.08543244 acc: 96.09375\n",
      "training time 0.017023086547851562 all layer time: 0.004263401031494141 shape: (128, 10)\n",
      "epoch: 1 batch 89 loss: 0.10468622 acc: 96.875\n",
      "training time 0.01729607582092285 all layer time: 0.004113197326660156 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 90 loss: 0.09429228 acc: 97.65625\n",
      "training time 0.016940593719482422 all layer time: 0.004258871078491211 shape: (128, 10)\n",
      "epoch: 1 batch 91 loss: 0.1422236 acc: 95.3125\n",
      "training time 0.017229080200195312 all layer time: 0.0044708251953125 shape: (128, 10)\n",
      "epoch: 1 batch 92 loss: 0.12647748 acc: 96.875\n",
      "training time 0.016856908798217773 all layer time: 0.004317522048950195 shape: (128, 10)\n",
      "epoch: 1 batch 93 loss: 0.04635443 acc: 99.21875\n",
      "training time 0.01729440689086914 all layer time: 0.00432586669921875 shape: (128, 10)\n",
      "epoch: 1 batch 94 loss: 0.09560431 acc: 96.09375\n",
      "training time 0.017182588577270508 all layer time: 0.004176616668701172 shape: (128, 10)\n",
      "epoch: 1 batch 95 loss: 0.16220358 acc: 95.3125\n",
      "training time 0.017225980758666992 all layer time: 0.0043032169342041016 shape: (128, 10)\n",
      "epoch: 1 batch 96 loss: 0.09516691 acc: 96.875\n",
      "training time 0.017420053482055664 all layer time: 0.004287004470825195 shape: (128, 10)\n",
      "epoch: 1 batch 97 loss: 0.040154956 acc: 99.21875\n",
      "training time 0.01716017723083496 all layer time: 0.004266262054443359 shape: (128, 10)\n",
      "epoch: 1 batch 98 loss: 0.17759065 acc: 95.3125\n",
      "training time 0.017453670501708984 all layer time: 0.004221677780151367 shape: (128, 10)\n",
      "epoch: 1 batch 99 loss: 0.13040257 acc: 95.3125\n",
      "training time 0.017231225967407227 all layer time: 0.004297494888305664 shape: (128, 10)\n",
      "epoch: 1 batch 100 loss: 0.05784353 acc: 98.4375\n",
      "training time 0.017945528030395508 all layer time: 0.0045697689056396484 shape: (128, 10)\n",
      "epoch: 1 batch 101 loss: 0.16064245 acc: 93.75\n",
      "training time 0.017453908920288086 all layer time: 0.00420379638671875 shape: (128, 10)\n",
      "epoch: 1 batch 102 loss: 0.16255096 acc: 93.75\n",
      "training time 0.017125844955444336 all layer time: 0.004130363464355469 shape: (128, 10)\n",
      "epoch: 1 batch 103 loss: 0.043279126 acc: 98.4375\n",
      "training time 0.01697683334350586 all layer time: 0.004435539245605469 shape: (128, 10)\n",
      "epoch: 1 batch 104 loss: 0.08623403 acc: 96.875\n",
      "training time 0.016787290573120117 all layer time: 0.004250288009643555 shape: (128, 10)\n",
      "epoch: 1 batch 105 loss: 0.08106364 acc: 98.4375\n",
      "training time 0.01706099510192871 all layer time: 0.004246234893798828 shape: (128, 10)\n",
      "epoch: 1 batch 106 loss: 0.108066365 acc: 97.65625\n",
      "training time 0.016911983489990234 all layer time: 0.0042417049407958984 shape: (128, 10)\n",
      "epoch: 1 batch 107 loss: 0.07078506 acc: 97.65625\n",
      "training time 0.017072677612304688 all layer time: 0.004390239715576172 shape: (128, 10)\n",
      "epoch: 1 batch 108 loss: 0.068210915 acc: 96.09375\n",
      "training time 0.017075777053833008 all layer time: 0.0043179988861083984 shape: (128, 10)\n",
      "epoch: 1 batch 109 loss: 0.14102222 acc: 95.3125\n",
      "training time 0.01748490333557129 all layer time: 0.004273891448974609 shape: (128, 10)\n",
      "epoch: 1 batch 110 loss: 0.05780011 acc: 97.65625\n",
      "training time 0.016921520233154297 all layer time: 0.0043141841888427734 shape: (128, 10)\n",
      "epoch: 1 batch 111 loss: 0.11339168 acc: 96.875\n",
      "training time 0.017172813415527344 all layer time: 0.004209995269775391 shape: (128, 10)\n",
      "epoch: 1 batch 112 loss: 0.116054006 acc: 97.65625\n",
      "training time 0.01731109619140625 all layer time: 0.004227638244628906 shape: (128, 10)\n",
      "epoch: 1 batch 113 loss: 0.12210422 acc: 97.65625\n",
      "training time 0.016906261444091797 all layer time: 0.0043392181396484375 shape: (128, 10)\n",
      "epoch: 1 batch 114 loss: 0.10359782 acc: 96.09375\n",
      "training time 0.017130136489868164 all layer time: 0.004481792449951172 shape: (128, 10)\n",
      "epoch: 1 batch 115 loss: 0.21109258 acc: 92.96875\n",
      "training time 0.01697230339050293 all layer time: 0.004269838333129883 shape: (128, 10)\n",
      "epoch: 1 batch 116 loss: 0.039815553 acc: 99.21875\n",
      "training time 0.01689457893371582 all layer time: 0.004319429397583008 shape: (128, 10)\n",
      "epoch: 1 batch 117 loss: 0.029761763 acc: 98.4375\n",
      "training time 0.01706862449645996 all layer time: 0.004296541213989258 shape: (128, 10)\n",
      "epoch: 1 batch 118 loss: 0.12694687 acc: 96.09375\n",
      "training time 0.01681351661682129 all layer time: 0.004324913024902344 shape: (128, 10)\n",
      "epoch: 1 batch 119 loss: 0.10010413 acc: 98.4375\n",
      "training time 0.017261028289794922 all layer time: 0.004241228103637695 shape: (128, 10)\n",
      "epoch: 1 batch 120 loss: 0.10991021 acc: 96.875\n",
      "training time 0.01713728904724121 all layer time: 0.004308462142944336 shape: (128, 10)\n",
      "epoch: 1 batch 121 loss: 0.09064006 acc: 98.4375\n",
      "training time 0.017167091369628906 all layer time: 0.004456043243408203 shape: (128, 10)\n",
      "epoch: 1 batch 122 loss: 0.050665352 acc: 99.21875\n",
      "training time 0.018051862716674805 all layer time: 0.004433870315551758 shape: (128, 10)\n",
      "epoch: 1 batch 123 loss: 0.17780629 acc: 94.53125\n",
      "training time 0.0168001651763916 all layer time: 0.0041887760162353516 shape: (128, 10)\n",
      "epoch: 1 batch 124 loss: 0.17798604 acc: 96.09375\n",
      "training time 0.016811847686767578 all layer time: 0.004203319549560547 shape: (128, 10)\n",
      "epoch: 1 batch 125 loss: 0.07675543 acc: 97.65625\n",
      "training time 0.01720142364501953 all layer time: 0.004380941390991211 shape: (128, 10)\n",
      "epoch: 1 batch 126 loss: 0.09050182 acc: 96.09375\n",
      "training time 0.018259763717651367 all layer time: 0.004270076751708984 shape: (128, 10)\n",
      "epoch: 1 batch 127 loss: 0.07404372 acc: 97.65625\n",
      "training time 0.017658472061157227 all layer time: 0.004186868667602539 shape: (128, 10)\n",
      "epoch: 1 batch 128 loss: 0.036870934 acc: 98.4375\n",
      "training time 0.016741037368774414 all layer time: 0.004203081130981445 shape: (128, 10)\n",
      "epoch: 1 batch 129 loss: 0.123006806 acc: 96.875\n",
      "training time 0.017024517059326172 all layer time: 0.004286289215087891 shape: (128, 10)\n",
      "epoch: 1 batch 130 loss: 0.16012877 acc: 94.53125\n",
      "training time 0.01650524139404297 all layer time: 0.004258871078491211 shape: (128, 10)\n",
      "epoch: 1 batch 131 loss: 0.056837134 acc: 97.65625\n",
      "training time 0.01775336265563965 all layer time: 0.0043604373931884766 shape: (128, 10)\n",
      "epoch: 1 batch 132 loss: 0.07652707 acc: 97.65625\n",
      "training time 0.016997337341308594 all layer time: 0.004164218902587891 shape: (128, 10)\n",
      "epoch: 1 batch 133 loss: 0.11995371 acc: 95.3125\n",
      "training time 0.01707768440246582 all layer time: 0.00414276123046875 shape: (128, 10)\n",
      "epoch: 1 batch 134 loss: 0.08325188 acc: 96.875\n",
      "training time 0.017163753509521484 all layer time: 0.004125833511352539 shape: (128, 10)\n",
      "epoch: 1 batch 135 loss: 0.06863016 acc: 98.4375\n",
      "training time 0.01650094985961914 all layer time: 0.004115104675292969 shape: (128, 10)\n",
      "epoch: 1 batch 136 loss: 0.057567276 acc: 97.65625\n",
      "training time 0.01661968231201172 all layer time: 0.004101753234863281 shape: (128, 10)\n",
      "epoch: 1 batch 137 loss: 0.098501086 acc: 97.65625\n",
      "training time 0.016907930374145508 all layer time: 0.004141330718994141 shape: (128, 10)\n",
      "epoch: 1 batch 138 loss: 0.16812795 acc: 95.3125\n",
      "training time 0.016480684280395508 all layer time: 0.004308938980102539 shape: (128, 10)\n",
      "epoch: 1 batch 139 loss: 0.08964425 acc: 96.875\n",
      "training time 0.017174243927001953 all layer time: 0.0042073726654052734 shape: (128, 10)\n",
      "epoch: 1 batch 140 loss: 0.07222395 acc: 97.65625\n",
      "training time 0.016773223876953125 all layer time: 0.004205226898193359 shape: (128, 10)\n",
      "epoch: 1 batch 141 loss: 0.054319512 acc: 98.4375\n",
      "training time 0.016980886459350586 all layer time: 0.004304170608520508 shape: (128, 10)\n",
      "epoch: 1 batch 142 loss: 0.04342928 acc: 98.4375\n",
      "training time 0.016717195510864258 all layer time: 0.004158735275268555 shape: (128, 10)\n",
      "epoch: 1 batch 143 loss: 0.120155536 acc: 97.65625\n",
      "training time 0.0169374942779541 all layer time: 0.004526853561401367 shape: (128, 10)\n",
      "epoch: 1 batch 144 loss: 0.06807493 acc: 98.4375\n",
      "training time 0.017177581787109375 all layer time: 0.004162788391113281 shape: (128, 10)\n",
      "epoch: 1 batch 145 loss: 0.073299386 acc: 96.09375\n",
      "training time 0.01693248748779297 all layer time: 0.004146099090576172 shape: (128, 10)\n",
      "epoch: 1 batch 146 loss: 0.023840126 acc: 99.21875\n",
      "training time 0.017094135284423828 all layer time: 0.004210710525512695 shape: (128, 10)\n",
      "epoch: 1 batch 147 loss: 0.04023324 acc: 98.4375\n",
      "training time 0.016801118850708008 all layer time: 0.0044994354248046875 shape: (128, 10)\n",
      "epoch: 1 batch 148 loss: 0.020440863 acc: 99.21875\n",
      "training time 0.016588926315307617 all layer time: 0.004281282424926758 shape: (128, 10)\n",
      "epoch: 1 batch 149 loss: 0.17859796 acc: 95.3125\n",
      "training time 0.017449617385864258 all layer time: 0.004458904266357422 shape: (128, 10)\n",
      "epoch: 1 batch 150 loss: 0.18690243 acc: 96.875\n",
      "training time 0.017512798309326172 all layer time: 0.004197597503662109 shape: (128, 10)\n",
      "epoch: 1 batch 151 loss: 0.18700925 acc: 94.53125\n",
      "training time 0.0172119140625 all layer time: 0.004187822341918945 shape: (128, 10)\n",
      "epoch: 1 batch 152 loss: 0.07437681 acc: 97.65625\n",
      "training time 0.01826000213623047 all layer time: 0.0043637752532958984 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 153 loss: 0.027902529 acc: 100.0\n",
      "training time 0.0173494815826416 all layer time: 0.004200458526611328 shape: (128, 10)\n",
      "epoch: 1 batch 154 loss: 0.07897232 acc: 96.875\n",
      "training time 0.017133712768554688 all layer time: 0.0043561458587646484 shape: (128, 10)\n",
      "epoch: 1 batch 155 loss: 0.056763932 acc: 97.65625\n",
      "training time 0.017325639724731445 all layer time: 0.004141330718994141 shape: (128, 10)\n",
      "epoch: 1 batch 156 loss: 0.08660614 acc: 96.875\n",
      "training time 0.01672053337097168 all layer time: 0.00429224967956543 shape: (128, 10)\n",
      "epoch: 1 batch 157 loss: 0.079017855 acc: 96.875\n",
      "training time 0.017463207244873047 all layer time: 0.00432133674621582 shape: (128, 10)\n",
      "epoch: 1 batch 158 loss: 0.118164964 acc: 97.65625\n",
      "training time 0.01717996597290039 all layer time: 0.004209756851196289 shape: (128, 10)\n",
      "epoch: 1 batch 159 loss: 0.015246555 acc: 100.0\n",
      "training time 0.01725459098815918 all layer time: 0.004209280014038086 shape: (128, 10)\n",
      "epoch: 1 batch 160 loss: 0.07566996 acc: 99.21875\n",
      "training time 0.01705765724182129 all layer time: 0.004331111907958984 shape: (128, 10)\n",
      "epoch: 1 batch 161 loss: 0.15140061 acc: 95.3125\n",
      "training time 0.017302989959716797 all layer time: 0.004224538803100586 shape: (128, 10)\n",
      "epoch: 1 batch 162 loss: 0.060183678 acc: 99.21875\n",
      "training time 0.017214536666870117 all layer time: 0.004113674163818359 shape: (128, 10)\n",
      "epoch: 1 batch 163 loss: 0.14048178 acc: 96.875\n",
      "training time 0.01719808578491211 all layer time: 0.0044803619384765625 shape: (128, 10)\n",
      "epoch: 1 batch 164 loss: 0.085685864 acc: 97.65625\n",
      "training time 0.017301559448242188 all layer time: 0.004349470138549805 shape: (128, 10)\n",
      "epoch: 1 batch 165 loss: 0.082472 acc: 98.4375\n",
      "training time 0.01725006103515625 all layer time: 0.004467964172363281 shape: (128, 10)\n",
      "epoch: 1 batch 166 loss: 0.07432659 acc: 97.65625\n",
      "training time 0.017422199249267578 all layer time: 0.004202365875244141 shape: (128, 10)\n",
      "epoch: 1 batch 167 loss: 0.044673104 acc: 100.0\n",
      "training time 0.01710677146911621 all layer time: 0.0043451786041259766 shape: (128, 10)\n",
      "epoch: 1 batch 168 loss: 0.07972732 acc: 97.65625\n",
      "training time 0.017117977142333984 all layer time: 0.004335641860961914 shape: (128, 10)\n",
      "epoch: 1 batch 169 loss: 0.05576533 acc: 98.4375\n",
      "training time 0.01704263687133789 all layer time: 0.004251956939697266 shape: (128, 10)\n",
      "epoch: 1 batch 170 loss: 0.053843025 acc: 98.4375\n",
      "training time 0.017337799072265625 all layer time: 0.0042989253997802734 shape: (128, 10)\n",
      "epoch: 1 batch 171 loss: 0.123171635 acc: 96.09375\n",
      "training time 0.017203092575073242 all layer time: 0.004509449005126953 shape: (128, 10)\n",
      "epoch: 1 batch 172 loss: 0.071531095 acc: 97.65625\n",
      "training time 0.01688098907470703 all layer time: 0.004392862319946289 shape: (128, 10)\n",
      "epoch: 1 batch 173 loss: 0.12961388 acc: 97.65625\n",
      "training time 0.017507076263427734 all layer time: 0.0041866302490234375 shape: (128, 10)\n",
      "epoch: 1 batch 174 loss: 0.040188167 acc: 99.21875\n",
      "training time 0.017241477966308594 all layer time: 0.004198312759399414 shape: (128, 10)\n",
      "epoch: 1 batch 175 loss: 0.11346925 acc: 96.09375\n",
      "training time 0.01735210418701172 all layer time: 0.00415802001953125 shape: (128, 10)\n",
      "epoch: 1 batch 176 loss: 0.17620668 acc: 94.53125\n",
      "training time 0.01693415641784668 all layer time: 0.004430294036865234 shape: (128, 10)\n",
      "epoch: 1 batch 177 loss: 0.107622 acc: 96.875\n",
      "training time 0.01687908172607422 all layer time: 0.0042111873626708984 shape: (128, 10)\n",
      "epoch: 1 batch 178 loss: 0.038806714 acc: 98.4375\n",
      "training time 0.017185688018798828 all layer time: 0.004334926605224609 shape: (128, 10)\n",
      "epoch: 1 batch 179 loss: 0.038137913 acc: 99.21875\n",
      "training time 0.016968488693237305 all layer time: 0.004240751266479492 shape: (128, 10)\n",
      "epoch: 1 batch 180 loss: 0.07878865 acc: 96.875\n",
      "training time 0.017156124114990234 all layer time: 0.004264354705810547 shape: (128, 10)\n",
      "epoch: 1 batch 181 loss: 0.036940288 acc: 99.21875\n",
      "training time 0.01709771156311035 all layer time: 0.004414796829223633 shape: (128, 10)\n",
      "epoch: 1 batch 182 loss: 0.08501422 acc: 97.65625\n",
      "training time 0.017417192459106445 all layer time: 0.004390716552734375 shape: (128, 10)\n",
      "epoch: 1 batch 183 loss: 0.08080266 acc: 98.4375\n",
      "training time 0.01764512062072754 all layer time: 0.004275798797607422 shape: (128, 10)\n",
      "epoch: 1 batch 184 loss: 0.08355065 acc: 97.65625\n",
      "training time 0.016973018646240234 all layer time: 0.004266262054443359 shape: (128, 10)\n",
      "epoch: 1 batch 185 loss: 0.10440502 acc: 95.3125\n",
      "training time 0.017172574996948242 all layer time: 0.0043337345123291016 shape: (128, 10)\n",
      "epoch: 1 batch 186 loss: 0.17180112 acc: 95.3125\n",
      "training time 0.017070531845092773 all layer time: 0.004221439361572266 shape: (128, 10)\n",
      "epoch: 1 batch 187 loss: 0.09148243 acc: 97.65625\n",
      "training time 0.018039703369140625 all layer time: 0.004247903823852539 shape: (128, 10)\n",
      "epoch: 1 batch 188 loss: 0.06818347 acc: 98.4375\n",
      "training time 0.0168306827545166 all layer time: 0.004241466522216797 shape: (128, 10)\n",
      "epoch: 1 batch 189 loss: 0.026502475 acc: 99.21875\n",
      "training time 0.017305374145507812 all layer time: 0.004321575164794922 shape: (128, 10)\n",
      "epoch: 1 batch 190 loss: 0.027233094 acc: 99.21875\n",
      "training time 0.017439842224121094 all layer time: 0.004343748092651367 shape: (128, 10)\n",
      "epoch: 1 batch 191 loss: 0.047334347 acc: 98.4375\n",
      "training time 0.018105506896972656 all layer time: 0.004337787628173828 shape: (128, 10)\n",
      "epoch: 1 batch 192 loss: 0.09514072 acc: 97.65625\n",
      "training time 0.017403602600097656 all layer time: 0.004173755645751953 shape: (128, 10)\n",
      "epoch: 1 batch 193 loss: 0.13875842 acc: 98.4375\n",
      "training time 0.017279624938964844 all layer time: 0.004203081130981445 shape: (128, 10)\n",
      "epoch: 1 batch 194 loss: 0.08921461 acc: 96.875\n",
      "training time 0.01746058464050293 all layer time: 0.004303932189941406 shape: (128, 10)\n",
      "epoch: 1 batch 195 loss: 0.01013958 acc: 100.0\n",
      "training time 0.017427444458007812 all layer time: 0.0043184757232666016 shape: (128, 10)\n",
      "epoch: 1 batch 196 loss: 0.06981567 acc: 96.875\n",
      "training time 0.017124414443969727 all layer time: 0.004257678985595703 shape: (128, 10)\n",
      "epoch: 1 batch 197 loss: 0.0441743 acc: 98.4375\n",
      "training time 0.01728034019470215 all layer time: 0.004148960113525391 shape: (128, 10)\n",
      "epoch: 1 batch 198 loss: 0.016161488 acc: 100.0\n",
      "training time 0.017724990844726562 all layer time: 0.004195451736450195 shape: (128, 10)\n",
      "epoch: 1 batch 199 loss: 0.13354436 acc: 98.4375\n",
      "training time 0.018502473831176758 all layer time: 0.004156827926635742 shape: (128, 10)\n",
      "epoch: 1 batch 200 loss: 0.10909583 acc: 98.4375\n",
      "training time 0.01838374137878418 all layer time: 0.004301786422729492 shape: (128, 10)\n",
      "epoch: 1 batch 201 loss: 0.13409045 acc: 97.65625\n",
      "training time 0.016949176788330078 all layer time: 0.004181861877441406 shape: (128, 10)\n",
      "epoch: 1 batch 202 loss: 0.037122242 acc: 99.21875\n",
      "training time 0.016431093215942383 all layer time: 0.004236698150634766 shape: (128, 10)\n",
      "epoch: 1 batch 203 loss: 0.07907146 acc: 98.4375\n",
      "training time 0.017163515090942383 all layer time: 0.00443267822265625 shape: (128, 10)\n",
      "epoch: 1 batch 204 loss: 0.021157824 acc: 100.0\n",
      "training time 0.016885995864868164 all layer time: 0.004342079162597656 shape: (128, 10)\n",
      "epoch: 1 batch 205 loss: 0.07207066 acc: 99.21875\n",
      "training time 0.017173051834106445 all layer time: 0.004361391067504883 shape: (128, 10)\n",
      "epoch: 1 batch 206 loss: 0.19779217 acc: 93.75\n",
      "training time 0.016869068145751953 all layer time: 0.0042607784271240234 shape: (128, 10)\n",
      "epoch: 1 batch 207 loss: 0.15842342 acc: 96.875\n",
      "training time 0.01660895347595215 all layer time: 0.00424504280090332 shape: (128, 10)\n",
      "epoch: 1 batch 208 loss: 0.25353304 acc: 94.53125\n",
      "training time 0.017060279846191406 all layer time: 0.004411220550537109 shape: (128, 10)\n",
      "epoch: 1 batch 209 loss: 0.11284363 acc: 97.65625\n",
      "training time 0.016982316970825195 all layer time: 0.004322052001953125 shape: (128, 10)\n",
      "epoch: 1 batch 210 loss: 0.13234477 acc: 98.4375\n",
      "training time 0.0169830322265625 all layer time: 0.004313230514526367 shape: (128, 10)\n",
      "epoch: 1 batch 211 loss: 0.050857186 acc: 98.4375\n",
      "training time 0.016837596893310547 all layer time: 0.0042705535888671875 shape: (128, 10)\n",
      "epoch: 1 batch 212 loss: 0.18386078 acc: 94.53125\n",
      "training time 0.017087459564208984 all layer time: 0.004367828369140625 shape: (128, 10)\n",
      "epoch: 1 batch 213 loss: 0.050456475 acc: 97.65625\n",
      "training time 0.01787400245666504 all layer time: 0.004199504852294922 shape: (128, 10)\n",
      "epoch: 1 batch 214 loss: 0.0637216 acc: 97.65625\n",
      "training time 0.016913414001464844 all layer time: 0.004206657409667969 shape: (128, 10)\n",
      "epoch: 1 batch 215 loss: 0.06001292 acc: 96.875\n",
      "training time 0.01723003387451172 all layer time: 0.004149198532104492 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 216 loss: 0.055107333 acc: 98.4375\n",
      "training time 0.017168521881103516 all layer time: 0.004160881042480469 shape: (128, 10)\n",
      "epoch: 1 batch 217 loss: 0.11066229 acc: 96.09375\n",
      "training time 0.017650604248046875 all layer time: 0.004482746124267578 shape: (128, 10)\n",
      "epoch: 1 batch 218 loss: 0.026960364 acc: 99.21875\n",
      "training time 0.017373085021972656 all layer time: 0.00420832633972168 shape: (128, 10)\n",
      "epoch: 1 batch 219 loss: 0.079059154 acc: 96.875\n",
      "training time 0.01755809783935547 all layer time: 0.004252910614013672 shape: (128, 10)\n",
      "epoch: 1 batch 220 loss: 0.04659725 acc: 98.4375\n",
      "training time 0.017252206802368164 all layer time: 0.004122495651245117 shape: (128, 10)\n",
      "epoch: 1 batch 221 loss: 0.08600258 acc: 98.4375\n",
      "training time 0.017146825790405273 all layer time: 0.004146575927734375 shape: (128, 10)\n",
      "epoch: 1 batch 222 loss: 0.04902181 acc: 99.21875\n",
      "training time 0.017831802368164062 all layer time: 0.004216432571411133 shape: (128, 10)\n",
      "epoch: 1 batch 223 loss: 0.21570733 acc: 94.53125\n",
      "training time 0.017848968505859375 all layer time: 0.0042095184326171875 shape: (128, 10)\n",
      "epoch: 1 batch 224 loss: 0.1699352 acc: 96.875\n",
      "training time 0.016804933547973633 all layer time: 0.004195690155029297 shape: (128, 10)\n",
      "epoch: 1 batch 225 loss: 0.05692087 acc: 99.21875\n",
      "training time 0.01716446876525879 all layer time: 0.004314899444580078 shape: (128, 10)\n",
      "epoch: 1 batch 226 loss: 0.064926386 acc: 98.4375\n",
      "training time 0.017283201217651367 all layer time: 0.004135847091674805 shape: (128, 10)\n",
      "epoch: 1 batch 227 loss: 0.0861598 acc: 97.65625\n",
      "training time 0.01736164093017578 all layer time: 0.004174709320068359 shape: (128, 10)\n",
      "epoch: 1 batch 228 loss: 0.07223953 acc: 97.65625\n",
      "training time 0.017231464385986328 all layer time: 0.004369497299194336 shape: (128, 10)\n",
      "epoch: 1 batch 229 loss: 0.061787937 acc: 98.4375\n",
      "training time 0.017642974853515625 all layer time: 0.0043163299560546875 shape: (128, 10)\n",
      "epoch: 1 batch 230 loss: 0.03444711 acc: 97.65625\n",
      "training time 0.017580509185791016 all layer time: 0.004347801208496094 shape: (128, 10)\n",
      "epoch: 1 batch 231 loss: 0.08250175 acc: 99.21875\n",
      "training time 0.017256498336791992 all layer time: 0.004224061965942383 shape: (128, 10)\n",
      "epoch: 1 batch 232 loss: 0.07955689 acc: 96.875\n",
      "training time 0.016885757446289062 all layer time: 0.004152774810791016 shape: (128, 10)\n",
      "epoch: 1 batch 233 loss: 0.07901804 acc: 97.65625\n",
      "training time 0.017610788345336914 all layer time: 0.0042383670806884766 shape: (128, 10)\n",
      "epoch: 1 batch 234 loss: 0.080027804 acc: 97.65625\n",
      "training time 0.01717090606689453 all layer time: 0.004403352737426758 shape: (128, 10)\n",
      "epoch: 1 batch 235 loss: 0.0641271 acc: 98.4375\n",
      "training time 0.017330169677734375 all layer time: 0.004313945770263672 shape: (128, 10)\n",
      "epoch: 1 batch 236 loss: 0.04602439 acc: 97.65625\n",
      "training time 0.0169675350189209 all layer time: 0.004397869110107422 shape: (128, 10)\n",
      "epoch: 1 batch 237 loss: 0.052264806 acc: 96.875\n",
      "training time 0.017259597778320312 all layer time: 0.004317760467529297 shape: (128, 10)\n",
      "epoch: 1 batch 238 loss: 0.09860805 acc: 96.09375\n",
      "training time 0.017035245895385742 all layer time: 0.004378795623779297 shape: (128, 10)\n",
      "epoch: 1 batch 239 loss: 0.10973224 acc: 98.4375\n",
      "training time 0.018019437789916992 all layer time: 0.0042612552642822266 shape: (128, 10)\n",
      "epoch: 1 batch 240 loss: 0.07902953 acc: 96.09375\n",
      "training time 0.017249345779418945 all layer time: 0.004339933395385742 shape: (128, 10)\n",
      "epoch: 1 batch 241 loss: 0.09223673 acc: 98.4375\n",
      "training time 0.01651287078857422 all layer time: 0.004265785217285156 shape: (128, 10)\n",
      "epoch: 1 batch 242 loss: 0.0341946 acc: 98.4375\n",
      "training time 0.01715397834777832 all layer time: 0.00433802604675293 shape: (128, 10)\n",
      "epoch: 1 batch 243 loss: 0.18430832 acc: 96.09375\n",
      "training time 0.017705917358398438 all layer time: 0.004302024841308594 shape: (128, 10)\n",
      "epoch: 1 batch 244 loss: 0.24919969 acc: 93.75\n",
      "training time 0.017894268035888672 all layer time: 0.004315853118896484 shape: (128, 10)\n",
      "epoch: 1 batch 245 loss: 0.09155064 acc: 98.4375\n",
      "training time 0.01702117919921875 all layer time: 0.004282474517822266 shape: (128, 10)\n",
      "epoch: 1 batch 246 loss: 0.100383125 acc: 96.09375\n",
      "training time 0.017237424850463867 all layer time: 0.004343271255493164 shape: (128, 10)\n",
      "epoch: 1 batch 247 loss: 0.18373296 acc: 96.09375\n",
      "training time 0.016823530197143555 all layer time: 0.004159688949584961 shape: (128, 10)\n",
      "epoch: 1 batch 248 loss: 0.14215729 acc: 95.3125\n",
      "training time 0.01771688461303711 all layer time: 0.004234790802001953 shape: (128, 10)\n",
      "epoch: 1 batch 249 loss: 0.06499081 acc: 99.21875\n",
      "training time 0.016760826110839844 all layer time: 0.004122018814086914 shape: (128, 10)\n",
      "epoch: 1 batch 250 loss: 0.07612882 acc: 97.65625\n",
      "training time 0.017040014266967773 all layer time: 0.004308462142944336 shape: (128, 10)\n",
      "epoch: 1 batch 251 loss: 0.058626037 acc: 98.4375\n",
      "training time 0.01700305938720703 all layer time: 0.004347085952758789 shape: (128, 10)\n",
      "epoch: 1 batch 252 loss: 0.13018319 acc: 94.53125\n",
      "training time 0.01766514778137207 all layer time: 0.0043218135833740234 shape: (128, 10)\n",
      "epoch: 1 batch 253 loss: 0.13166739 acc: 96.875\n",
      "training time 0.01758289337158203 all layer time: 0.004340171813964844 shape: (128, 10)\n",
      "epoch: 1 batch 254 loss: 0.035571307 acc: 100.0\n",
      "training time 0.01762104034423828 all layer time: 0.004224061965942383 shape: (128, 10)\n",
      "epoch: 1 batch 255 loss: 0.062334627 acc: 97.65625\n",
      "training time 0.017499446868896484 all layer time: 0.0041539669036865234 shape: (128, 10)\n",
      "epoch: 1 batch 256 loss: 0.15604067 acc: 96.09375\n",
      "training time 0.016991376876831055 all layer time: 0.0044307708740234375 shape: (128, 10)\n",
      "epoch: 1 batch 257 loss: 0.051142856 acc: 99.21875\n",
      "training time 0.017809629440307617 all layer time: 0.0043354034423828125 shape: (128, 10)\n",
      "epoch: 1 batch 258 loss: 0.07367928 acc: 96.875\n",
      "training time 0.01696014404296875 all layer time: 0.004418611526489258 shape: (128, 10)\n",
      "epoch: 1 batch 259 loss: 0.0685999 acc: 98.4375\n",
      "training time 0.016762495040893555 all layer time: 0.004334211349487305 shape: (128, 10)\n",
      "epoch: 1 batch 260 loss: 0.14018123 acc: 95.3125\n",
      "training time 0.017311811447143555 all layer time: 0.004606485366821289 shape: (128, 10)\n",
      "epoch: 1 batch 261 loss: 0.06258198 acc: 97.65625\n",
      "training time 0.01755833625793457 all layer time: 0.004189014434814453 shape: (128, 10)\n",
      "epoch: 1 batch 262 loss: 0.07323892 acc: 98.4375\n",
      "training time 0.016932964324951172 all layer time: 0.004162788391113281 shape: (128, 10)\n",
      "epoch: 1 batch 263 loss: 0.10693833 acc: 96.875\n",
      "training time 0.017308473587036133 all layer time: 0.0043201446533203125 shape: (128, 10)\n",
      "epoch: 1 batch 264 loss: 0.048653126 acc: 98.4375\n",
      "training time 0.017312288284301758 all layer time: 0.004220485687255859 shape: (128, 10)\n",
      "epoch: 1 batch 265 loss: 0.035862952 acc: 99.21875\n",
      "training time 0.017889022827148438 all layer time: 0.004106760025024414 shape: (128, 10)\n",
      "epoch: 1 batch 266 loss: 0.04191035 acc: 99.21875\n",
      "training time 0.017225980758666992 all layer time: 0.004328250885009766 shape: (128, 10)\n",
      "epoch: 1 batch 267 loss: 0.022686182 acc: 99.21875\n",
      "training time 0.017380952835083008 all layer time: 0.0043125152587890625 shape: (128, 10)\n",
      "epoch: 1 batch 268 loss: 0.10604119 acc: 96.875\n",
      "training time 0.017376184463500977 all layer time: 0.004162311553955078 shape: (128, 10)\n",
      "epoch: 1 batch 269 loss: 0.04217731 acc: 98.4375\n",
      "training time 0.01720905303955078 all layer time: 0.004347562789916992 shape: (128, 10)\n",
      "epoch: 1 batch 270 loss: 0.12145032 acc: 96.875\n",
      "training time 0.017173051834106445 all layer time: 0.004351615905761719 shape: (128, 10)\n",
      "epoch: 1 batch 271 loss: 0.10942099 acc: 96.09375\n",
      "training time 0.017074108123779297 all layer time: 0.004262208938598633 shape: (128, 10)\n",
      "epoch: 1 batch 272 loss: 0.16325322 acc: 93.75\n",
      "training time 0.016915559768676758 all layer time: 0.0042302608489990234 shape: (128, 10)\n",
      "epoch: 1 batch 273 loss: 0.063517556 acc: 99.21875\n",
      "training time 0.017170429229736328 all layer time: 0.004334926605224609 shape: (128, 10)\n",
      "epoch: 1 batch 274 loss: 0.02837649 acc: 99.21875\n",
      "training time 0.0174715518951416 all layer time: 0.0044100284576416016 shape: (128, 10)\n",
      "epoch: 1 batch 275 loss: 0.10048225 acc: 96.875\n",
      "training time 0.017867565155029297 all layer time: 0.004246950149536133 shape: (128, 10)\n",
      "epoch: 1 batch 276 loss: 0.06907527 acc: 97.65625\n",
      "training time 0.017080068588256836 all layer time: 0.0041980743408203125 shape: (128, 10)\n",
      "epoch: 1 batch 277 loss: 0.13306773 acc: 98.4375\n",
      "training time 0.017429590225219727 all layer time: 0.004233837127685547 shape: (128, 10)\n",
      "epoch: 1 batch 278 loss: 0.12716569 acc: 95.3125\n",
      "training time 0.01725459098815918 all layer time: 0.004185914993286133 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 279 loss: 0.034761667 acc: 98.4375\n",
      "training time 0.017758846282958984 all layer time: 0.004341840744018555 shape: (128, 10)\n",
      "epoch: 1 batch 280 loss: 0.0912199 acc: 97.65625\n",
      "training time 0.016908645629882812 all layer time: 0.004283905029296875 shape: (128, 10)\n",
      "epoch: 1 batch 281 loss: 0.085778594 acc: 96.875\n",
      "training time 0.017270565032958984 all layer time: 0.004230976104736328 shape: (128, 10)\n",
      "epoch: 1 batch 282 loss: 0.084427126 acc: 97.65625\n",
      "training time 0.016811609268188477 all layer time: 0.004418373107910156 shape: (128, 10)\n",
      "epoch: 1 batch 283 loss: 0.088392615 acc: 98.4375\n",
      "training time 0.017014741897583008 all layer time: 0.004199981689453125 shape: (128, 10)\n",
      "epoch: 1 batch 284 loss: 0.057121485 acc: 97.65625\n",
      "training time 0.016934871673583984 all layer time: 0.0042514801025390625 shape: (128, 10)\n",
      "epoch: 1 batch 285 loss: 0.07049361 acc: 96.09375\n",
      "training time 0.01695847511291504 all layer time: 0.0041561126708984375 shape: (128, 10)\n",
      "epoch: 1 batch 286 loss: 0.05126784 acc: 97.65625\n",
      "training time 0.01738882064819336 all layer time: 0.004271745681762695 shape: (128, 10)\n",
      "epoch: 1 batch 287 loss: 0.10189642 acc: 96.875\n",
      "training time 0.01741957664489746 all layer time: 0.004284381866455078 shape: (128, 10)\n",
      "epoch: 1 batch 288 loss: 0.027297169 acc: 99.21875\n",
      "training time 0.016832590103149414 all layer time: 0.0042302608489990234 shape: (128, 10)\n",
      "epoch: 1 batch 289 loss: 0.15615658 acc: 97.65625\n",
      "training time 0.01709604263305664 all layer time: 0.004253387451171875 shape: (128, 10)\n",
      "epoch: 1 batch 290 loss: 0.09035623 acc: 97.65625\n",
      "training time 0.01761174201965332 all layer time: 0.004416465759277344 shape: (128, 10)\n",
      "epoch: 1 batch 291 loss: 0.10863269 acc: 96.875\n",
      "training time 0.017796993255615234 all layer time: 0.0042607784271240234 shape: (128, 10)\n",
      "epoch: 1 batch 292 loss: 0.09612803 acc: 96.875\n",
      "training time 0.01740550994873047 all layer time: 0.004247903823852539 shape: (128, 10)\n",
      "epoch: 1 batch 293 loss: 0.08201577 acc: 96.875\n",
      "training time 0.017298221588134766 all layer time: 0.004314422607421875 shape: (128, 10)\n",
      "epoch: 1 batch 294 loss: 0.09092261 acc: 93.75\n",
      "training time 0.017413616180419922 all layer time: 0.004167795181274414 shape: (128, 10)\n",
      "epoch: 1 batch 295 loss: 0.17683068 acc: 96.09375\n",
      "training time 0.01704859733581543 all layer time: 0.004485130310058594 shape: (128, 10)\n",
      "epoch: 1 batch 296 loss: 0.055524226 acc: 98.4375\n",
      "training time 0.016657352447509766 all layer time: 0.0042705535888671875 shape: (128, 10)\n",
      "epoch: 1 batch 297 loss: 0.06505392 acc: 96.875\n",
      "training time 0.01743006706237793 all layer time: 0.0041806697845458984 shape: (128, 10)\n",
      "epoch: 1 batch 298 loss: 0.06359357 acc: 98.4375\n",
      "training time 0.01682734489440918 all layer time: 0.004170894622802734 shape: (128, 10)\n",
      "epoch: 1 batch 299 loss: 0.11882056 acc: 96.875\n",
      "training time 0.017160415649414062 all layer time: 0.004116058349609375 shape: (128, 10)\n",
      "epoch: 1 batch 300 loss: 0.118742034 acc: 96.875\n",
      "training time 0.017934560775756836 all layer time: 0.004288196563720703 shape: (128, 10)\n",
      "epoch: 1 batch 301 loss: 0.057657544 acc: 97.65625\n",
      "training time 0.016718626022338867 all layer time: 0.004263401031494141 shape: (128, 10)\n",
      "epoch: 1 batch 302 loss: 0.08729184 acc: 96.09375\n",
      "training time 0.016693115234375 all layer time: 0.0040836334228515625 shape: (128, 10)\n",
      "epoch: 1 batch 303 loss: 0.03742339 acc: 99.21875\n",
      "training time 0.017725467681884766 all layer time: 0.004094600677490234 shape: (128, 10)\n",
      "epoch: 1 batch 304 loss: 0.0650705 acc: 96.875\n",
      "training time 0.017369508743286133 all layer time: 0.00654292106628418 shape: (128, 10)\n",
      "epoch: 1 batch 305 loss: 0.05396271 acc: 97.65625\n",
      "training time 0.016959667205810547 all layer time: 0.004237651824951172 shape: (128, 10)\n",
      "epoch: 1 batch 306 loss: 0.12182966 acc: 97.65625\n",
      "training time 0.017035722732543945 all layer time: 0.004498958587646484 shape: (128, 10)\n",
      "epoch: 1 batch 307 loss: 0.2927848 acc: 89.84375\n",
      "training time 0.016903162002563477 all layer time: 0.004165172576904297 shape: (128, 10)\n",
      "epoch: 1 batch 308 loss: 0.07143271 acc: 99.21875\n",
      "training time 0.01783299446105957 all layer time: 0.004282951354980469 shape: (128, 10)\n",
      "epoch: 1 batch 309 loss: 0.19853422 acc: 95.3125\n",
      "training time 0.01795172691345215 all layer time: 0.004208564758300781 shape: (128, 10)\n",
      "epoch: 1 batch 310 loss: 0.073185444 acc: 97.65625\n",
      "training time 0.01683831214904785 all layer time: 0.004136800765991211 shape: (128, 10)\n",
      "epoch: 1 batch 311 loss: 0.028818365 acc: 100.0\n",
      "training time 0.017066478729248047 all layer time: 0.0042073726654052734 shape: (128, 10)\n",
      "epoch: 1 batch 312 loss: 0.08771776 acc: 98.4375\n",
      "training time 0.016975879669189453 all layer time: 0.004147529602050781 shape: (128, 10)\n",
      "epoch: 1 batch 313 loss: 0.12286648 acc: 98.4375\n",
      "training time 0.01738262176513672 all layer time: 0.004130363464355469 shape: (128, 10)\n",
      "epoch: 1 batch 314 loss: 0.08121523 acc: 97.65625\n",
      "training time 0.016877174377441406 all layer time: 0.00427699089050293 shape: (128, 10)\n",
      "epoch: 1 batch 315 loss: 0.074140675 acc: 97.65625\n",
      "training time 0.01692795753479004 all layer time: 0.004237174987792969 shape: (128, 10)\n",
      "epoch: 1 batch 316 loss: 0.034875114 acc: 99.21875\n",
      "training time 0.01667952537536621 all layer time: 0.0042569637298583984 shape: (128, 10)\n",
      "epoch: 1 batch 317 loss: 0.07274976 acc: 98.4375\n",
      "training time 0.016977548599243164 all layer time: 0.0042607784271240234 shape: (128, 10)\n",
      "epoch: 1 batch 318 loss: 0.10538305 acc: 96.09375\n",
      "training time 0.017312049865722656 all layer time: 0.004286050796508789 shape: (128, 10)\n",
      "epoch: 1 batch 319 loss: 0.040452234 acc: 97.65625\n",
      "training time 0.016888856887817383 all layer time: 0.004212379455566406 shape: (128, 10)\n",
      "epoch: 1 batch 320 loss: 0.06355698 acc: 97.65625\n",
      "training time 0.016614198684692383 all layer time: 0.0042896270751953125 shape: (128, 10)\n",
      "epoch: 1 batch 321 loss: 0.05780638 acc: 98.4375\n",
      "training time 0.017785310745239258 all layer time: 0.0043947696685791016 shape: (128, 10)\n",
      "epoch: 1 batch 322 loss: 0.14109772 acc: 96.875\n",
      "training time 0.018093347549438477 all layer time: 0.0043299198150634766 shape: (128, 10)\n",
      "epoch: 1 batch 323 loss: 0.17219009 acc: 93.75\n",
      "training time 0.016714096069335938 all layer time: 0.004103899002075195 shape: (128, 10)\n",
      "epoch: 1 batch 324 loss: 0.11308819 acc: 95.3125\n",
      "training time 0.016663312911987305 all layer time: 0.004208564758300781 shape: (128, 10)\n",
      "epoch: 1 batch 325 loss: 0.053085305 acc: 96.875\n",
      "training time 0.01731395721435547 all layer time: 0.004401445388793945 shape: (128, 10)\n",
      "epoch: 1 batch 326 loss: 0.033487365 acc: 99.21875\n",
      "training time 0.017541170120239258 all layer time: 0.004431009292602539 shape: (128, 10)\n",
      "epoch: 1 batch 327 loss: 0.084282644 acc: 96.875\n",
      "training time 0.016809940338134766 all layer time: 0.0041086673736572266 shape: (128, 10)\n",
      "epoch: 1 batch 328 loss: 0.121239014 acc: 96.875\n",
      "training time 0.017586946487426758 all layer time: 0.004153728485107422 shape: (128, 10)\n",
      "epoch: 1 batch 329 loss: 0.07664715 acc: 98.4375\n",
      "training time 0.017162561416625977 all layer time: 0.0044286251068115234 shape: (128, 10)\n",
      "epoch: 1 batch 330 loss: 0.11608952 acc: 95.3125\n",
      "training time 0.017296552658081055 all layer time: 0.004142284393310547 shape: (128, 10)\n",
      "epoch: 1 batch 331 loss: 0.079390034 acc: 98.4375\n",
      "training time 0.017479658126831055 all layer time: 0.0040934085845947266 shape: (128, 10)\n",
      "epoch: 1 batch 332 loss: 0.10251223 acc: 98.4375\n",
      "training time 0.017310380935668945 all layer time: 0.004207134246826172 shape: (128, 10)\n",
      "epoch: 1 batch 333 loss: 0.089416325 acc: 96.875\n",
      "training time 0.017042160034179688 all layer time: 0.0042819976806640625 shape: (128, 10)\n",
      "epoch: 1 batch 334 loss: 0.09589851 acc: 98.4375\n",
      "training time 0.01694345474243164 all layer time: 0.004484653472900391 shape: (128, 10)\n",
      "epoch: 1 batch 335 loss: 0.07672647 acc: 97.65625\n",
      "training time 0.01719045639038086 all layer time: 0.004145622253417969 shape: (128, 10)\n",
      "epoch: 1 batch 336 loss: 0.075628445 acc: 97.65625\n",
      "training time 0.016836881637573242 all layer time: 0.004157543182373047 shape: (128, 10)\n",
      "epoch: 1 batch 337 loss: 0.0642845 acc: 96.875\n",
      "training time 0.017394542694091797 all layer time: 0.004068136215209961 shape: (128, 10)\n",
      "epoch: 1 batch 338 loss: 0.017157272 acc: 100.0\n",
      "training time 0.01713275909423828 all layer time: 0.004506826400756836 shape: (128, 10)\n",
      "epoch: 1 batch 339 loss: 0.08760117 acc: 98.4375\n",
      "training time 0.01713728904724121 all layer time: 0.004212379455566406 shape: (128, 10)\n",
      "epoch: 1 batch 340 loss: 0.06755174 acc: 98.4375\n",
      "training time 0.017251968383789062 all layer time: 0.00436711311340332 shape: (128, 10)\n",
      "epoch: 1 batch 341 loss: 0.06880501 acc: 97.65625\n",
      "training time 0.01693272590637207 all layer time: 0.004257678985595703 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 342 loss: 0.058836542 acc: 97.65625\n",
      "training time 0.017056941986083984 all layer time: 0.0042493343353271484 shape: (128, 10)\n",
      "epoch: 1 batch 343 loss: 0.1274204 acc: 97.65625\n",
      "training time 0.017728805541992188 all layer time: 0.00431060791015625 shape: (128, 10)\n",
      "epoch: 1 batch 344 loss: 0.08744755 acc: 96.875\n",
      "training time 0.0174105167388916 all layer time: 0.0043184757232666016 shape: (128, 10)\n",
      "epoch: 1 batch 345 loss: 0.056156322 acc: 97.65625\n",
      "training time 0.016996383666992188 all layer time: 0.004171848297119141 shape: (128, 10)\n",
      "epoch: 1 batch 346 loss: 0.03730616 acc: 99.21875\n",
      "training time 0.016878604888916016 all layer time: 0.004201650619506836 shape: (128, 10)\n",
      "epoch: 1 batch 347 loss: 0.12831469 acc: 96.875\n",
      "training time 0.017518281936645508 all layer time: 0.004324913024902344 shape: (128, 10)\n",
      "epoch: 1 batch 348 loss: 0.019315219 acc: 100.0\n",
      "training time 0.01773667335510254 all layer time: 0.004194498062133789 shape: (128, 10)\n",
      "epoch: 1 batch 349 loss: 0.05301047 acc: 99.21875\n",
      "training time 0.016887903213500977 all layer time: 0.004322528839111328 shape: (128, 10)\n",
      "epoch: 1 batch 350 loss: 0.096549034 acc: 97.65625\n",
      "training time 0.017279624938964844 all layer time: 0.004217624664306641 shape: (128, 10)\n",
      "epoch: 1 batch 351 loss: 0.07172809 acc: 98.4375\n",
      "training time 0.017162799835205078 all layer time: 0.0043849945068359375 shape: (128, 10)\n",
      "epoch: 1 batch 352 loss: 0.051728237 acc: 98.4375\n",
      "training time 0.017989635467529297 all layer time: 0.004267215728759766 shape: (128, 10)\n",
      "epoch: 1 batch 353 loss: 0.051310368 acc: 97.65625\n",
      "training time 0.01729440689086914 all layer time: 0.004218101501464844 shape: (128, 10)\n",
      "epoch: 1 batch 354 loss: 0.0638736 acc: 97.65625\n",
      "training time 0.017063379287719727 all layer time: 0.004256248474121094 shape: (128, 10)\n",
      "epoch: 1 batch 355 loss: 0.06322938 acc: 98.4375\n",
      "training time 0.01706242561340332 all layer time: 0.004374980926513672 shape: (128, 10)\n",
      "epoch: 1 batch 356 loss: 0.06705807 acc: 98.4375\n",
      "training time 0.018415451049804688 all layer time: 0.004368305206298828 shape: (128, 10)\n",
      "epoch: 1 batch 357 loss: 0.14635387 acc: 96.875\n",
      "training time 0.01711249351501465 all layer time: 0.0042874813079833984 shape: (128, 10)\n",
      "epoch: 1 batch 358 loss: 0.17063843 acc: 96.09375\n",
      "training time 0.016890287399291992 all layer time: 0.004417896270751953 shape: (128, 10)\n",
      "epoch: 1 batch 359 loss: 0.09742038 acc: 96.875\n",
      "training time 0.016973495483398438 all layer time: 0.004328012466430664 shape: (128, 10)\n",
      "epoch: 1 batch 360 loss: 0.16938101 acc: 96.09375\n",
      "training time 0.01710033416748047 all layer time: 0.004373073577880859 shape: (128, 10)\n",
      "epoch: 1 batch 361 loss: 0.12750785 acc: 94.53125\n",
      "training time 0.01702570915222168 all layer time: 0.0042726993560791016 shape: (128, 10)\n",
      "epoch: 1 batch 362 loss: 0.15616587 acc: 95.3125\n",
      "training time 0.016879558563232422 all layer time: 0.004273891448974609 shape: (128, 10)\n",
      "epoch: 1 batch 363 loss: 0.06830719 acc: 97.65625\n",
      "training time 0.016986846923828125 all layer time: 0.004233121871948242 shape: (128, 10)\n",
      "epoch: 1 batch 364 loss: 0.03713657 acc: 100.0\n",
      "training time 0.017193078994750977 all layer time: 0.004398345947265625 shape: (128, 10)\n",
      "epoch: 1 batch 365 loss: 0.11631121 acc: 96.09375\n",
      "training time 0.01706552505493164 all layer time: 0.0044209957122802734 shape: (128, 10)\n",
      "epoch: 1 batch 366 loss: 0.06118838 acc: 97.65625\n",
      "training time 0.016637086868286133 all layer time: 0.004264354705810547 shape: (128, 10)\n",
      "epoch: 1 batch 367 loss: 0.15098336 acc: 94.53125\n",
      "training time 0.016939640045166016 all layer time: 0.004235267639160156 shape: (128, 10)\n",
      "epoch: 1 batch 368 loss: 0.11132551 acc: 96.09375\n",
      "training time 0.01695728302001953 all layer time: 0.004239559173583984 shape: (128, 10)\n",
      "epoch: 1 batch 369 loss: 0.1482149 acc: 96.875\n",
      "training time 0.017679691314697266 all layer time: 0.004307985305786133 shape: (128, 10)\n",
      "epoch: 1 batch 370 loss: 0.12774596 acc: 96.09375\n",
      "training time 0.017007112503051758 all layer time: 0.004342317581176758 shape: (128, 10)\n",
      "epoch: 1 batch 371 loss: 0.17428541 acc: 95.3125\n",
      "training time 0.01720428466796875 all layer time: 0.004281044006347656 shape: (128, 10)\n",
      "epoch: 1 batch 372 loss: 0.0840513 acc: 97.65625\n",
      "training time 0.016946792602539062 all layer time: 0.004466533660888672 shape: (128, 10)\n",
      "epoch: 1 batch 373 loss: 0.07874956 acc: 98.4375\n",
      "training time 0.017560243606567383 all layer time: 0.004333019256591797 shape: (128, 10)\n",
      "epoch: 1 batch 374 loss: 0.11393297 acc: 96.09375\n",
      "training time 0.017764568328857422 all layer time: 0.004223346710205078 shape: (128, 10)\n",
      "epoch: 1 batch 375 loss: 0.070738554 acc: 96.875\n",
      "training time 0.016807079315185547 all layer time: 0.0042765140533447266 shape: (128, 10)\n",
      "epoch: 1 batch 376 loss: 0.017513787 acc: 100.0\n",
      "training time 0.016566038131713867 all layer time: 0.004515409469604492 shape: (128, 10)\n",
      "epoch: 1 batch 377 loss: 0.1161937 acc: 96.875\n",
      "training time 0.017151832580566406 all layer time: 0.0042879581451416016 shape: (128, 10)\n",
      "epoch: 1 batch 378 loss: 0.02407926 acc: 99.21875\n",
      "training time 0.017554044723510742 all layer time: 0.004399299621582031 shape: (128, 10)\n",
      "epoch: 1 batch 379 loss: 0.066945374 acc: 99.21875\n",
      "training time 0.016879558563232422 all layer time: 0.004263639450073242 shape: (128, 10)\n",
      "epoch: 1 batch 380 loss: 0.04125123 acc: 98.4375\n",
      "training time 0.01681804656982422 all layer time: 0.004167079925537109 shape: (128, 10)\n",
      "epoch: 1 batch 381 loss: 0.018255081 acc: 100.0\n",
      "training time 0.016861915588378906 all layer time: 0.0041675567626953125 shape: (128, 10)\n",
      "epoch: 1 batch 382 loss: 0.22316697 acc: 92.1875\n",
      "training time 0.016713619232177734 all layer time: 0.0042667388916015625 shape: (128, 10)\n",
      "epoch: 1 batch 383 loss: 0.09876631 acc: 96.09375\n",
      "training time 0.01754903793334961 all layer time: 0.004369258880615234 shape: (128, 10)\n",
      "epoch: 1 batch 384 loss: 0.066351496 acc: 97.65625\n",
      "training time 0.017137527465820312 all layer time: 0.004449605941772461 shape: (128, 10)\n",
      "epoch: 1 batch 385 loss: 0.025011176 acc: 100.0\n",
      "training time 0.017361879348754883 all layer time: 0.00426936149597168 shape: (128, 10)\n",
      "epoch: 1 batch 386 loss: 0.09990926 acc: 96.09375\n",
      "training time 0.01736283302307129 all layer time: 0.004427909851074219 shape: (128, 10)\n",
      "epoch: 1 batch 387 loss: 0.17341186 acc: 92.96875\n",
      "training time 0.017713069915771484 all layer time: 0.004402875900268555 shape: (128, 10)\n",
      "epoch: 1 batch 388 loss: 0.042639036 acc: 98.4375\n",
      "training time 0.017516613006591797 all layer time: 0.004206418991088867 shape: (128, 10)\n",
      "epoch: 1 batch 389 loss: 0.154148 acc: 96.09375\n",
      "training time 0.01716327667236328 all layer time: 0.0040853023529052734 shape: (128, 10)\n",
      "epoch: 1 batch 390 loss: 0.038733833 acc: 98.4375\n",
      "training time 0.01673436164855957 all layer time: 0.004351615905761719 shape: (128, 10)\n",
      "epoch: 1 batch 391 loss: 0.04539506 acc: 99.21875\n",
      "training time 0.017626285552978516 all layer time: 0.004662036895751953 shape: (128, 10)\n",
      "epoch: 1 batch 392 loss: 0.09775773 acc: 98.4375\n",
      "training time 0.017338275909423828 all layer time: 0.004572391510009766 shape: (128, 10)\n",
      "epoch: 1 batch 393 loss: 0.1510721 acc: 95.3125\n",
      "training time 0.016963958740234375 all layer time: 0.00429081916809082 shape: (128, 10)\n",
      "epoch: 1 batch 394 loss: 0.06874611 acc: 99.21875\n",
      "training time 0.016832828521728516 all layer time: 0.004194736480712891 shape: (128, 10)\n",
      "epoch: 1 batch 395 loss: 0.04722901 acc: 98.4375\n",
      "training time 0.01677417755126953 all layer time: 0.004219770431518555 shape: (128, 10)\n",
      "epoch: 1 batch 396 loss: 0.076513365 acc: 97.65625\n",
      "training time 0.01723766326904297 all layer time: 0.00412750244140625 shape: (128, 10)\n",
      "epoch: 1 batch 397 loss: 0.058176734 acc: 98.4375\n",
      "training time 0.01711559295654297 all layer time: 0.004416227340698242 shape: (128, 10)\n",
      "epoch: 1 batch 398 loss: 0.016831484 acc: 99.21875\n",
      "training time 0.01689887046813965 all layer time: 0.004190921783447266 shape: (128, 10)\n",
      "epoch: 1 batch 399 loss: 0.04514364 acc: 98.4375\n",
      "training time 0.016921520233154297 all layer time: 0.00449681282043457 shape: (128, 10)\n",
      "epoch: 1 batch 400 loss: 0.102415144 acc: 96.09375\n",
      "training time 0.01747918128967285 all layer time: 0.004517555236816406 shape: (128, 10)\n",
      "epoch: 1 batch 401 loss: 0.01618265 acc: 99.21875\n",
      "training time 0.01705765724182129 all layer time: 0.0041959285736083984 shape: (128, 10)\n",
      "epoch: 1 batch 402 loss: 0.06683279 acc: 97.65625\n",
      "training time 0.01675724983215332 all layer time: 0.004309654235839844 shape: (128, 10)\n",
      "epoch: 1 batch 403 loss: 0.04390386 acc: 98.4375\n",
      "training time 0.017655372619628906 all layer time: 0.0047054290771484375 shape: (128, 10)\n",
      "epoch: 1 batch 404 loss: 0.048992608 acc: 99.21875\n",
      "training time 0.01812434196472168 all layer time: 0.00420379638671875 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 405 loss: 0.08048362 acc: 98.4375\n",
      "training time 0.016746044158935547 all layer time: 0.004381895065307617 shape: (128, 10)\n",
      "epoch: 1 batch 406 loss: 0.11351344 acc: 96.09375\n",
      "training time 0.016879558563232422 all layer time: 0.004281282424926758 shape: (128, 10)\n",
      "epoch: 1 batch 407 loss: 0.12521225 acc: 95.3125\n",
      "training time 0.0170590877532959 all layer time: 0.0044345855712890625 shape: (128, 10)\n",
      "epoch: 1 batch 408 loss: 0.1536176 acc: 97.65625\n",
      "training time 0.016535043716430664 all layer time: 0.004156589508056641 shape: (128, 10)\n",
      "epoch: 1 batch 409 loss: 0.020713443 acc: 99.21875\n",
      "training time 0.017499685287475586 all layer time: 0.004206657409667969 shape: (128, 10)\n",
      "epoch: 1 batch 410 loss: 0.032138076 acc: 98.4375\n",
      "training time 0.016937732696533203 all layer time: 0.004307270050048828 shape: (128, 10)\n",
      "epoch: 1 batch 411 loss: 0.09194887 acc: 97.65625\n",
      "training time 0.016815662384033203 all layer time: 0.004345893859863281 shape: (128, 10)\n",
      "epoch: 1 batch 412 loss: 0.082017094 acc: 97.65625\n",
      "training time 0.017421722412109375 all layer time: 0.004300594329833984 shape: (128, 10)\n",
      "epoch: 1 batch 413 loss: 0.2541035 acc: 95.3125\n",
      "training time 0.01743292808532715 all layer time: 0.00428318977355957 shape: (128, 10)\n",
      "epoch: 1 batch 414 loss: 0.027836055 acc: 99.21875\n",
      "training time 0.017035961151123047 all layer time: 0.0042765140533447266 shape: (128, 10)\n",
      "epoch: 1 batch 415 loss: 0.12670194 acc: 98.4375\n",
      "training time 0.017203807830810547 all layer time: 0.004242420196533203 shape: (128, 10)\n",
      "epoch: 1 batch 416 loss: 0.02970629 acc: 99.21875\n",
      "training time 0.01749444007873535 all layer time: 0.004386425018310547 shape: (128, 10)\n",
      "epoch: 1 batch 417 loss: 0.0779962 acc: 97.65625\n",
      "training time 0.016832828521728516 all layer time: 0.0042591094970703125 shape: (128, 10)\n",
      "epoch: 1 batch 418 loss: 0.06960229 acc: 98.4375\n",
      "training time 0.017119884490966797 all layer time: 0.004164218902587891 shape: (128, 10)\n",
      "epoch: 1 batch 419 loss: 0.08444537 acc: 97.65625\n",
      "training time 0.016961097717285156 all layer time: 0.004128694534301758 shape: (128, 10)\n",
      "epoch: 1 batch 420 loss: 0.16256431 acc: 97.65625\n",
      "training time 0.017153501510620117 all layer time: 0.0042760372161865234 shape: (128, 10)\n",
      "epoch: 1 batch 421 loss: 0.115176335 acc: 96.875\n",
      "training time 0.01757502555847168 all layer time: 0.004325389862060547 shape: (128, 10)\n",
      "epoch: 1 batch 422 loss: 0.064847074 acc: 97.65625\n",
      "training time 0.017222166061401367 all layer time: 0.004204750061035156 shape: (128, 10)\n",
      "epoch: 1 batch 423 loss: 0.035511244 acc: 99.21875\n",
      "training time 0.0173337459564209 all layer time: 0.0043332576751708984 shape: (128, 10)\n",
      "epoch: 1 batch 424 loss: 0.038608667 acc: 99.21875\n",
      "training time 0.01689910888671875 all layer time: 0.004304647445678711 shape: (128, 10)\n",
      "epoch: 1 batch 425 loss: 0.04852938 acc: 98.4375\n",
      "training time 0.017473697662353516 all layer time: 0.004317283630371094 shape: (128, 10)\n",
      "epoch: 1 batch 426 loss: 0.053986624 acc: 98.4375\n",
      "training time 0.017317533493041992 all layer time: 0.004345893859863281 shape: (128, 10)\n",
      "epoch: 1 batch 427 loss: 0.036727443 acc: 99.21875\n",
      "training time 0.017017364501953125 all layer time: 0.00415492057800293 shape: (128, 10)\n",
      "epoch: 1 batch 428 loss: 0.07066056 acc: 97.65625\n",
      "training time 0.016760587692260742 all layer time: 0.0042552947998046875 shape: (128, 10)\n",
      "epoch: 1 batch 429 loss: 0.120026164 acc: 95.3125\n",
      "training time 0.016880273818969727 all layer time: 0.004250288009643555 shape: (128, 10)\n",
      "epoch: 1 batch 430 loss: 0.022266848 acc: 99.21875\n",
      "training time 0.01688671112060547 all layer time: 0.004322528839111328 shape: (128, 10)\n",
      "epoch: 1 batch 431 loss: 0.022478405 acc: 100.0\n",
      "training time 0.017392635345458984 all layer time: 0.004204988479614258 shape: (128, 10)\n",
      "epoch: 1 batch 432 loss: 0.06899911 acc: 98.4375\n",
      "training time 0.016826391220092773 all layer time: 0.004333972930908203 shape: (128, 10)\n",
      "epoch: 1 batch 433 loss: 0.052535467 acc: 97.65625\n",
      "training time 0.017119407653808594 all layer time: 0.004241943359375 shape: (128, 10)\n",
      "epoch: 1 batch 434 loss: 0.047848925 acc: 99.21875\n",
      "training time 0.017452716827392578 all layer time: 0.00464320182800293 shape: (128, 10)\n",
      "epoch: 1 batch 435 loss: 0.10790316 acc: 95.3125\n",
      "training time 0.016905546188354492 all layer time: 0.004294395446777344 shape: (128, 10)\n",
      "epoch: 1 batch 436 loss: 0.0756236 acc: 96.875\n",
      "training time 0.017554759979248047 all layer time: 0.004415988922119141 shape: (128, 10)\n",
      "epoch: 1 batch 437 loss: 0.03186551 acc: 98.4375\n",
      "training time 0.017578840255737305 all layer time: 0.004301309585571289 shape: (128, 10)\n",
      "epoch: 1 batch 438 loss: 0.06147402 acc: 98.4375\n",
      "training time 0.016873836517333984 all layer time: 0.00420689582824707 shape: (128, 10)\n",
      "epoch: 1 batch 439 loss: 0.07744623 acc: 96.875\n",
      "training time 0.01774764060974121 all layer time: 0.004183530807495117 shape: (128, 10)\n",
      "epoch: 1 batch 440 loss: 0.025897156 acc: 99.21875\n",
      "training time 0.016847610473632812 all layer time: 0.0041577816009521484 shape: (128, 10)\n",
      "epoch: 1 batch 441 loss: 0.09047604 acc: 97.65625\n",
      "training time 0.01681232452392578 all layer time: 0.0042150020599365234 shape: (128, 10)\n",
      "epoch: 1 batch 442 loss: 0.06665418 acc: 96.875\n",
      "training time 0.01709580421447754 all layer time: 0.004246711730957031 shape: (128, 10)\n",
      "epoch: 1 batch 443 loss: 0.10089494 acc: 97.65625\n",
      "training time 0.016576051712036133 all layer time: 0.00419163703918457 shape: (128, 10)\n",
      "epoch: 1 batch 444 loss: 0.034154966 acc: 98.4375\n",
      "training time 0.017500877380371094 all layer time: 0.004177093505859375 shape: (128, 10)\n",
      "epoch: 1 batch 445 loss: 0.043836262 acc: 99.21875\n",
      "training time 0.017139673233032227 all layer time: 0.00426173210144043 shape: (128, 10)\n",
      "epoch: 1 batch 446 loss: 0.0027667782 acc: 100.0\n",
      "training time 0.017130374908447266 all layer time: 0.004173755645751953 shape: (128, 10)\n",
      "epoch: 1 batch 447 loss: 0.10031048 acc: 97.65625\n",
      "training time 0.017747879028320312 all layer time: 0.004227638244628906 shape: (128, 10)\n",
      "epoch: 1 batch 448 loss: 0.036703832 acc: 98.4375\n",
      "training time 0.01793050765991211 all layer time: 0.004378795623779297 shape: (128, 10)\n",
      "epoch: 1 batch 449 loss: 0.056140985 acc: 98.4375\n",
      "training time 0.017093896865844727 all layer time: 0.004381656646728516 shape: (128, 10)\n",
      "epoch: 1 batch 450 loss: 0.0973434 acc: 96.875\n",
      "training time 0.01728534698486328 all layer time: 0.004278898239135742 shape: (128, 10)\n",
      "epoch: 1 batch 451 loss: 0.08992546 acc: 97.65625\n",
      "training time 0.01753997802734375 all layer time: 0.00433349609375 shape: (128, 10)\n",
      "epoch: 1 batch 452 loss: 0.08886488 acc: 96.09375\n",
      "training time 0.017018556594848633 all layer time: 0.004271268844604492 shape: (128, 10)\n",
      "epoch: 1 batch 453 loss: 0.08220479 acc: 96.875\n",
      "training time 0.017217159271240234 all layer time: 0.004311561584472656 shape: (128, 10)\n",
      "epoch: 1 batch 454 loss: 0.0068551274 acc: 100.0\n",
      "training time 0.017128705978393555 all layer time: 0.00426173210144043 shape: (128, 10)\n",
      "epoch: 1 batch 455 loss: 0.021612216 acc: 99.21875\n",
      "training time 0.017964601516723633 all layer time: 0.004286766052246094 shape: (128, 10)\n",
      "epoch: 1 batch 456 loss: 0.05446621 acc: 98.4375\n",
      "training time 0.016811609268188477 all layer time: 0.004287004470825195 shape: (128, 10)\n",
      "epoch: 1 batch 457 loss: 0.01865131 acc: 99.21875\n",
      "training time 0.016913175582885742 all layer time: 0.004223346710205078 shape: (128, 10)\n",
      "epoch: 1 batch 458 loss: 0.035555482 acc: 98.4375\n",
      "training time 0.016953706741333008 all layer time: 0.0041654109954833984 shape: (128, 10)\n",
      "epoch: 1 batch 459 loss: 0.07952905 acc: 97.65625\n",
      "training time 0.01671147346496582 all layer time: 0.0043447017669677734 shape: (128, 10)\n",
      "epoch: 1 batch 460 loss: 0.013434282 acc: 100.0\n",
      "training time 0.016841411590576172 all layer time: 0.004164218902587891 shape: (128, 10)\n",
      "epoch: 1 batch 461 loss: 0.003234535 acc: 100.0\n",
      "training time 0.017426013946533203 all layer time: 0.0041332244873046875 shape: (128, 10)\n",
      "epoch: 1 batch 462 loss: 0.0032285415 acc: 100.0\n",
      "training time 0.016872882843017578 all layer time: 0.004211902618408203 shape: (128, 10)\n",
      "epoch: 1 batch 463 loss: 0.041498724 acc: 99.21875\n",
      "training time 0.016802310943603516 all layer time: 0.004305124282836914 shape: (128, 10)\n",
      "epoch: 1 batch 464 loss: 0.038587775 acc: 99.21875\n",
      "training time 0.01714611053466797 all layer time: 0.004233121871948242 shape: (128, 10)\n",
      "epoch: 1 batch 465 loss: 0.0038969533 acc: 100.0\n",
      "training time 0.01739358901977539 all layer time: 0.0042057037353515625 shape: (128, 10)\n",
      "epoch: 1 batch 466 loss: 0.36042815 acc: 92.96875\n",
      "training time 0.01693892478942871 all layer time: 0.00410914421081543 shape: (128, 10)\n",
      "epoch: 1 batch 467 loss: 0.01040287 acc: 100.0\n",
      "training time 0.01718878746032715 all layer time: 0.004228353500366211 shape: (128, 10)\n",
      "before\n",
      "[0.03248993 0.         0.         0.07987152 0.03592135 0.\n",
      " 0.         0.         0.         0.0181312  0.         0.03251849\n",
      " 0.00435253 0.00276487 0.03190462 0.02166074 0.         0.\n",
      " 0.04598112 0.07327609 0.01283341 0.         0.         0.06642461\n",
      " 0.         0.0592512  0.         0.04745357 0.06115036 0.\n",
      " 0.06252579 0.01236281]\n",
      "after\n",
      "[0.03268323 0.         0.         0.07986496 0.03599294 0.\n",
      " 0.         0.         0.         0.01830978 0.         0.03267937\n",
      " 0.00447435 0.00251772 0.03188559 0.02121767 0.         0.\n",
      " 0.04585801 0.07328352 0.01263096 0.         0.         0.06642315\n",
      " 0.         0.05914993 0.         0.04769197 0.0613355  0.\n",
      " 0.06235917 0.01254006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch 0 loss: 0.0975013 acc: 97.65625\n",
      "training time 0.017226696014404297 all layer time: 0.004228115081787109 shape: (128, 10)\n",
      "epoch: 2 batch 1 loss: 0.11534647 acc: 96.09375\n",
      "training time 0.017786741256713867 all layer time: 0.004499197006225586 shape: (128, 10)\n",
      "epoch: 2 batch 2 loss: 0.01710248 acc: 100.0\n",
      "training time 0.017635345458984375 all layer time: 0.0042455196380615234 shape: (128, 10)\n",
      "epoch: 2 batch 3 loss: 0.14814132 acc: 95.3125\n",
      "training time 0.0170900821685791 all layer time: 0.004266023635864258 shape: (128, 10)\n",
      "epoch: 2 batch 4 loss: 0.053220645 acc: 98.4375\n",
      "training time 0.017002105712890625 all layer time: 0.004168510437011719 shape: (128, 10)\n",
      "epoch: 2 batch 5 loss: 0.053052783 acc: 99.21875\n",
      "training time 0.0170748233795166 all layer time: 0.0044841766357421875 shape: (128, 10)\n",
      "epoch: 2 batch 6 loss: 0.1607641 acc: 96.09375\n",
      "training time 0.017815351486206055 all layer time: 0.004231929779052734 shape: (128, 10)\n",
      "epoch: 2 batch 7 loss: 0.09110238 acc: 96.09375\n",
      "training time 0.016930580139160156 all layer time: 0.0043201446533203125 shape: (128, 10)\n",
      "epoch: 2 batch 8 loss: 0.12883934 acc: 95.3125\n",
      "training time 0.016801834106445312 all layer time: 0.004405975341796875 shape: (128, 10)\n",
      "epoch: 2 batch 9 loss: 0.045333967 acc: 99.21875\n",
      "training time 0.0175173282623291 all layer time: 0.004180192947387695 shape: (128, 10)\n",
      "epoch: 2 batch 10 loss: 0.07310663 acc: 97.65625\n",
      "training time 0.01842331886291504 all layer time: 0.004235744476318359 shape: (128, 10)\n",
      "epoch: 2 batch 11 loss: 0.033532284 acc: 99.21875\n",
      "training time 0.0169527530670166 all layer time: 0.0041735172271728516 shape: (128, 10)\n",
      "epoch: 2 batch 12 loss: 0.07533616 acc: 98.4375\n",
      "training time 0.017409324645996094 all layer time: 0.004374265670776367 shape: (128, 10)\n",
      "epoch: 2 batch 13 loss: 0.069644876 acc: 99.21875\n",
      "training time 0.016971588134765625 all layer time: 0.0042591094970703125 shape: (128, 10)\n",
      "epoch: 2 batch 14 loss: 0.03270064 acc: 99.21875\n",
      "training time 0.016927242279052734 all layer time: 0.004283428192138672 shape: (128, 10)\n",
      "epoch: 2 batch 15 loss: 0.06197006 acc: 98.4375\n",
      "training time 0.017170429229736328 all layer time: 0.0042266845703125 shape: (128, 10)\n",
      "epoch: 2 batch 16 loss: 0.05470983 acc: 98.4375\n",
      "training time 0.017465591430664062 all layer time: 0.004270315170288086 shape: (128, 10)\n",
      "epoch: 2 batch 17 loss: 0.057566825 acc: 97.65625\n",
      "training time 0.017075777053833008 all layer time: 0.004271745681762695 shape: (128, 10)\n",
      "epoch: 2 batch 18 loss: 0.0477399 acc: 98.4375\n",
      "training time 0.01686406135559082 all layer time: 0.004324913024902344 shape: (128, 10)\n",
      "epoch: 2 batch 19 loss: 0.021517996 acc: 100.0\n",
      "training time 0.01680898666381836 all layer time: 0.004245281219482422 shape: (128, 10)\n",
      "epoch: 2 batch 20 loss: 0.1460545 acc: 97.65625\n",
      "training time 0.016939640045166016 all layer time: 0.004161357879638672 shape: (128, 10)\n",
      "epoch: 2 batch 21 loss: 0.121977724 acc: 96.09375\n",
      "training time 0.017007827758789062 all layer time: 0.004219770431518555 shape: (128, 10)\n",
      "epoch: 2 batch 22 loss: 0.08755022 acc: 97.65625\n",
      "training time 0.01637887954711914 all layer time: 0.004289865493774414 shape: (128, 10)\n",
      "epoch: 2 batch 23 loss: 0.057879955 acc: 98.4375\n",
      "training time 0.017212629318237305 all layer time: 0.004246950149536133 shape: (128, 10)\n",
      "epoch: 2 batch 24 loss: 0.025813028 acc: 99.21875\n",
      "training time 0.01695394515991211 all layer time: 0.0042307376861572266 shape: (128, 10)\n",
      "epoch: 2 batch 25 loss: 0.0676911 acc: 97.65625\n",
      "training time 0.017016172409057617 all layer time: 0.004172563552856445 shape: (128, 10)\n",
      "epoch: 2 batch 26 loss: 0.033813473 acc: 98.4375\n",
      "training time 0.017026185989379883 all layer time: 0.00420832633972168 shape: (128, 10)\n",
      "epoch: 2 batch 27 loss: 0.05285568 acc: 98.4375\n",
      "training time 0.01691412925720215 all layer time: 0.004470109939575195 shape: (128, 10)\n",
      "epoch: 2 batch 28 loss: 0.1005274 acc: 97.65625\n",
      "training time 0.01764225959777832 all layer time: 0.004313468933105469 shape: (128, 10)\n",
      "epoch: 2 batch 29 loss: 0.091217 acc: 97.65625\n",
      "training time 0.017213821411132812 all layer time: 0.004407405853271484 shape: (128, 10)\n",
      "epoch: 2 batch 30 loss: 0.00793048 acc: 100.0\n",
      "training time 0.017188549041748047 all layer time: 0.004198312759399414 shape: (128, 10)\n",
      "epoch: 2 batch 31 loss: 0.054314848 acc: 98.4375\n",
      "training time 0.017087697982788086 all layer time: 0.004336357116699219 shape: (128, 10)\n",
      "epoch: 2 batch 32 loss: 0.1361372 acc: 96.09375\n",
      "training time 0.016806364059448242 all layer time: 0.0044023990631103516 shape: (128, 10)\n",
      "epoch: 2 batch 33 loss: 0.065373465 acc: 98.4375\n",
      "training time 0.017142295837402344 all layer time: 0.004338979721069336 shape: (128, 10)\n",
      "epoch: 2 batch 34 loss: 0.16251516 acc: 95.3125\n",
      "training time 0.016736507415771484 all layer time: 0.00435948371887207 shape: (128, 10)\n",
      "epoch: 2 batch 35 loss: 0.021018982 acc: 100.0\n",
      "training time 0.017317771911621094 all layer time: 0.004309654235839844 shape: (128, 10)\n",
      "epoch: 2 batch 36 loss: 0.07543116 acc: 96.09375\n",
      "training time 0.01697707176208496 all layer time: 0.004465818405151367 shape: (128, 10)\n",
      "epoch: 2 batch 37 loss: 0.040417798 acc: 98.4375\n",
      "training time 0.01647806167602539 all layer time: 0.004351377487182617 shape: (128, 10)\n",
      "epoch: 2 batch 38 loss: 0.049355302 acc: 98.4375\n",
      "training time 0.01739788055419922 all layer time: 0.004185676574707031 shape: (128, 10)\n",
      "epoch: 2 batch 39 loss: 0.031647317 acc: 98.4375\n",
      "training time 0.016893863677978516 all layer time: 0.004280567169189453 shape: (128, 10)\n",
      "epoch: 2 batch 40 loss: 0.037693672 acc: 98.4375\n",
      "training time 0.017656326293945312 all layer time: 0.0042591094970703125 shape: (128, 10)\n",
      "epoch: 2 batch 41 loss: 0.16575226 acc: 96.875\n",
      "training time 0.017746686935424805 all layer time: 0.004205942153930664 shape: (128, 10)\n",
      "epoch: 2 batch 42 loss: 0.047452744 acc: 99.21875\n",
      "training time 0.01672673225402832 all layer time: 0.004383087158203125 shape: (128, 10)\n",
      "epoch: 2 batch 43 loss: 0.11549626 acc: 96.875\n",
      "training time 0.017320871353149414 all layer time: 0.004314899444580078 shape: (128, 10)\n",
      "epoch: 2 batch 44 loss: 0.079229526 acc: 96.875\n",
      "training time 0.016919851303100586 all layer time: 0.004256486892700195 shape: (128, 10)\n",
      "epoch: 2 batch 45 loss: 0.07146396 acc: 96.875\n",
      "training time 0.01757979393005371 all layer time: 0.0041713714599609375 shape: (128, 10)\n",
      "epoch: 2 batch 46 loss: 0.057667747 acc: 99.21875\n",
      "training time 0.01690220832824707 all layer time: 0.0042378902435302734 shape: (128, 10)\n",
      "epoch: 2 batch 47 loss: 0.08412084 acc: 98.4375\n",
      "training time 0.017088651657104492 all layer time: 0.0041658878326416016 shape: (128, 10)\n",
      "epoch: 2 batch 48 loss: 0.057572044 acc: 99.21875\n",
      "training time 0.017105579376220703 all layer time: 0.004261016845703125 shape: (128, 10)\n",
      "epoch: 2 batch 49 loss: 0.045477048 acc: 98.4375\n",
      "training time 0.01789093017578125 all layer time: 0.00434112548828125 shape: (128, 10)\n",
      "epoch: 2 batch 50 loss: 0.058596723 acc: 98.4375\n",
      "training time 0.01723504066467285 all layer time: 0.004228353500366211 shape: (128, 10)\n",
      "epoch: 2 batch 51 loss: 0.011503772 acc: 99.21875\n",
      "training time 0.01739645004272461 all layer time: 0.0043506622314453125 shape: (128, 10)\n",
      "epoch: 2 batch 52 loss: 0.049781546 acc: 98.4375\n",
      "training time 0.017199277877807617 all layer time: 0.004279136657714844 shape: (128, 10)\n",
      "epoch: 2 batch 53 loss: 0.13702604 acc: 94.53125\n",
      "training time 0.0168759822845459 all layer time: 0.004209041595458984 shape: (128, 10)\n",
      "epoch: 2 batch 54 loss: 0.14611514 acc: 96.875\n",
      "training time 0.017397403717041016 all layer time: 0.00426793098449707 shape: (128, 10)\n",
      "epoch: 2 batch 55 loss: 0.06602068 acc: 96.875\n",
      "training time 0.017425060272216797 all layer time: 0.004333019256591797 shape: (128, 10)\n",
      "epoch: 2 batch 56 loss: 0.10219283 acc: 97.65625\n",
      "training time 0.016913652420043945 all layer time: 0.004149198532104492 shape: (128, 10)\n",
      "epoch: 2 batch 57 loss: 0.070128456 acc: 98.4375\n",
      "training time 0.016608715057373047 all layer time: 0.004204988479614258 shape: (128, 10)\n",
      "epoch: 2 batch 58 loss: 0.049788117 acc: 98.4375\n",
      "training time 0.017974853515625 all layer time: 0.004274129867553711 shape: (128, 10)\n",
      "epoch: 2 batch 59 loss: 0.04459548 acc: 97.65625\n",
      "training time 0.0173189640045166 all layer time: 0.004177570343017578 shape: (128, 10)\n",
      "epoch: 2 batch 60 loss: 0.04101029 acc: 98.4375\n",
      "training time 0.01647210121154785 all layer time: 0.004177570343017578 shape: (128, 10)\n",
      "epoch: 2 batch 61 loss: 0.07832741 acc: 96.875\n",
      "training time 0.01732468605041504 all layer time: 0.004278659820556641 shape: (128, 10)\n",
      "epoch: 2 batch 62 loss: 0.022243785 acc: 99.21875\n",
      "training time 0.01770329475402832 all layer time: 0.004160404205322266 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch 63 loss: 0.049882136 acc: 97.65625\n",
      "training time 0.01688551902770996 all layer time: 0.004235267639160156 shape: (128, 10)\n",
      "epoch: 2 batch 64 loss: 0.1526533 acc: 94.53125\n",
      "training time 0.016559123992919922 all layer time: 0.004288434982299805 shape: (128, 10)\n",
      "epoch: 2 batch 65 loss: 0.019466843 acc: 100.0\n",
      "training time 0.016876697540283203 all layer time: 0.004279136657714844 shape: (128, 10)\n",
      "epoch: 2 batch 66 loss: 0.055959836 acc: 97.65625\n",
      "training time 0.017701387405395508 all layer time: 0.0042765140533447266 shape: (128, 10)\n",
      "epoch: 2 batch 67 loss: 0.0663422 acc: 97.65625\n",
      "training time 0.017368078231811523 all layer time: 0.0042383670806884766 shape: (128, 10)\n",
      "epoch: 2 batch 68 loss: 0.22832216 acc: 95.3125\n",
      "training time 0.016941070556640625 all layer time: 0.0042209625244140625 shape: (128, 10)\n",
      "epoch: 2 batch 69 loss: 0.16201508 acc: 94.53125\n",
      "training time 0.01707625389099121 all layer time: 0.004259347915649414 shape: (128, 10)\n",
      "epoch: 2 batch 70 loss: 0.036727138 acc: 98.4375\n",
      "training time 0.01706719398498535 all layer time: 0.004280805587768555 shape: (128, 10)\n",
      "epoch: 2 batch 71 loss: 0.11202356 acc: 96.875\n",
      "training time 0.017670392990112305 all layer time: 0.0042095184326171875 shape: (128, 10)\n",
      "epoch: 2 batch 72 loss: 0.06746347 acc: 98.4375\n",
      "training time 0.016966581344604492 all layer time: 0.004279136657714844 shape: (128, 10)\n",
      "epoch: 2 batch 73 loss: 0.17997868 acc: 94.53125\n",
      "training time 0.016811132431030273 all layer time: 0.004288911819458008 shape: (128, 10)\n",
      "epoch: 2 batch 74 loss: 0.050790593 acc: 97.65625\n",
      "training time 0.017015695571899414 all layer time: 0.0042154788970947266 shape: (128, 10)\n",
      "epoch: 2 batch 75 loss: 0.02557392 acc: 99.21875\n",
      "training time 0.017714738845825195 all layer time: 0.004273653030395508 shape: (128, 10)\n",
      "epoch: 2 batch 76 loss: 0.036278 acc: 98.4375\n",
      "training time 0.017315387725830078 all layer time: 0.004244327545166016 shape: (128, 10)\n",
      "epoch: 2 batch 77 loss: 0.017920705 acc: 99.21875\n",
      "training time 0.017310619354248047 all layer time: 0.0044100284576416016 shape: (128, 10)\n",
      "epoch: 2 batch 78 loss: 0.03194918 acc: 99.21875\n",
      "training time 0.01708698272705078 all layer time: 0.004374027252197266 shape: (128, 10)\n",
      "epoch: 2 batch 79 loss: 0.103838354 acc: 96.09375\n",
      "training time 0.01779770851135254 all layer time: 0.0042645931243896484 shape: (128, 10)\n",
      "epoch: 2 batch 80 loss: 0.10344249 acc: 95.3125\n",
      "training time 0.017360210418701172 all layer time: 0.004280567169189453 shape: (128, 10)\n",
      "epoch: 2 batch 81 loss: 0.007491396 acc: 100.0\n",
      "training time 0.017090797424316406 all layer time: 0.0043795108795166016 shape: (128, 10)\n",
      "epoch: 2 batch 82 loss: 0.034661934 acc: 99.21875\n",
      "training time 0.017536640167236328 all layer time: 0.004231929779052734 shape: (128, 10)\n",
      "epoch: 2 batch 83 loss: 0.019989261 acc: 99.21875\n",
      "training time 0.017323970794677734 all layer time: 0.004652738571166992 shape: (128, 10)\n",
      "epoch: 2 batch 84 loss: 0.066501856 acc: 95.3125\n",
      "training time 0.016977310180664062 all layer time: 0.00419306755065918 shape: (128, 10)\n",
      "epoch: 2 batch 85 loss: 0.10413114 acc: 98.4375\n",
      "training time 0.017214298248291016 all layer time: 0.004315853118896484 shape: (128, 10)\n",
      "epoch: 2 batch 86 loss: 0.07485865 acc: 96.875\n",
      "training time 0.01680731773376465 all layer time: 0.004204273223876953 shape: (128, 10)\n",
      "epoch: 2 batch 87 loss: 0.054860048 acc: 99.21875\n",
      "training time 0.017170190811157227 all layer time: 0.004185676574707031 shape: (128, 10)\n",
      "epoch: 2 batch 88 loss: 0.029710164 acc: 98.4375\n",
      "training time 0.017064809799194336 all layer time: 0.004476070404052734 shape: (128, 10)\n",
      "epoch: 2 batch 89 loss: 0.041498415 acc: 98.4375\n",
      "training time 0.01706981658935547 all layer time: 0.004290342330932617 shape: (128, 10)\n",
      "epoch: 2 batch 90 loss: 0.11784227 acc: 97.65625\n",
      "training time 0.016762256622314453 all layer time: 0.004191398620605469 shape: (128, 10)\n",
      "epoch: 2 batch 91 loss: 0.07666001 acc: 98.4375\n",
      "training time 0.01717543601989746 all layer time: 0.004266023635864258 shape: (128, 10)\n",
      "epoch: 2 batch 92 loss: 0.094143316 acc: 97.65625\n",
      "training time 0.016831636428833008 all layer time: 0.004194736480712891 shape: (128, 10)\n",
      "epoch: 2 batch 93 loss: 0.04384982 acc: 97.65625\n",
      "training time 0.017071008682250977 all layer time: 0.0043942928314208984 shape: (128, 10)\n",
      "epoch: 2 batch 94 loss: 0.036876533 acc: 99.21875\n",
      "training time 0.016796350479125977 all layer time: 0.004369258880615234 shape: (128, 10)\n",
      "epoch: 2 batch 95 loss: 0.06377083 acc: 99.21875\n",
      "training time 0.01679086685180664 all layer time: 0.00441431999206543 shape: (128, 10)\n",
      "epoch: 2 batch 96 loss: 0.0750172 acc: 96.875\n",
      "training time 0.016956090927124023 all layer time: 0.004316091537475586 shape: (128, 10)\n",
      "epoch: 2 batch 97 loss: 0.03517002 acc: 98.4375\n",
      "training time 0.016662120819091797 all layer time: 0.0041887760162353516 shape: (128, 10)\n",
      "epoch: 2 batch 98 loss: 0.138789 acc: 95.3125\n",
      "training time 0.017054319381713867 all layer time: 0.004197597503662109 shape: (128, 10)\n",
      "epoch: 2 batch 99 loss: 0.04209948 acc: 98.4375\n",
      "training time 0.016798973083496094 all layer time: 0.0041811466217041016 shape: (128, 10)\n",
      "epoch: 2 batch 100 loss: 0.057765096 acc: 96.875\n",
      "training time 0.016672134399414062 all layer time: 0.004182577133178711 shape: (128, 10)\n",
      "epoch: 2 batch 101 loss: 0.11132005 acc: 97.65625\n",
      "training time 0.01720714569091797 all layer time: 0.004467487335205078 shape: (128, 10)\n",
      "epoch: 2 batch 102 loss: 0.096348695 acc: 96.875\n",
      "training time 0.016719818115234375 all layer time: 0.004263162612915039 shape: (128, 10)\n",
      "epoch: 2 batch 103 loss: 0.05157609 acc: 98.4375\n",
      "training time 0.017026185989379883 all layer time: 0.00425410270690918 shape: (128, 10)\n",
      "epoch: 2 batch 104 loss: 0.06776516 acc: 96.875\n",
      "training time 0.016937255859375 all layer time: 0.004143238067626953 shape: (128, 10)\n",
      "epoch: 2 batch 105 loss: 0.058891587 acc: 97.65625\n",
      "training time 0.019094467163085938 all layer time: 0.004292726516723633 shape: (128, 10)\n",
      "epoch: 2 batch 106 loss: 0.043466732 acc: 98.4375\n",
      "training time 0.01757025718688965 all layer time: 0.00424957275390625 shape: (128, 10)\n",
      "epoch: 2 batch 107 loss: 0.024996977 acc: 99.21875\n",
      "training time 0.0172269344329834 all layer time: 0.004233360290527344 shape: (128, 10)\n",
      "epoch: 2 batch 108 loss: 0.07159074 acc: 96.875\n",
      "training time 0.017136812210083008 all layer time: 0.004232645034790039 shape: (128, 10)\n",
      "epoch: 2 batch 109 loss: 0.12913465 acc: 96.09375\n",
      "training time 0.01740288734436035 all layer time: 0.004431486129760742 shape: (128, 10)\n",
      "epoch: 2 batch 110 loss: 0.06966223 acc: 98.4375\n",
      "training time 0.01719212532043457 all layer time: 0.004175901412963867 shape: (128, 10)\n",
      "epoch: 2 batch 111 loss: 0.032094564 acc: 98.4375\n",
      "training time 0.017006397247314453 all layer time: 0.004240512847900391 shape: (128, 10)\n",
      "epoch: 2 batch 112 loss: 0.077169046 acc: 98.4375\n",
      "training time 0.016897916793823242 all layer time: 0.0041806697845458984 shape: (128, 10)\n",
      "epoch: 2 batch 113 loss: 0.071297534 acc: 98.4375\n",
      "training time 0.01723790168762207 all layer time: 0.004192352294921875 shape: (128, 10)\n",
      "epoch: 2 batch 114 loss: 0.09041551 acc: 97.65625\n",
      "training time 0.017879724502563477 all layer time: 0.004243135452270508 shape: (128, 10)\n",
      "epoch: 2 batch 115 loss: 0.110962786 acc: 94.53125\n",
      "training time 0.017754077911376953 all layer time: 0.00435948371887207 shape: (128, 10)\n",
      "epoch: 2 batch 116 loss: 0.038167946 acc: 99.21875\n",
      "training time 0.017025232315063477 all layer time: 0.004265785217285156 shape: (128, 10)\n",
      "epoch: 2 batch 117 loss: 0.027452243 acc: 98.4375\n",
      "training time 0.01717209815979004 all layer time: 0.00423121452331543 shape: (128, 10)\n",
      "epoch: 2 batch 118 loss: 0.13039325 acc: 97.65625\n",
      "training time 0.016913414001464844 all layer time: 0.004441022872924805 shape: (128, 10)\n",
      "epoch: 2 batch 119 loss: 0.05528894 acc: 98.4375\n",
      "training time 0.017667770385742188 all layer time: 0.004203081130981445 shape: (128, 10)\n",
      "epoch: 2 batch 120 loss: 0.102129176 acc: 96.09375\n",
      "training time 0.01653599739074707 all layer time: 0.0043010711669921875 shape: (128, 10)\n",
      "epoch: 2 batch 121 loss: 0.059340704 acc: 99.21875\n",
      "training time 0.016916513442993164 all layer time: 0.00418543815612793 shape: (128, 10)\n",
      "epoch: 2 batch 122 loss: 0.03963201 acc: 97.65625\n",
      "training time 0.01709294319152832 all layer time: 0.004332304000854492 shape: (128, 10)\n",
      "epoch: 2 batch 123 loss: 0.08734465 acc: 96.875\n",
      "training time 0.01738452911376953 all layer time: 0.004376411437988281 shape: (128, 10)\n",
      "epoch: 2 batch 124 loss: 0.14540245 acc: 98.4375\n",
      "training time 0.016956567764282227 all layer time: 0.004331350326538086 shape: (128, 10)\n",
      "epoch: 2 batch 125 loss: 0.049555354 acc: 99.21875\n",
      "training time 0.016814231872558594 all layer time: 0.00432276725769043 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch 126 loss: 0.027902732 acc: 99.21875\n",
      "training time 0.01717066764831543 all layer time: 0.0042760372161865234 shape: (128, 10)\n",
      "epoch: 2 batch 127 loss: 0.04933376 acc: 98.4375\n",
      "training time 0.018136024475097656 all layer time: 0.004423379898071289 shape: (128, 10)\n",
      "epoch: 2 batch 128 loss: 0.029082024 acc: 99.21875\n",
      "training time 0.017109155654907227 all layer time: 0.0041506290435791016 shape: (128, 10)\n",
      "epoch: 2 batch 129 loss: 0.048043486 acc: 98.4375\n",
      "training time 0.01677250862121582 all layer time: 0.004437923431396484 shape: (128, 10)\n",
      "epoch: 2 batch 130 loss: 0.07966355 acc: 96.875\n",
      "training time 0.017248868942260742 all layer time: 0.0044231414794921875 shape: (128, 10)\n",
      "epoch: 2 batch 131 loss: 0.0483986 acc: 97.65625\n",
      "training time 0.01713871955871582 all layer time: 0.0042896270751953125 shape: (128, 10)\n",
      "epoch: 2 batch 132 loss: 0.06727434 acc: 97.65625\n",
      "training time 0.019281387329101562 all layer time: 0.004354715347290039 shape: (128, 10)\n",
      "epoch: 2 batch 133 loss: 0.044152953 acc: 98.4375\n",
      "training time 0.016881704330444336 all layer time: 0.004281759262084961 shape: (128, 10)\n",
      "epoch: 2 batch 134 loss: 0.0790317 acc: 97.65625\n",
      "training time 0.016712427139282227 all layer time: 0.004212379455566406 shape: (128, 10)\n",
      "epoch: 2 batch 135 loss: 0.03439402 acc: 97.65625\n",
      "training time 0.017098188400268555 all layer time: 0.004484653472900391 shape: (128, 10)\n",
      "epoch: 2 batch 136 loss: 0.071035005 acc: 96.875\n",
      "training time 0.018013954162597656 all layer time: 0.004237174987792969 shape: (128, 10)\n",
      "epoch: 2 batch 137 loss: 0.09327907 acc: 96.875\n",
      "training time 0.017190933227539062 all layer time: 0.004183292388916016 shape: (128, 10)\n",
      "epoch: 2 batch 138 loss: 0.09350327 acc: 97.65625\n",
      "training time 0.01722884178161621 all layer time: 0.004419088363647461 shape: (128, 10)\n",
      "epoch: 2 batch 139 loss: 0.075498775 acc: 98.4375\n",
      "training time 0.016799211502075195 all layer time: 0.004271745681762695 shape: (128, 10)\n",
      "epoch: 2 batch 140 loss: 0.08591756 acc: 96.09375\n",
      "training time 0.02068495750427246 all layer time: 0.004226207733154297 shape: (128, 10)\n",
      "epoch: 2 batch 141 loss: 0.030193962 acc: 100.0\n",
      "training time 0.0169069766998291 all layer time: 0.0043332576751708984 shape: (128, 10)\n",
      "epoch: 2 batch 142 loss: 0.035927884 acc: 98.4375\n",
      "training time 0.016994237899780273 all layer time: 0.0043065547943115234 shape: (128, 10)\n",
      "epoch: 2 batch 143 loss: 0.14501587 acc: 96.875\n",
      "training time 0.016794681549072266 all layer time: 0.004193305969238281 shape: (128, 10)\n",
      "epoch: 2 batch 144 loss: 0.05586099 acc: 98.4375\n",
      "training time 0.017775774002075195 all layer time: 0.004255533218383789 shape: (128, 10)\n",
      "epoch: 2 batch 145 loss: 0.042279396 acc: 98.4375\n",
      "training time 0.017650365829467773 all layer time: 0.004293918609619141 shape: (128, 10)\n",
      "epoch: 2 batch 146 loss: 0.026583515 acc: 99.21875\n",
      "training time 0.016767263412475586 all layer time: 0.004214763641357422 shape: (128, 10)\n",
      "epoch: 2 batch 147 loss: 0.030216668 acc: 98.4375\n",
      "training time 0.016807079315185547 all layer time: 0.004187107086181641 shape: (128, 10)\n",
      "epoch: 2 batch 148 loss: 0.03846667 acc: 99.21875\n",
      "training time 0.01696038246154785 all layer time: 0.004446744918823242 shape: (128, 10)\n",
      "epoch: 2 batch 149 loss: 0.07835746 acc: 96.875\n",
      "training time 0.016610383987426758 all layer time: 0.0042724609375 shape: (128, 10)\n",
      "epoch: 2 batch 150 loss: 0.077066824 acc: 97.65625\n",
      "training time 0.017046689987182617 all layer time: 0.004235982894897461 shape: (128, 10)\n",
      "epoch: 2 batch 151 loss: 0.10250132 acc: 96.875\n",
      "training time 0.01768636703491211 all layer time: 0.004221677780151367 shape: (128, 10)\n",
      "epoch: 2 batch 152 loss: 0.02524329 acc: 99.21875\n",
      "training time 0.016941308975219727 all layer time: 0.004342317581176758 shape: (128, 10)\n",
      "epoch: 2 batch 153 loss: 0.036185626 acc: 99.21875\n",
      "training time 0.01693868637084961 all layer time: 0.0043833255767822266 shape: (128, 10)\n",
      "epoch: 2 batch 154 loss: 0.011931885 acc: 100.0\n",
      "training time 0.01709604263305664 all layer time: 0.0042994022369384766 shape: (128, 10)\n",
      "epoch: 2 batch 155 loss: 0.027043393 acc: 99.21875\n",
      "training time 0.016558170318603516 all layer time: 0.0040776729583740234 shape: (128, 10)\n",
      "epoch: 2 batch 156 loss: 0.07369498 acc: 98.4375\n",
      "training time 0.01681995391845703 all layer time: 0.00424957275390625 shape: (128, 10)\n",
      "epoch: 2 batch 157 loss: 0.09991468 acc: 96.09375\n",
      "training time 0.017229318618774414 all layer time: 0.004284381866455078 shape: (128, 10)\n",
      "epoch: 2 batch 158 loss: 0.120124996 acc: 97.65625\n",
      "training time 0.017044782638549805 all layer time: 0.004135608673095703 shape: (128, 10)\n",
      "epoch: 2 batch 159 loss: 0.008477358 acc: 100.0\n",
      "training time 0.01736164093017578 all layer time: 0.004239797592163086 shape: (128, 10)\n",
      "epoch: 2 batch 160 loss: 0.008941308 acc: 100.0\n",
      "training time 0.017052412033081055 all layer time: 0.004284858703613281 shape: (128, 10)\n",
      "epoch: 2 batch 161 loss: 0.14879182 acc: 96.875\n",
      "training time 0.01714158058166504 all layer time: 0.004404306411743164 shape: (128, 10)\n",
      "epoch: 2 batch 162 loss: 0.07717647 acc: 97.65625\n",
      "training time 0.017424583435058594 all layer time: 0.004228353500366211 shape: (128, 10)\n",
      "epoch: 2 batch 163 loss: 0.11027893 acc: 96.875\n",
      "training time 0.017141103744506836 all layer time: 0.0041980743408203125 shape: (128, 10)\n",
      "epoch: 2 batch 164 loss: 0.04783664 acc: 99.21875\n",
      "training time 0.017041444778442383 all layer time: 0.004200458526611328 shape: (128, 10)\n",
      "epoch: 2 batch 165 loss: 0.05520262 acc: 97.65625\n",
      "training time 0.016939878463745117 all layer time: 0.004302024841308594 shape: (128, 10)\n",
      "epoch: 2 batch 166 loss: 0.046485465 acc: 97.65625\n",
      "training time 0.01743483543395996 all layer time: 0.0042684078216552734 shape: (128, 10)\n",
      "epoch: 2 batch 167 loss: 0.030572444 acc: 99.21875\n",
      "training time 0.016948938369750977 all layer time: 0.0043108463287353516 shape: (128, 10)\n",
      "epoch: 2 batch 168 loss: 0.047704533 acc: 98.4375\n",
      "training time 0.016991853713989258 all layer time: 0.004211902618408203 shape: (128, 10)\n",
      "epoch: 2 batch 169 loss: 0.04536008 acc: 97.65625\n",
      "training time 0.01709151268005371 all layer time: 0.004206657409667969 shape: (128, 10)\n",
      "epoch: 2 batch 170 loss: 0.016894164 acc: 100.0\n",
      "training time 0.017532825469970703 all layer time: 0.004330873489379883 shape: (128, 10)\n",
      "epoch: 2 batch 171 loss: 0.11720301 acc: 97.65625\n",
      "training time 0.017461538314819336 all layer time: 0.004205465316772461 shape: (128, 10)\n",
      "epoch: 2 batch 172 loss: 0.04312661 acc: 98.4375\n",
      "training time 0.01697683334350586 all layer time: 0.004321098327636719 shape: (128, 10)\n",
      "epoch: 2 batch 173 loss: 0.06365913 acc: 98.4375\n",
      "training time 0.017136812210083008 all layer time: 0.004463911056518555 shape: (128, 10)\n",
      "epoch: 2 batch 174 loss: 0.04953866 acc: 98.4375\n",
      "training time 0.017397642135620117 all layer time: 0.004334688186645508 shape: (128, 10)\n",
      "epoch: 2 batch 175 loss: 0.1267654 acc: 95.3125\n",
      "training time 0.017100095748901367 all layer time: 0.004326343536376953 shape: (128, 10)\n",
      "epoch: 2 batch 176 loss: 0.1071205 acc: 96.09375\n",
      "training time 0.01694798469543457 all layer time: 0.0042493343353271484 shape: (128, 10)\n",
      "epoch: 2 batch 177 loss: 0.039719045 acc: 99.21875\n",
      "training time 0.017162561416625977 all layer time: 0.004242658615112305 shape: (128, 10)\n",
      "epoch: 2 batch 178 loss: 0.014494265 acc: 99.21875\n",
      "training time 0.01691269874572754 all layer time: 0.004384517669677734 shape: (128, 10)\n",
      "epoch: 2 batch 179 loss: 0.0136519335 acc: 100.0\n",
      "training time 0.017378807067871094 all layer time: 0.004523754119873047 shape: (128, 10)\n",
      "epoch: 2 batch 180 loss: 0.030083362 acc: 98.4375\n",
      "training time 0.017346620559692383 all layer time: 0.004239797592163086 shape: (128, 10)\n",
      "epoch: 2 batch 181 loss: 0.048853643 acc: 97.65625\n",
      "training time 0.01735830307006836 all layer time: 0.004328012466430664 shape: (128, 10)\n",
      "epoch: 2 batch 182 loss: 0.01632974 acc: 100.0\n",
      "training time 0.016829490661621094 all layer time: 0.0041866302490234375 shape: (128, 10)\n",
      "epoch: 2 batch 183 loss: 0.02914318 acc: 98.4375\n",
      "training time 0.01738119125366211 all layer time: 0.004399776458740234 shape: (128, 10)\n",
      "epoch: 2 batch 184 loss: 0.07206964 acc: 97.65625\n",
      "training time 0.0169522762298584 all layer time: 0.00419163703918457 shape: (128, 10)\n",
      "epoch: 2 batch 185 loss: 0.07921721 acc: 97.65625\n",
      "training time 0.01690983772277832 all layer time: 0.0042302608489990234 shape: (128, 10)\n",
      "epoch: 2 batch 186 loss: 0.20787983 acc: 96.875\n",
      "training time 0.01689934730529785 all layer time: 0.00429844856262207 shape: (128, 10)\n",
      "epoch: 2 batch 187 loss: 0.080327176 acc: 98.4375\n",
      "training time 0.016948699951171875 all layer time: 0.0044591426849365234 shape: (128, 10)\n",
      "epoch: 2 batch 188 loss: 0.033321805 acc: 98.4375\n",
      "training time 0.016873598098754883 all layer time: 0.004353761672973633 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch 189 loss: 0.03801238 acc: 99.21875\n",
      "training time 0.016826868057250977 all layer time: 0.004421234130859375 shape: (128, 10)\n",
      "epoch: 2 batch 190 loss: 0.0093824165 acc: 100.0\n",
      "training time 0.01692795753479004 all layer time: 0.0041522979736328125 shape: (128, 10)\n",
      "epoch: 2 batch 191 loss: 0.022258764 acc: 99.21875\n",
      "training time 0.016994476318359375 all layer time: 0.004213571548461914 shape: (128, 10)\n",
      "epoch: 2 batch 192 loss: 0.10689309 acc: 96.875\n",
      "training time 0.017610788345336914 all layer time: 0.00426793098449707 shape: (128, 10)\n",
      "epoch: 2 batch 193 loss: 0.15532361 acc: 98.4375\n",
      "training time 0.01751852035522461 all layer time: 0.004325389862060547 shape: (128, 10)\n",
      "epoch: 2 batch 194 loss: 0.11810379 acc: 95.3125\n",
      "training time 0.017383098602294922 all layer time: 0.004540920257568359 shape: (128, 10)\n",
      "epoch: 2 batch 195 loss: 0.00805436 acc: 100.0\n",
      "training time 0.017097949981689453 all layer time: 0.004512310028076172 shape: (128, 10)\n",
      "epoch: 2 batch 196 loss: 0.058236495 acc: 97.65625\n",
      "training time 0.018507957458496094 all layer time: 0.004308462142944336 shape: (128, 10)\n",
      "epoch: 2 batch 197 loss: 0.044330724 acc: 99.21875\n",
      "training time 0.01682877540588379 all layer time: 0.004183530807495117 shape: (128, 10)\n",
      "epoch: 2 batch 198 loss: 0.008889247 acc: 100.0\n",
      "training time 0.016843557357788086 all layer time: 0.004242658615112305 shape: (128, 10)\n",
      "epoch: 2 batch 199 loss: 0.12946951 acc: 96.875\n",
      "training time 0.01675868034362793 all layer time: 0.004304647445678711 shape: (128, 10)\n",
      "epoch: 2 batch 200 loss: 0.058971435 acc: 98.4375\n",
      "training time 0.017894506454467773 all layer time: 0.004239082336425781 shape: (128, 10)\n",
      "epoch: 2 batch 201 loss: 0.10388818 acc: 97.65625\n",
      "training time 0.018735647201538086 all layer time: 0.0043659210205078125 shape: (128, 10)\n",
      "epoch: 2 batch 202 loss: 0.029752092 acc: 99.21875\n",
      "training time 0.01707911491394043 all layer time: 0.004110813140869141 shape: (128, 10)\n",
      "epoch: 2 batch 203 loss: 0.047906622 acc: 98.4375\n",
      "training time 0.016954898834228516 all layer time: 0.004270076751708984 shape: (128, 10)\n",
      "epoch: 2 batch 204 loss: 0.021637738 acc: 99.21875\n",
      "training time 0.017433643341064453 all layer time: 0.004296541213989258 shape: (128, 10)\n",
      "epoch: 2 batch 205 loss: 0.06269716 acc: 99.21875\n",
      "training time 0.017794132232666016 all layer time: 0.004279613494873047 shape: (128, 10)\n",
      "epoch: 2 batch 206 loss: 0.10560667 acc: 96.875\n",
      "training time 0.017522573471069336 all layer time: 0.004286527633666992 shape: (128, 10)\n",
      "epoch: 2 batch 207 loss: 0.1574266 acc: 96.875\n",
      "training time 0.016975879669189453 all layer time: 0.0042417049407958984 shape: (128, 10)\n",
      "epoch: 2 batch 208 loss: 0.24399006 acc: 95.3125\n",
      "training time 0.016611814498901367 all layer time: 0.004375457763671875 shape: (128, 10)\n",
      "epoch: 2 batch 209 loss: 0.045688853 acc: 98.4375\n",
      "training time 0.01703500747680664 all layer time: 0.004556894302368164 shape: (128, 10)\n",
      "epoch: 2 batch 210 loss: 0.09595126 acc: 97.65625\n",
      "training time 0.01823139190673828 all layer time: 0.004348278045654297 shape: (128, 10)\n",
      "epoch: 2 batch 211 loss: 0.032331683 acc: 100.0\n",
      "training time 0.017061233520507812 all layer time: 0.0042841434478759766 shape: (128, 10)\n",
      "epoch: 2 batch 212 loss: 0.15142918 acc: 95.3125\n",
      "training time 0.016861677169799805 all layer time: 0.004205226898193359 shape: (128, 10)\n",
      "epoch: 2 batch 213 loss: 0.036240894 acc: 97.65625\n",
      "training time 0.01694631576538086 all layer time: 0.004271745681762695 shape: (128, 10)\n",
      "epoch: 2 batch 214 loss: 0.06570357 acc: 98.4375\n",
      "training time 0.01821112632751465 all layer time: 0.0042417049407958984 shape: (128, 10)\n",
      "epoch: 2 batch 215 loss: 0.06590265 acc: 98.4375\n",
      "training time 0.016852140426635742 all layer time: 0.004228830337524414 shape: (128, 10)\n",
      "epoch: 2 batch 216 loss: 0.03124569 acc: 99.21875\n",
      "training time 0.01683807373046875 all layer time: 0.004338979721069336 shape: (128, 10)\n",
      "epoch: 2 batch 217 loss: 0.064233966 acc: 98.4375\n",
      "training time 0.017077207565307617 all layer time: 0.0040836334228515625 shape: (128, 10)\n",
      "epoch: 2 batch 218 loss: 0.020149123 acc: 99.21875\n",
      "training time 0.01696634292602539 all layer time: 0.004250764846801758 shape: (128, 10)\n",
      "epoch: 2 batch 219 loss: 0.032858603 acc: 99.21875\n",
      "training time 0.016632795333862305 all layer time: 0.004225015640258789 shape: (128, 10)\n",
      "epoch: 2 batch 220 loss: 0.12451843 acc: 94.53125\n",
      "training time 0.01732611656188965 all layer time: 0.00424504280090332 shape: (128, 10)\n",
      "epoch: 2 batch 221 loss: 0.060097136 acc: 97.65625\n",
      "training time 0.017302274703979492 all layer time: 0.00418543815612793 shape: (128, 10)\n",
      "epoch: 2 batch 222 loss: 0.038160775 acc: 98.4375\n",
      "training time 0.017061233520507812 all layer time: 0.004318714141845703 shape: (128, 10)\n",
      "epoch: 2 batch 223 loss: 0.14509767 acc: 96.09375\n",
      "training time 0.017760038375854492 all layer time: 0.0042896270751953125 shape: (128, 10)\n",
      "epoch: 2 batch 224 loss: 0.0935249 acc: 98.4375\n",
      "training time 0.017153263092041016 all layer time: 0.004205942153930664 shape: (128, 10)\n",
      "epoch: 2 batch 225 loss: 0.02766367 acc: 100.0\n",
      "training time 0.016419410705566406 all layer time: 0.004239559173583984 shape: (128, 10)\n",
      "epoch: 2 batch 226 loss: 0.075687855 acc: 97.65625\n",
      "training time 0.016990184783935547 all layer time: 0.004528999328613281 shape: (128, 10)\n",
      "epoch: 2 batch 227 loss: 0.031934503 acc: 100.0\n",
      "training time 0.017236709594726562 all layer time: 0.004469394683837891 shape: (128, 10)\n",
      "epoch: 2 batch 228 loss: 0.026255514 acc: 100.0\n",
      "training time 0.01741194725036621 all layer time: 0.0043315887451171875 shape: (128, 10)\n",
      "epoch: 2 batch 229 loss: 0.07557334 acc: 98.4375\n",
      "training time 0.01692509651184082 all layer time: 0.004279613494873047 shape: (128, 10)\n",
      "epoch: 2 batch 230 loss: 0.012474628 acc: 100.0\n",
      "training time 0.017129182815551758 all layer time: 0.00421595573425293 shape: (128, 10)\n",
      "epoch: 2 batch 231 loss: 0.020692743 acc: 99.21875\n",
      "training time 0.01718592643737793 all layer time: 0.004216432571411133 shape: (128, 10)\n",
      "epoch: 2 batch 232 loss: 0.098496646 acc: 96.09375\n",
      "training time 0.016672134399414062 all layer time: 0.004254579544067383 shape: (128, 10)\n",
      "epoch: 2 batch 233 loss: 0.06830355 acc: 96.875\n",
      "training time 0.016733646392822266 all layer time: 0.004201412200927734 shape: (128, 10)\n",
      "epoch: 2 batch 234 loss: 0.0766475 acc: 97.65625\n",
      "training time 0.016721010208129883 all layer time: 0.0042269229888916016 shape: (128, 10)\n",
      "epoch: 2 batch 235 loss: 0.059612162 acc: 97.65625\n",
      "training time 0.017251014709472656 all layer time: 0.004332780838012695 shape: (128, 10)\n",
      "epoch: 2 batch 236 loss: 0.041339025 acc: 97.65625\n",
      "training time 0.016887664794921875 all layer time: 0.00417780876159668 shape: (128, 10)\n",
      "epoch: 2 batch 237 loss: 0.03146714 acc: 99.21875\n",
      "training time 0.017127275466918945 all layer time: 0.004327535629272461 shape: (128, 10)\n",
      "epoch: 2 batch 238 loss: 0.035466697 acc: 99.21875\n",
      "training time 0.016878604888916016 all layer time: 0.004311561584472656 shape: (128, 10)\n",
      "epoch: 2 batch 239 loss: 0.053397626 acc: 96.875\n",
      "training time 0.017070531845092773 all layer time: 0.0042133331298828125 shape: (128, 10)\n",
      "epoch: 2 batch 240 loss: 0.10721584 acc: 96.09375\n",
      "training time 0.016997814178466797 all layer time: 0.00438380241394043 shape: (128, 10)\n",
      "epoch: 2 batch 241 loss: 0.06103599 acc: 98.4375\n",
      "training time 0.017578125 all layer time: 0.004281520843505859 shape: (128, 10)\n",
      "epoch: 2 batch 242 loss: 0.057673242 acc: 97.65625\n",
      "training time 0.016790390014648438 all layer time: 0.00421595573425293 shape: (128, 10)\n",
      "epoch: 2 batch 243 loss: 0.100798905 acc: 97.65625\n",
      "training time 0.0170590877532959 all layer time: 0.004052877426147461 shape: (128, 10)\n",
      "epoch: 2 batch 244 loss: 0.1266064 acc: 94.53125\n",
      "training time 0.0171811580657959 all layer time: 0.004422664642333984 shape: (128, 10)\n",
      "epoch: 2 batch 245 loss: 0.02833942 acc: 100.0\n",
      "training time 0.016811370849609375 all layer time: 0.0043714046478271484 shape: (128, 10)\n",
      "epoch: 2 batch 246 loss: 0.09118051 acc: 95.3125\n",
      "training time 0.016695261001586914 all layer time: 0.0042307376861572266 shape: (128, 10)\n",
      "epoch: 2 batch 247 loss: 0.15029494 acc: 94.53125\n",
      "training time 0.016770601272583008 all layer time: 0.004221677780151367 shape: (128, 10)\n",
      "epoch: 2 batch 248 loss: 0.10452698 acc: 97.65625\n",
      "training time 0.016984939575195312 all layer time: 0.0043201446533203125 shape: (128, 10)\n",
      "epoch: 2 batch 249 loss: 0.04732353 acc: 99.21875\n",
      "training time 0.016933917999267578 all layer time: 0.004163980484008789 shape: (128, 10)\n",
      "epoch: 2 batch 250 loss: 0.078360386 acc: 97.65625\n",
      "training time 0.016985416412353516 all layer time: 0.004455089569091797 shape: (128, 10)\n",
      "epoch: 2 batch 251 loss: 0.022137487 acc: 100.0\n",
      "training time 0.01678776741027832 all layer time: 0.004312753677368164 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch 252 loss: 0.10581018 acc: 97.65625\n",
      "training time 0.017478227615356445 all layer time: 0.00417780876159668 shape: (128, 10)\n",
      "epoch: 2 batch 253 loss: 0.09607599 acc: 96.09375\n",
      "training time 0.01779007911682129 all layer time: 0.004343509674072266 shape: (128, 10)\n",
      "epoch: 2 batch 254 loss: 0.044765405 acc: 98.4375\n",
      "training time 0.01797652244567871 all layer time: 0.004245281219482422 shape: (128, 10)\n",
      "epoch: 2 batch 255 loss: 0.05552397 acc: 97.65625\n",
      "training time 0.01717376708984375 all layer time: 0.004171133041381836 shape: (128, 10)\n",
      "epoch: 2 batch 256 loss: 0.09104519 acc: 97.65625\n",
      "training time 0.01691269874572754 all layer time: 0.0042040348052978516 shape: (128, 10)\n",
      "epoch: 2 batch 257 loss: 0.012909916 acc: 99.21875\n",
      "training time 0.016737937927246094 all layer time: 0.004270792007446289 shape: (128, 10)\n",
      "epoch: 2 batch 258 loss: 0.08007512 acc: 96.875\n",
      "training time 0.018117904663085938 all layer time: 0.0043642520904541016 shape: (128, 10)\n",
      "epoch: 2 batch 259 loss: 0.034786608 acc: 99.21875\n",
      "training time 0.016642093658447266 all layer time: 0.004243612289428711 shape: (128, 10)\n",
      "epoch: 2 batch 260 loss: 0.11594732 acc: 96.875\n",
      "training time 0.016785621643066406 all layer time: 0.004175424575805664 shape: (128, 10)\n",
      "epoch: 2 batch 261 loss: 0.11001031 acc: 98.4375\n",
      "training time 0.01770472526550293 all layer time: 0.004193782806396484 shape: (128, 10)\n",
      "epoch: 2 batch 262 loss: 0.0405502 acc: 98.4375\n",
      "training time 0.01711249351501465 all layer time: 0.0043065547943115234 shape: (128, 10)\n",
      "epoch: 2 batch 263 loss: 0.0361897 acc: 99.21875\n",
      "training time 0.01714468002319336 all layer time: 0.004155874252319336 shape: (128, 10)\n",
      "epoch: 2 batch 264 loss: 0.028790962 acc: 98.4375\n",
      "training time 0.016903400421142578 all layer time: 0.004251241683959961 shape: (128, 10)\n",
      "epoch: 2 batch 265 loss: 0.015506582 acc: 100.0\n",
      "training time 0.017302989959716797 all layer time: 0.004351139068603516 shape: (128, 10)\n",
      "epoch: 2 batch 266 loss: 0.041298695 acc: 98.4375\n",
      "training time 0.017293453216552734 all layer time: 0.004380226135253906 shape: (128, 10)\n",
      "epoch: 2 batch 267 loss: 0.011387967 acc: 100.0\n",
      "training time 0.017302274703979492 all layer time: 0.004396677017211914 shape: (128, 10)\n",
      "epoch: 2 batch 268 loss: 0.07168174 acc: 97.65625\n",
      "training time 0.017169475555419922 all layer time: 0.004266262054443359 shape: (128, 10)\n",
      "epoch: 2 batch 269 loss: 0.064584196 acc: 98.4375\n",
      "training time 0.017965316772460938 all layer time: 0.0042726993560791016 shape: (128, 10)\n",
      "epoch: 2 batch 270 loss: 0.104203254 acc: 96.875\n",
      "training time 0.017870426177978516 all layer time: 0.004563331604003906 shape: (128, 10)\n",
      "epoch: 2 batch 271 loss: 0.14842299 acc: 94.53125\n",
      "training time 0.018139123916625977 all layer time: 0.004241228103637695 shape: (128, 10)\n",
      "epoch: 2 batch 272 loss: 0.11561709 acc: 96.09375\n",
      "training time 0.0172116756439209 all layer time: 0.004452705383300781 shape: (128, 10)\n",
      "epoch: 2 batch 273 loss: 0.05631589 acc: 97.65625\n",
      "training time 0.01708245277404785 all layer time: 0.004309892654418945 shape: (128, 10)\n",
      "epoch: 2 batch 274 loss: 0.050761186 acc: 98.4375\n",
      "training time 0.01733541488647461 all layer time: 0.004491090774536133 shape: (128, 10)\n",
      "epoch: 2 batch 275 loss: 0.11401483 acc: 96.875\n",
      "training time 0.017675399780273438 all layer time: 0.004380702972412109 shape: (128, 10)\n",
      "epoch: 2 batch 276 loss: 0.02595924 acc: 99.21875\n",
      "training time 0.01717352867126465 all layer time: 0.004252433776855469 shape: (128, 10)\n",
      "epoch: 2 batch 277 loss: 0.191243 acc: 97.65625\n",
      "training time 0.016588926315307617 all layer time: 0.004100322723388672 shape: (128, 10)\n",
      "epoch: 2 batch 278 loss: 0.09530844 acc: 98.4375\n",
      "training time 0.017165660858154297 all layer time: 0.004276752471923828 shape: (128, 10)\n",
      "epoch: 2 batch 279 loss: 0.025567248 acc: 100.0\n",
      "training time 0.01735973358154297 all layer time: 0.004129886627197266 shape: (128, 10)\n",
      "epoch: 2 batch 280 loss: 0.119039185 acc: 96.09375\n",
      "training time 0.017489910125732422 all layer time: 0.004183292388916016 shape: (128, 10)\n",
      "epoch: 2 batch 281 loss: 0.0749876 acc: 97.65625\n",
      "training time 0.01703023910522461 all layer time: 0.0043485164642333984 shape: (128, 10)\n",
      "epoch: 2 batch 282 loss: 0.03930099 acc: 98.4375\n",
      "training time 0.017391681671142578 all layer time: 0.004248619079589844 shape: (128, 10)\n",
      "epoch: 2 batch 283 loss: 0.01352651 acc: 100.0\n",
      "training time 0.016984224319458008 all layer time: 0.004344940185546875 shape: (128, 10)\n",
      "epoch: 2 batch 284 loss: 0.09474641 acc: 95.3125\n",
      "training time 0.017873048782348633 all layer time: 0.00421452522277832 shape: (128, 10)\n",
      "epoch: 2 batch 285 loss: 0.025859382 acc: 100.0\n",
      "training time 0.016753673553466797 all layer time: 0.004228115081787109 shape: (128, 10)\n",
      "epoch: 2 batch 286 loss: 0.039037473 acc: 99.21875\n",
      "training time 0.01690816879272461 all layer time: 0.00410771369934082 shape: (128, 10)\n",
      "epoch: 2 batch 287 loss: 0.09883815 acc: 96.09375\n",
      "training time 0.01708221435546875 all layer time: 0.004290103912353516 shape: (128, 10)\n",
      "epoch: 2 batch 288 loss: 0.029681245 acc: 97.65625\n",
      "training time 0.01685190200805664 all layer time: 0.0043218135833740234 shape: (128, 10)\n",
      "epoch: 2 batch 289 loss: 0.1495198 acc: 96.09375\n",
      "training time 0.017174482345581055 all layer time: 0.004416227340698242 shape: (128, 10)\n",
      "epoch: 2 batch 290 loss: 0.03502355 acc: 98.4375\n",
      "training time 0.017294883728027344 all layer time: 0.004467487335205078 shape: (128, 10)\n",
      "epoch: 2 batch 291 loss: 0.05625014 acc: 98.4375\n",
      "training time 0.01743459701538086 all layer time: 0.004274845123291016 shape: (128, 10)\n",
      "epoch: 2 batch 292 loss: 0.12127559 acc: 95.3125\n",
      "training time 0.017540454864501953 all layer time: 0.004277229309082031 shape: (128, 10)\n",
      "epoch: 2 batch 293 loss: 0.040400606 acc: 99.21875\n",
      "training time 0.01728677749633789 all layer time: 0.004281044006347656 shape: (128, 10)\n",
      "epoch: 2 batch 294 loss: 0.090863615 acc: 96.875\n",
      "training time 0.017041683197021484 all layer time: 0.00410008430480957 shape: (128, 10)\n",
      "epoch: 2 batch 295 loss: 0.07582235 acc: 98.4375\n",
      "training time 0.0165097713470459 all layer time: 0.004262447357177734 shape: (128, 10)\n",
      "epoch: 2 batch 296 loss: 0.074331164 acc: 95.3125\n",
      "training time 0.01693892478942871 all layer time: 0.004439115524291992 shape: (128, 10)\n",
      "epoch: 2 batch 297 loss: 0.020474289 acc: 100.0\n",
      "training time 0.01759934425354004 all layer time: 0.0042455196380615234 shape: (128, 10)\n",
      "epoch: 2 batch 298 loss: 0.028366312 acc: 99.21875\n",
      "training time 0.016361713409423828 all layer time: 0.004302501678466797 shape: (128, 10)\n",
      "epoch: 2 batch 299 loss: 0.07494827 acc: 96.875\n",
      "training time 0.017212629318237305 all layer time: 0.00418400764465332 shape: (128, 10)\n",
      "epoch: 2 batch 300 loss: 0.11144666 acc: 97.65625\n",
      "training time 0.017337322235107422 all layer time: 0.004161357879638672 shape: (128, 10)\n",
      "epoch: 2 batch 301 loss: 0.08028841 acc: 97.65625\n",
      "training time 0.017314672470092773 all layer time: 0.0042018890380859375 shape: (128, 10)\n",
      "epoch: 2 batch 302 loss: 0.0768438 acc: 97.65625\n",
      "training time 0.017406463623046875 all layer time: 0.004178047180175781 shape: (128, 10)\n",
      "epoch: 2 batch 303 loss: 0.012612155 acc: 100.0\n",
      "training time 0.017537355422973633 all layer time: 0.004206180572509766 shape: (128, 10)\n",
      "epoch: 2 batch 304 loss: 0.05507514 acc: 98.4375\n",
      "training time 0.016620397567749023 all layer time: 0.004403352737426758 shape: (128, 10)\n",
      "epoch: 2 batch 305 loss: 0.017566903 acc: 100.0\n",
      "training time 0.017249584197998047 all layer time: 0.004395723342895508 shape: (128, 10)\n",
      "epoch: 2 batch 306 loss: 0.13908571 acc: 96.875\n",
      "training time 0.01667046546936035 all layer time: 0.004332304000854492 shape: (128, 10)\n",
      "epoch: 2 batch 307 loss: 0.15249181 acc: 95.3125\n",
      "training time 0.016963958740234375 all layer time: 0.004288434982299805 shape: (128, 10)\n",
      "epoch: 2 batch 308 loss: 0.045379277 acc: 99.21875\n",
      "training time 0.01712942123413086 all layer time: 0.004338502883911133 shape: (128, 10)\n",
      "epoch: 2 batch 309 loss: 0.08817837 acc: 97.65625\n",
      "training time 0.01696181297302246 all layer time: 0.004494905471801758 shape: (128, 10)\n",
      "epoch: 2 batch 310 loss: 0.05347313 acc: 98.4375\n",
      "training time 0.017837047576904297 all layer time: 0.004302263259887695 shape: (128, 10)\n",
      "epoch: 2 batch 311 loss: 0.017119072 acc: 100.0\n",
      "training time 0.016845226287841797 all layer time: 0.004388332366943359 shape: (128, 10)\n",
      "epoch: 2 batch 312 loss: 0.049647678 acc: 98.4375\n",
      "training time 0.017061710357666016 all layer time: 0.0044176578521728516 shape: (128, 10)\n",
      "epoch: 2 batch 313 loss: 0.06546444 acc: 98.4375\n",
      "training time 0.017022132873535156 all layer time: 0.004258155822753906 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch 314 loss: 0.052285243 acc: 99.21875\n",
      "training time 0.01718592643737793 all layer time: 0.004207611083984375 shape: (128, 10)\n",
      "epoch: 2 batch 315 loss: 0.043770473 acc: 97.65625\n",
      "training time 0.017696142196655273 all layer time: 0.004363536834716797 shape: (128, 10)\n",
      "epoch: 2 batch 316 loss: 0.020078924 acc: 99.21875\n",
      "training time 0.016465187072753906 all layer time: 0.004293918609619141 shape: (128, 10)\n",
      "epoch: 2 batch 317 loss: 0.061245065 acc: 96.875\n",
      "training time 0.016985654830932617 all layer time: 0.004151821136474609 shape: (128, 10)\n",
      "epoch: 2 batch 318 loss: 0.06892291 acc: 98.4375\n",
      "training time 0.01688098907470703 all layer time: 0.0043833255767822266 shape: (128, 10)\n",
      "epoch: 2 batch 319 loss: 0.02654656 acc: 99.21875\n",
      "training time 0.01660299301147461 all layer time: 0.004236936569213867 shape: (128, 10)\n",
      "epoch: 2 batch 320 loss: 0.07601623 acc: 96.875\n",
      "training time 0.016569137573242188 all layer time: 0.004225969314575195 shape: (128, 10)\n",
      "epoch: 2 batch 321 loss: 0.038064986 acc: 99.21875\n",
      "training time 0.017246246337890625 all layer time: 0.004191875457763672 shape: (128, 10)\n",
      "epoch: 2 batch 322 loss: 0.16838393 acc: 96.09375\n",
      "training time 0.017795562744140625 all layer time: 0.004100799560546875 shape: (128, 10)\n",
      "epoch: 2 batch 323 loss: 0.17499173 acc: 94.53125\n",
      "training time 0.017737388610839844 all layer time: 0.004350185394287109 shape: (128, 10)\n",
      "epoch: 2 batch 324 loss: 0.11052712 acc: 96.875\n",
      "training time 0.01694202423095703 all layer time: 0.004236936569213867 shape: (128, 10)\n",
      "epoch: 2 batch 325 loss: 0.031547844 acc: 100.0\n",
      "training time 0.016821861267089844 all layer time: 0.004316091537475586 shape: (128, 10)\n",
      "epoch: 2 batch 326 loss: 0.03950421 acc: 98.4375\n",
      "training time 0.016960859298706055 all layer time: 0.0042073726654052734 shape: (128, 10)\n",
      "epoch: 2 batch 327 loss: 0.052445296 acc: 98.4375\n",
      "training time 0.017278432846069336 all layer time: 0.004357814788818359 shape: (128, 10)\n",
      "epoch: 2 batch 328 loss: 0.07346281 acc: 97.65625\n",
      "training time 0.017018795013427734 all layer time: 0.004233837127685547 shape: (128, 10)\n",
      "epoch: 2 batch 329 loss: 0.05456752 acc: 97.65625\n",
      "training time 0.017154216766357422 all layer time: 0.004342079162597656 shape: (128, 10)\n",
      "epoch: 2 batch 330 loss: 0.058369268 acc: 96.875\n",
      "training time 0.01676654815673828 all layer time: 0.004297971725463867 shape: (128, 10)\n",
      "epoch: 2 batch 331 loss: 0.11054993 acc: 97.65625\n",
      "training time 0.016842365264892578 all layer time: 0.004328489303588867 shape: (128, 10)\n",
      "epoch: 2 batch 332 loss: 0.073603876 acc: 96.09375\n",
      "training time 0.017195940017700195 all layer time: 0.0054950714111328125 shape: (128, 10)\n",
      "epoch: 2 batch 333 loss: 0.09497028 acc: 96.875\n",
      "training time 0.016939640045166016 all layer time: 0.004373788833618164 shape: (128, 10)\n",
      "epoch: 2 batch 334 loss: 0.1295597 acc: 96.875\n",
      "training time 0.017404794692993164 all layer time: 0.004168510437011719 shape: (128, 10)\n",
      "epoch: 2 batch 335 loss: 0.046679493 acc: 98.4375\n",
      "training time 0.017801523208618164 all layer time: 0.004283428192138672 shape: (128, 10)\n",
      "epoch: 2 batch 336 loss: 0.13514248 acc: 95.3125\n",
      "training time 0.0175478458404541 all layer time: 0.004309415817260742 shape: (128, 10)\n",
      "epoch: 2 batch 337 loss: 0.019034922 acc: 100.0\n",
      "training time 0.01718902587890625 all layer time: 0.00435328483581543 shape: (128, 10)\n",
      "epoch: 2 batch 338 loss: 0.017436758 acc: 100.0\n",
      "training time 0.0168612003326416 all layer time: 0.004260540008544922 shape: (128, 10)\n",
      "epoch: 2 batch 339 loss: 0.101531275 acc: 98.4375\n",
      "training time 0.0170133113861084 all layer time: 0.004198551177978516 shape: (128, 10)\n",
      "epoch: 2 batch 340 loss: 0.061755948 acc: 97.65625\n",
      "training time 0.01695561408996582 all layer time: 0.004214048385620117 shape: (128, 10)\n",
      "epoch: 2 batch 341 loss: 0.06792182 acc: 96.875\n",
      "training time 0.017992258071899414 all layer time: 0.0043871402740478516 shape: (128, 10)\n",
      "epoch: 2 batch 342 loss: 0.050100222 acc: 98.4375\n",
      "training time 0.016831398010253906 all layer time: 0.0041790008544921875 shape: (128, 10)\n",
      "epoch: 2 batch 343 loss: 0.07398302 acc: 98.4375\n",
      "training time 0.0173494815826416 all layer time: 0.004273891448974609 shape: (128, 10)\n",
      "epoch: 2 batch 344 loss: 0.033664048 acc: 100.0\n",
      "training time 0.0171053409576416 all layer time: 0.0043108463287353516 shape: (128, 10)\n",
      "epoch: 2 batch 345 loss: 0.040380202 acc: 98.4375\n",
      "training time 0.017563581466674805 all layer time: 0.004488229751586914 shape: (128, 10)\n",
      "epoch: 2 batch 346 loss: 0.050705966 acc: 99.21875\n",
      "training time 0.01703786849975586 all layer time: 0.004222393035888672 shape: (128, 10)\n",
      "epoch: 2 batch 347 loss: 0.13253255 acc: 96.875\n",
      "training time 0.01689434051513672 all layer time: 0.004296064376831055 shape: (128, 10)\n",
      "epoch: 2 batch 348 loss: 0.023193317 acc: 99.21875\n",
      "training time 0.016583919525146484 all layer time: 0.004316568374633789 shape: (128, 10)\n",
      "epoch: 2 batch 349 loss: 0.052330267 acc: 98.4375\n",
      "training time 0.016919612884521484 all layer time: 0.004287242889404297 shape: (128, 10)\n",
      "epoch: 2 batch 350 loss: 0.09027998 acc: 97.65625\n",
      "training time 0.017060041427612305 all layer time: 0.004214286804199219 shape: (128, 10)\n",
      "epoch: 2 batch 351 loss: 0.03753181 acc: 98.4375\n",
      "training time 0.017123699188232422 all layer time: 0.0041844844818115234 shape: (128, 10)\n",
      "epoch: 2 batch 352 loss: 0.03395197 acc: 99.21875\n",
      "training time 0.01693272590637207 all layer time: 0.0042667388916015625 shape: (128, 10)\n",
      "epoch: 2 batch 353 loss: 0.014342621 acc: 99.21875\n",
      "training time 0.017178058624267578 all layer time: 0.004330873489379883 shape: (128, 10)\n",
      "epoch: 2 batch 354 loss: 0.10552728 acc: 97.65625\n",
      "training time 0.017314910888671875 all layer time: 0.0043408870697021484 shape: (128, 10)\n",
      "epoch: 2 batch 355 loss: 0.07679256 acc: 97.65625\n",
      "training time 0.01711106300354004 all layer time: 0.004134416580200195 shape: (128, 10)\n",
      "epoch: 2 batch 356 loss: 0.050389454 acc: 98.4375\n",
      "training time 0.017070770263671875 all layer time: 0.004307270050048828 shape: (128, 10)\n",
      "epoch: 2 batch 357 loss: 0.09054983 acc: 96.875\n",
      "training time 0.017299413681030273 all layer time: 0.004369497299194336 shape: (128, 10)\n",
      "epoch: 2 batch 358 loss: 0.11111091 acc: 94.53125\n",
      "training time 0.017344951629638672 all layer time: 0.0042819976806640625 shape: (128, 10)\n",
      "epoch: 2 batch 359 loss: 0.078591645 acc: 97.65625\n",
      "training time 0.01700901985168457 all layer time: 0.004210472106933594 shape: (128, 10)\n",
      "epoch: 2 batch 360 loss: 0.073048756 acc: 99.21875\n",
      "training time 0.01701068878173828 all layer time: 0.004323720932006836 shape: (128, 10)\n",
      "epoch: 2 batch 361 loss: 0.13203123 acc: 95.3125\n",
      "training time 0.016791105270385742 all layer time: 0.004283428192138672 shape: (128, 10)\n",
      "epoch: 2 batch 362 loss: 0.08615662 acc: 96.875\n",
      "training time 0.016867637634277344 all layer time: 0.00421905517578125 shape: (128, 10)\n",
      "epoch: 2 batch 363 loss: 0.04125744 acc: 99.21875\n",
      "training time 0.01725602149963379 all layer time: 0.00415492057800293 shape: (128, 10)\n",
      "epoch: 2 batch 364 loss: 0.022417147 acc: 100.0\n",
      "training time 0.017339706420898438 all layer time: 0.0041010379791259766 shape: (128, 10)\n",
      "epoch: 2 batch 365 loss: 0.106139615 acc: 96.875\n",
      "training time 0.017220020294189453 all layer time: 0.004461050033569336 shape: (128, 10)\n",
      "epoch: 2 batch 366 loss: 0.03060458 acc: 99.21875\n",
      "training time 0.017278194427490234 all layer time: 0.004436492919921875 shape: (128, 10)\n",
      "epoch: 2 batch 367 loss: 0.097524635 acc: 95.3125\n",
      "training time 0.017015695571899414 all layer time: 0.0044307708740234375 shape: (128, 10)\n",
      "epoch: 2 batch 368 loss: 0.04842 acc: 98.4375\n",
      "training time 0.01727461814880371 all layer time: 0.004255533218383789 shape: (128, 10)\n",
      "epoch: 2 batch 369 loss: 0.14389421 acc: 96.875\n",
      "training time 0.01755213737487793 all layer time: 0.004206180572509766 shape: (128, 10)\n",
      "epoch: 2 batch 370 loss: 0.081541404 acc: 96.09375\n",
      "training time 0.017101526260375977 all layer time: 0.00416874885559082 shape: (128, 10)\n",
      "epoch: 2 batch 371 loss: 0.0763329 acc: 96.09375\n",
      "training time 0.017275333404541016 all layer time: 0.0042171478271484375 shape: (128, 10)\n",
      "epoch: 2 batch 372 loss: 0.017098581 acc: 100.0\n",
      "training time 0.0171356201171875 all layer time: 0.004205226898193359 shape: (128, 10)\n",
      "epoch: 2 batch 373 loss: 0.05232043 acc: 99.21875\n",
      "training time 0.017189502716064453 all layer time: 0.0043408870697021484 shape: (128, 10)\n",
      "epoch: 2 batch 374 loss: 0.055946413 acc: 98.4375\n",
      "training time 0.016750335693359375 all layer time: 0.00434112548828125 shape: (128, 10)\n",
      "epoch: 2 batch 375 loss: 0.04136197 acc: 99.21875\n",
      "training time 0.017015933990478516 all layer time: 0.0044672489166259766 shape: (128, 10)\n",
      "epoch: 2 batch 376 loss: 0.021673232 acc: 99.21875\n",
      "training time 0.017391204833984375 all layer time: 0.0043222904205322266 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch 377 loss: 0.09843659 acc: 97.65625\n",
      "training time 0.017427682876586914 all layer time: 0.004190206527709961 shape: (128, 10)\n",
      "epoch: 2 batch 378 loss: 0.011305928 acc: 100.0\n",
      "training time 0.017493009567260742 all layer time: 0.004255771636962891 shape: (128, 10)\n",
      "epoch: 2 batch 379 loss: 0.023391988 acc: 100.0\n",
      "training time 0.017163991928100586 all layer time: 0.004239797592163086 shape: (128, 10)\n",
      "epoch: 2 batch 380 loss: 0.02211948 acc: 99.21875\n",
      "training time 0.017258167266845703 all layer time: 0.0041005611419677734 shape: (128, 10)\n",
      "epoch: 2 batch 381 loss: 0.02173864 acc: 99.21875\n",
      "training time 0.01695561408996582 all layer time: 0.004187822341918945 shape: (128, 10)\n",
      "epoch: 2 batch 382 loss: 0.15233217 acc: 95.3125\n",
      "training time 0.016813278198242188 all layer time: 0.0042264461517333984 shape: (128, 10)\n",
      "epoch: 2 batch 383 loss: 0.08423487 acc: 96.09375\n",
      "training time 0.01695108413696289 all layer time: 0.004204750061035156 shape: (128, 10)\n",
      "epoch: 2 batch 384 loss: 0.09845973 acc: 96.875\n",
      "training time 0.01867818832397461 all layer time: 0.004300117492675781 shape: (128, 10)\n",
      "epoch: 2 batch 385 loss: 0.023610367 acc: 99.21875\n",
      "training time 0.017011165618896484 all layer time: 0.0042667388916015625 shape: (128, 10)\n",
      "epoch: 2 batch 386 loss: 0.104955465 acc: 96.875\n",
      "training time 0.016927719116210938 all layer time: 0.004193782806396484 shape: (128, 10)\n",
      "epoch: 2 batch 387 loss: 0.1100323 acc: 96.875\n",
      "training time 0.017192840576171875 all layer time: 0.004212141036987305 shape: (128, 10)\n",
      "epoch: 2 batch 388 loss: 0.032702707 acc: 99.21875\n",
      "training time 0.017073869705200195 all layer time: 0.004229545593261719 shape: (128, 10)\n",
      "epoch: 2 batch 389 loss: 0.08920163 acc: 95.3125\n",
      "training time 0.01743912696838379 all layer time: 0.004088640213012695 shape: (128, 10)\n",
      "epoch: 2 batch 390 loss: 0.031294324 acc: 100.0\n",
      "training time 0.01671910285949707 all layer time: 0.004244327545166016 shape: (128, 10)\n",
      "epoch: 2 batch 391 loss: 0.027886964 acc: 100.0\n",
      "training time 0.016843795776367188 all layer time: 0.004244327545166016 shape: (128, 10)\n",
      "epoch: 2 batch 392 loss: 0.08555965 acc: 96.875\n",
      "training time 0.017090797424316406 all layer time: 0.00423431396484375 shape: (128, 10)\n",
      "epoch: 2 batch 393 loss: 0.1463454 acc: 92.96875\n",
      "training time 0.01785731315612793 all layer time: 0.004255771636962891 shape: (128, 10)\n",
      "epoch: 2 batch 394 loss: 0.037075013 acc: 99.21875\n",
      "training time 0.017389774322509766 all layer time: 0.004175662994384766 shape: (128, 10)\n",
      "epoch: 2 batch 395 loss: 0.019126637 acc: 99.21875\n",
      "training time 0.016982316970825195 all layer time: 0.004158496856689453 shape: (128, 10)\n",
      "epoch: 2 batch 396 loss: 0.056946255 acc: 98.4375\n",
      "training time 0.01676487922668457 all layer time: 0.00424647331237793 shape: (128, 10)\n",
      "epoch: 2 batch 397 loss: 0.07318884 acc: 97.65625\n",
      "training time 0.01780223846435547 all layer time: 0.0045664310455322266 shape: (128, 10)\n",
      "epoch: 2 batch 398 loss: 0.02340404 acc: 99.21875\n",
      "training time 0.017239093780517578 all layer time: 0.004293203353881836 shape: (128, 10)\n",
      "epoch: 2 batch 399 loss: 0.042860758 acc: 97.65625\n",
      "training time 0.016875028610229492 all layer time: 0.004276275634765625 shape: (128, 10)\n",
      "epoch: 2 batch 400 loss: 0.14808048 acc: 96.09375\n",
      "training time 0.017439603805541992 all layer time: 0.004285097122192383 shape: (128, 10)\n",
      "epoch: 2 batch 401 loss: 0.022498388 acc: 98.4375\n",
      "training time 0.018149137496948242 all layer time: 0.004567384719848633 shape: (128, 10)\n",
      "epoch: 2 batch 402 loss: 0.06388514 acc: 96.875\n",
      "training time 0.017498254776000977 all layer time: 0.0042879581451416016 shape: (128, 10)\n",
      "epoch: 2 batch 403 loss: 0.08688669 acc: 98.4375\n",
      "training time 0.01711893081665039 all layer time: 0.00438690185546875 shape: (128, 10)\n",
      "epoch: 2 batch 404 loss: 0.06360106 acc: 99.21875\n",
      "training time 0.01743006706237793 all layer time: 0.0042667388916015625 shape: (128, 10)\n",
      "epoch: 2 batch 405 loss: 0.08171819 acc: 98.4375\n",
      "training time 0.01744222640991211 all layer time: 0.0044252872467041016 shape: (128, 10)\n",
      "epoch: 2 batch 406 loss: 0.08537018 acc: 96.875\n",
      "training time 0.017589330673217773 all layer time: 0.004247426986694336 shape: (128, 10)\n",
      "epoch: 2 batch 407 loss: 0.11417477 acc: 96.875\n",
      "training time 0.017180919647216797 all layer time: 0.004339933395385742 shape: (128, 10)\n",
      "epoch: 2 batch 408 loss: 0.13141535 acc: 96.875\n",
      "training time 0.017486095428466797 all layer time: 0.004191160202026367 shape: (128, 10)\n",
      "epoch: 2 batch 409 loss: 0.023176637 acc: 100.0\n",
      "training time 0.017148733139038086 all layer time: 0.0043125152587890625 shape: (128, 10)\n",
      "epoch: 2 batch 410 loss: 0.019085936 acc: 99.21875\n",
      "training time 0.017048358917236328 all layer time: 0.004311561584472656 shape: (128, 10)\n",
      "epoch: 2 batch 411 loss: 0.050501227 acc: 99.21875\n",
      "training time 0.01711869239807129 all layer time: 0.004378557205200195 shape: (128, 10)\n",
      "epoch: 2 batch 412 loss: 0.076670974 acc: 98.4375\n",
      "training time 0.017216920852661133 all layer time: 0.004323482513427734 shape: (128, 10)\n",
      "epoch: 2 batch 413 loss: 0.16653052 acc: 96.09375\n",
      "training time 0.017136812210083008 all layer time: 0.004233598709106445 shape: (128, 10)\n",
      "epoch: 2 batch 414 loss: 0.037083194 acc: 98.4375\n",
      "training time 0.017518043518066406 all layer time: 0.004131317138671875 shape: (128, 10)\n",
      "epoch: 2 batch 415 loss: 0.120197065 acc: 96.09375\n",
      "training time 0.016845703125 all layer time: 0.004439830780029297 shape: (128, 10)\n",
      "epoch: 2 batch 416 loss: 0.010555625 acc: 100.0\n",
      "training time 0.016826629638671875 all layer time: 0.0041539669036865234 shape: (128, 10)\n",
      "epoch: 2 batch 417 loss: 0.08867756 acc: 96.875\n",
      "training time 0.017066001892089844 all layer time: 0.00423431396484375 shape: (128, 10)\n",
      "epoch: 2 batch 418 loss: 0.015188183 acc: 99.21875\n",
      "training time 0.01705193519592285 all layer time: 0.004294872283935547 shape: (128, 10)\n",
      "epoch: 2 batch 419 loss: 0.10033326 acc: 96.875\n",
      "training time 0.01769089698791504 all layer time: 0.004297971725463867 shape: (128, 10)\n",
      "epoch: 2 batch 420 loss: 0.15932915 acc: 95.3125\n",
      "training time 0.017127037048339844 all layer time: 0.004231691360473633 shape: (128, 10)\n",
      "epoch: 2 batch 421 loss: 0.05212532 acc: 97.65625\n",
      "training time 0.017095327377319336 all layer time: 0.004206418991088867 shape: (128, 10)\n",
      "epoch: 2 batch 422 loss: 0.055357963 acc: 98.4375\n",
      "training time 0.01683187484741211 all layer time: 0.004224061965942383 shape: (128, 10)\n",
      "epoch: 2 batch 423 loss: 0.033905365 acc: 98.4375\n",
      "training time 0.01762247085571289 all layer time: 0.004248142242431641 shape: (128, 10)\n",
      "epoch: 2 batch 424 loss: 0.03976074 acc: 98.4375\n",
      "training time 0.017505884170532227 all layer time: 0.004299163818359375 shape: (128, 10)\n",
      "epoch: 2 batch 425 loss: 0.045549855 acc: 99.21875\n",
      "training time 0.0171663761138916 all layer time: 0.004180192947387695 shape: (128, 10)\n",
      "epoch: 2 batch 426 loss: 0.043256182 acc: 99.21875\n",
      "training time 0.01692485809326172 all layer time: 0.0042400360107421875 shape: (128, 10)\n",
      "epoch: 2 batch 427 loss: 0.049192205 acc: 97.65625\n",
      "training time 0.01786017417907715 all layer time: 0.00438237190246582 shape: (128, 10)\n",
      "epoch: 2 batch 428 loss: 0.061689116 acc: 97.65625\n",
      "training time 0.01760101318359375 all layer time: 0.0044896602630615234 shape: (128, 10)\n",
      "epoch: 2 batch 429 loss: 0.029282495 acc: 100.0\n",
      "training time 0.016980886459350586 all layer time: 0.004224538803100586 shape: (128, 10)\n",
      "epoch: 2 batch 430 loss: 0.02631288 acc: 99.21875\n",
      "training time 0.016962528228759766 all layer time: 0.004313945770263672 shape: (128, 10)\n",
      "epoch: 2 batch 431 loss: 0.028314004 acc: 99.21875\n",
      "training time 0.017584562301635742 all layer time: 0.004251718521118164 shape: (128, 10)\n",
      "epoch: 2 batch 432 loss: 0.05997644 acc: 98.4375\n",
      "training time 0.0171968936920166 all layer time: 0.004548072814941406 shape: (128, 10)\n",
      "epoch: 2 batch 433 loss: 0.021709487 acc: 100.0\n",
      "training time 0.01691269874572754 all layer time: 0.004120349884033203 shape: (128, 10)\n",
      "epoch: 2 batch 434 loss: 0.029014772 acc: 98.4375\n",
      "training time 0.017251014709472656 all layer time: 0.004494190216064453 shape: (128, 10)\n",
      "epoch: 2 batch 435 loss: 0.04744371 acc: 98.4375\n",
      "training time 0.016825437545776367 all layer time: 0.0042400360107421875 shape: (128, 10)\n",
      "epoch: 2 batch 436 loss: 0.06690203 acc: 96.875\n",
      "training time 0.0173337459564209 all layer time: 0.004236698150634766 shape: (128, 10)\n",
      "epoch: 2 batch 437 loss: 0.020746812 acc: 99.21875\n",
      "training time 0.01699686050415039 all layer time: 0.004598140716552734 shape: (128, 10)\n",
      "epoch: 2 batch 438 loss: 0.038242944 acc: 99.21875\n",
      "training time 0.01710820198059082 all layer time: 0.0043370723724365234 shape: (128, 10)\n",
      "epoch: 2 batch 439 loss: 0.082514584 acc: 97.65625\n",
      "training time 0.01685619354248047 all layer time: 0.004299640655517578 shape: (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 batch 440 loss: 0.04191071 acc: 99.21875\n",
      "training time 0.017086267471313477 all layer time: 0.0042226314544677734 shape: (128, 10)\n",
      "epoch: 2 batch 441 loss: 0.07643046 acc: 97.65625\n",
      "training time 0.017267942428588867 all layer time: 0.004179477691650391 shape: (128, 10)\n",
      "epoch: 2 batch 442 loss: 0.053510208 acc: 98.4375\n",
      "training time 0.01687455177307129 all layer time: 0.004344940185546875 shape: (128, 10)\n",
      "epoch: 2 batch 443 loss: 0.077983454 acc: 96.875\n",
      "training time 0.01704096794128418 all layer time: 0.0042018890380859375 shape: (128, 10)\n",
      "epoch: 2 batch 444 loss: 0.06729982 acc: 97.65625\n",
      "training time 0.01761937141418457 all layer time: 0.0042688846588134766 shape: (128, 10)\n",
      "epoch: 2 batch 445 loss: 0.046110433 acc: 98.4375\n",
      "training time 0.016768217086791992 all layer time: 0.004172325134277344 shape: (128, 10)\n",
      "epoch: 2 batch 446 loss: 0.008566316 acc: 100.0\n",
      "training time 0.017474889755249023 all layer time: 0.004242420196533203 shape: (128, 10)\n",
      "epoch: 2 batch 447 loss: 0.08563073 acc: 96.09375\n",
      "training time 0.016877174377441406 all layer time: 0.004282236099243164 shape: (128, 10)\n",
      "epoch: 2 batch 448 loss: 0.060588084 acc: 97.65625\n",
      "training time 0.017101049423217773 all layer time: 0.0043125152587890625 shape: (128, 10)\n",
      "epoch: 2 batch 449 loss: 0.022262221 acc: 100.0\n",
      "training time 0.0175168514251709 all layer time: 0.004237174987792969 shape: (128, 10)\n",
      "epoch: 2 batch 450 loss: 0.04424147 acc: 99.21875\n",
      "training time 0.016964435577392578 all layer time: 0.0042400360107421875 shape: (128, 10)\n",
      "epoch: 2 batch 451 loss: 0.106600195 acc: 97.65625\n",
      "training time 0.01653003692626953 all layer time: 0.004259824752807617 shape: (128, 10)\n",
      "epoch: 2 batch 452 loss: 0.056248352 acc: 98.4375\n",
      "training time 0.01727747917175293 all layer time: 0.004477024078369141 shape: (128, 10)\n",
      "epoch: 2 batch 453 loss: 0.036432985 acc: 98.4375\n",
      "training time 0.01798081398010254 all layer time: 0.00440216064453125 shape: (128, 10)\n",
      "epoch: 2 batch 454 loss: 0.0027383044 acc: 100.0\n",
      "training time 0.01654529571533203 all layer time: 0.004328250885009766 shape: (128, 10)\n",
      "epoch: 2 batch 455 loss: 0.011525753 acc: 100.0\n",
      "training time 0.017370223999023438 all layer time: 0.004366874694824219 shape: (128, 10)\n",
      "epoch: 2 batch 456 loss: 0.026530772 acc: 99.21875\n",
      "training time 0.01693558692932129 all layer time: 0.004240751266479492 shape: (128, 10)\n",
      "epoch: 2 batch 457 loss: 0.0067200284 acc: 100.0\n",
      "training time 0.01715564727783203 all layer time: 0.00433802604675293 shape: (128, 10)\n",
      "epoch: 2 batch 458 loss: 0.0066827945 acc: 100.0\n",
      "training time 0.017353057861328125 all layer time: 0.0042607784271240234 shape: (128, 10)\n",
      "epoch: 2 batch 459 loss: 0.05137083 acc: 97.65625\n",
      "training time 0.01698017120361328 all layer time: 0.0042569637298583984 shape: (128, 10)\n",
      "epoch: 2 batch 460 loss: 0.0023497338 acc: 100.0\n",
      "training time 0.017095565795898438 all layer time: 0.004326581954956055 shape: (128, 10)\n",
      "epoch: 2 batch 461 loss: 0.0018605448 acc: 100.0\n",
      "training time 0.017688751220703125 all layer time: 0.004407405853271484 shape: (128, 10)\n",
      "epoch: 2 batch 462 loss: 0.0015926885 acc: 100.0\n",
      "training time 0.01736736297607422 all layer time: 0.00436854362487793 shape: (128, 10)\n",
      "epoch: 2 batch 463 loss: 0.045600686 acc: 98.4375\n",
      "training time 0.01711249351501465 all layer time: 0.004317760467529297 shape: (128, 10)\n",
      "epoch: 2 batch 464 loss: 0.025823722 acc: 100.0\n",
      "training time 0.017253875732421875 all layer time: 0.004356861114501953 shape: (128, 10)\n",
      "epoch: 2 batch 465 loss: 0.00291683 acc: 100.0\n",
      "training time 0.017285585403442383 all layer time: 0.004271507263183594 shape: (128, 10)\n",
      "epoch: 2 batch 466 loss: 0.25520402 acc: 94.53125\n",
      "training time 0.017535686492919922 all layer time: 0.004415988922119141 shape: (128, 10)\n",
      "epoch: 2 batch 467 loss: 0.012575943 acc: 100.0\n",
      "training time 0.017210006713867188 all layer time: 0.004240989685058594 shape: (128, 10)\n",
      "before\n",
      "[0.0260438  0.         0.         0.07481608 0.03618006 0.\n",
      " 0.         0.00037677 0.         0.02194145 0.         0.03495533\n",
      " 0.0109588  0.00170013 0.03731549 0.01487227 0.         0.\n",
      " 0.05138506 0.07107769 0.01474862 0.         0.         0.06366953\n",
      " 0.         0.05969064 0.         0.05294572 0.06732398 0.\n",
      " 0.06366065 0.02358042]\n",
      "after\n",
      "[0.02634172 0.         0.         0.07523811 0.0362772  0.\n",
      " 0.         0.         0.         0.02236722 0.         0.03511516\n",
      " 0.011319   0.00137858 0.03681935 0.01495694 0.         0.\n",
      " 0.05085367 0.07105694 0.01443977 0.         0.         0.06350697\n",
      " 0.         0.05947938 0.         0.05337152 0.06716904 0.\n",
      " 0.06368347 0.02342833]\n",
      "Test loss: 0.03895613919443713\n",
      "Test accuracy: 0.9866999983787537\n"
     ]
    }
   ],
   "source": [
    "# from the official keras documentation: https://keras.io/examples/mnist_cnn/ get the MNIST part\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keract import get_activations\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#model definition\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "'''\n",
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(data)\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "A weighted version of keras.objectives.categorical_crossentropy\n",
    "\n",
    "Variables:\n",
    "    weights: numpy array of shape (C,) where C is the number of classes\n",
    "\n",
    "Usage:\n",
    "    weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "    loss = weighted_categorical_crossentropy(weights)\n",
    "    model.compile(loss=loss,optimizer='adam')\n",
    "\"\"\"\n",
    "def weighted_categorical_crossentropy(weights, input_tensor):\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        \n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print('layer names:')\n",
    "for layer in model.layers:\n",
    "    print(layer.name)\n",
    "    \n",
    "get_first_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[0].output])\n",
    "get_second_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[1].output])\n",
    "\n",
    "get_last_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[7].output])\n",
    "print(len(model.layers))\n",
    "\n",
    "weights = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "input_tensor = Input(shape=(batch_size,))\n",
    "\n",
    "model.compile(loss=weighted_categorical_crossentropy(weights,input_tensor),\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 3\n",
    "n_batches = math.floor( x_train.shape[0] / batch_size)\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    for i in range(0,n_batches):\n",
    "        imgs = x_train[i*batch_size:(i+1)*batch_size]\n",
    "        labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        intermediates_before = get_first_layer_output([imgs])\n",
    "        \n",
    "        start_time1 = time.time()\n",
    "        loss = model.train_on_batch(imgs,labels)\n",
    "        timer1 = time.time() - start_time1\n",
    "        print(\"epoch: \" + str(e) + \" batch \" + str(i) + \" loss: \" + str(loss[0]) + \" acc: \" + str( 100*loss[1]))\n",
    "        \n",
    "        intermediates_after = get_first_layer_output([imgs])\n",
    "        \n",
    "        start_time2 = time.time()\n",
    "        tmp = get_last_layer_output([imgs])[0]\n",
    "        \n",
    "        timer2 = time.time() - start_time2\n",
    "        print('training time ' + str(timer1) + ' all layer time: ' + str(timer2) + ' shape: ' + str(tmp.shape))\n",
    "    \n",
    "    print('before')\n",
    "    print(intermediates_before[0][0][0][0])\n",
    "    print('after')\n",
    "    print(intermediates_after[0][0][0][0])\n",
    "\n",
    "'''\n",
    "for i in range(batches):\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    imgs = x_train[idx]\n",
    "    labels = y_train[idx] \n",
    "    loss = model.train_on_batch(imgs,labels)\n",
    "    print(\" loss: \" + str(loss[0]) + \" acc: \" + str( 100*loss[1]))\n",
    "'''   \n",
    "    \n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.578585, acc.: 46.88%] [G loss: 0.699700]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.354314, acc.: 78.12%] [G loss: 0.723980]\n",
      "2 [D loss: 0.317373, acc.: 93.75%] [G loss: 0.717782]\n",
      "3 [D loss: 0.287278, acc.: 100.00%] [G loss: 0.822010]\n",
      "4 [D loss: 0.295507, acc.: 87.50%] [G loss: 0.922056]\n",
      "5 [D loss: 0.282318, acc.: 96.88%] [G loss: 1.122179]\n",
      "6 [D loss: 0.237372, acc.: 93.75%] [G loss: 1.292230]\n",
      "7 [D loss: 0.177808, acc.: 100.00%] [G loss: 1.474605]\n",
      "8 [D loss: 0.163413, acc.: 100.00%] [G loss: 1.670371]\n",
      "9 [D loss: 0.147224, acc.: 100.00%] [G loss: 1.777976]\n",
      "10 [D loss: 0.169061, acc.: 96.88%] [G loss: 2.014380]\n",
      "11 [D loss: 0.113736, acc.: 100.00%] [G loss: 2.182868]\n",
      "12 [D loss: 0.093973, acc.: 100.00%] [G loss: 2.331012]\n",
      "13 [D loss: 0.065767, acc.: 100.00%] [G loss: 2.482884]\n",
      "14 [D loss: 0.079054, acc.: 100.00%] [G loss: 2.598572]\n",
      "15 [D loss: 0.062686, acc.: 100.00%] [G loss: 2.669461]\n",
      "16 [D loss: 0.072377, acc.: 100.00%] [G loss: 2.699196]\n",
      "17 [D loss: 0.045911, acc.: 100.00%] [G loss: 2.788925]\n",
      "18 [D loss: 0.044632, acc.: 100.00%] [G loss: 2.855821]\n",
      "19 [D loss: 0.042971, acc.: 100.00%] [G loss: 2.923053]\n",
      "20 [D loss: 0.029973, acc.: 100.00%] [G loss: 2.994072]\n",
      "21 [D loss: 0.042950, acc.: 100.00%] [G loss: 3.063731]\n",
      "22 [D loss: 0.034214, acc.: 100.00%] [G loss: 3.243902]\n",
      "23 [D loss: 0.029763, acc.: 100.00%] [G loss: 3.076954]\n",
      "24 [D loss: 0.034413, acc.: 100.00%] [G loss: 3.178521]\n",
      "25 [D loss: 0.024127, acc.: 100.00%] [G loss: 3.352545]\n",
      "26 [D loss: 0.032773, acc.: 100.00%] [G loss: 3.361336]\n",
      "27 [D loss: 0.032851, acc.: 100.00%] [G loss: 3.413721]\n",
      "28 [D loss: 0.021017, acc.: 100.00%] [G loss: 3.514873]\n",
      "29 [D loss: 0.014622, acc.: 100.00%] [G loss: 3.579936]\n",
      "30 [D loss: 0.027089, acc.: 100.00%] [G loss: 3.513592]\n",
      "31 [D loss: 0.017234, acc.: 100.00%] [G loss: 3.580800]\n",
      "32 [D loss: 0.024580, acc.: 100.00%] [G loss: 3.550110]\n",
      "33 [D loss: 0.015915, acc.: 100.00%] [G loss: 3.616603]\n",
      "34 [D loss: 0.019889, acc.: 100.00%] [G loss: 3.710729]\n",
      "35 [D loss: 0.026633, acc.: 100.00%] [G loss: 3.659901]\n",
      "36 [D loss: 0.013374, acc.: 100.00%] [G loss: 3.572917]\n",
      "37 [D loss: 0.022369, acc.: 100.00%] [G loss: 3.949333]\n",
      "38 [D loss: 0.025170, acc.: 100.00%] [G loss: 3.908626]\n",
      "39 [D loss: 0.013254, acc.: 100.00%] [G loss: 4.053905]\n",
      "40 [D loss: 0.015888, acc.: 100.00%] [G loss: 3.858311]\n",
      "41 [D loss: 0.013320, acc.: 100.00%] [G loss: 3.830083]\n",
      "42 [D loss: 0.019573, acc.: 100.00%] [G loss: 3.932526]\n",
      "43 [D loss: 0.019190, acc.: 100.00%] [G loss: 3.878027]\n",
      "44 [D loss: 0.012951, acc.: 100.00%] [G loss: 3.839694]\n",
      "45 [D loss: 0.010301, acc.: 100.00%] [G loss: 4.175656]\n",
      "46 [D loss: 0.021548, acc.: 100.00%] [G loss: 3.923019]\n",
      "47 [D loss: 0.018448, acc.: 100.00%] [G loss: 4.134804]\n",
      "48 [D loss: 0.010127, acc.: 100.00%] [G loss: 4.001863]\n",
      "49 [D loss: 0.012992, acc.: 100.00%] [G loss: 4.191327]\n",
      "50 [D loss: 0.021265, acc.: 100.00%] [G loss: 4.040501]\n",
      "51 [D loss: 0.015779, acc.: 100.00%] [G loss: 4.258946]\n",
      "52 [D loss: 0.014599, acc.: 100.00%] [G loss: 4.400567]\n",
      "53 [D loss: 0.014305, acc.: 100.00%] [G loss: 4.226403]\n",
      "54 [D loss: 0.011176, acc.: 100.00%] [G loss: 4.227910]\n",
      "55 [D loss: 0.014824, acc.: 100.00%] [G loss: 4.430920]\n",
      "56 [D loss: 0.008268, acc.: 100.00%] [G loss: 4.346193]\n",
      "57 [D loss: 0.011491, acc.: 100.00%] [G loss: 4.309917]\n",
      "58 [D loss: 0.011834, acc.: 100.00%] [G loss: 4.327695]\n",
      "59 [D loss: 0.013321, acc.: 100.00%] [G loss: 4.247735]\n",
      "60 [D loss: 0.009762, acc.: 100.00%] [G loss: 4.343680]\n",
      "61 [D loss: 0.009764, acc.: 100.00%] [G loss: 4.216566]\n",
      "62 [D loss: 0.019940, acc.: 100.00%] [G loss: 4.320264]\n",
      "63 [D loss: 0.008622, acc.: 100.00%] [G loss: 4.408942]\n",
      "64 [D loss: 0.013101, acc.: 100.00%] [G loss: 4.470275]\n",
      "65 [D loss: 0.014527, acc.: 100.00%] [G loss: 4.404842]\n",
      "66 [D loss: 0.009758, acc.: 100.00%] [G loss: 4.544692]\n",
      "67 [D loss: 0.006309, acc.: 100.00%] [G loss: 4.435243]\n",
      "68 [D loss: 0.009264, acc.: 100.00%] [G loss: 4.315001]\n",
      "69 [D loss: 0.017848, acc.: 100.00%] [G loss: 4.552242]\n",
      "70 [D loss: 0.015442, acc.: 100.00%] [G loss: 4.689888]\n",
      "71 [D loss: 0.011885, acc.: 100.00%] [G loss: 4.592043]\n",
      "72 [D loss: 0.019049, acc.: 100.00%] [G loss: 4.781352]\n",
      "73 [D loss: 0.010151, acc.: 100.00%] [G loss: 4.722620]\n",
      "74 [D loss: 0.010295, acc.: 100.00%] [G loss: 4.523609]\n",
      "75 [D loss: 0.014299, acc.: 100.00%] [G loss: 4.675009]\n",
      "76 [D loss: 0.011132, acc.: 100.00%] [G loss: 4.603380]\n",
      "77 [D loss: 0.009695, acc.: 100.00%] [G loss: 4.575303]\n",
      "78 [D loss: 0.011367, acc.: 100.00%] [G loss: 4.737556]\n",
      "79 [D loss: 0.009415, acc.: 100.00%] [G loss: 4.565213]\n",
      "80 [D loss: 0.009839, acc.: 100.00%] [G loss: 4.694991]\n",
      "81 [D loss: 0.012401, acc.: 100.00%] [G loss: 4.728321]\n",
      "82 [D loss: 0.006913, acc.: 100.00%] [G loss: 4.760692]\n",
      "83 [D loss: 0.009940, acc.: 100.00%] [G loss: 4.597241]\n",
      "84 [D loss: 0.007579, acc.: 100.00%] [G loss: 4.692306]\n",
      "85 [D loss: 0.018396, acc.: 100.00%] [G loss: 4.791230]\n",
      "86 [D loss: 0.013894, acc.: 100.00%] [G loss: 4.766806]\n",
      "87 [D loss: 0.011208, acc.: 100.00%] [G loss: 4.786083]\n",
      "88 [D loss: 0.014547, acc.: 100.00%] [G loss: 4.932344]\n",
      "89 [D loss: 0.009660, acc.: 100.00%] [G loss: 4.655262]\n",
      "90 [D loss: 0.007842, acc.: 100.00%] [G loss: 4.991631]\n",
      "91 [D loss: 0.015403, acc.: 100.00%] [G loss: 4.897261]\n",
      "92 [D loss: 0.027850, acc.: 100.00%] [G loss: 5.286861]\n",
      "93 [D loss: 0.010860, acc.: 100.00%] [G loss: 5.214090]\n",
      "94 [D loss: 0.022860, acc.: 100.00%] [G loss: 4.930447]\n",
      "95 [D loss: 0.010612, acc.: 100.00%] [G loss: 4.831007]\n",
      "96 [D loss: 0.008017, acc.: 100.00%] [G loss: 5.098093]\n",
      "97 [D loss: 0.010157, acc.: 100.00%] [G loss: 5.146238]\n",
      "98 [D loss: 0.024201, acc.: 100.00%] [G loss: 5.483281]\n",
      "99 [D loss: 0.023557, acc.: 100.00%] [G loss: 4.619657]\n",
      "100 [D loss: 0.021359, acc.: 100.00%] [G loss: 4.807951]\n",
      "101 [D loss: 0.013702, acc.: 100.00%] [G loss: 5.089011]\n",
      "102 [D loss: 0.010057, acc.: 100.00%] [G loss: 5.001290]\n",
      "103 [D loss: 0.015517, acc.: 100.00%] [G loss: 5.012558]\n",
      "104 [D loss: 0.017113, acc.: 100.00%] [G loss: 5.367805]\n",
      "105 [D loss: 0.019612, acc.: 100.00%] [G loss: 5.078423]\n",
      "106 [D loss: 0.012702, acc.: 100.00%] [G loss: 5.070868]\n",
      "107 [D loss: 0.010402, acc.: 100.00%] [G loss: 5.033226]\n",
      "108 [D loss: 0.031397, acc.: 100.00%] [G loss: 5.334124]\n",
      "109 [D loss: 0.013630, acc.: 100.00%] [G loss: 5.182091]\n",
      "110 [D loss: 0.033652, acc.: 100.00%] [G loss: 5.160572]\n",
      "111 [D loss: 0.055476, acc.: 100.00%] [G loss: 5.606319]\n",
      "112 [D loss: 0.047601, acc.: 100.00%] [G loss: 4.512602]\n",
      "113 [D loss: 0.162584, acc.: 90.62%] [G loss: 6.037320]\n",
      "114 [D loss: 0.062373, acc.: 96.88%] [G loss: 5.077969]\n",
      "115 [D loss: 0.037899, acc.: 100.00%] [G loss: 5.465611]\n",
      "116 [D loss: 0.020251, acc.: 100.00%] [G loss: 4.982738]\n",
      "117 [D loss: 0.228152, acc.: 93.75%] [G loss: 5.679245]\n",
      "118 [D loss: 0.715204, acc.: 78.12%] [G loss: 4.071417]\n",
      "119 [D loss: 0.285605, acc.: 87.50%] [G loss: 3.543065]\n",
      "120 [D loss: 0.064599, acc.: 100.00%] [G loss: 4.228205]\n",
      "121 [D loss: 0.138836, acc.: 96.88%] [G loss: 5.320818]\n",
      "122 [D loss: 0.024380, acc.: 100.00%] [G loss: 5.443755]\n",
      "123 [D loss: 0.022567, acc.: 100.00%] [G loss: 5.207184]\n",
      "124 [D loss: 0.055809, acc.: 100.00%] [G loss: 4.629608]\n",
      "125 [D loss: 0.024925, acc.: 100.00%] [G loss: 4.484179]\n",
      "126 [D loss: 0.211393, acc.: 90.62%] [G loss: 4.983322]\n",
      "127 [D loss: 0.227694, acc.: 90.62%] [G loss: 3.101767]\n",
      "128 [D loss: 0.203619, acc.: 93.75%] [G loss: 3.840929]\n",
      "129 [D loss: 0.080669, acc.: 96.88%] [G loss: 3.984814]\n",
      "130 [D loss: 0.031173, acc.: 100.00%] [G loss: 4.380082]\n",
      "131 [D loss: 0.036423, acc.: 100.00%] [G loss: 4.569640]\n",
      "132 [D loss: 0.127580, acc.: 96.88%] [G loss: 4.612339]\n",
      "133 [D loss: 0.537329, acc.: 75.00%] [G loss: 3.382812]\n",
      "134 [D loss: 0.231438, acc.: 90.62%] [G loss: 4.352389]\n",
      "135 [D loss: 0.029270, acc.: 100.00%] [G loss: 4.571016]\n",
      "136 [D loss: 0.143262, acc.: 90.62%] [G loss: 4.138582]\n",
      "137 [D loss: 0.044015, acc.: 100.00%] [G loss: 3.899905]\n",
      "138 [D loss: 0.156540, acc.: 93.75%] [G loss: 4.011226]\n",
      "139 [D loss: 0.164863, acc.: 90.62%] [G loss: 4.717526]\n",
      "140 [D loss: 0.133439, acc.: 96.88%] [G loss: 3.489234]\n",
      "141 [D loss: 0.300020, acc.: 90.62%] [G loss: 3.912390]\n",
      "142 [D loss: 0.058032, acc.: 100.00%] [G loss: 4.488497]\n",
      "143 [D loss: 0.290567, acc.: 84.38%] [G loss: 3.838162]\n",
      "144 [D loss: 0.053611, acc.: 100.00%] [G loss: 4.318035]\n",
      "145 [D loss: 0.703971, acc.: 68.75%] [G loss: 3.248144]\n",
      "146 [D loss: 0.199009, acc.: 90.62%] [G loss: 4.589735]\n",
      "147 [D loss: 0.595680, acc.: 65.62%] [G loss: 2.413754]\n",
      "148 [D loss: 0.547254, acc.: 75.00%] [G loss: 2.106058]\n",
      "149 [D loss: 0.290421, acc.: 81.25%] [G loss: 3.296897]\n",
      "150 [D loss: 0.085944, acc.: 96.88%] [G loss: 4.539362]\n",
      "151 [D loss: 0.079309, acc.: 93.75%] [G loss: 5.386492]\n",
      "152 [D loss: 0.093002, acc.: 100.00%] [G loss: 4.452722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 [D loss: 0.036151, acc.: 100.00%] [G loss: 4.390170]\n",
      "154 [D loss: 0.036772, acc.: 100.00%] [G loss: 3.964340]\n",
      "155 [D loss: 0.122014, acc.: 93.75%] [G loss: 4.466368]\n",
      "156 [D loss: 0.177821, acc.: 96.88%] [G loss: 3.783077]\n",
      "157 [D loss: 0.102290, acc.: 96.88%] [G loss: 3.757785]\n",
      "158 [D loss: 0.067973, acc.: 100.00%] [G loss: 3.860507]\n",
      "159 [D loss: 0.131097, acc.: 96.88%] [G loss: 3.026674]\n",
      "160 [D loss: 0.090804, acc.: 96.88%] [G loss: 3.267917]\n",
      "161 [D loss: 0.109892, acc.: 100.00%] [G loss: 3.795675]\n",
      "162 [D loss: 0.065348, acc.: 100.00%] [G loss: 4.025987]\n",
      "163 [D loss: 0.422384, acc.: 81.25%] [G loss: 3.281295]\n",
      "164 [D loss: 0.070885, acc.: 96.88%] [G loss: 3.891161]\n",
      "165 [D loss: 0.506459, acc.: 81.25%] [G loss: 2.160962]\n",
      "166 [D loss: 0.468851, acc.: 75.00%] [G loss: 3.406263]\n",
      "167 [D loss: 0.070547, acc.: 96.88%] [G loss: 4.258386]\n",
      "168 [D loss: 0.517047, acc.: 78.12%] [G loss: 2.142956]\n",
      "169 [D loss: 0.225999, acc.: 87.50%] [G loss: 3.208400]\n",
      "170 [D loss: 0.056963, acc.: 100.00%] [G loss: 3.684173]\n",
      "171 [D loss: 0.077396, acc.: 100.00%] [G loss: 4.202624]\n",
      "172 [D loss: 0.210084, acc.: 90.62%] [G loss: 3.039125]\n",
      "173 [D loss: 0.139093, acc.: 93.75%] [G loss: 3.934927]\n",
      "174 [D loss: 0.192102, acc.: 90.62%] [G loss: 3.408977]\n",
      "175 [D loss: 0.211015, acc.: 90.62%] [G loss: 3.105343]\n",
      "176 [D loss: 0.091170, acc.: 100.00%] [G loss: 2.953341]\n",
      "177 [D loss: 0.165840, acc.: 90.62%] [G loss: 4.028949]\n",
      "178 [D loss: 0.184333, acc.: 100.00%] [G loss: 3.251074]\n",
      "179 [D loss: 0.078849, acc.: 100.00%] [G loss: 3.377034]\n",
      "180 [D loss: 0.157387, acc.: 96.88%] [G loss: 3.351741]\n",
      "181 [D loss: 0.209621, acc.: 87.50%] [G loss: 4.554438]\n",
      "182 [D loss: 0.802775, acc.: 65.62%] [G loss: 2.000021]\n",
      "183 [D loss: 0.504495, acc.: 75.00%] [G loss: 3.053505]\n",
      "184 [D loss: 0.064958, acc.: 100.00%] [G loss: 5.024813]\n",
      "185 [D loss: 0.194194, acc.: 93.75%] [G loss: 3.236316]\n",
      "186 [D loss: 0.111741, acc.: 96.88%] [G loss: 3.173080]\n",
      "187 [D loss: 0.105746, acc.: 96.88%] [G loss: 3.447903]\n",
      "188 [D loss: 0.161918, acc.: 96.88%] [G loss: 3.157983]\n",
      "189 [D loss: 0.112916, acc.: 100.00%] [G loss: 3.506847]\n",
      "190 [D loss: 0.092304, acc.: 100.00%] [G loss: 3.879931]\n",
      "191 [D loss: 0.143313, acc.: 100.00%] [G loss: 3.363167]\n",
      "192 [D loss: 0.198634, acc.: 93.75%] [G loss: 3.510172]\n",
      "193 [D loss: 0.186483, acc.: 96.88%] [G loss: 3.128443]\n",
      "194 [D loss: 0.545953, acc.: 81.25%] [G loss: 1.867389]\n",
      "195 [D loss: 0.339826, acc.: 78.12%] [G loss: 4.135964]\n",
      "196 [D loss: 1.005203, acc.: 50.00%] [G loss: 1.736209]\n",
      "197 [D loss: 0.152840, acc.: 90.62%] [G loss: 2.808588]\n",
      "198 [D loss: 0.117880, acc.: 100.00%] [G loss: 3.344624]\n",
      "199 [D loss: 0.201502, acc.: 93.75%] [G loss: 3.582405]\n",
      "200 [D loss: 0.216047, acc.: 93.75%] [G loss: 2.748168]\n",
      "201 [D loss: 0.326244, acc.: 81.25%] [G loss: 3.462496]\n",
      "202 [D loss: 0.634046, acc.: 68.75%] [G loss: 2.664929]\n",
      "203 [D loss: 0.094215, acc.: 100.00%] [G loss: 3.121888]\n",
      "204 [D loss: 0.130959, acc.: 93.75%] [G loss: 3.533502]\n",
      "205 [D loss: 0.191420, acc.: 93.75%] [G loss: 3.178650]\n",
      "206 [D loss: 0.247626, acc.: 87.50%] [G loss: 3.765563]\n",
      "207 [D loss: 0.373614, acc.: 78.12%] [G loss: 3.145106]\n",
      "208 [D loss: 0.117397, acc.: 100.00%] [G loss: 3.497609]\n",
      "209 [D loss: 0.366192, acc.: 81.25%] [G loss: 3.819998]\n",
      "210 [D loss: 0.357710, acc.: 87.50%] [G loss: 2.858631]\n",
      "211 [D loss: 0.132784, acc.: 100.00%] [G loss: 3.889380]\n",
      "212 [D loss: 0.228000, acc.: 93.75%] [G loss: 3.245371]\n",
      "213 [D loss: 0.149246, acc.: 93.75%] [G loss: 3.439595]\n",
      "214 [D loss: 0.303385, acc.: 90.62%] [G loss: 3.324906]\n",
      "215 [D loss: 0.409076, acc.: 87.50%] [G loss: 3.924244]\n",
      "216 [D loss: 1.075384, acc.: 53.12%] [G loss: 1.803538]\n",
      "217 [D loss: 0.544853, acc.: 78.12%] [G loss: 2.766370]\n",
      "218 [D loss: 0.069386, acc.: 100.00%] [G loss: 4.913792]\n",
      "219 [D loss: 0.720050, acc.: 62.50%] [G loss: 1.616421]\n",
      "220 [D loss: 0.264863, acc.: 87.50%] [G loss: 2.055108]\n",
      "221 [D loss: 0.186134, acc.: 90.62%] [G loss: 3.914687]\n",
      "222 [D loss: 0.210159, acc.: 93.75%] [G loss: 3.032754]\n",
      "223 [D loss: 0.166640, acc.: 96.88%] [G loss: 3.463297]\n",
      "224 [D loss: 0.538390, acc.: 71.88%] [G loss: 3.252426]\n",
      "225 [D loss: 0.252544, acc.: 90.62%] [G loss: 3.473038]\n",
      "226 [D loss: 1.290686, acc.: 40.62%] [G loss: 1.412632]\n",
      "227 [D loss: 0.302577, acc.: 84.38%] [G loss: 2.812195]\n",
      "228 [D loss: 0.327678, acc.: 90.62%] [G loss: 2.156262]\n",
      "229 [D loss: 0.564884, acc.: 65.62%] [G loss: 2.302833]\n",
      "230 [D loss: 0.232151, acc.: 90.62%] [G loss: 3.179171]\n",
      "231 [D loss: 0.947160, acc.: 53.12%] [G loss: 1.709008]\n",
      "232 [D loss: 0.237261, acc.: 87.50%] [G loss: 3.281050]\n",
      "233 [D loss: 0.457981, acc.: 81.25%] [G loss: 1.959672]\n",
      "234 [D loss: 0.393768, acc.: 84.38%] [G loss: 2.067222]\n",
      "235 [D loss: 0.249803, acc.: 96.88%] [G loss: 2.981655]\n",
      "236 [D loss: 0.465604, acc.: 81.25%] [G loss: 1.938041]\n",
      "237 [D loss: 0.374786, acc.: 81.25%] [G loss: 2.455424]\n",
      "238 [D loss: 0.603597, acc.: 65.62%] [G loss: 2.237458]\n",
      "239 [D loss: 0.465168, acc.: 68.75%] [G loss: 3.553001]\n",
      "240 [D loss: 1.179191, acc.: 28.12%] [G loss: 1.230496]\n",
      "241 [D loss: 0.424345, acc.: 75.00%] [G loss: 2.444780]\n",
      "242 [D loss: 0.273275, acc.: 90.62%] [G loss: 3.505309]\n",
      "243 [D loss: 0.835871, acc.: 53.12%] [G loss: 0.885127]\n",
      "244 [D loss: 0.613393, acc.: 68.75%] [G loss: 1.786834]\n",
      "245 [D loss: 0.202776, acc.: 93.75%] [G loss: 2.930116]\n",
      "246 [D loss: 0.722190, acc.: 59.38%] [G loss: 1.744487]\n",
      "247 [D loss: 0.337724, acc.: 84.38%] [G loss: 2.578650]\n",
      "248 [D loss: 0.668980, acc.: 59.38%] [G loss: 1.819269]\n",
      "249 [D loss: 0.236169, acc.: 90.62%] [G loss: 2.435131]\n",
      "250 [D loss: 0.470076, acc.: 78.12%] [G loss: 2.152476]\n",
      "251 [D loss: 0.249033, acc.: 93.75%] [G loss: 2.864526]\n",
      "252 [D loss: 0.596542, acc.: 68.75%] [G loss: 1.866981]\n",
      "253 [D loss: 0.231856, acc.: 96.88%] [G loss: 2.575285]\n",
      "254 [D loss: 0.486293, acc.: 71.88%] [G loss: 2.448258]\n",
      "255 [D loss: 0.477238, acc.: 78.12%] [G loss: 2.172832]\n",
      "256 [D loss: 0.310264, acc.: 87.50%] [G loss: 2.849808]\n",
      "257 [D loss: 0.518239, acc.: 75.00%] [G loss: 2.030437]\n",
      "258 [D loss: 0.474775, acc.: 68.75%] [G loss: 2.668856]\n",
      "259 [D loss: 0.397909, acc.: 84.38%] [G loss: 2.186585]\n",
      "260 [D loss: 0.387630, acc.: 90.62%] [G loss: 2.378746]\n",
      "261 [D loss: 0.510880, acc.: 75.00%] [G loss: 2.437349]\n",
      "262 [D loss: 0.473592, acc.: 81.25%] [G loss: 2.116868]\n",
      "263 [D loss: 0.560224, acc.: 78.12%] [G loss: 1.661454]\n",
      "264 [D loss: 0.639027, acc.: 62.50%] [G loss: 2.517031]\n",
      "265 [D loss: 0.780622, acc.: 46.88%] [G loss: 1.064588]\n",
      "266 [D loss: 0.358415, acc.: 87.50%] [G loss: 2.313519]\n",
      "267 [D loss: 0.394493, acc.: 84.38%] [G loss: 2.431357]\n",
      "268 [D loss: 0.509829, acc.: 71.88%] [G loss: 2.041675]\n",
      "269 [D loss: 0.555693, acc.: 68.75%] [G loss: 2.589622]\n",
      "270 [D loss: 0.798430, acc.: 46.88%] [G loss: 1.850379]\n",
      "271 [D loss: 0.482403, acc.: 78.12%] [G loss: 2.191542]\n",
      "272 [D loss: 0.884839, acc.: 53.12%] [G loss: 0.964816]\n",
      "273 [D loss: 0.561052, acc.: 68.75%] [G loss: 2.315781]\n",
      "274 [D loss: 0.830145, acc.: 40.62%] [G loss: 1.273108]\n",
      "275 [D loss: 0.542775, acc.: 62.50%] [G loss: 2.295891]\n",
      "276 [D loss: 0.746059, acc.: 46.88%] [G loss: 1.694781]\n",
      "277 [D loss: 0.705253, acc.: 68.75%] [G loss: 1.756454]\n",
      "278 [D loss: 0.534178, acc.: 68.75%] [G loss: 2.193650]\n",
      "279 [D loss: 1.161340, acc.: 25.00%] [G loss: 0.762724]\n",
      "280 [D loss: 0.557652, acc.: 59.38%] [G loss: 1.740217]\n",
      "281 [D loss: 0.520191, acc.: 78.12%] [G loss: 1.801191]\n",
      "282 [D loss: 0.512850, acc.: 78.12%] [G loss: 1.910145]\n",
      "283 [D loss: 0.764615, acc.: 40.62%] [G loss: 1.390778]\n",
      "284 [D loss: 0.463818, acc.: 75.00%] [G loss: 2.159881]\n",
      "285 [D loss: 0.739370, acc.: 46.88%] [G loss: 1.444633]\n",
      "286 [D loss: 0.548547, acc.: 71.88%] [G loss: 1.830392]\n",
      "287 [D loss: 0.739442, acc.: 53.12%] [G loss: 1.313418]\n",
      "288 [D loss: 0.661892, acc.: 59.38%] [G loss: 1.400877]\n",
      "289 [D loss: 0.802566, acc.: 46.88%] [G loss: 1.021065]\n",
      "290 [D loss: 0.593937, acc.: 59.38%] [G loss: 1.859794]\n",
      "291 [D loss: 0.816141, acc.: 50.00%] [G loss: 1.277503]\n",
      "292 [D loss: 0.660041, acc.: 59.38%] [G loss: 1.435930]\n",
      "293 [D loss: 0.622200, acc.: 62.50%] [G loss: 1.565266]\n",
      "294 [D loss: 0.661172, acc.: 65.62%] [G loss: 1.228026]\n",
      "295 [D loss: 0.819474, acc.: 40.62%] [G loss: 1.094594]\n",
      "296 [D loss: 0.575196, acc.: 65.62%] [G loss: 1.387713]\n",
      "297 [D loss: 0.864222, acc.: 40.62%] [G loss: 1.091751]\n",
      "298 [D loss: 0.510870, acc.: 78.12%] [G loss: 1.547797]\n",
      "299 [D loss: 0.680882, acc.: 46.88%] [G loss: 1.146685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 [D loss: 0.614422, acc.: 71.88%] [G loss: 1.043989]\n",
      "301 [D loss: 0.648880, acc.: 59.38%] [G loss: 1.309413]\n",
      "302 [D loss: 0.618780, acc.: 65.62%] [G loss: 1.352860]\n",
      "303 [D loss: 0.652710, acc.: 68.75%] [G loss: 1.165978]\n",
      "304 [D loss: 0.988769, acc.: 28.12%] [G loss: 0.641015]\n",
      "305 [D loss: 0.709336, acc.: 43.75%] [G loss: 0.984588]\n",
      "306 [D loss: 0.574284, acc.: 62.50%] [G loss: 1.352177]\n",
      "307 [D loss: 1.019247, acc.: 15.62%] [G loss: 0.611964]\n",
      "308 [D loss: 0.693678, acc.: 50.00%] [G loss: 0.876972]\n",
      "309 [D loss: 0.525812, acc.: 71.88%] [G loss: 1.215150]\n",
      "310 [D loss: 0.702845, acc.: 59.38%] [G loss: 1.019110]\n",
      "311 [D loss: 0.656394, acc.: 56.25%] [G loss: 1.026459]\n",
      "312 [D loss: 0.655777, acc.: 59.38%] [G loss: 1.017629]\n",
      "313 [D loss: 0.648723, acc.: 53.12%] [G loss: 1.336263]\n",
      "314 [D loss: 0.732070, acc.: 43.75%] [G loss: 0.991071]\n",
      "315 [D loss: 0.783628, acc.: 37.50%] [G loss: 0.808321]\n",
      "316 [D loss: 0.662905, acc.: 53.12%] [G loss: 1.007113]\n",
      "317 [D loss: 0.818440, acc.: 34.38%] [G loss: 0.904645]\n",
      "318 [D loss: 0.764291, acc.: 43.75%] [G loss: 0.806014]\n",
      "319 [D loss: 0.824097, acc.: 40.62%] [G loss: 0.716576]\n",
      "320 [D loss: 0.689076, acc.: 43.75%] [G loss: 0.883070]\n",
      "321 [D loss: 0.706723, acc.: 53.12%] [G loss: 0.953833]\n",
      "322 [D loss: 0.823659, acc.: 40.62%] [G loss: 0.730036]\n",
      "323 [D loss: 0.710576, acc.: 50.00%] [G loss: 0.828366]\n",
      "324 [D loss: 0.662027, acc.: 53.12%] [G loss: 0.943825]\n",
      "325 [D loss: 0.789773, acc.: 25.00%] [G loss: 0.736742]\n",
      "326 [D loss: 0.729016, acc.: 46.88%] [G loss: 0.749119]\n",
      "327 [D loss: 0.746023, acc.: 46.88%] [G loss: 0.714974]\n",
      "328 [D loss: 0.758212, acc.: 37.50%] [G loss: 0.787501]\n",
      "329 [D loss: 0.721258, acc.: 50.00%] [G loss: 0.814475]\n",
      "330 [D loss: 0.788096, acc.: 28.12%] [G loss: 0.665885]\n",
      "331 [D loss: 0.749352, acc.: 43.75%] [G loss: 0.687488]\n",
      "332 [D loss: 0.702950, acc.: 50.00%] [G loss: 0.736244]\n",
      "333 [D loss: 0.745994, acc.: 40.62%] [G loss: 0.687328]\n",
      "334 [D loss: 0.673603, acc.: 53.12%] [G loss: 0.747758]\n",
      "335 [D loss: 0.695720, acc.: 43.75%] [G loss: 0.797861]\n",
      "336 [D loss: 0.699450, acc.: 46.88%] [G loss: 0.814380]\n",
      "337 [D loss: 0.784939, acc.: 31.25%] [G loss: 0.710046]\n",
      "338 [D loss: 0.776775, acc.: 46.88%] [G loss: 0.662409]\n",
      "339 [D loss: 0.670597, acc.: 50.00%] [G loss: 0.721603]\n",
      "340 [D loss: 0.726859, acc.: 40.62%] [G loss: 0.693153]\n",
      "341 [D loss: 0.711528, acc.: 40.62%] [G loss: 0.678592]\n",
      "342 [D loss: 0.735794, acc.: 43.75%] [G loss: 0.704445]\n",
      "343 [D loss: 0.668342, acc.: 53.12%] [G loss: 0.682370]\n",
      "344 [D loss: 0.730954, acc.: 34.38%] [G loss: 0.698653]\n",
      "345 [D loss: 0.682822, acc.: 46.88%] [G loss: 0.702920]\n",
      "346 [D loss: 0.776130, acc.: 34.38%] [G loss: 0.646149]\n",
      "347 [D loss: 0.706017, acc.: 46.88%] [G loss: 0.678032]\n",
      "348 [D loss: 0.714572, acc.: 43.75%] [G loss: 0.703386]\n",
      "349 [D loss: 0.743254, acc.: 37.50%] [G loss: 0.677887]\n",
      "350 [D loss: 0.694857, acc.: 46.88%] [G loss: 0.692568]\n",
      "351 [D loss: 0.671687, acc.: 59.38%] [G loss: 0.700213]\n",
      "352 [D loss: 0.698152, acc.: 43.75%] [G loss: 0.701289]\n",
      "353 [D loss: 0.735915, acc.: 37.50%] [G loss: 0.695838]\n",
      "354 [D loss: 0.717299, acc.: 40.62%] [G loss: 0.668214]\n",
      "355 [D loss: 0.703418, acc.: 46.88%] [G loss: 0.676589]\n",
      "356 [D loss: 0.700172, acc.: 53.12%] [G loss: 0.709257]\n",
      "357 [D loss: 0.721564, acc.: 46.88%] [G loss: 0.659085]\n",
      "358 [D loss: 0.703504, acc.: 40.62%] [G loss: 0.717922]\n",
      "359 [D loss: 0.674347, acc.: 59.38%] [G loss: 0.717199]\n",
      "360 [D loss: 0.685213, acc.: 53.12%] [G loss: 0.734813]\n",
      "361 [D loss: 0.715066, acc.: 46.88%] [G loss: 0.682048]\n",
      "362 [D loss: 0.719080, acc.: 53.12%] [G loss: 0.686317]\n",
      "363 [D loss: 0.662556, acc.: 53.12%] [G loss: 0.695879]\n",
      "364 [D loss: 0.714903, acc.: 40.62%] [G loss: 0.670725]\n",
      "365 [D loss: 0.696550, acc.: 53.12%] [G loss: 0.661003]\n",
      "366 [D loss: 0.700739, acc.: 46.88%] [G loss: 0.674221]\n",
      "367 [D loss: 0.687075, acc.: 46.88%] [G loss: 0.679597]\n",
      "368 [D loss: 0.702558, acc.: 46.88%] [G loss: 0.675978]\n",
      "369 [D loss: 0.691192, acc.: 53.12%] [G loss: 0.663796]\n",
      "370 [D loss: 0.670464, acc.: 50.00%] [G loss: 0.662674]\n",
      "371 [D loss: 0.730708, acc.: 40.62%] [G loss: 0.653379]\n",
      "372 [D loss: 0.706949, acc.: 46.88%] [G loss: 0.685047]\n",
      "373 [D loss: 0.689561, acc.: 50.00%] [G loss: 0.668322]\n",
      "374 [D loss: 0.716321, acc.: 46.88%] [G loss: 0.651790]\n",
      "375 [D loss: 0.696731, acc.: 50.00%] [G loss: 0.639166]\n",
      "376 [D loss: 0.646994, acc.: 50.00%] [G loss: 0.678292]\n",
      "377 [D loss: 0.684100, acc.: 46.88%] [G loss: 0.687408]\n",
      "378 [D loss: 0.684301, acc.: 40.62%] [G loss: 0.665136]\n",
      "379 [D loss: 0.686110, acc.: 50.00%] [G loss: 0.679833]\n",
      "380 [D loss: 0.675183, acc.: 46.88%] [G loss: 0.687325]\n",
      "381 [D loss: 0.660240, acc.: 53.12%] [G loss: 0.710325]\n",
      "382 [D loss: 0.674509, acc.: 50.00%] [G loss: 0.704904]\n",
      "383 [D loss: 0.705126, acc.: 46.88%] [G loss: 0.676692]\n",
      "384 [D loss: 0.699019, acc.: 46.88%] [G loss: 0.670097]\n",
      "385 [D loss: 0.721693, acc.: 43.75%] [G loss: 0.651706]\n",
      "386 [D loss: 0.698630, acc.: 46.88%] [G loss: 0.667942]\n",
      "387 [D loss: 0.690303, acc.: 50.00%] [G loss: 0.678070]\n",
      "388 [D loss: 0.681767, acc.: 50.00%] [G loss: 0.661992]\n",
      "389 [D loss: 0.687534, acc.: 53.12%] [G loss: 0.671036]\n",
      "390 [D loss: 0.651322, acc.: 56.25%] [G loss: 0.671933]\n",
      "391 [D loss: 0.670239, acc.: 53.12%] [G loss: 0.677923]\n",
      "392 [D loss: 0.668331, acc.: 46.88%] [G loss: 0.684047]\n",
      "393 [D loss: 0.663440, acc.: 56.25%] [G loss: 0.671847]\n",
      "394 [D loss: 0.666790, acc.: 50.00%] [G loss: 0.698082]\n",
      "395 [D loss: 0.683902, acc.: 53.12%] [G loss: 0.667050]\n",
      "396 [D loss: 0.740637, acc.: 40.62%] [G loss: 0.653618]\n",
      "397 [D loss: 0.669795, acc.: 50.00%] [G loss: 0.652566]\n",
      "398 [D loss: 0.683074, acc.: 46.88%] [G loss: 0.647598]\n",
      "399 [D loss: 0.659044, acc.: 50.00%] [G loss: 0.667870]\n",
      "400 [D loss: 0.691002, acc.: 46.88%] [G loss: 0.662646]\n",
      "401 [D loss: 0.682192, acc.: 53.12%] [G loss: 0.663292]\n",
      "402 [D loss: 0.662247, acc.: 53.12%] [G loss: 0.636935]\n",
      "403 [D loss: 0.679849, acc.: 50.00%] [G loss: 0.642380]\n",
      "404 [D loss: 0.665450, acc.: 46.88%] [G loss: 0.652254]\n",
      "405 [D loss: 0.713344, acc.: 40.62%] [G loss: 0.662009]\n",
      "406 [D loss: 0.690003, acc.: 50.00%] [G loss: 0.673187]\n",
      "407 [D loss: 0.645944, acc.: 50.00%] [G loss: 0.651204]\n",
      "408 [D loss: 0.674821, acc.: 46.88%] [G loss: 0.655721]\n",
      "409 [D loss: 0.695497, acc.: 43.75%] [G loss: 0.671471]\n",
      "410 [D loss: 0.690805, acc.: 43.75%] [G loss: 0.667352]\n",
      "411 [D loss: 0.664827, acc.: 53.12%] [G loss: 0.669170]\n",
      "412 [D loss: 0.654930, acc.: 50.00%] [G loss: 0.653934]\n",
      "413 [D loss: 0.661433, acc.: 50.00%] [G loss: 0.652278]\n",
      "414 [D loss: 0.678122, acc.: 50.00%] [G loss: 0.642292]\n",
      "415 [D loss: 0.658955, acc.: 50.00%] [G loss: 0.676766]\n",
      "416 [D loss: 0.690439, acc.: 40.62%] [G loss: 0.646575]\n",
      "417 [D loss: 0.664258, acc.: 46.88%] [G loss: 0.659055]\n",
      "418 [D loss: 0.671680, acc.: 50.00%] [G loss: 0.651754]\n",
      "419 [D loss: 0.663033, acc.: 50.00%] [G loss: 0.663165]\n",
      "420 [D loss: 0.685664, acc.: 46.88%] [G loss: 0.685015]\n",
      "421 [D loss: 0.662931, acc.: 50.00%] [G loss: 0.684399]\n",
      "422 [D loss: 0.653330, acc.: 53.12%] [G loss: 0.683547]\n",
      "423 [D loss: 0.681018, acc.: 56.25%] [G loss: 0.673026]\n",
      "424 [D loss: 0.653239, acc.: 50.00%] [G loss: 0.688339]\n",
      "425 [D loss: 0.648028, acc.: 53.12%] [G loss: 0.674718]\n",
      "426 [D loss: 0.635047, acc.: 59.38%] [G loss: 0.657973]\n",
      "427 [D loss: 0.674254, acc.: 53.12%] [G loss: 0.656720]\n",
      "428 [D loss: 0.654660, acc.: 50.00%] [G loss: 0.652920]\n",
      "429 [D loss: 0.678181, acc.: 40.62%] [G loss: 0.677021]\n",
      "430 [D loss: 0.657058, acc.: 50.00%] [G loss: 0.679321]\n",
      "431 [D loss: 0.675557, acc.: 50.00%] [G loss: 0.689310]\n",
      "432 [D loss: 0.668564, acc.: 56.25%] [G loss: 0.686721]\n",
      "433 [D loss: 0.674702, acc.: 53.12%] [G loss: 0.655586]\n",
      "434 [D loss: 0.653125, acc.: 50.00%] [G loss: 0.658681]\n",
      "435 [D loss: 0.638797, acc.: 50.00%] [G loss: 0.665592]\n",
      "436 [D loss: 0.643051, acc.: 50.00%] [G loss: 0.657539]\n",
      "437 [D loss: 0.654886, acc.: 46.88%] [G loss: 0.674004]\n",
      "438 [D loss: 0.652910, acc.: 43.75%] [G loss: 0.667296]\n",
      "439 [D loss: 0.670013, acc.: 50.00%] [G loss: 0.662461]\n",
      "440 [D loss: 0.658365, acc.: 53.12%] [G loss: 0.674049]\n",
      "441 [D loss: 0.660123, acc.: 46.88%] [G loss: 0.670312]\n",
      "442 [D loss: 0.651999, acc.: 43.75%] [G loss: 0.690239]\n",
      "443 [D loss: 0.647674, acc.: 53.12%] [G loss: 0.678968]\n",
      "444 [D loss: 0.638471, acc.: 56.25%] [G loss: 0.689790]\n",
      "445 [D loss: 0.625301, acc.: 68.75%] [G loss: 0.710579]\n",
      "446 [D loss: 0.670517, acc.: 56.25%] [G loss: 0.698800]\n",
      "447 [D loss: 0.660177, acc.: 50.00%] [G loss: 0.699167]\n",
      "448 [D loss: 0.660583, acc.: 43.75%] [G loss: 0.698428]\n",
      "449 [D loss: 0.648540, acc.: 50.00%] [G loss: 0.701566]\n",
      "450 [D loss: 0.661092, acc.: 46.88%] [G loss: 0.700310]\n",
      "451 [D loss: 0.657841, acc.: 50.00%] [G loss: 0.706434]\n",
      "452 [D loss: 0.642192, acc.: 53.12%] [G loss: 0.693676]\n",
      "453 [D loss: 0.660002, acc.: 50.00%] [G loss: 0.696659]\n",
      "454 [D loss: 0.649549, acc.: 50.00%] [G loss: 0.684525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 [D loss: 0.652827, acc.: 46.88%] [G loss: 0.669142]\n",
      "456 [D loss: 0.642753, acc.: 53.12%] [G loss: 0.676230]\n",
      "457 [D loss: 0.661266, acc.: 53.12%] [G loss: 0.679545]\n",
      "458 [D loss: 0.641959, acc.: 50.00%] [G loss: 0.685549]\n",
      "459 [D loss: 0.650294, acc.: 46.88%] [G loss: 0.711323]\n",
      "460 [D loss: 0.666655, acc.: 46.88%] [G loss: 0.718420]\n",
      "461 [D loss: 0.639564, acc.: 59.38%] [G loss: 0.714015]\n",
      "462 [D loss: 0.675105, acc.: 53.12%] [G loss: 0.718786]\n",
      "463 [D loss: 0.648691, acc.: 56.25%] [G loss: 0.711882]\n",
      "464 [D loss: 0.659541, acc.: 59.38%] [G loss: 0.700144]\n",
      "465 [D loss: 0.651519, acc.: 53.12%] [G loss: 0.690145]\n",
      "466 [D loss: 0.648673, acc.: 53.12%] [G loss: 0.684901]\n",
      "467 [D loss: 0.634449, acc.: 46.88%] [G loss: 0.684778]\n",
      "468 [D loss: 0.641606, acc.: 53.12%] [G loss: 0.702032]\n",
      "469 [D loss: 0.647515, acc.: 56.25%] [G loss: 0.735998]\n",
      "470 [D loss: 0.627141, acc.: 53.12%] [G loss: 0.725786]\n",
      "471 [D loss: 0.631551, acc.: 65.62%] [G loss: 0.721684]\n",
      "472 [D loss: 0.649233, acc.: 53.12%] [G loss: 0.700181]\n",
      "473 [D loss: 0.650460, acc.: 56.25%] [G loss: 0.704529]\n",
      "474 [D loss: 0.650768, acc.: 56.25%] [G loss: 0.700011]\n",
      "475 [D loss: 0.641891, acc.: 65.62%] [G loss: 0.715073]\n",
      "476 [D loss: 0.624787, acc.: 65.62%] [G loss: 0.727812]\n",
      "477 [D loss: 0.646297, acc.: 59.38%] [G loss: 0.731199]\n",
      "478 [D loss: 0.640409, acc.: 56.25%] [G loss: 0.719736]\n",
      "479 [D loss: 0.669108, acc.: 46.88%] [G loss: 0.715993]\n",
      "480 [D loss: 0.614530, acc.: 59.38%] [G loss: 0.712752]\n",
      "481 [D loss: 0.625635, acc.: 56.25%] [G loss: 0.721683]\n",
      "482 [D loss: 0.660324, acc.: 65.62%] [G loss: 0.741843]\n",
      "483 [D loss: 0.660952, acc.: 53.12%] [G loss: 0.736184]\n",
      "484 [D loss: 0.635668, acc.: 50.00%] [G loss: 0.729107]\n",
      "485 [D loss: 0.630068, acc.: 62.50%] [G loss: 0.757639]\n",
      "486 [D loss: 0.641494, acc.: 68.75%] [G loss: 0.780755]\n",
      "487 [D loss: 0.667214, acc.: 62.50%] [G loss: 0.746001]\n",
      "488 [D loss: 0.640900, acc.: 62.50%] [G loss: 0.786625]\n",
      "489 [D loss: 0.655404, acc.: 50.00%] [G loss: 0.760232]\n",
      "490 [D loss: 0.645646, acc.: 62.50%] [G loss: 0.712327]\n",
      "491 [D loss: 0.652132, acc.: 53.12%] [G loss: 0.711042]\n",
      "492 [D loss: 0.686086, acc.: 56.25%] [G loss: 0.727066]\n",
      "493 [D loss: 0.652881, acc.: 65.62%] [G loss: 0.733224]\n",
      "494 [D loss: 0.670542, acc.: 53.12%] [G loss: 0.703345]\n",
      "495 [D loss: 0.642725, acc.: 56.25%] [G loss: 0.723943]\n",
      "496 [D loss: 0.631313, acc.: 53.12%] [G loss: 0.757161]\n",
      "497 [D loss: 0.639123, acc.: 56.25%] [G loss: 0.753581]\n",
      "498 [D loss: 0.644237, acc.: 59.38%] [G loss: 0.766329]\n",
      "499 [D loss: 0.654580, acc.: 46.88%] [G loss: 0.760716]\n",
      "500 [D loss: 0.636212, acc.: 68.75%] [G loss: 0.742947]\n",
      "501 [D loss: 0.662093, acc.: 62.50%] [G loss: 0.719188]\n",
      "502 [D loss: 0.646270, acc.: 59.38%] [G loss: 0.706660]\n",
      "503 [D loss: 0.662088, acc.: 46.88%] [G loss: 0.692565]\n",
      "504 [D loss: 0.697958, acc.: 56.25%] [G loss: 0.702136]\n",
      "505 [D loss: 0.671574, acc.: 46.88%] [G loss: 0.712256]\n",
      "506 [D loss: 0.672816, acc.: 43.75%] [G loss: 0.712284]\n",
      "507 [D loss: 0.652847, acc.: 56.25%] [G loss: 0.680375]\n",
      "508 [D loss: 0.685803, acc.: 50.00%] [G loss: 0.694537]\n",
      "509 [D loss: 0.688902, acc.: 56.25%] [G loss: 0.714152]\n",
      "510 [D loss: 0.684896, acc.: 53.12%] [G loss: 0.712796]\n",
      "511 [D loss: 0.674566, acc.: 53.12%] [G loss: 0.685827]\n",
      "512 [D loss: 0.703304, acc.: 40.62%] [G loss: 0.683914]\n",
      "513 [D loss: 0.678623, acc.: 46.88%] [G loss: 0.692441]\n",
      "514 [D loss: 0.654639, acc.: 53.12%] [G loss: 0.665450]\n",
      "515 [D loss: 0.658820, acc.: 53.12%] [G loss: 0.671015]\n",
      "516 [D loss: 0.662521, acc.: 50.00%] [G loss: 0.680703]\n",
      "517 [D loss: 0.644550, acc.: 56.25%] [G loss: 0.684877]\n",
      "518 [D loss: 0.674015, acc.: 53.12%] [G loss: 0.720188]\n",
      "519 [D loss: 0.628745, acc.: 62.50%] [G loss: 0.738402]\n",
      "520 [D loss: 0.657163, acc.: 68.75%] [G loss: 0.726932]\n",
      "521 [D loss: 0.626681, acc.: 71.88%] [G loss: 0.749699]\n",
      "522 [D loss: 0.582518, acc.: 78.12%] [G loss: 0.809210]\n",
      "523 [D loss: 0.613763, acc.: 75.00%] [G loss: 0.796650]\n",
      "524 [D loss: 0.681379, acc.: 56.25%] [G loss: 0.782271]\n",
      "525 [D loss: 0.641027, acc.: 59.38%] [G loss: 0.795635]\n",
      "526 [D loss: 0.652947, acc.: 53.12%] [G loss: 0.722595]\n",
      "527 [D loss: 0.621835, acc.: 50.00%] [G loss: 0.766937]\n",
      "528 [D loss: 0.645436, acc.: 59.38%] [G loss: 0.780096]\n",
      "529 [D loss: 0.689139, acc.: 46.88%] [G loss: 0.751269]\n",
      "530 [D loss: 0.638067, acc.: 50.00%] [G loss: 0.754399]\n",
      "531 [D loss: 0.630825, acc.: 62.50%] [G loss: 0.710678]\n",
      "532 [D loss: 0.638865, acc.: 71.88%] [G loss: 0.726625]\n",
      "533 [D loss: 0.656376, acc.: 56.25%] [G loss: 0.715878]\n",
      "534 [D loss: 0.623711, acc.: 75.00%] [G loss: 0.730011]\n",
      "535 [D loss: 0.670130, acc.: 59.38%] [G loss: 0.720241]\n",
      "536 [D loss: 0.664410, acc.: 56.25%] [G loss: 0.742937]\n",
      "537 [D loss: 0.614682, acc.: 62.50%] [G loss: 0.735603]\n",
      "538 [D loss: 0.637917, acc.: 62.50%] [G loss: 0.761024]\n",
      "539 [D loss: 0.625467, acc.: 56.25%] [G loss: 0.778454]\n",
      "540 [D loss: 0.639356, acc.: 53.12%] [G loss: 0.783834]\n",
      "541 [D loss: 0.639642, acc.: 62.50%] [G loss: 0.777356]\n",
      "542 [D loss: 0.626128, acc.: 62.50%] [G loss: 0.800860]\n",
      "543 [D loss: 0.645654, acc.: 75.00%] [G loss: 0.722728]\n",
      "544 [D loss: 0.672530, acc.: 56.25%] [G loss: 0.782010]\n",
      "545 [D loss: 0.680256, acc.: 62.50%] [G loss: 0.761436]\n",
      "546 [D loss: 0.639410, acc.: 62.50%] [G loss: 0.731301]\n",
      "547 [D loss: 0.665355, acc.: 59.38%] [G loss: 0.709431]\n",
      "548 [D loss: 0.708126, acc.: 50.00%] [G loss: 0.755121]\n",
      "549 [D loss: 0.665133, acc.: 59.38%] [G loss: 0.726561]\n",
      "550 [D loss: 0.720964, acc.: 50.00%] [G loss: 0.711689]\n",
      "551 [D loss: 0.658992, acc.: 59.38%] [G loss: 0.693702]\n",
      "552 [D loss: 0.639981, acc.: 56.25%] [G loss: 0.688058]\n",
      "553 [D loss: 0.664969, acc.: 53.12%] [G loss: 0.713171]\n",
      "554 [D loss: 0.664945, acc.: 56.25%] [G loss: 0.723096]\n",
      "555 [D loss: 0.700993, acc.: 53.12%] [G loss: 0.728682]\n",
      "556 [D loss: 0.686438, acc.: 59.38%] [G loss: 0.742478]\n",
      "557 [D loss: 0.657367, acc.: 46.88%] [G loss: 0.749263]\n",
      "558 [D loss: 0.620931, acc.: 62.50%] [G loss: 0.755951]\n",
      "559 [D loss: 0.659927, acc.: 53.12%] [G loss: 0.761140]\n",
      "560 [D loss: 0.665068, acc.: 53.12%] [G loss: 0.739616]\n",
      "561 [D loss: 0.669646, acc.: 46.88%] [G loss: 0.706291]\n",
      "562 [D loss: 0.656219, acc.: 53.12%] [G loss: 0.719260]\n",
      "563 [D loss: 0.646324, acc.: 56.25%] [G loss: 0.759203]\n",
      "564 [D loss: 0.627770, acc.: 56.25%] [G loss: 0.783095]\n",
      "565 [D loss: 0.658967, acc.: 65.62%] [G loss: 0.782023]\n",
      "566 [D loss: 0.623814, acc.: 59.38%] [G loss: 0.772726]\n",
      "567 [D loss: 0.654831, acc.: 59.38%] [G loss: 0.735157]\n",
      "568 [D loss: 0.620948, acc.: 56.25%] [G loss: 0.739763]\n",
      "569 [D loss: 0.675725, acc.: 56.25%] [G loss: 0.723554]\n",
      "570 [D loss: 0.655598, acc.: 46.88%] [G loss: 0.713341]\n",
      "571 [D loss: 0.661812, acc.: 53.12%] [G loss: 0.723132]\n",
      "572 [D loss: 0.621041, acc.: 53.12%] [G loss: 0.711099]\n",
      "573 [D loss: 0.605451, acc.: 62.50%] [G loss: 0.704607]\n",
      "574 [D loss: 0.655177, acc.: 56.25%] [G loss: 0.713582]\n",
      "575 [D loss: 0.652425, acc.: 59.38%] [G loss: 0.742771]\n",
      "576 [D loss: 0.637531, acc.: 59.38%] [G loss: 0.786976]\n",
      "577 [D loss: 0.677154, acc.: 46.88%] [G loss: 0.770884]\n",
      "578 [D loss: 0.670067, acc.: 43.75%] [G loss: 0.784349]\n",
      "579 [D loss: 0.672055, acc.: 53.12%] [G loss: 0.740817]\n",
      "580 [D loss: 0.679323, acc.: 50.00%] [G loss: 0.734432]\n",
      "581 [D loss: 0.624944, acc.: 53.12%] [G loss: 0.778551]\n",
      "582 [D loss: 0.618841, acc.: 59.38%] [G loss: 0.795425]\n",
      "583 [D loss: 0.666069, acc.: 65.62%] [G loss: 0.782073]\n",
      "584 [D loss: 0.681336, acc.: 46.88%] [G loss: 0.730739]\n",
      "585 [D loss: 0.644528, acc.: 50.00%] [G loss: 0.731943]\n",
      "586 [D loss: 0.667713, acc.: 53.12%] [G loss: 0.720471]\n",
      "587 [D loss: 0.633089, acc.: 62.50%] [G loss: 0.731491]\n",
      "588 [D loss: 0.641237, acc.: 62.50%] [G loss: 0.740367]\n",
      "589 [D loss: 0.626920, acc.: 56.25%] [G loss: 0.746550]\n",
      "590 [D loss: 0.633417, acc.: 59.38%] [G loss: 0.734685]\n",
      "591 [D loss: 0.615185, acc.: 65.62%] [G loss: 0.760751]\n",
      "592 [D loss: 0.622330, acc.: 75.00%] [G loss: 0.756848]\n",
      "593 [D loss: 0.652892, acc.: 56.25%] [G loss: 0.776149]\n",
      "594 [D loss: 0.642836, acc.: 75.00%] [G loss: 0.799833]\n",
      "595 [D loss: 0.648855, acc.: 68.75%] [G loss: 0.774240]\n",
      "596 [D loss: 0.679245, acc.: 59.38%] [G loss: 0.726965]\n",
      "597 [D loss: 0.650023, acc.: 53.12%] [G loss: 0.736911]\n",
      "598 [D loss: 0.660782, acc.: 46.88%] [G loss: 0.758167]\n",
      "599 [D loss: 0.630978, acc.: 68.75%] [G loss: 0.761256]\n",
      "600 [D loss: 0.681137, acc.: 53.12%] [G loss: 0.755218]\n",
      "601 [D loss: 0.652621, acc.: 59.38%] [G loss: 0.695518]\n",
      "602 [D loss: 0.639169, acc.: 62.50%] [G loss: 0.690559]\n",
      "603 [D loss: 0.671956, acc.: 56.25%] [G loss: 0.708849]\n",
      "604 [D loss: 0.661194, acc.: 62.50%] [G loss: 0.714720]\n",
      "605 [D loss: 0.674243, acc.: 59.38%] [G loss: 0.708803]\n",
      "606 [D loss: 0.623804, acc.: 71.88%] [G loss: 0.721857]\n",
      "607 [D loss: 0.665476, acc.: 59.38%] [G loss: 0.730377]\n",
      "608 [D loss: 0.655877, acc.: 65.62%] [G loss: 0.751464]\n",
      "609 [D loss: 0.650145, acc.: 71.88%] [G loss: 0.758269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610 [D loss: 0.671688, acc.: 59.38%] [G loss: 0.767247]\n",
      "611 [D loss: 0.698468, acc.: 50.00%] [G loss: 0.740401]\n",
      "612 [D loss: 0.636120, acc.: 68.75%] [G loss: 0.747355]\n",
      "613 [D loss: 0.629310, acc.: 62.50%] [G loss: 0.763318]\n",
      "614 [D loss: 0.661867, acc.: 59.38%] [G loss: 0.749556]\n",
      "615 [D loss: 0.637801, acc.: 59.38%] [G loss: 0.770595]\n",
      "616 [D loss: 0.621306, acc.: 59.38%] [G loss: 0.790053]\n",
      "617 [D loss: 0.680568, acc.: 53.12%] [G loss: 0.767466]\n",
      "618 [D loss: 0.643474, acc.: 59.38%] [G loss: 0.729946]\n",
      "619 [D loss: 0.645216, acc.: 65.62%] [G loss: 0.728329]\n",
      "620 [D loss: 0.701717, acc.: 50.00%] [G loss: 0.752656]\n",
      "621 [D loss: 0.637530, acc.: 71.88%] [G loss: 0.732518]\n",
      "622 [D loss: 0.683573, acc.: 46.88%] [G loss: 0.711436]\n",
      "623 [D loss: 0.659865, acc.: 62.50%] [G loss: 0.732630]\n",
      "624 [D loss: 0.648191, acc.: 62.50%] [G loss: 0.723773]\n",
      "625 [D loss: 0.647245, acc.: 68.75%] [G loss: 0.708238]\n",
      "626 [D loss: 0.636931, acc.: 56.25%] [G loss: 0.718082]\n",
      "627 [D loss: 0.654629, acc.: 56.25%] [G loss: 0.725242]\n",
      "628 [D loss: 0.647656, acc.: 62.50%] [G loss: 0.743764]\n",
      "629 [D loss: 0.677159, acc.: 62.50%] [G loss: 0.778481]\n",
      "630 [D loss: 0.707556, acc.: 53.12%] [G loss: 0.749997]\n",
      "631 [D loss: 0.655658, acc.: 56.25%] [G loss: 0.722788]\n",
      "632 [D loss: 0.671224, acc.: 65.62%] [G loss: 0.706792]\n",
      "633 [D loss: 0.625364, acc.: 68.75%] [G loss: 0.688901]\n",
      "634 [D loss: 0.659145, acc.: 59.38%] [G loss: 0.678709]\n",
      "635 [D loss: 0.658651, acc.: 56.25%] [G loss: 0.702246]\n",
      "636 [D loss: 0.675887, acc.: 46.88%] [G loss: 0.703683]\n",
      "637 [D loss: 0.640127, acc.: 62.50%] [G loss: 0.710328]\n",
      "638 [D loss: 0.647394, acc.: 59.38%] [G loss: 0.739053]\n",
      "639 [D loss: 0.655732, acc.: 56.25%] [G loss: 0.743650]\n",
      "640 [D loss: 0.662000, acc.: 62.50%] [G loss: 0.749868]\n",
      "641 [D loss: 0.654605, acc.: 53.12%] [G loss: 0.730539]\n",
      "642 [D loss: 0.641616, acc.: 62.50%] [G loss: 0.748660]\n",
      "643 [D loss: 0.682252, acc.: 46.88%] [G loss: 0.759480]\n",
      "644 [D loss: 0.685187, acc.: 56.25%] [G loss: 0.724075]\n",
      "645 [D loss: 0.652587, acc.: 56.25%] [G loss: 0.738486]\n",
      "646 [D loss: 0.656878, acc.: 62.50%] [G loss: 0.758254]\n",
      "647 [D loss: 0.632895, acc.: 62.50%] [G loss: 0.774186]\n",
      "648 [D loss: 0.655332, acc.: 59.38%] [G loss: 0.792911]\n",
      "649 [D loss: 0.633231, acc.: 68.75%] [G loss: 0.775143]\n",
      "650 [D loss: 0.678636, acc.: 50.00%] [G loss: 0.756397]\n",
      "651 [D loss: 0.655987, acc.: 62.50%] [G loss: 0.759562]\n",
      "652 [D loss: 0.680578, acc.: 53.12%] [G loss: 0.727994]\n",
      "653 [D loss: 0.655867, acc.: 65.62%] [G loss: 0.745329]\n",
      "654 [D loss: 0.628229, acc.: 62.50%] [G loss: 0.757153]\n",
      "655 [D loss: 0.675220, acc.: 62.50%] [G loss: 0.783725]\n",
      "656 [D loss: 0.647606, acc.: 65.62%] [G loss: 0.771227]\n",
      "657 [D loss: 0.649556, acc.: 56.25%] [G loss: 0.757751]\n",
      "658 [D loss: 0.648183, acc.: 62.50%] [G loss: 0.753007]\n",
      "659 [D loss: 0.605706, acc.: 65.62%] [G loss: 0.730400]\n",
      "660 [D loss: 0.630604, acc.: 68.75%] [G loss: 0.738575]\n",
      "661 [D loss: 0.623804, acc.: 71.88%] [G loss: 0.730928]\n",
      "662 [D loss: 0.662693, acc.: 62.50%] [G loss: 0.722565]\n",
      "663 [D loss: 0.659772, acc.: 62.50%] [G loss: 0.715296]\n",
      "664 [D loss: 0.605434, acc.: 68.75%] [G loss: 0.739035]\n",
      "665 [D loss: 0.595765, acc.: 68.75%] [G loss: 0.750757]\n",
      "666 [D loss: 0.651164, acc.: 62.50%] [G loss: 0.742633]\n",
      "667 [D loss: 0.588038, acc.: 84.38%] [G loss: 0.737450]\n",
      "668 [D loss: 0.636626, acc.: 59.38%] [G loss: 0.740075]\n",
      "669 [D loss: 0.585297, acc.: 75.00%] [G loss: 0.741338]\n",
      "670 [D loss: 0.622106, acc.: 78.12%] [G loss: 0.732618]\n",
      "671 [D loss: 0.676061, acc.: 53.12%] [G loss: 0.720289]\n",
      "672 [D loss: 0.594124, acc.: 65.62%] [G loss: 0.743700]\n",
      "673 [D loss: 0.618605, acc.: 62.50%] [G loss: 0.772272]\n",
      "674 [D loss: 0.574351, acc.: 75.00%] [G loss: 0.820769]\n",
      "675 [D loss: 0.665280, acc.: 59.38%] [G loss: 0.815097]\n",
      "676 [D loss: 0.638728, acc.: 75.00%] [G loss: 0.790746]\n",
      "677 [D loss: 0.628099, acc.: 68.75%] [G loss: 0.748508]\n",
      "678 [D loss: 0.601301, acc.: 75.00%] [G loss: 0.761879]\n",
      "679 [D loss: 0.611388, acc.: 59.38%] [G loss: 0.772521]\n",
      "680 [D loss: 0.602212, acc.: 78.12%] [G loss: 0.773939]\n",
      "681 [D loss: 0.626201, acc.: 68.75%] [G loss: 0.764791]\n",
      "682 [D loss: 0.633537, acc.: 68.75%] [G loss: 0.764423]\n",
      "683 [D loss: 0.607150, acc.: 71.88%] [G loss: 0.774844]\n",
      "684 [D loss: 0.667450, acc.: 53.12%] [G loss: 0.739442]\n",
      "685 [D loss: 0.675098, acc.: 56.25%] [G loss: 0.748835]\n",
      "686 [D loss: 0.596064, acc.: 75.00%] [G loss: 0.757029]\n",
      "687 [D loss: 0.629713, acc.: 62.50%] [G loss: 0.753029]\n",
      "688 [D loss: 0.641978, acc.: 59.38%] [G loss: 0.732109]\n",
      "689 [D loss: 0.648670, acc.: 59.38%] [G loss: 0.751283]\n",
      "690 [D loss: 0.622786, acc.: 68.75%] [G loss: 0.737939]\n",
      "691 [D loss: 0.603887, acc.: 78.12%] [G loss: 0.741041]\n",
      "692 [D loss: 0.633940, acc.: 68.75%] [G loss: 0.735819]\n",
      "693 [D loss: 0.669520, acc.: 59.38%] [G loss: 0.776100]\n",
      "694 [D loss: 0.665570, acc.: 59.38%] [G loss: 0.794445]\n",
      "695 [D loss: 0.671534, acc.: 40.62%] [G loss: 0.815090]\n",
      "696 [D loss: 0.651708, acc.: 68.75%] [G loss: 0.810256]\n",
      "697 [D loss: 0.617525, acc.: 71.88%] [G loss: 0.736668]\n",
      "698 [D loss: 0.641337, acc.: 59.38%] [G loss: 0.697937]\n",
      "699 [D loss: 0.663416, acc.: 59.38%] [G loss: 0.698734]\n",
      "700 [D loss: 0.664322, acc.: 68.75%] [G loss: 0.695154]\n",
      "701 [D loss: 0.677413, acc.: 53.12%] [G loss: 0.711164]\n",
      "702 [D loss: 0.657560, acc.: 50.00%] [G loss: 0.741001]\n",
      "703 [D loss: 0.665934, acc.: 68.75%] [G loss: 0.785507]\n",
      "704 [D loss: 0.625874, acc.: 62.50%] [G loss: 0.815569]\n",
      "705 [D loss: 0.660313, acc.: 53.12%] [G loss: 0.770781]\n",
      "706 [D loss: 0.600668, acc.: 81.25%] [G loss: 0.810094]\n",
      "707 [D loss: 0.591303, acc.: 75.00%] [G loss: 0.831228]\n",
      "708 [D loss: 0.628384, acc.: 65.62%] [G loss: 0.750602]\n",
      "709 [D loss: 0.629354, acc.: 68.75%] [G loss: 0.743008]\n",
      "710 [D loss: 0.614988, acc.: 65.62%] [G loss: 0.789824]\n",
      "711 [D loss: 0.594980, acc.: 68.75%] [G loss: 0.768639]\n",
      "712 [D loss: 0.631476, acc.: 75.00%] [G loss: 0.790962]\n",
      "713 [D loss: 0.638392, acc.: 65.62%] [G loss: 0.796962]\n",
      "714 [D loss: 0.630476, acc.: 56.25%] [G loss: 0.813661]\n",
      "715 [D loss: 0.623891, acc.: 65.62%] [G loss: 0.786618]\n",
      "716 [D loss: 0.654547, acc.: 59.38%] [G loss: 0.760426]\n",
      "717 [D loss: 0.640474, acc.: 65.62%] [G loss: 0.759397]\n",
      "718 [D loss: 0.712552, acc.: 46.88%] [G loss: 0.754811]\n",
      "719 [D loss: 0.654576, acc.: 56.25%] [G loss: 0.784794]\n",
      "720 [D loss: 0.710159, acc.: 59.38%] [G loss: 0.735473]\n",
      "721 [D loss: 0.648203, acc.: 71.88%] [G loss: 0.720529]\n",
      "722 [D loss: 0.639842, acc.: 56.25%] [G loss: 0.716660]\n",
      "723 [D loss: 0.645267, acc.: 59.38%] [G loss: 0.719041]\n",
      "724 [D loss: 0.650391, acc.: 65.62%] [G loss: 0.761183]\n",
      "725 [D loss: 0.620546, acc.: 71.88%] [G loss: 0.743002]\n",
      "726 [D loss: 0.645464, acc.: 65.62%] [G loss: 0.755298]\n",
      "727 [D loss: 0.674134, acc.: 59.38%] [G loss: 0.775665]\n",
      "728 [D loss: 0.615261, acc.: 68.75%] [G loss: 0.823807]\n",
      "729 [D loss: 0.684198, acc.: 59.38%] [G loss: 0.823590]\n",
      "730 [D loss: 0.694904, acc.: 50.00%] [G loss: 0.808565]\n",
      "731 [D loss: 0.608031, acc.: 75.00%] [G loss: 0.782744]\n",
      "732 [D loss: 0.655658, acc.: 56.25%] [G loss: 0.728739]\n",
      "733 [D loss: 0.627968, acc.: 65.62%] [G loss: 0.765527]\n",
      "734 [D loss: 0.619865, acc.: 68.75%] [G loss: 0.801415]\n",
      "735 [D loss: 0.679348, acc.: 50.00%] [G loss: 0.800568]\n",
      "736 [D loss: 0.684707, acc.: 53.12%] [G loss: 0.819456]\n",
      "737 [D loss: 0.622997, acc.: 71.88%] [G loss: 0.785939]\n",
      "738 [D loss: 0.687666, acc.: 59.38%] [G loss: 0.783716]\n",
      "739 [D loss: 0.637417, acc.: 59.38%] [G loss: 0.783075]\n",
      "740 [D loss: 0.659382, acc.: 50.00%] [G loss: 0.741290]\n",
      "741 [D loss: 0.652812, acc.: 59.38%] [G loss: 0.724079]\n",
      "742 [D loss: 0.632131, acc.: 65.62%] [G loss: 0.761403]\n",
      "743 [D loss: 0.603123, acc.: 81.25%] [G loss: 0.748748]\n",
      "744 [D loss: 0.640621, acc.: 71.88%] [G loss: 0.786380]\n",
      "745 [D loss: 0.666550, acc.: 53.12%] [G loss: 0.769701]\n",
      "746 [D loss: 0.677063, acc.: 56.25%] [G loss: 0.773472]\n",
      "747 [D loss: 0.620934, acc.: 71.88%] [G loss: 0.831393]\n",
      "748 [D loss: 0.622417, acc.: 71.88%] [G loss: 0.832652]\n",
      "749 [D loss: 0.591744, acc.: 81.25%] [G loss: 0.835231]\n",
      "750 [D loss: 0.658208, acc.: 59.38%] [G loss: 0.771699]\n",
      "751 [D loss: 0.594396, acc.: 71.88%] [G loss: 0.782736]\n",
      "752 [D loss: 0.670524, acc.: 56.25%] [G loss: 0.797591]\n",
      "753 [D loss: 0.598071, acc.: 59.38%] [G loss: 0.775922]\n",
      "754 [D loss: 0.622946, acc.: 68.75%] [G loss: 0.755822]\n",
      "755 [D loss: 0.627253, acc.: 71.88%] [G loss: 0.792008]\n",
      "756 [D loss: 0.599630, acc.: 75.00%] [G loss: 0.763794]\n",
      "757 [D loss: 0.635665, acc.: 62.50%] [G loss: 0.777119]\n",
      "758 [D loss: 0.668407, acc.: 46.88%] [G loss: 0.718843]\n",
      "759 [D loss: 0.639224, acc.: 59.38%] [G loss: 0.775695]\n",
      "760 [D loss: 0.579365, acc.: 87.50%] [G loss: 0.758838]\n",
      "761 [D loss: 0.641835, acc.: 59.38%] [G loss: 0.775455]\n",
      "762 [D loss: 0.612255, acc.: 68.75%] [G loss: 0.774229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763 [D loss: 0.637462, acc.: 65.62%] [G loss: 0.784589]\n",
      "764 [D loss: 0.682406, acc.: 53.12%] [G loss: 0.749093]\n",
      "765 [D loss: 0.713288, acc.: 50.00%] [G loss: 0.748215]\n",
      "766 [D loss: 0.641025, acc.: 59.38%] [G loss: 0.744901]\n",
      "767 [D loss: 0.625280, acc.: 56.25%] [G loss: 0.743869]\n",
      "768 [D loss: 0.634579, acc.: 59.38%] [G loss: 0.770559]\n",
      "769 [D loss: 0.668927, acc.: 56.25%] [G loss: 0.781342]\n",
      "770 [D loss: 0.638917, acc.: 62.50%] [G loss: 0.767672]\n",
      "771 [D loss: 0.677318, acc.: 62.50%] [G loss: 0.767099]\n",
      "772 [D loss: 0.618942, acc.: 62.50%] [G loss: 0.756525]\n",
      "773 [D loss: 0.623195, acc.: 59.38%] [G loss: 0.772472]\n",
      "774 [D loss: 0.634898, acc.: 68.75%] [G loss: 0.781272]\n",
      "775 [D loss: 0.637397, acc.: 62.50%] [G loss: 0.788803]\n",
      "776 [D loss: 0.630292, acc.: 56.25%] [G loss: 0.773293]\n",
      "777 [D loss: 0.633010, acc.: 71.88%] [G loss: 0.783179]\n",
      "778 [D loss: 0.637233, acc.: 56.25%] [G loss: 0.790463]\n",
      "779 [D loss: 0.611114, acc.: 78.12%] [G loss: 0.776461]\n",
      "780 [D loss: 0.635977, acc.: 59.38%] [G loss: 0.771934]\n",
      "781 [D loss: 0.654045, acc.: 65.62%] [G loss: 0.754397]\n",
      "782 [D loss: 0.631311, acc.: 65.62%] [G loss: 0.757060]\n",
      "783 [D loss: 0.594285, acc.: 71.88%] [G loss: 0.766616]\n",
      "784 [D loss: 0.639585, acc.: 62.50%] [G loss: 0.756891]\n",
      "785 [D loss: 0.586946, acc.: 68.75%] [G loss: 0.727991]\n",
      "786 [D loss: 0.611228, acc.: 75.00%] [G loss: 0.731984]\n",
      "787 [D loss: 0.622164, acc.: 62.50%] [G loss: 0.706566]\n",
      "788 [D loss: 0.660526, acc.: 62.50%] [G loss: 0.731407]\n",
      "789 [D loss: 0.600677, acc.: 75.00%] [G loss: 0.744067]\n",
      "790 [D loss: 0.632292, acc.: 71.88%] [G loss: 0.804231]\n",
      "791 [D loss: 0.583854, acc.: 78.12%] [G loss: 0.800421]\n",
      "792 [D loss: 0.591995, acc.: 71.88%] [G loss: 0.824121]\n",
      "793 [D loss: 0.593320, acc.: 68.75%] [G loss: 0.810784]\n",
      "794 [D loss: 0.665551, acc.: 46.88%] [G loss: 0.847736]\n",
      "795 [D loss: 0.602278, acc.: 68.75%] [G loss: 0.834508]\n",
      "796 [D loss: 0.636457, acc.: 59.38%] [G loss: 0.785399]\n",
      "797 [D loss: 0.654167, acc.: 59.38%] [G loss: 0.739004]\n",
      "798 [D loss: 0.578866, acc.: 71.88%] [G loss: 0.764710]\n",
      "799 [D loss: 0.604562, acc.: 71.88%] [G loss: 0.777515]\n",
      "800 [D loss: 0.673287, acc.: 53.12%] [G loss: 0.739172]\n",
      "801 [D loss: 0.710642, acc.: 59.38%] [G loss: 0.746673]\n",
      "802 [D loss: 0.601068, acc.: 75.00%] [G loss: 0.733148]\n",
      "803 [D loss: 0.648123, acc.: 59.38%] [G loss: 0.805082]\n",
      "804 [D loss: 0.665601, acc.: 56.25%] [G loss: 0.811387]\n",
      "805 [D loss: 0.639015, acc.: 59.38%] [G loss: 0.814352]\n",
      "806 [D loss: 0.640256, acc.: 65.62%] [G loss: 0.788292]\n",
      "807 [D loss: 0.618859, acc.: 71.88%] [G loss: 0.774702]\n",
      "808 [D loss: 0.624042, acc.: 62.50%] [G loss: 0.736474]\n",
      "809 [D loss: 0.622417, acc.: 65.62%] [G loss: 0.756293]\n",
      "810 [D loss: 0.721270, acc.: 34.38%] [G loss: 0.739120]\n",
      "811 [D loss: 0.666705, acc.: 50.00%] [G loss: 0.745315]\n",
      "812 [D loss: 0.629421, acc.: 68.75%] [G loss: 0.779478]\n",
      "813 [D loss: 0.653544, acc.: 62.50%] [G loss: 0.712386]\n",
      "814 [D loss: 0.634677, acc.: 53.12%] [G loss: 0.721182]\n",
      "815 [D loss: 0.667608, acc.: 56.25%] [G loss: 0.749964]\n",
      "816 [D loss: 0.643822, acc.: 62.50%] [G loss: 0.752954]\n",
      "817 [D loss: 0.698649, acc.: 65.62%] [G loss: 0.774293]\n",
      "818 [D loss: 0.663095, acc.: 59.38%] [G loss: 0.814761]\n",
      "819 [D loss: 0.663172, acc.: 50.00%] [G loss: 0.858085]\n",
      "820 [D loss: 0.665671, acc.: 59.38%] [G loss: 0.801207]\n",
      "821 [D loss: 0.661483, acc.: 62.50%] [G loss: 0.797886]\n",
      "822 [D loss: 0.656346, acc.: 56.25%] [G loss: 0.727564]\n",
      "823 [D loss: 0.670864, acc.: 56.25%] [G loss: 0.734860]\n",
      "824 [D loss: 0.670029, acc.: 56.25%] [G loss: 0.738131]\n",
      "825 [D loss: 0.613773, acc.: 65.62%] [G loss: 0.786163]\n",
      "826 [D loss: 0.631354, acc.: 75.00%] [G loss: 0.748491]\n",
      "827 [D loss: 0.731244, acc.: 40.62%] [G loss: 0.712534]\n",
      "828 [D loss: 0.650770, acc.: 62.50%] [G loss: 0.731952]\n",
      "829 [D loss: 0.653269, acc.: 59.38%] [G loss: 0.778119]\n",
      "830 [D loss: 0.661324, acc.: 65.62%] [G loss: 0.786476]\n",
      "831 [D loss: 0.702708, acc.: 43.75%] [G loss: 0.767548]\n",
      "832 [D loss: 0.624464, acc.: 62.50%] [G loss: 0.763027]\n",
      "833 [D loss: 0.652069, acc.: 62.50%] [G loss: 0.767012]\n",
      "834 [D loss: 0.640163, acc.: 71.88%] [G loss: 0.739123]\n",
      "835 [D loss: 0.644995, acc.: 65.62%] [G loss: 0.711410]\n",
      "836 [D loss: 0.663291, acc.: 59.38%] [G loss: 0.775152]\n",
      "837 [D loss: 0.638174, acc.: 59.38%] [G loss: 0.724297]\n",
      "838 [D loss: 0.669505, acc.: 65.62%] [G loss: 0.728820]\n",
      "839 [D loss: 0.630945, acc.: 68.75%] [G loss: 0.770433]\n",
      "840 [D loss: 0.629486, acc.: 62.50%] [G loss: 0.769806]\n",
      "841 [D loss: 0.644283, acc.: 59.38%] [G loss: 0.760267]\n",
      "842 [D loss: 0.664962, acc.: 62.50%] [G loss: 0.795706]\n",
      "843 [D loss: 0.623656, acc.: 65.62%] [G loss: 0.802081]\n",
      "844 [D loss: 0.704513, acc.: 50.00%] [G loss: 0.798878]\n",
      "845 [D loss: 0.700372, acc.: 50.00%] [G loss: 0.764018]\n",
      "846 [D loss: 0.704615, acc.: 43.75%] [G loss: 0.767586]\n",
      "847 [D loss: 0.638506, acc.: 65.62%] [G loss: 0.738731]\n",
      "848 [D loss: 0.684254, acc.: 56.25%] [G loss: 0.742973]\n",
      "849 [D loss: 0.660136, acc.: 56.25%] [G loss: 0.731283]\n",
      "850 [D loss: 0.694057, acc.: 46.88%] [G loss: 0.730938]\n",
      "851 [D loss: 0.669765, acc.: 46.88%] [G loss: 0.719610]\n",
      "852 [D loss: 0.671052, acc.: 56.25%] [G loss: 0.782050]\n",
      "853 [D loss: 0.640064, acc.: 62.50%] [G loss: 0.764702]\n",
      "854 [D loss: 0.653225, acc.: 53.12%] [G loss: 0.781534]\n",
      "855 [D loss: 0.682891, acc.: 43.75%] [G loss: 0.748455]\n",
      "856 [D loss: 0.657351, acc.: 65.62%] [G loss: 0.781076]\n",
      "857 [D loss: 0.620995, acc.: 62.50%] [G loss: 0.756606]\n",
      "858 [D loss: 0.658209, acc.: 59.38%] [G loss: 0.798632]\n",
      "859 [D loss: 0.628254, acc.: 59.38%] [G loss: 0.814167]\n",
      "860 [D loss: 0.657532, acc.: 59.38%] [G loss: 0.805398]\n",
      "861 [D loss: 0.611645, acc.: 68.75%] [G loss: 0.795591]\n",
      "862 [D loss: 0.631759, acc.: 62.50%] [G loss: 0.741105]\n",
      "863 [D loss: 0.668762, acc.: 46.88%] [G loss: 0.791533]\n",
      "864 [D loss: 0.618713, acc.: 62.50%] [G loss: 0.778477]\n",
      "865 [D loss: 0.607176, acc.: 75.00%] [G loss: 0.781024]\n",
      "866 [D loss: 0.634401, acc.: 68.75%] [G loss: 0.728955]\n",
      "867 [D loss: 0.634604, acc.: 65.62%] [G loss: 0.677157]\n",
      "868 [D loss: 0.647922, acc.: 65.62%] [G loss: 0.699526]\n",
      "869 [D loss: 0.655267, acc.: 50.00%] [G loss: 0.750032]\n",
      "870 [D loss: 0.647822, acc.: 62.50%] [G loss: 0.731292]\n",
      "871 [D loss: 0.630921, acc.: 71.88%] [G loss: 0.742416]\n",
      "872 [D loss: 0.636817, acc.: 65.62%] [G loss: 0.726240]\n",
      "873 [D loss: 0.652899, acc.: 56.25%] [G loss: 0.746378]\n",
      "874 [D loss: 0.669120, acc.: 46.88%] [G loss: 0.786924]\n",
      "875 [D loss: 0.645399, acc.: 56.25%] [G loss: 0.786380]\n",
      "876 [D loss: 0.606128, acc.: 90.62%] [G loss: 0.789163]\n",
      "877 [D loss: 0.652486, acc.: 62.50%] [G loss: 0.747212]\n",
      "878 [D loss: 0.656707, acc.: 62.50%] [G loss: 0.746397]\n",
      "879 [D loss: 0.573950, acc.: 78.12%] [G loss: 0.781288]\n",
      "880 [D loss: 0.611920, acc.: 65.62%] [G loss: 0.795721]\n",
      "881 [D loss: 0.572606, acc.: 81.25%] [G loss: 0.765970]\n",
      "882 [D loss: 0.657775, acc.: 50.00%] [G loss: 0.824208]\n",
      "883 [D loss: 0.629544, acc.: 71.88%] [G loss: 0.771669]\n",
      "884 [D loss: 0.635010, acc.: 71.88%] [G loss: 0.777249]\n",
      "885 [D loss: 0.596094, acc.: 68.75%] [G loss: 0.801776]\n",
      "886 [D loss: 0.596982, acc.: 75.00%] [G loss: 0.769419]\n",
      "887 [D loss: 0.648683, acc.: 59.38%] [G loss: 0.764552]\n",
      "888 [D loss: 0.646353, acc.: 53.12%] [G loss: 0.794673]\n",
      "889 [D loss: 0.592087, acc.: 75.00%] [G loss: 0.789572]\n",
      "890 [D loss: 0.717168, acc.: 43.75%] [G loss: 0.712286]\n",
      "891 [D loss: 0.581084, acc.: 68.75%] [G loss: 0.736210]\n",
      "892 [D loss: 0.618165, acc.: 65.62%] [G loss: 0.777130]\n",
      "893 [D loss: 0.677156, acc.: 62.50%] [G loss: 0.804057]\n",
      "894 [D loss: 0.588370, acc.: 75.00%] [G loss: 0.812083]\n",
      "895 [D loss: 0.661137, acc.: 62.50%] [G loss: 0.802071]\n",
      "896 [D loss: 0.645351, acc.: 71.88%] [G loss: 0.770725]\n",
      "897 [D loss: 0.625186, acc.: 62.50%] [G loss: 0.773562]\n",
      "898 [D loss: 0.630430, acc.: 62.50%] [G loss: 0.754183]\n",
      "899 [D loss: 0.645950, acc.: 59.38%] [G loss: 0.744068]\n",
      "900 [D loss: 0.627333, acc.: 62.50%] [G loss: 0.784097]\n",
      "901 [D loss: 0.614462, acc.: 75.00%] [G loss: 0.782310]\n",
      "902 [D loss: 0.638186, acc.: 50.00%] [G loss: 0.790519]\n",
      "903 [D loss: 0.653136, acc.: 71.88%] [G loss: 0.782335]\n",
      "904 [D loss: 0.610131, acc.: 78.12%] [G loss: 0.808315]\n",
      "905 [D loss: 0.656889, acc.: 65.62%] [G loss: 0.804058]\n",
      "906 [D loss: 0.586908, acc.: 75.00%] [G loss: 0.780923]\n",
      "907 [D loss: 0.636681, acc.: 59.38%] [G loss: 0.770577]\n",
      "908 [D loss: 0.692675, acc.: 53.12%] [G loss: 0.762945]\n",
      "909 [D loss: 0.592014, acc.: 68.75%] [G loss: 0.776103]\n",
      "910 [D loss: 0.639672, acc.: 68.75%] [G loss: 0.781466]\n",
      "911 [D loss: 0.632086, acc.: 71.88%] [G loss: 0.785786]\n",
      "912 [D loss: 0.606877, acc.: 71.88%] [G loss: 0.784088]\n",
      "913 [D loss: 0.651079, acc.: 59.38%] [G loss: 0.745524]\n",
      "914 [D loss: 0.620603, acc.: 65.62%] [G loss: 0.754479]\n",
      "915 [D loss: 0.631825, acc.: 65.62%] [G loss: 0.789710]\n",
      "916 [D loss: 0.663805, acc.: 62.50%] [G loss: 0.810438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917 [D loss: 0.657491, acc.: 62.50%] [G loss: 0.796192]\n",
      "918 [D loss: 0.588667, acc.: 68.75%] [G loss: 0.804036]\n",
      "919 [D loss: 0.599449, acc.: 81.25%] [G loss: 0.825940]\n",
      "920 [D loss: 0.709418, acc.: 50.00%] [G loss: 0.803618]\n",
      "921 [D loss: 0.702906, acc.: 43.75%] [G loss: 0.760154]\n",
      "922 [D loss: 0.655651, acc.: 53.12%] [G loss: 0.794055]\n",
      "923 [D loss: 0.607273, acc.: 65.62%] [G loss: 0.800623]\n",
      "924 [D loss: 0.699419, acc.: 53.12%] [G loss: 0.757725]\n",
      "925 [D loss: 0.699724, acc.: 53.12%] [G loss: 0.799448]\n",
      "926 [D loss: 0.637899, acc.: 68.75%] [G loss: 0.799949]\n",
      "927 [D loss: 0.605325, acc.: 78.12%] [G loss: 0.738268]\n",
      "928 [D loss: 0.629114, acc.: 59.38%] [G loss: 0.754175]\n",
      "929 [D loss: 0.643017, acc.: 56.25%] [G loss: 0.756878]\n",
      "930 [D loss: 0.620188, acc.: 75.00%] [G loss: 0.755862]\n",
      "931 [D loss: 0.617936, acc.: 68.75%] [G loss: 0.764526]\n",
      "932 [D loss: 0.605342, acc.: 71.88%] [G loss: 0.732027]\n",
      "933 [D loss: 0.660515, acc.: 68.75%] [G loss: 0.734464]\n",
      "934 [D loss: 0.619012, acc.: 68.75%] [G loss: 0.731223]\n",
      "935 [D loss: 0.661183, acc.: 59.38%] [G loss: 0.737568]\n",
      "936 [D loss: 0.641964, acc.: 50.00%] [G loss: 0.756185]\n",
      "937 [D loss: 0.631679, acc.: 59.38%] [G loss: 0.833857]\n",
      "938 [D loss: 0.622829, acc.: 68.75%] [G loss: 0.821138]\n",
      "939 [D loss: 0.668255, acc.: 62.50%] [G loss: 0.816593]\n",
      "940 [D loss: 0.638267, acc.: 56.25%] [G loss: 0.856934]\n",
      "941 [D loss: 0.689730, acc.: 46.88%] [G loss: 0.791840]\n",
      "942 [D loss: 0.622096, acc.: 59.38%] [G loss: 0.811108]\n",
      "943 [D loss: 0.609899, acc.: 59.38%] [G loss: 0.825090]\n",
      "944 [D loss: 0.602346, acc.: 56.25%] [G loss: 0.839157]\n",
      "945 [D loss: 0.652499, acc.: 71.88%] [G loss: 0.777430]\n",
      "946 [D loss: 0.684815, acc.: 50.00%] [G loss: 0.736817]\n",
      "947 [D loss: 0.650499, acc.: 65.62%] [G loss: 0.739621]\n",
      "948 [D loss: 0.622946, acc.: 75.00%] [G loss: 0.746668]\n",
      "949 [D loss: 0.588284, acc.: 78.12%] [G loss: 0.788540]\n",
      "950 [D loss: 0.686376, acc.: 59.38%] [G loss: 0.755492]\n",
      "951 [D loss: 0.702412, acc.: 50.00%] [G loss: 0.821399]\n",
      "952 [D loss: 0.669151, acc.: 50.00%] [G loss: 0.850581]\n",
      "953 [D loss: 0.633869, acc.: 75.00%] [G loss: 0.830029]\n",
      "954 [D loss: 0.667606, acc.: 59.38%] [G loss: 0.793889]\n",
      "955 [D loss: 0.630606, acc.: 59.38%] [G loss: 0.768594]\n",
      "956 [D loss: 0.656190, acc.: 56.25%] [G loss: 0.732490]\n",
      "957 [D loss: 0.636121, acc.: 62.50%] [G loss: 0.777011]\n",
      "958 [D loss: 0.649379, acc.: 71.88%] [G loss: 0.810379]\n",
      "959 [D loss: 0.632365, acc.: 62.50%] [G loss: 0.794188]\n",
      "960 [D loss: 0.613286, acc.: 81.25%] [G loss: 0.824948]\n",
      "961 [D loss: 0.669313, acc.: 59.38%] [G loss: 0.831025]\n",
      "962 [D loss: 0.652733, acc.: 53.12%] [G loss: 0.839124]\n",
      "963 [D loss: 0.628271, acc.: 59.38%] [G loss: 0.807029]\n",
      "964 [D loss: 0.611014, acc.: 71.88%] [G loss: 0.831890]\n",
      "965 [D loss: 0.615423, acc.: 75.00%] [G loss: 0.798800]\n",
      "966 [D loss: 0.641100, acc.: 62.50%] [G loss: 0.797064]\n",
      "967 [D loss: 0.660468, acc.: 56.25%] [G loss: 0.743208]\n",
      "968 [D loss: 0.649058, acc.: 68.75%] [G loss: 0.814212]\n",
      "969 [D loss: 0.675688, acc.: 50.00%] [G loss: 0.779328]\n",
      "970 [D loss: 0.556638, acc.: 78.12%] [G loss: 0.798258]\n",
      "971 [D loss: 0.604717, acc.: 68.75%] [G loss: 0.805940]\n",
      "972 [D loss: 0.613812, acc.: 71.88%] [G loss: 0.771067]\n",
      "973 [D loss: 0.612695, acc.: 56.25%] [G loss: 0.776363]\n",
      "974 [D loss: 0.612754, acc.: 68.75%] [G loss: 0.792994]\n",
      "975 [D loss: 0.641971, acc.: 62.50%] [G loss: 0.808285]\n",
      "976 [D loss: 0.635444, acc.: 65.62%] [G loss: 0.797602]\n",
      "977 [D loss: 0.624399, acc.: 62.50%] [G loss: 0.750278]\n",
      "978 [D loss: 0.643017, acc.: 62.50%] [G loss: 0.770771]\n",
      "979 [D loss: 0.646935, acc.: 68.75%] [G loss: 0.783449]\n",
      "980 [D loss: 0.639254, acc.: 59.38%] [G loss: 0.791048]\n",
      "981 [D loss: 0.615459, acc.: 68.75%] [G loss: 0.776333]\n",
      "982 [D loss: 0.606762, acc.: 65.62%] [G loss: 0.772491]\n",
      "983 [D loss: 0.635507, acc.: 59.38%] [G loss: 0.768110]\n",
      "984 [D loss: 0.666472, acc.: 53.12%] [G loss: 0.763374]\n",
      "985 [D loss: 0.668646, acc.: 53.12%] [G loss: 0.757907]\n",
      "986 [D loss: 0.690770, acc.: 50.00%] [G loss: 0.747536]\n",
      "987 [D loss: 0.663571, acc.: 62.50%] [G loss: 0.811094]\n",
      "988 [D loss: 0.651691, acc.: 68.75%] [G loss: 0.785508]\n",
      "989 [D loss: 0.671424, acc.: 56.25%] [G loss: 0.818268]\n",
      "990 [D loss: 0.644593, acc.: 59.38%] [G loss: 0.779791]\n",
      "991 [D loss: 0.632500, acc.: 56.25%] [G loss: 0.833756]\n",
      "992 [D loss: 0.650912, acc.: 62.50%] [G loss: 0.835539]\n",
      "993 [D loss: 0.687483, acc.: 53.12%] [G loss: 0.845128]\n",
      "994 [D loss: 0.641436, acc.: 65.62%] [G loss: 0.779862]\n",
      "995 [D loss: 0.657966, acc.: 68.75%] [G loss: 0.859027]\n",
      "996 [D loss: 0.672513, acc.: 50.00%] [G loss: 0.841619]\n",
      "997 [D loss: 0.699831, acc.: 50.00%] [G loss: 0.797562]\n",
      "998 [D loss: 0.649295, acc.: 59.38%] [G loss: 0.816673]\n",
      "999 [D loss: 0.641122, acc.: 68.75%] [G loss: 0.778483]\n",
      "1000 [D loss: 0.648425, acc.: 68.75%] [G loss: 0.756306]\n",
      "1001 [D loss: 0.675892, acc.: 46.88%] [G loss: 0.764620]\n",
      "1002 [D loss: 0.597744, acc.: 68.75%] [G loss: 0.831489]\n",
      "1003 [D loss: 0.616536, acc.: 71.88%] [G loss: 0.803459]\n",
      "1004 [D loss: 0.681173, acc.: 56.25%] [G loss: 0.698523]\n",
      "1005 [D loss: 0.662214, acc.: 56.25%] [G loss: 0.776582]\n",
      "1006 [D loss: 0.636594, acc.: 59.38%] [G loss: 0.805627]\n",
      "1007 [D loss: 0.627661, acc.: 65.62%] [G loss: 0.783441]\n",
      "1008 [D loss: 0.659824, acc.: 62.50%] [G loss: 0.781156]\n",
      "1009 [D loss: 0.654083, acc.: 62.50%] [G loss: 0.726450]\n",
      "1010 [D loss: 0.663196, acc.: 56.25%] [G loss: 0.748757]\n",
      "1011 [D loss: 0.627886, acc.: 65.62%] [G loss: 0.801376]\n",
      "1012 [D loss: 0.640730, acc.: 62.50%] [G loss: 0.798541]\n",
      "1013 [D loss: 0.603719, acc.: 75.00%] [G loss: 0.831899]\n",
      "1014 [D loss: 0.698406, acc.: 59.38%] [G loss: 0.839225]\n",
      "1015 [D loss: 0.627102, acc.: 59.38%] [G loss: 0.880499]\n",
      "1016 [D loss: 0.582547, acc.: 87.50%] [G loss: 0.876045]\n",
      "1017 [D loss: 0.626628, acc.: 71.88%] [G loss: 0.860620]\n",
      "1018 [D loss: 0.577003, acc.: 71.88%] [G loss: 0.880388]\n",
      "1019 [D loss: 0.677325, acc.: 56.25%] [G loss: 0.831692]\n",
      "1020 [D loss: 0.635265, acc.: 59.38%] [G loss: 0.823018]\n",
      "1021 [D loss: 0.633795, acc.: 62.50%] [G loss: 0.841852]\n",
      "1022 [D loss: 0.630196, acc.: 65.62%] [G loss: 0.853436]\n",
      "1023 [D loss: 0.670653, acc.: 46.88%] [G loss: 0.847037]\n",
      "1024 [D loss: 0.667846, acc.: 56.25%] [G loss: 0.830931]\n",
      "1025 [D loss: 0.678717, acc.: 53.12%] [G loss: 0.785894]\n",
      "1026 [D loss: 0.637373, acc.: 65.62%] [G loss: 0.781326]\n",
      "1027 [D loss: 0.606777, acc.: 75.00%] [G loss: 0.810982]\n",
      "1028 [D loss: 0.656400, acc.: 56.25%] [G loss: 0.791857]\n",
      "1029 [D loss: 0.677656, acc.: 43.75%] [G loss: 0.796707]\n",
      "1030 [D loss: 0.629847, acc.: 71.88%] [G loss: 0.786345]\n",
      "1031 [D loss: 0.664147, acc.: 59.38%] [G loss: 0.782337]\n",
      "1032 [D loss: 0.630027, acc.: 59.38%] [G loss: 0.787336]\n",
      "1033 [D loss: 0.586096, acc.: 68.75%] [G loss: 0.829638]\n",
      "1034 [D loss: 0.639616, acc.: 65.62%] [G loss: 0.815431]\n",
      "1035 [D loss: 0.607752, acc.: 75.00%] [G loss: 0.803872]\n",
      "1036 [D loss: 0.635971, acc.: 68.75%] [G loss: 0.836044]\n",
      "1037 [D loss: 0.677127, acc.: 62.50%] [G loss: 0.808201]\n",
      "1038 [D loss: 0.670836, acc.: 53.12%] [G loss: 0.774261]\n",
      "1039 [D loss: 0.638988, acc.: 62.50%] [G loss: 0.800143]\n",
      "1040 [D loss: 0.682842, acc.: 56.25%] [G loss: 0.785402]\n",
      "1041 [D loss: 0.618596, acc.: 71.88%] [G loss: 0.819355]\n",
      "1042 [D loss: 0.602197, acc.: 81.25%] [G loss: 0.828629]\n",
      "1043 [D loss: 0.624568, acc.: 68.75%] [G loss: 0.831495]\n",
      "1044 [D loss: 0.677987, acc.: 53.12%] [G loss: 0.825950]\n",
      "1045 [D loss: 0.606971, acc.: 65.62%] [G loss: 0.809702]\n",
      "1046 [D loss: 0.622015, acc.: 62.50%] [G loss: 0.827526]\n",
      "1047 [D loss: 0.569480, acc.: 78.12%] [G loss: 0.848665]\n",
      "1048 [D loss: 0.594042, acc.: 78.12%] [G loss: 0.880948]\n",
      "1049 [D loss: 0.555717, acc.: 81.25%] [G loss: 0.865252]\n",
      "1050 [D loss: 0.620758, acc.: 62.50%] [G loss: 0.863963]\n",
      "1051 [D loss: 0.652983, acc.: 65.62%] [G loss: 0.825867]\n",
      "1052 [D loss: 0.622658, acc.: 65.62%] [G loss: 0.804696]\n",
      "1053 [D loss: 0.625146, acc.: 71.88%] [G loss: 0.818266]\n",
      "1054 [D loss: 0.638016, acc.: 65.62%] [G loss: 0.807165]\n",
      "1055 [D loss: 0.620443, acc.: 62.50%] [G loss: 0.795179]\n",
      "1056 [D loss: 0.600457, acc.: 71.88%] [G loss: 0.836430]\n",
      "1057 [D loss: 0.569364, acc.: 78.12%] [G loss: 0.814947]\n",
      "1058 [D loss: 0.629216, acc.: 71.88%] [G loss: 0.829651]\n",
      "1059 [D loss: 0.584692, acc.: 84.38%] [G loss: 0.836827]\n",
      "1060 [D loss: 0.582976, acc.: 84.38%] [G loss: 0.828778]\n",
      "1061 [D loss: 0.635390, acc.: 65.62%] [G loss: 0.809279]\n",
      "1062 [D loss: 0.584024, acc.: 75.00%] [G loss: 0.834913]\n",
      "1063 [D loss: 0.637852, acc.: 71.88%] [G loss: 0.811889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064 [D loss: 0.689970, acc.: 53.12%] [G loss: 0.809226]\n",
      "1065 [D loss: 0.623435, acc.: 78.12%] [G loss: 0.817591]\n",
      "1066 [D loss: 0.628074, acc.: 68.75%] [G loss: 0.796404]\n",
      "1067 [D loss: 0.577900, acc.: 71.88%] [G loss: 0.840961]\n",
      "1068 [D loss: 0.590561, acc.: 71.88%] [G loss: 0.864513]\n",
      "1069 [D loss: 0.655830, acc.: 62.50%] [G loss: 0.834610]\n",
      "1070 [D loss: 0.647993, acc.: 65.62%] [G loss: 0.848035]\n",
      "1071 [D loss: 0.617680, acc.: 75.00%] [G loss: 0.817187]\n",
      "1072 [D loss: 0.610314, acc.: 71.88%] [G loss: 0.864845]\n",
      "1073 [D loss: 0.627136, acc.: 71.88%] [G loss: 0.834241]\n",
      "1074 [D loss: 0.656507, acc.: 56.25%] [G loss: 0.838535]\n",
      "1075 [D loss: 0.616009, acc.: 56.25%] [G loss: 0.824657]\n",
      "1076 [D loss: 0.647599, acc.: 65.62%] [G loss: 0.809439]\n",
      "1077 [D loss: 0.645650, acc.: 59.38%] [G loss: 0.797189]\n",
      "1078 [D loss: 0.643722, acc.: 71.88%] [G loss: 0.800794]\n",
      "1079 [D loss: 0.598690, acc.: 59.38%] [G loss: 0.799689]\n",
      "1080 [D loss: 0.621815, acc.: 59.38%] [G loss: 0.862755]\n",
      "1081 [D loss: 0.641780, acc.: 71.88%] [G loss: 0.838127]\n",
      "1082 [D loss: 0.616824, acc.: 75.00%] [G loss: 0.811693]\n",
      "1083 [D loss: 0.609814, acc.: 71.88%] [G loss: 0.782920]\n",
      "1084 [D loss: 0.675975, acc.: 53.12%] [G loss: 0.797796]\n",
      "1085 [D loss: 0.610585, acc.: 68.75%] [G loss: 0.854955]\n",
      "1086 [D loss: 0.582780, acc.: 81.25%] [G loss: 0.839295]\n",
      "1087 [D loss: 0.628469, acc.: 53.12%] [G loss: 0.864185]\n",
      "1088 [D loss: 0.667165, acc.: 56.25%] [G loss: 0.811068]\n",
      "1089 [D loss: 0.600367, acc.: 71.88%] [G loss: 0.788101]\n",
      "1090 [D loss: 0.621206, acc.: 71.88%] [G loss: 0.790093]\n",
      "1091 [D loss: 0.640793, acc.: 62.50%] [G loss: 0.857916]\n",
      "1092 [D loss: 0.651581, acc.: 56.25%] [G loss: 0.824334]\n",
      "1093 [D loss: 0.594231, acc.: 75.00%] [G loss: 0.844112]\n",
      "1094 [D loss: 0.570909, acc.: 78.12%] [G loss: 0.844832]\n",
      "1095 [D loss: 0.624725, acc.: 68.75%] [G loss: 0.808761]\n",
      "1096 [D loss: 0.641112, acc.: 59.38%] [G loss: 0.827702]\n",
      "1097 [D loss: 0.579659, acc.: 81.25%] [G loss: 0.840519]\n",
      "1098 [D loss: 0.629449, acc.: 50.00%] [G loss: 0.851650]\n",
      "1099 [D loss: 0.611345, acc.: 65.62%] [G loss: 0.806549]\n",
      "1100 [D loss: 0.590442, acc.: 75.00%] [G loss: 0.813077]\n",
      "1101 [D loss: 0.629968, acc.: 62.50%] [G loss: 0.818547]\n",
      "1102 [D loss: 0.692209, acc.: 56.25%] [G loss: 0.789063]\n",
      "1103 [D loss: 0.591013, acc.: 71.88%] [G loss: 0.825667]\n",
      "1104 [D loss: 0.635800, acc.: 78.12%] [G loss: 0.816533]\n",
      "1105 [D loss: 0.634986, acc.: 65.62%] [G loss: 0.805612]\n",
      "1106 [D loss: 0.587655, acc.: 71.88%] [G loss: 0.853222]\n",
      "1107 [D loss: 0.566143, acc.: 75.00%] [G loss: 0.914951]\n",
      "1108 [D loss: 0.632217, acc.: 68.75%] [G loss: 0.860721]\n",
      "1109 [D loss: 0.667629, acc.: 50.00%] [G loss: 0.772959]\n",
      "1110 [D loss: 0.617077, acc.: 65.62%] [G loss: 0.826720]\n",
      "1111 [D loss: 0.632952, acc.: 59.38%] [G loss: 0.843747]\n",
      "1112 [D loss: 0.551904, acc.: 81.25%] [G loss: 0.901882]\n",
      "1113 [D loss: 0.610603, acc.: 65.62%] [G loss: 0.917817]\n",
      "1114 [D loss: 0.621805, acc.: 81.25%] [G loss: 0.866475]\n",
      "1115 [D loss: 0.619604, acc.: 56.25%] [G loss: 0.849624]\n",
      "1116 [D loss: 0.583585, acc.: 71.88%] [G loss: 0.803228]\n",
      "1117 [D loss: 0.578713, acc.: 65.62%] [G loss: 0.838169]\n",
      "1118 [D loss: 0.622366, acc.: 62.50%] [G loss: 0.815067]\n",
      "1119 [D loss: 0.563272, acc.: 78.12%] [G loss: 0.896886]\n",
      "1120 [D loss: 0.584938, acc.: 68.75%] [G loss: 0.874874]\n",
      "1121 [D loss: 0.678099, acc.: 62.50%] [G loss: 0.856400]\n",
      "1122 [D loss: 0.627712, acc.: 62.50%] [G loss: 0.820006]\n",
      "1123 [D loss: 0.631080, acc.: 65.62%] [G loss: 0.768108]\n",
      "1124 [D loss: 0.645938, acc.: 65.62%] [G loss: 0.760732]\n",
      "1125 [D loss: 0.596080, acc.: 71.88%] [G loss: 0.827645]\n",
      "1126 [D loss: 0.631247, acc.: 68.75%] [G loss: 0.831948]\n",
      "1127 [D loss: 0.610907, acc.: 75.00%] [G loss: 0.900493]\n",
      "1128 [D loss: 0.598137, acc.: 78.12%] [G loss: 0.882545]\n",
      "1129 [D loss: 0.610738, acc.: 65.62%] [G loss: 0.847426]\n",
      "1130 [D loss: 0.609073, acc.: 75.00%] [G loss: 0.911996]\n",
      "1131 [D loss: 0.588020, acc.: 71.88%] [G loss: 0.900428]\n",
      "1132 [D loss: 0.578907, acc.: 68.75%] [G loss: 0.905724]\n",
      "1133 [D loss: 0.596213, acc.: 75.00%] [G loss: 0.829233]\n",
      "1134 [D loss: 0.626642, acc.: 75.00%] [G loss: 0.837635]\n",
      "1135 [D loss: 0.663401, acc.: 59.38%] [G loss: 0.820111]\n",
      "1136 [D loss: 0.567816, acc.: 68.75%] [G loss: 0.846695]\n",
      "1137 [D loss: 0.608328, acc.: 71.88%] [G loss: 0.893319]\n",
      "1138 [D loss: 0.597951, acc.: 68.75%] [G loss: 0.883176]\n",
      "1139 [D loss: 0.577537, acc.: 75.00%] [G loss: 0.865799]\n",
      "1140 [D loss: 0.618452, acc.: 65.62%] [G loss: 0.877235]\n",
      "1141 [D loss: 0.643991, acc.: 59.38%] [G loss: 0.826972]\n",
      "1142 [D loss: 0.624301, acc.: 65.62%] [G loss: 0.809873]\n",
      "1143 [D loss: 0.561525, acc.: 78.12%] [G loss: 0.844170]\n",
      "1144 [D loss: 0.569112, acc.: 71.88%] [G loss: 0.846763]\n",
      "1145 [D loss: 0.616762, acc.: 65.62%] [G loss: 0.883006]\n",
      "1146 [D loss: 0.587098, acc.: 81.25%] [G loss: 0.810595]\n",
      "1147 [D loss: 0.572437, acc.: 75.00%] [G loss: 0.879231]\n",
      "1148 [D loss: 0.634437, acc.: 65.62%] [G loss: 0.875574]\n",
      "1149 [D loss: 0.671795, acc.: 56.25%] [G loss: 0.839065]\n",
      "1150 [D loss: 0.607352, acc.: 65.62%] [G loss: 0.837236]\n",
      "1151 [D loss: 0.565916, acc.: 78.12%] [G loss: 0.870945]\n",
      "1152 [D loss: 0.573243, acc.: 75.00%] [G loss: 0.867429]\n",
      "1153 [D loss: 0.585881, acc.: 75.00%] [G loss: 0.809403]\n",
      "1154 [D loss: 0.541706, acc.: 81.25%] [G loss: 0.821987]\n",
      "1155 [D loss: 0.579429, acc.: 62.50%] [G loss: 0.781817]\n",
      "1156 [D loss: 0.611326, acc.: 68.75%] [G loss: 0.775138]\n",
      "1157 [D loss: 0.619538, acc.: 62.50%] [G loss: 0.858725]\n",
      "1158 [D loss: 0.692880, acc.: 56.25%] [G loss: 0.798987]\n",
      "1159 [D loss: 0.631834, acc.: 56.25%] [G loss: 0.789293]\n",
      "1160 [D loss: 0.614151, acc.: 71.88%] [G loss: 0.847689]\n",
      "1161 [D loss: 0.594920, acc.: 75.00%] [G loss: 0.885822]\n",
      "1162 [D loss: 0.713782, acc.: 53.12%] [G loss: 0.857372]\n",
      "1163 [D loss: 0.667575, acc.: 62.50%] [G loss: 0.808834]\n",
      "1164 [D loss: 0.621544, acc.: 65.62%] [G loss: 0.781902]\n",
      "1165 [D loss: 0.642895, acc.: 62.50%] [G loss: 0.797368]\n",
      "1166 [D loss: 0.587401, acc.: 75.00%] [G loss: 0.828558]\n",
      "1167 [D loss: 0.621911, acc.: 62.50%] [G loss: 0.789654]\n",
      "1168 [D loss: 0.621910, acc.: 62.50%] [G loss: 0.836287]\n",
      "1169 [D loss: 0.729131, acc.: 46.88%] [G loss: 0.784010]\n",
      "1170 [D loss: 0.625719, acc.: 65.62%] [G loss: 0.822167]\n",
      "1171 [D loss: 0.610763, acc.: 65.62%] [G loss: 0.863786]\n",
      "1172 [D loss: 0.602814, acc.: 68.75%] [G loss: 0.799177]\n",
      "1173 [D loss: 0.668725, acc.: 53.12%] [G loss: 0.752280]\n",
      "1174 [D loss: 0.648432, acc.: 62.50%] [G loss: 0.805618]\n",
      "1175 [D loss: 0.649610, acc.: 59.38%] [G loss: 0.812453]\n",
      "1176 [D loss: 0.606390, acc.: 59.38%] [G loss: 0.830722]\n",
      "1177 [D loss: 0.623461, acc.: 75.00%] [G loss: 0.820510]\n",
      "1178 [D loss: 0.618446, acc.: 71.88%] [G loss: 0.798345]\n",
      "1179 [D loss: 0.651302, acc.: 62.50%] [G loss: 0.829277]\n",
      "1180 [D loss: 0.591199, acc.: 71.88%] [G loss: 0.856940]\n",
      "1181 [D loss: 0.627836, acc.: 53.12%] [G loss: 0.851787]\n",
      "1182 [D loss: 0.641190, acc.: 65.62%] [G loss: 0.832983]\n",
      "1183 [D loss: 0.611548, acc.: 65.62%] [G loss: 0.859572]\n",
      "1184 [D loss: 0.659320, acc.: 65.62%] [G loss: 0.795356]\n",
      "1185 [D loss: 0.697143, acc.: 46.88%] [G loss: 0.783102]\n",
      "1186 [D loss: 0.656258, acc.: 59.38%] [G loss: 0.823152]\n",
      "1187 [D loss: 0.685138, acc.: 46.88%] [G loss: 0.846248]\n",
      "1188 [D loss: 0.593453, acc.: 68.75%] [G loss: 0.896740]\n",
      "1189 [D loss: 0.609124, acc.: 62.50%] [G loss: 0.842270]\n",
      "1190 [D loss: 0.641323, acc.: 62.50%] [G loss: 0.832837]\n",
      "1191 [D loss: 0.612092, acc.: 78.12%] [G loss: 0.839459]\n",
      "1192 [D loss: 0.686569, acc.: 53.12%] [G loss: 0.829618]\n",
      "1193 [D loss: 0.598079, acc.: 71.88%] [G loss: 0.869037]\n",
      "1194 [D loss: 0.572596, acc.: 90.62%] [G loss: 0.896120]\n",
      "1195 [D loss: 0.623423, acc.: 65.62%] [G loss: 0.913684]\n",
      "1196 [D loss: 0.570703, acc.: 81.25%] [G loss: 0.847136]\n",
      "1197 [D loss: 0.697920, acc.: 59.38%] [G loss: 0.819577]\n",
      "1198 [D loss: 0.582240, acc.: 71.88%] [G loss: 0.822620]\n",
      "1199 [D loss: 0.637303, acc.: 56.25%] [G loss: 0.883359]\n",
      "1200 [D loss: 0.659576, acc.: 62.50%] [G loss: 0.840557]\n",
      "1201 [D loss: 0.704418, acc.: 53.12%] [G loss: 0.754286]\n",
      "1202 [D loss: 0.606344, acc.: 59.38%] [G loss: 0.777932]\n",
      "1203 [D loss: 0.583149, acc.: 75.00%] [G loss: 0.793786]\n",
      "1204 [D loss: 0.643666, acc.: 62.50%] [G loss: 0.731782]\n",
      "1205 [D loss: 0.620056, acc.: 65.62%] [G loss: 0.762787]\n",
      "1206 [D loss: 0.627118, acc.: 65.62%] [G loss: 0.790668]\n",
      "1207 [D loss: 0.640009, acc.: 68.75%] [G loss: 0.763365]\n",
      "1208 [D loss: 0.602516, acc.: 68.75%] [G loss: 0.816002]\n",
      "1209 [D loss: 0.646784, acc.: 56.25%] [G loss: 0.884225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1210 [D loss: 0.640839, acc.: 59.38%] [G loss: 0.835848]\n",
      "1211 [D loss: 0.628294, acc.: 75.00%] [G loss: 0.804667]\n",
      "1212 [D loss: 0.719651, acc.: 50.00%] [G loss: 0.778430]\n",
      "1213 [D loss: 0.617320, acc.: 59.38%] [G loss: 0.816501]\n",
      "1214 [D loss: 0.593835, acc.: 65.62%] [G loss: 0.794078]\n",
      "1215 [D loss: 0.630111, acc.: 65.62%] [G loss: 0.830007]\n",
      "1216 [D loss: 0.592039, acc.: 78.12%] [G loss: 0.860715]\n",
      "1217 [D loss: 0.591645, acc.: 71.88%] [G loss: 0.811591]\n",
      "1218 [D loss: 0.593570, acc.: 68.75%] [G loss: 0.808092]\n",
      "1219 [D loss: 0.621747, acc.: 75.00%] [G loss: 0.829384]\n",
      "1220 [D loss: 0.606082, acc.: 75.00%] [G loss: 0.834279]\n",
      "1221 [D loss: 0.620746, acc.: 75.00%] [G loss: 0.827082]\n",
      "1222 [D loss: 0.599097, acc.: 71.88%] [G loss: 0.797752]\n",
      "1223 [D loss: 0.633054, acc.: 59.38%] [G loss: 0.770126]\n",
      "1224 [D loss: 0.610647, acc.: 65.62%] [G loss: 0.817722]\n",
      "1225 [D loss: 0.655182, acc.: 65.62%] [G loss: 0.840229]\n",
      "1226 [D loss: 0.617686, acc.: 68.75%] [G loss: 0.841326]\n",
      "1227 [D loss: 0.651232, acc.: 71.88%] [G loss: 0.757475]\n",
      "1228 [D loss: 0.666203, acc.: 68.75%] [G loss: 0.798938]\n",
      "1229 [D loss: 0.657183, acc.: 59.38%] [G loss: 0.808460]\n",
      "1230 [D loss: 0.593533, acc.: 71.88%] [G loss: 0.873025]\n",
      "1231 [D loss: 0.623814, acc.: 65.62%] [G loss: 0.803301]\n",
      "1232 [D loss: 0.656123, acc.: 53.12%] [G loss: 0.787103]\n",
      "1233 [D loss: 0.669813, acc.: 50.00%] [G loss: 0.801410]\n",
      "1234 [D loss: 0.627368, acc.: 59.38%] [G loss: 0.793335]\n",
      "1235 [D loss: 0.652025, acc.: 59.38%] [G loss: 0.875010]\n",
      "1236 [D loss: 0.673154, acc.: 53.12%] [G loss: 0.787247]\n",
      "1237 [D loss: 0.612012, acc.: 75.00%] [G loss: 0.837076]\n",
      "1238 [D loss: 0.621561, acc.: 75.00%] [G loss: 0.884560]\n",
      "1239 [D loss: 0.650581, acc.: 71.88%] [G loss: 0.895369]\n",
      "1240 [D loss: 0.582806, acc.: 78.12%] [G loss: 0.862167]\n",
      "1241 [D loss: 0.615132, acc.: 65.62%] [G loss: 0.862146]\n",
      "1242 [D loss: 0.712867, acc.: 46.88%] [G loss: 0.831814]\n",
      "1243 [D loss: 0.610349, acc.: 75.00%] [G loss: 0.794692]\n",
      "1244 [D loss: 0.648028, acc.: 56.25%] [G loss: 0.773126]\n",
      "1245 [D loss: 0.629550, acc.: 62.50%] [G loss: 0.815605]\n",
      "1246 [D loss: 0.652452, acc.: 65.62%] [G loss: 0.787576]\n",
      "1247 [D loss: 0.647977, acc.: 68.75%] [G loss: 0.796522]\n",
      "1248 [D loss: 0.632277, acc.: 65.62%] [G loss: 0.828144]\n",
      "1249 [D loss: 0.623309, acc.: 68.75%] [G loss: 0.751094]\n",
      "1250 [D loss: 0.644566, acc.: 59.38%] [G loss: 0.784388]\n",
      "1251 [D loss: 0.641666, acc.: 68.75%] [G loss: 0.791731]\n",
      "1252 [D loss: 0.629682, acc.: 68.75%] [G loss: 0.863917]\n",
      "1253 [D loss: 0.671098, acc.: 62.50%] [G loss: 0.773228]\n",
      "1254 [D loss: 0.671654, acc.: 53.12%] [G loss: 0.827138]\n",
      "1255 [D loss: 0.642993, acc.: 59.38%] [G loss: 0.766039]\n",
      "1256 [D loss: 0.617705, acc.: 68.75%] [G loss: 0.789046]\n",
      "1257 [D loss: 0.601706, acc.: 78.12%] [G loss: 0.776046]\n",
      "1258 [D loss: 0.616407, acc.: 71.88%] [G loss: 0.788784]\n",
      "1259 [D loss: 0.590080, acc.: 65.62%] [G loss: 0.815871]\n",
      "1260 [D loss: 0.657429, acc.: 53.12%] [G loss: 0.865391]\n",
      "1261 [D loss: 0.648171, acc.: 62.50%] [G loss: 0.878425]\n",
      "1262 [D loss: 0.609152, acc.: 68.75%] [G loss: 0.826207]\n",
      "1263 [D loss: 0.610320, acc.: 65.62%] [G loss: 0.780855]\n",
      "1264 [D loss: 0.674650, acc.: 59.38%] [G loss: 0.794722]\n",
      "1265 [D loss: 0.591837, acc.: 68.75%] [G loss: 0.848540]\n",
      "1266 [D loss: 0.641502, acc.: 62.50%] [G loss: 0.866345]\n",
      "1267 [D loss: 0.638830, acc.: 65.62%] [G loss: 0.844036]\n",
      "1268 [D loss: 0.646039, acc.: 68.75%] [G loss: 0.856867]\n",
      "1269 [D loss: 0.613166, acc.: 68.75%] [G loss: 0.860406]\n",
      "1270 [D loss: 0.584003, acc.: 81.25%] [G loss: 0.874122]\n",
      "1271 [D loss: 0.630045, acc.: 65.62%] [G loss: 0.913297]\n",
      "1272 [D loss: 0.597978, acc.: 75.00%] [G loss: 0.812577]\n",
      "1273 [D loss: 0.578652, acc.: 90.62%] [G loss: 0.810543]\n",
      "1274 [D loss: 0.590123, acc.: 71.88%] [G loss: 0.839949]\n",
      "1275 [D loss: 0.586924, acc.: 75.00%] [G loss: 0.839546]\n",
      "1276 [D loss: 0.621736, acc.: 75.00%] [G loss: 0.845755]\n",
      "1277 [D loss: 0.562538, acc.: 75.00%] [G loss: 0.842206]\n",
      "1278 [D loss: 0.594769, acc.: 68.75%] [G loss: 0.879131]\n",
      "1279 [D loss: 0.634267, acc.: 62.50%] [G loss: 0.859906]\n",
      "1280 [D loss: 0.647528, acc.: 62.50%] [G loss: 0.872589]\n",
      "1281 [D loss: 0.632205, acc.: 65.62%] [G loss: 0.864817]\n",
      "1282 [D loss: 0.583955, acc.: 71.88%] [G loss: 0.858936]\n",
      "1283 [D loss: 0.641111, acc.: 56.25%] [G loss: 0.839925]\n",
      "1284 [D loss: 0.589766, acc.: 65.62%] [G loss: 0.782484]\n",
      "1285 [D loss: 0.576519, acc.: 65.62%] [G loss: 0.902361]\n",
      "1286 [D loss: 0.589499, acc.: 68.75%] [G loss: 0.939770]\n",
      "1287 [D loss: 0.627321, acc.: 62.50%] [G loss: 0.897082]\n",
      "1288 [D loss: 0.627708, acc.: 56.25%] [G loss: 0.845075]\n",
      "1289 [D loss: 0.581964, acc.: 78.12%] [G loss: 0.843889]\n",
      "1290 [D loss: 0.638802, acc.: 59.38%] [G loss: 0.875946]\n",
      "1291 [D loss: 0.580859, acc.: 75.00%] [G loss: 0.844570]\n",
      "1292 [D loss: 0.660340, acc.: 65.62%] [G loss: 0.869542]\n",
      "1293 [D loss: 0.610567, acc.: 71.88%] [G loss: 0.882167]\n",
      "1294 [D loss: 0.587940, acc.: 68.75%] [G loss: 0.864586]\n",
      "1295 [D loss: 0.575006, acc.: 71.88%] [G loss: 0.892751]\n",
      "1296 [D loss: 0.616630, acc.: 71.88%] [G loss: 0.899626]\n",
      "1297 [D loss: 0.638399, acc.: 65.62%] [G loss: 0.881053]\n",
      "1298 [D loss: 0.620192, acc.: 65.62%] [G loss: 0.871240]\n",
      "1299 [D loss: 0.602900, acc.: 75.00%] [G loss: 0.900568]\n",
      "1300 [D loss: 0.594985, acc.: 75.00%] [G loss: 0.851924]\n",
      "1301 [D loss: 0.588986, acc.: 71.88%] [G loss: 0.829081]\n",
      "1302 [D loss: 0.659962, acc.: 62.50%] [G loss: 0.828183]\n",
      "1303 [D loss: 0.573895, acc.: 71.88%] [G loss: 0.870436]\n",
      "1304 [D loss: 0.583721, acc.: 78.12%] [G loss: 0.856604]\n",
      "1305 [D loss: 0.686232, acc.: 56.25%] [G loss: 0.841752]\n",
      "1306 [D loss: 0.537517, acc.: 81.25%] [G loss: 0.822449]\n",
      "1307 [D loss: 0.607172, acc.: 71.88%] [G loss: 0.816815]\n",
      "1308 [D loss: 0.521953, acc.: 87.50%] [G loss: 0.835186]\n",
      "1309 [D loss: 0.635857, acc.: 65.62%] [G loss: 0.872779]\n",
      "1310 [D loss: 0.613072, acc.: 59.38%] [G loss: 0.918148]\n",
      "1311 [D loss: 0.607695, acc.: 71.88%] [G loss: 0.866745]\n",
      "1312 [D loss: 0.672302, acc.: 59.38%] [G loss: 0.849607]\n",
      "1313 [D loss: 0.567087, acc.: 75.00%] [G loss: 0.869317]\n",
      "1314 [D loss: 0.638862, acc.: 59.38%] [G loss: 0.851358]\n",
      "1315 [D loss: 0.642664, acc.: 62.50%] [G loss: 0.820425]\n",
      "1316 [D loss: 0.602819, acc.: 59.38%] [G loss: 0.876355]\n",
      "1317 [D loss: 0.582093, acc.: 75.00%] [G loss: 0.837888]\n",
      "1318 [D loss: 0.669030, acc.: 65.62%] [G loss: 0.870777]\n",
      "1319 [D loss: 0.646862, acc.: 62.50%] [G loss: 0.840148]\n",
      "1320 [D loss: 0.646421, acc.: 62.50%] [G loss: 0.834816]\n",
      "1321 [D loss: 0.610086, acc.: 56.25%] [G loss: 0.819385]\n",
      "1322 [D loss: 0.628861, acc.: 68.75%] [G loss: 0.806074]\n",
      "1323 [D loss: 0.618874, acc.: 71.88%] [G loss: 0.792747]\n",
      "1324 [D loss: 0.642247, acc.: 62.50%] [G loss: 0.814867]\n",
      "1325 [D loss: 0.582160, acc.: 78.12%] [G loss: 0.834524]\n",
      "1326 [D loss: 0.612005, acc.: 62.50%] [G loss: 0.893453]\n",
      "1327 [D loss: 0.549989, acc.: 78.12%] [G loss: 0.888632]\n",
      "1328 [D loss: 0.639780, acc.: 71.88%] [G loss: 0.898964]\n",
      "1329 [D loss: 0.591146, acc.: 71.88%] [G loss: 0.995026]\n",
      "1330 [D loss: 0.693538, acc.: 53.12%] [G loss: 0.910771]\n",
      "1331 [D loss: 0.636203, acc.: 53.12%] [G loss: 0.788136]\n",
      "1332 [D loss: 0.589834, acc.: 68.75%] [G loss: 0.859469]\n",
      "1333 [D loss: 0.627374, acc.: 75.00%] [G loss: 0.789269]\n",
      "1334 [D loss: 0.564660, acc.: 78.12%] [G loss: 0.829686]\n",
      "1335 [D loss: 0.595771, acc.: 65.62%] [G loss: 0.837928]\n",
      "1336 [D loss: 0.617266, acc.: 56.25%] [G loss: 0.841380]\n",
      "1337 [D loss: 0.596519, acc.: 75.00%] [G loss: 0.883855]\n",
      "1338 [D loss: 0.589579, acc.: 71.88%] [G loss: 0.891755]\n",
      "1339 [D loss: 0.587917, acc.: 65.62%] [G loss: 0.883429]\n",
      "1340 [D loss: 0.627362, acc.: 59.38%] [G loss: 0.875998]\n",
      "1341 [D loss: 0.592382, acc.: 68.75%] [G loss: 0.832626]\n",
      "1342 [D loss: 0.638636, acc.: 62.50%] [G loss: 0.899192]\n",
      "1343 [D loss: 0.581249, acc.: 81.25%] [G loss: 0.962842]\n",
      "1344 [D loss: 0.640101, acc.: 68.75%] [G loss: 0.861282]\n",
      "1345 [D loss: 0.629202, acc.: 59.38%] [G loss: 0.853523]\n",
      "1346 [D loss: 0.612624, acc.: 68.75%] [G loss: 0.888274]\n",
      "1347 [D loss: 0.642006, acc.: 65.62%] [G loss: 0.810647]\n",
      "1348 [D loss: 0.615950, acc.: 68.75%] [G loss: 0.816741]\n",
      "1349 [D loss: 0.646630, acc.: 71.88%] [G loss: 0.838887]\n",
      "1350 [D loss: 0.575476, acc.: 71.88%] [G loss: 0.905924]\n",
      "1351 [D loss: 0.641580, acc.: 56.25%] [G loss: 0.918341]\n",
      "1352 [D loss: 0.584894, acc.: 75.00%] [G loss: 0.895608]\n",
      "1353 [D loss: 0.625634, acc.: 59.38%] [G loss: 0.933049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1354 [D loss: 0.607705, acc.: 68.75%] [G loss: 0.950259]\n",
      "1355 [D loss: 0.608276, acc.: 71.88%] [G loss: 0.875807]\n",
      "1356 [D loss: 0.643176, acc.: 62.50%] [G loss: 0.849361]\n",
      "1357 [D loss: 0.601767, acc.: 59.38%] [G loss: 0.839369]\n",
      "1358 [D loss: 0.701555, acc.: 62.50%] [G loss: 0.808888]\n",
      "1359 [D loss: 0.623607, acc.: 65.62%] [G loss: 0.855680]\n",
      "1360 [D loss: 0.609301, acc.: 65.62%] [G loss: 0.844936]\n",
      "1361 [D loss: 0.617643, acc.: 71.88%] [G loss: 0.845365]\n",
      "1362 [D loss: 0.607467, acc.: 75.00%] [G loss: 0.891205]\n",
      "1363 [D loss: 0.602101, acc.: 78.12%] [G loss: 0.878301]\n",
      "1364 [D loss: 0.583550, acc.: 75.00%] [G loss: 0.876332]\n",
      "1365 [D loss: 0.611114, acc.: 65.62%] [G loss: 0.925155]\n",
      "1366 [D loss: 0.610977, acc.: 65.62%] [G loss: 0.916172]\n",
      "1367 [D loss: 0.603193, acc.: 68.75%] [G loss: 0.812849]\n",
      "1368 [D loss: 0.581205, acc.: 71.88%] [G loss: 0.868811]\n",
      "1369 [D loss: 0.599599, acc.: 68.75%] [G loss: 0.840241]\n",
      "1370 [D loss: 0.536544, acc.: 84.38%] [G loss: 0.799751]\n",
      "1371 [D loss: 0.571880, acc.: 75.00%] [G loss: 0.860996]\n",
      "1372 [D loss: 0.649224, acc.: 59.38%] [G loss: 0.937970]\n",
      "1373 [D loss: 0.579122, acc.: 71.88%] [G loss: 0.932257]\n",
      "1374 [D loss: 0.542087, acc.: 84.38%] [G loss: 0.917224]\n",
      "1375 [D loss: 0.616205, acc.: 62.50%] [G loss: 0.896617]\n",
      "1376 [D loss: 0.661919, acc.: 56.25%] [G loss: 0.949729]\n",
      "1377 [D loss: 0.587370, acc.: 65.62%] [G loss: 0.908953]\n",
      "1378 [D loss: 0.587636, acc.: 68.75%] [G loss: 0.907703]\n",
      "1379 [D loss: 0.608891, acc.: 65.62%] [G loss: 0.958965]\n",
      "1380 [D loss: 0.646116, acc.: 43.75%] [G loss: 0.913730]\n",
      "1381 [D loss: 0.517515, acc.: 81.25%] [G loss: 0.935928]\n",
      "1382 [D loss: 0.563258, acc.: 81.25%] [G loss: 0.965953]\n",
      "1383 [D loss: 0.552975, acc.: 78.12%] [G loss: 0.928844]\n",
      "1384 [D loss: 0.620406, acc.: 56.25%] [G loss: 0.988951]\n",
      "1385 [D loss: 0.712669, acc.: 56.25%] [G loss: 0.893397]\n",
      "1386 [D loss: 0.595439, acc.: 78.12%] [G loss: 0.916982]\n",
      "1387 [D loss: 0.618810, acc.: 65.62%] [G loss: 0.918866]\n",
      "1388 [D loss: 0.641129, acc.: 62.50%] [G loss: 0.919811]\n",
      "1389 [D loss: 0.605862, acc.: 68.75%] [G loss: 0.931632]\n",
      "1390 [D loss: 0.654123, acc.: 65.62%] [G loss: 0.870852]\n",
      "1391 [D loss: 0.599195, acc.: 75.00%] [G loss: 0.834953]\n",
      "1392 [D loss: 0.625667, acc.: 62.50%] [G loss: 0.817692]\n",
      "1393 [D loss: 0.587413, acc.: 71.88%] [G loss: 0.856051]\n",
      "1394 [D loss: 0.639420, acc.: 65.62%] [G loss: 0.789510]\n",
      "1395 [D loss: 0.656292, acc.: 59.38%] [G loss: 0.809462]\n",
      "1396 [D loss: 0.546507, acc.: 75.00%] [G loss: 0.866593]\n",
      "1397 [D loss: 0.618439, acc.: 56.25%] [G loss: 0.878629]\n",
      "1398 [D loss: 0.597312, acc.: 71.88%] [G loss: 0.913197]\n",
      "1399 [D loss: 0.598125, acc.: 78.12%] [G loss: 0.941121]\n",
      "1400 [D loss: 0.647628, acc.: 59.38%] [G loss: 0.974448]\n",
      "1401 [D loss: 0.699249, acc.: 50.00%] [G loss: 0.871587]\n",
      "1402 [D loss: 0.684224, acc.: 53.12%] [G loss: 0.937869]\n",
      "1403 [D loss: 0.634571, acc.: 71.88%] [G loss: 0.838157]\n",
      "1404 [D loss: 0.635752, acc.: 56.25%] [G loss: 0.864221]\n",
      "1405 [D loss: 0.625266, acc.: 56.25%] [G loss: 0.842927]\n",
      "1406 [D loss: 0.604585, acc.: 65.62%] [G loss: 0.891696]\n",
      "1407 [D loss: 0.699897, acc.: 53.12%] [G loss: 0.817230]\n",
      "1408 [D loss: 0.603932, acc.: 68.75%] [G loss: 0.855053]\n",
      "1409 [D loss: 0.682113, acc.: 56.25%] [G loss: 0.828683]\n",
      "1410 [D loss: 0.594765, acc.: 75.00%] [G loss: 0.843466]\n",
      "1411 [D loss: 0.621027, acc.: 65.62%] [G loss: 0.851995]\n",
      "1412 [D loss: 0.608564, acc.: 62.50%] [G loss: 0.839836]\n",
      "1413 [D loss: 0.604705, acc.: 71.88%] [G loss: 0.881685]\n",
      "1414 [D loss: 0.625037, acc.: 68.75%] [G loss: 0.928982]\n",
      "1415 [D loss: 0.551351, acc.: 87.50%] [G loss: 0.946101]\n",
      "1416 [D loss: 0.598528, acc.: 75.00%] [G loss: 0.934506]\n",
      "1417 [D loss: 0.617095, acc.: 71.88%] [G loss: 0.850824]\n",
      "1418 [D loss: 0.541026, acc.: 81.25%] [G loss: 0.823972]\n",
      "1419 [D loss: 0.566264, acc.: 71.88%] [G loss: 0.854675]\n",
      "1420 [D loss: 0.679186, acc.: 46.88%] [G loss: 0.835417]\n",
      "1421 [D loss: 0.658156, acc.: 59.38%] [G loss: 0.833835]\n",
      "1422 [D loss: 0.601875, acc.: 62.50%] [G loss: 0.903706]\n",
      "1423 [D loss: 0.553357, acc.: 84.38%] [G loss: 0.917057]\n",
      "1424 [D loss: 0.657660, acc.: 62.50%] [G loss: 0.926512]\n",
      "1425 [D loss: 0.686739, acc.: 50.00%] [G loss: 0.887622]\n",
      "1426 [D loss: 0.556485, acc.: 68.75%] [G loss: 0.855698]\n",
      "1427 [D loss: 0.598433, acc.: 75.00%] [G loss: 0.879792]\n",
      "1428 [D loss: 0.550529, acc.: 81.25%] [G loss: 0.848848]\n",
      "1429 [D loss: 0.676746, acc.: 62.50%] [G loss: 0.897264]\n",
      "1430 [D loss: 0.553876, acc.: 81.25%] [G loss: 0.863460]\n",
      "1431 [D loss: 0.630633, acc.: 59.38%] [G loss: 0.888972]\n",
      "1432 [D loss: 0.614878, acc.: 71.88%] [G loss: 0.900578]\n",
      "1433 [D loss: 0.573186, acc.: 71.88%] [G loss: 0.935815]\n",
      "1434 [D loss: 0.580292, acc.: 84.38%] [G loss: 0.925630]\n",
      "1435 [D loss: 0.629520, acc.: 68.75%] [G loss: 0.907497]\n",
      "1436 [D loss: 0.615154, acc.: 71.88%] [G loss: 0.934868]\n",
      "1437 [D loss: 0.643467, acc.: 68.75%] [G loss: 0.893235]\n",
      "1438 [D loss: 0.620763, acc.: 71.88%] [G loss: 0.883921]\n",
      "1439 [D loss: 0.586419, acc.: 75.00%] [G loss: 0.870557]\n",
      "1440 [D loss: 0.569065, acc.: 75.00%] [G loss: 0.934459]\n",
      "1441 [D loss: 0.566895, acc.: 65.62%] [G loss: 0.917166]\n",
      "1442 [D loss: 0.531020, acc.: 90.62%] [G loss: 0.866509]\n",
      "1443 [D loss: 0.553154, acc.: 84.38%] [G loss: 0.857248]\n",
      "1444 [D loss: 0.595906, acc.: 75.00%] [G loss: 0.886611]\n",
      "1445 [D loss: 0.621190, acc.: 75.00%] [G loss: 0.864578]\n",
      "1446 [D loss: 0.661613, acc.: 62.50%] [G loss: 0.837347]\n",
      "1447 [D loss: 0.597512, acc.: 75.00%] [G loss: 0.900949]\n",
      "1448 [D loss: 0.650433, acc.: 59.38%] [G loss: 0.827060]\n",
      "1449 [D loss: 0.570405, acc.: 81.25%] [G loss: 0.876449]\n",
      "1450 [D loss: 0.502165, acc.: 81.25%] [G loss: 0.907571]\n",
      "1451 [D loss: 0.580458, acc.: 68.75%] [G loss: 0.901420]\n",
      "1452 [D loss: 0.561303, acc.: 71.88%] [G loss: 0.866376]\n",
      "1453 [D loss: 0.631570, acc.: 62.50%] [G loss: 0.848221]\n",
      "1454 [D loss: 0.620895, acc.: 62.50%] [G loss: 0.856912]\n",
      "1455 [D loss: 0.670102, acc.: 59.38%] [G loss: 0.929071]\n",
      "1456 [D loss: 0.560427, acc.: 90.62%] [G loss: 0.944533]\n",
      "1457 [D loss: 0.616375, acc.: 65.62%] [G loss: 0.873659]\n",
      "1458 [D loss: 0.590207, acc.: 71.88%] [G loss: 0.907822]\n",
      "1459 [D loss: 0.568529, acc.: 81.25%] [G loss: 0.870643]\n",
      "1460 [D loss: 0.654393, acc.: 53.12%] [G loss: 0.818096]\n",
      "1461 [D loss: 0.674061, acc.: 62.50%] [G loss: 0.791087]\n",
      "1462 [D loss: 0.585528, acc.: 71.88%] [G loss: 0.864879]\n",
      "1463 [D loss: 0.561443, acc.: 68.75%] [G loss: 0.861629]\n",
      "1464 [D loss: 0.610088, acc.: 75.00%] [G loss: 0.926659]\n",
      "1465 [D loss: 0.702322, acc.: 50.00%] [G loss: 0.992316]\n",
      "1466 [D loss: 0.643973, acc.: 71.88%] [G loss: 1.059338]\n",
      "1467 [D loss: 0.593216, acc.: 75.00%] [G loss: 0.990898]\n",
      "1468 [D loss: 0.633093, acc.: 59.38%] [G loss: 0.934457]\n",
      "1469 [D loss: 0.636689, acc.: 71.88%] [G loss: 0.855196]\n",
      "1470 [D loss: 0.680551, acc.: 56.25%] [G loss: 0.898525]\n",
      "1471 [D loss: 0.539984, acc.: 78.12%] [G loss: 0.874263]\n",
      "1472 [D loss: 0.566648, acc.: 75.00%] [G loss: 0.865758]\n",
      "1473 [D loss: 0.565692, acc.: 75.00%] [G loss: 0.895942]\n",
      "1474 [D loss: 0.592628, acc.: 65.62%] [G loss: 0.849715]\n",
      "1475 [D loss: 0.552450, acc.: 78.12%] [G loss: 0.944897]\n",
      "1476 [D loss: 0.653247, acc.: 68.75%] [G loss: 0.962771]\n",
      "1477 [D loss: 0.622310, acc.: 62.50%] [G loss: 0.902510]\n",
      "1478 [D loss: 0.624294, acc.: 56.25%] [G loss: 0.837906]\n",
      "1479 [D loss: 0.591698, acc.: 65.62%] [G loss: 0.860299]\n",
      "1480 [D loss: 0.583837, acc.: 68.75%] [G loss: 0.902166]\n",
      "1481 [D loss: 0.686043, acc.: 59.38%] [G loss: 0.898572]\n",
      "1482 [D loss: 0.661905, acc.: 65.62%] [G loss: 0.874149]\n",
      "1483 [D loss: 0.562804, acc.: 81.25%] [G loss: 0.875033]\n",
      "1484 [D loss: 0.599437, acc.: 62.50%] [G loss: 0.903934]\n",
      "1485 [D loss: 0.615911, acc.: 65.62%] [G loss: 0.907221]\n",
      "1486 [D loss: 0.634418, acc.: 71.88%] [G loss: 0.880273]\n",
      "1487 [D loss: 0.634784, acc.: 65.62%] [G loss: 0.818609]\n",
      "1488 [D loss: 0.552478, acc.: 84.38%] [G loss: 0.896571]\n",
      "1489 [D loss: 0.631814, acc.: 71.88%] [G loss: 0.858170]\n",
      "1490 [D loss: 0.587863, acc.: 68.75%] [G loss: 0.858009]\n",
      "1491 [D loss: 0.591526, acc.: 78.12%] [G loss: 0.832561]\n",
      "1492 [D loss: 0.548279, acc.: 84.38%] [G loss: 0.894085]\n",
      "1493 [D loss: 0.592045, acc.: 75.00%] [G loss: 0.873589]\n",
      "1494 [D loss: 0.629883, acc.: 59.38%] [G loss: 0.932182]\n",
      "1495 [D loss: 0.593567, acc.: 84.38%] [G loss: 0.910965]\n",
      "1496 [D loss: 0.598859, acc.: 75.00%] [G loss: 0.851010]\n",
      "1497 [D loss: 0.618591, acc.: 71.88%] [G loss: 0.807205]\n",
      "1498 [D loss: 0.521500, acc.: 81.25%] [G loss: 0.872361]\n",
      "1499 [D loss: 0.617329, acc.: 71.88%] [G loss: 0.976647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 [D loss: 0.525793, acc.: 81.25%] [G loss: 0.898973]\n",
      "1501 [D loss: 0.573511, acc.: 71.88%] [G loss: 0.832980]\n",
      "1502 [D loss: 0.556428, acc.: 78.12%] [G loss: 0.830876]\n",
      "1503 [D loss: 0.618513, acc.: 56.25%] [G loss: 0.864704]\n",
      "1504 [D loss: 0.603629, acc.: 71.88%] [G loss: 0.822294]\n",
      "1505 [D loss: 0.594418, acc.: 62.50%] [G loss: 0.879993]\n",
      "1506 [D loss: 0.583185, acc.: 84.38%] [G loss: 0.831622]\n",
      "1507 [D loss: 0.576406, acc.: 90.62%] [G loss: 0.855387]\n",
      "1508 [D loss: 0.605283, acc.: 78.12%] [G loss: 0.774959]\n",
      "1509 [D loss: 0.665929, acc.: 65.62%] [G loss: 0.907962]\n",
      "1510 [D loss: 0.653135, acc.: 59.38%] [G loss: 0.923324]\n",
      "1511 [D loss: 0.565923, acc.: 87.50%] [G loss: 0.869693]\n",
      "1512 [D loss: 0.638483, acc.: 68.75%] [G loss: 0.868636]\n",
      "1513 [D loss: 0.593005, acc.: 78.12%] [G loss: 0.935732]\n",
      "1514 [D loss: 0.535289, acc.: 75.00%] [G loss: 0.912048]\n",
      "1515 [D loss: 0.559828, acc.: 78.12%] [G loss: 0.931935]\n",
      "1516 [D loss: 0.525126, acc.: 78.12%] [G loss: 0.945076]\n",
      "1517 [D loss: 0.640141, acc.: 68.75%] [G loss: 0.969893]\n",
      "1518 [D loss: 0.571902, acc.: 71.88%] [G loss: 0.863772]\n",
      "1519 [D loss: 0.545884, acc.: 75.00%] [G loss: 0.867560]\n",
      "1520 [D loss: 0.628004, acc.: 59.38%] [G loss: 0.851352]\n",
      "1521 [D loss: 0.705390, acc.: 46.88%] [G loss: 0.907103]\n",
      "1522 [D loss: 0.658637, acc.: 65.62%] [G loss: 0.938681]\n",
      "1523 [D loss: 0.570141, acc.: 71.88%] [G loss: 0.999878]\n",
      "1524 [D loss: 0.605264, acc.: 71.88%] [G loss: 0.908356]\n",
      "1525 [D loss: 0.634406, acc.: 75.00%] [G loss: 0.896621]\n",
      "1526 [D loss: 0.590361, acc.: 68.75%] [G loss: 0.893803]\n",
      "1527 [D loss: 0.591656, acc.: 78.12%] [G loss: 0.884171]\n",
      "1528 [D loss: 0.609555, acc.: 71.88%] [G loss: 0.918046]\n",
      "1529 [D loss: 0.637266, acc.: 65.62%] [G loss: 0.867333]\n",
      "1530 [D loss: 0.622428, acc.: 62.50%] [G loss: 0.909340]\n",
      "1531 [D loss: 0.638331, acc.: 62.50%] [G loss: 0.912297]\n",
      "1532 [D loss: 0.616442, acc.: 65.62%] [G loss: 0.942408]\n",
      "1533 [D loss: 0.668326, acc.: 53.12%] [G loss: 0.936770]\n",
      "1534 [D loss: 0.564709, acc.: 75.00%] [G loss: 0.918369]\n",
      "1535 [D loss: 0.591008, acc.: 71.88%] [G loss: 0.926323]\n",
      "1536 [D loss: 0.609333, acc.: 71.88%] [G loss: 0.855174]\n",
      "1537 [D loss: 0.679798, acc.: 53.12%] [G loss: 0.840644]\n",
      "1538 [D loss: 0.665090, acc.: 65.62%] [G loss: 0.829479]\n",
      "1539 [D loss: 0.621852, acc.: 87.50%] [G loss: 0.824559]\n",
      "1540 [D loss: 0.560934, acc.: 81.25%] [G loss: 0.901558]\n",
      "1541 [D loss: 0.602181, acc.: 62.50%] [G loss: 0.931761]\n",
      "1542 [D loss: 0.494150, acc.: 71.88%] [G loss: 0.972318]\n",
      "1543 [D loss: 0.611587, acc.: 62.50%] [G loss: 0.939201]\n",
      "1544 [D loss: 0.522082, acc.: 81.25%] [G loss: 0.919240]\n",
      "1545 [D loss: 0.659476, acc.: 65.62%] [G loss: 0.901997]\n",
      "1546 [D loss: 0.648686, acc.: 75.00%] [G loss: 0.910610]\n",
      "1547 [D loss: 0.596175, acc.: 68.75%] [G loss: 0.923822]\n",
      "1548 [D loss: 0.514686, acc.: 87.50%] [G loss: 0.894421]\n",
      "1549 [D loss: 0.574041, acc.: 75.00%] [G loss: 0.852754]\n",
      "1550 [D loss: 0.593128, acc.: 65.62%] [G loss: 0.903645]\n",
      "1551 [D loss: 0.661500, acc.: 65.62%] [G loss: 0.941819]\n",
      "1552 [D loss: 0.581889, acc.: 84.38%] [G loss: 0.939162]\n",
      "1553 [D loss: 0.630325, acc.: 65.62%] [G loss: 0.967038]\n",
      "1554 [D loss: 0.622460, acc.: 68.75%] [G loss: 0.940427]\n",
      "1555 [D loss: 0.565165, acc.: 75.00%] [G loss: 0.840877]\n",
      "1556 [D loss: 0.554719, acc.: 78.12%] [G loss: 0.900307]\n",
      "1557 [D loss: 0.547394, acc.: 78.12%] [G loss: 0.823860]\n",
      "1558 [D loss: 0.658545, acc.: 65.62%] [G loss: 0.895218]\n",
      "1559 [D loss: 0.671942, acc.: 59.38%] [G loss: 0.929813]\n",
      "1560 [D loss: 0.646170, acc.: 56.25%] [G loss: 0.894613]\n",
      "1561 [D loss: 0.593480, acc.: 71.88%] [G loss: 0.880006]\n",
      "1562 [D loss: 0.551181, acc.: 75.00%] [G loss: 0.998072]\n",
      "1563 [D loss: 0.567195, acc.: 71.88%] [G loss: 0.916311]\n",
      "1564 [D loss: 0.538142, acc.: 81.25%] [G loss: 0.940763]\n",
      "1565 [D loss: 0.629034, acc.: 62.50%] [G loss: 0.867089]\n",
      "1566 [D loss: 0.578616, acc.: 68.75%] [G loss: 0.837521]\n",
      "1567 [D loss: 0.685665, acc.: 62.50%] [G loss: 0.833004]\n",
      "1568 [D loss: 0.596581, acc.: 68.75%] [G loss: 0.881704]\n",
      "1569 [D loss: 0.567932, acc.: 78.12%] [G loss: 0.872857]\n",
      "1570 [D loss: 0.538852, acc.: 78.12%] [G loss: 0.980811]\n",
      "1571 [D loss: 0.550249, acc.: 71.88%] [G loss: 0.972966]\n",
      "1572 [D loss: 0.643550, acc.: 62.50%] [G loss: 0.925932]\n",
      "1573 [D loss: 0.519766, acc.: 75.00%] [G loss: 0.960664]\n",
      "1574 [D loss: 0.632508, acc.: 56.25%] [G loss: 0.946263]\n",
      "1575 [D loss: 0.624530, acc.: 62.50%] [G loss: 0.857701]\n",
      "1576 [D loss: 0.601240, acc.: 65.62%] [G loss: 0.882514]\n",
      "1577 [D loss: 0.572508, acc.: 62.50%] [G loss: 0.920170]\n",
      "1578 [D loss: 0.594503, acc.: 75.00%] [G loss: 0.893380]\n",
      "1579 [D loss: 0.509246, acc.: 87.50%] [G loss: 0.913597]\n",
      "1580 [D loss: 0.637963, acc.: 65.62%] [G loss: 0.896780]\n",
      "1581 [D loss: 0.641087, acc.: 53.12%] [G loss: 0.881436]\n",
      "1582 [D loss: 0.564124, acc.: 81.25%] [G loss: 0.904449]\n",
      "1583 [D loss: 0.629278, acc.: 62.50%] [G loss: 0.864032]\n",
      "1584 [D loss: 0.605730, acc.: 68.75%] [G loss: 0.950432]\n",
      "1585 [D loss: 0.594160, acc.: 62.50%] [G loss: 0.921834]\n",
      "1586 [D loss: 0.592434, acc.: 75.00%] [G loss: 0.961324]\n",
      "1587 [D loss: 0.600757, acc.: 78.12%] [G loss: 0.881856]\n",
      "1588 [D loss: 0.622266, acc.: 71.88%] [G loss: 0.906551]\n",
      "1589 [D loss: 0.568732, acc.: 78.12%] [G loss: 0.954290]\n",
      "1590 [D loss: 0.559604, acc.: 75.00%] [G loss: 0.954887]\n",
      "1591 [D loss: 0.666615, acc.: 56.25%] [G loss: 0.874887]\n",
      "1592 [D loss: 0.578468, acc.: 71.88%] [G loss: 0.890412]\n",
      "1593 [D loss: 0.603286, acc.: 65.62%] [G loss: 0.903733]\n",
      "1594 [D loss: 0.706885, acc.: 53.12%] [G loss: 0.928048]\n",
      "1595 [D loss: 0.540086, acc.: 78.12%] [G loss: 0.988133]\n",
      "1596 [D loss: 0.641659, acc.: 71.88%] [G loss: 0.978771]\n",
      "1597 [D loss: 0.611769, acc.: 75.00%] [G loss: 0.905198]\n",
      "1598 [D loss: 0.547971, acc.: 78.12%] [G loss: 0.877550]\n",
      "1599 [D loss: 0.586806, acc.: 81.25%] [G loss: 0.863658]\n",
      "1600 [D loss: 0.566624, acc.: 71.88%] [G loss: 0.909268]\n",
      "1601 [D loss: 0.559044, acc.: 71.88%] [G loss: 0.944155]\n",
      "1602 [D loss: 0.614978, acc.: 68.75%] [G loss: 0.907545]\n",
      "1603 [D loss: 0.538145, acc.: 75.00%] [G loss: 0.910816]\n",
      "1604 [D loss: 0.601729, acc.: 75.00%] [G loss: 0.936781]\n",
      "1605 [D loss: 0.555388, acc.: 71.88%] [G loss: 0.947559]\n",
      "1606 [D loss: 0.556125, acc.: 78.12%] [G loss: 0.897988]\n",
      "1607 [D loss: 0.560032, acc.: 71.88%] [G loss: 0.973602]\n",
      "1608 [D loss: 0.582738, acc.: 71.88%] [G loss: 0.953310]\n",
      "1609 [D loss: 0.568138, acc.: 78.12%] [G loss: 1.032612]\n",
      "1610 [D loss: 0.590687, acc.: 81.25%] [G loss: 1.022452]\n",
      "1611 [D loss: 0.567209, acc.: 78.12%] [G loss: 1.105587]\n",
      "1612 [D loss: 0.654715, acc.: 68.75%] [G loss: 1.011201]\n",
      "1613 [D loss: 0.562114, acc.: 62.50%] [G loss: 1.017480]\n",
      "1614 [D loss: 0.505307, acc.: 71.88%] [G loss: 1.094527]\n",
      "1615 [D loss: 0.541113, acc.: 75.00%] [G loss: 1.060630]\n",
      "1616 [D loss: 0.658670, acc.: 62.50%] [G loss: 0.953817]\n",
      "1617 [D loss: 0.582981, acc.: 78.12%] [G loss: 0.853021]\n",
      "1618 [D loss: 0.619945, acc.: 62.50%] [G loss: 0.884602]\n",
      "1619 [D loss: 0.682062, acc.: 62.50%] [G loss: 0.889710]\n",
      "1620 [D loss: 0.601863, acc.: 65.62%] [G loss: 0.994786]\n",
      "1621 [D loss: 0.554280, acc.: 75.00%] [G loss: 0.957459]\n",
      "1622 [D loss: 0.625235, acc.: 65.62%] [G loss: 0.952903]\n",
      "1623 [D loss: 0.690956, acc.: 50.00%] [G loss: 0.893100]\n",
      "1624 [D loss: 0.609075, acc.: 65.62%] [G loss: 0.898209]\n",
      "1625 [D loss: 0.595180, acc.: 62.50%] [G loss: 0.866882]\n",
      "1626 [D loss: 0.688585, acc.: 59.38%] [G loss: 0.876003]\n",
      "1627 [D loss: 0.622504, acc.: 65.62%] [G loss: 0.843954]\n",
      "1628 [D loss: 0.592557, acc.: 71.88%] [G loss: 0.895905]\n",
      "1629 [D loss: 0.594721, acc.: 68.75%] [G loss: 0.837087]\n",
      "1630 [D loss: 0.575686, acc.: 78.12%] [G loss: 0.896352]\n",
      "1631 [D loss: 0.560432, acc.: 75.00%] [G loss: 0.969374]\n",
      "1632 [D loss: 0.542829, acc.: 84.38%] [G loss: 0.909857]\n",
      "1633 [D loss: 0.641968, acc.: 59.38%] [G loss: 0.870274]\n",
      "1634 [D loss: 0.602306, acc.: 71.88%] [G loss: 0.925697]\n",
      "1635 [D loss: 0.565498, acc.: 81.25%] [G loss: 0.947245]\n",
      "1636 [D loss: 0.652932, acc.: 59.38%] [G loss: 0.884509]\n",
      "1637 [D loss: 0.617020, acc.: 71.88%] [G loss: 0.860345]\n",
      "1638 [D loss: 0.551287, acc.: 78.12%] [G loss: 0.881257]\n",
      "1639 [D loss: 0.630678, acc.: 62.50%] [G loss: 0.897472]\n",
      "1640 [D loss: 0.678344, acc.: 59.38%] [G loss: 0.897589]\n",
      "1641 [D loss: 0.627550, acc.: 71.88%] [G loss: 0.963637]\n",
      "1642 [D loss: 0.658226, acc.: 59.38%] [G loss: 0.888454]\n",
      "1643 [D loss: 0.611226, acc.: 68.75%] [G loss: 0.924647]\n",
      "1644 [D loss: 0.558787, acc.: 78.12%] [G loss: 0.892394]\n",
      "1645 [D loss: 0.623039, acc.: 62.50%] [G loss: 0.954434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646 [D loss: 0.553870, acc.: 75.00%] [G loss: 0.880177]\n",
      "1647 [D loss: 0.574755, acc.: 71.88%] [G loss: 0.938133]\n",
      "1648 [D loss: 0.673987, acc.: 59.38%] [G loss: 0.797301]\n",
      "1649 [D loss: 0.544055, acc.: 75.00%] [G loss: 0.838387]\n",
      "1650 [D loss: 0.543015, acc.: 75.00%] [G loss: 0.947114]\n",
      "1651 [D loss: 0.662302, acc.: 62.50%] [G loss: 0.870701]\n",
      "1652 [D loss: 0.651001, acc.: 59.38%] [G loss: 0.879191]\n",
      "1653 [D loss: 0.595980, acc.: 71.88%] [G loss: 0.898138]\n",
      "1654 [D loss: 0.583699, acc.: 75.00%] [G loss: 0.900068]\n",
      "1655 [D loss: 0.746821, acc.: 50.00%] [G loss: 0.915959]\n",
      "1656 [D loss: 0.610058, acc.: 62.50%] [G loss: 0.854913]\n",
      "1657 [D loss: 0.612566, acc.: 71.88%] [G loss: 0.867194]\n",
      "1658 [D loss: 0.588815, acc.: 71.88%] [G loss: 0.864046]\n",
      "1659 [D loss: 0.512804, acc.: 78.12%] [G loss: 0.950096]\n",
      "1660 [D loss: 0.604898, acc.: 71.88%] [G loss: 0.885613]\n",
      "1661 [D loss: 0.655496, acc.: 62.50%] [G loss: 0.876203]\n",
      "1662 [D loss: 0.685257, acc.: 53.12%] [G loss: 0.882416]\n",
      "1663 [D loss: 0.555087, acc.: 81.25%] [G loss: 0.904366]\n",
      "1664 [D loss: 0.583469, acc.: 65.62%] [G loss: 0.923050]\n",
      "1665 [D loss: 0.591226, acc.: 62.50%] [G loss: 0.955291]\n",
      "1666 [D loss: 0.576913, acc.: 78.12%] [G loss: 1.002224]\n",
      "1667 [D loss: 0.523429, acc.: 84.38%] [G loss: 1.005490]\n",
      "1668 [D loss: 0.693496, acc.: 59.38%] [G loss: 0.887634]\n",
      "1669 [D loss: 0.625919, acc.: 62.50%] [G loss: 0.871825]\n",
      "1670 [D loss: 0.661907, acc.: 50.00%] [G loss: 0.827677]\n",
      "1671 [D loss: 0.589765, acc.: 59.38%] [G loss: 0.892333]\n",
      "1672 [D loss: 0.648851, acc.: 59.38%] [G loss: 0.836986]\n",
      "1673 [D loss: 0.609673, acc.: 75.00%] [G loss: 0.871385]\n",
      "1674 [D loss: 0.547823, acc.: 75.00%] [G loss: 0.885044]\n",
      "1675 [D loss: 0.591026, acc.: 75.00%] [G loss: 0.831495]\n",
      "1676 [D loss: 0.619461, acc.: 65.62%] [G loss: 0.887248]\n",
      "1677 [D loss: 0.558417, acc.: 75.00%] [G loss: 0.988436]\n",
      "1678 [D loss: 0.633370, acc.: 62.50%] [G loss: 0.873951]\n",
      "1679 [D loss: 0.590204, acc.: 78.12%] [G loss: 0.951540]\n",
      "1680 [D loss: 0.588478, acc.: 78.12%] [G loss: 0.960452]\n",
      "1681 [D loss: 0.564230, acc.: 75.00%] [G loss: 0.910830]\n",
      "1682 [D loss: 0.522567, acc.: 81.25%] [G loss: 0.990499]\n",
      "1683 [D loss: 0.633820, acc.: 65.62%] [G loss: 0.874218]\n",
      "1684 [D loss: 0.752159, acc.: 40.62%] [G loss: 0.866522]\n",
      "1685 [D loss: 0.608001, acc.: 68.75%] [G loss: 0.923242]\n",
      "1686 [D loss: 0.630084, acc.: 68.75%] [G loss: 0.950758]\n",
      "1687 [D loss: 0.647764, acc.: 53.12%] [G loss: 0.982634]\n",
      "1688 [D loss: 0.669283, acc.: 50.00%] [G loss: 0.865617]\n",
      "1689 [D loss: 0.689732, acc.: 53.12%] [G loss: 0.892669]\n",
      "1690 [D loss: 0.675866, acc.: 59.38%] [G loss: 0.935732]\n",
      "1691 [D loss: 0.602103, acc.: 71.88%] [G loss: 0.954915]\n",
      "1692 [D loss: 0.618737, acc.: 71.88%] [G loss: 0.942313]\n",
      "1693 [D loss: 0.698235, acc.: 50.00%] [G loss: 0.966954]\n",
      "1694 [D loss: 0.652816, acc.: 65.62%] [G loss: 0.927326]\n",
      "1695 [D loss: 0.639654, acc.: 56.25%] [G loss: 0.847241]\n",
      "1696 [D loss: 0.631822, acc.: 62.50%] [G loss: 0.845798]\n",
      "1697 [D loss: 0.652493, acc.: 46.88%] [G loss: 0.958624]\n",
      "1698 [D loss: 0.605243, acc.: 65.62%] [G loss: 0.969852]\n",
      "1699 [D loss: 0.592801, acc.: 75.00%] [G loss: 0.879411]\n",
      "1700 [D loss: 0.581049, acc.: 78.12%] [G loss: 0.909368]\n",
      "1701 [D loss: 0.564311, acc.: 81.25%] [G loss: 0.940842]\n",
      "1702 [D loss: 0.541945, acc.: 84.38%] [G loss: 0.914257]\n",
      "1703 [D loss: 0.551786, acc.: 71.88%] [G loss: 0.935783]\n",
      "1704 [D loss: 0.578527, acc.: 65.62%] [G loss: 0.976592]\n",
      "1705 [D loss: 0.612167, acc.: 53.12%] [G loss: 0.933534]\n",
      "1706 [D loss: 0.644962, acc.: 65.62%] [G loss: 0.920467]\n",
      "1707 [D loss: 0.709643, acc.: 53.12%] [G loss: 0.907863]\n",
      "1708 [D loss: 0.629216, acc.: 65.62%] [G loss: 0.977955]\n",
      "1709 [D loss: 0.561299, acc.: 68.75%] [G loss: 0.964101]\n",
      "1710 [D loss: 0.619152, acc.: 75.00%] [G loss: 1.016222]\n",
      "1711 [D loss: 0.641451, acc.: 65.62%] [G loss: 0.969315]\n",
      "1712 [D loss: 0.534501, acc.: 87.50%] [G loss: 0.967852]\n",
      "1713 [D loss: 0.586185, acc.: 78.12%] [G loss: 0.924228]\n",
      "1714 [D loss: 0.520510, acc.: 78.12%] [G loss: 0.968680]\n",
      "1715 [D loss: 0.582979, acc.: 68.75%] [G loss: 0.936322]\n",
      "1716 [D loss: 0.594696, acc.: 68.75%] [G loss: 0.960371]\n",
      "1717 [D loss: 0.559150, acc.: 71.88%] [G loss: 0.965288]\n",
      "1718 [D loss: 0.686672, acc.: 53.12%] [G loss: 0.889844]\n",
      "1719 [D loss: 0.614260, acc.: 65.62%] [G loss: 0.896075]\n",
      "1720 [D loss: 0.572412, acc.: 75.00%] [G loss: 0.957950]\n",
      "1721 [D loss: 0.533518, acc.: 75.00%] [G loss: 0.946195]\n",
      "1722 [D loss: 0.686128, acc.: 53.12%] [G loss: 0.861591]\n",
      "1723 [D loss: 0.622052, acc.: 68.75%] [G loss: 0.854057]\n",
      "1724 [D loss: 0.612954, acc.: 65.62%] [G loss: 0.930058]\n",
      "1725 [D loss: 0.569519, acc.: 78.12%] [G loss: 0.999056]\n",
      "1726 [D loss: 0.626096, acc.: 62.50%] [G loss: 0.923942]\n",
      "1727 [D loss: 0.552599, acc.: 75.00%] [G loss: 0.972703]\n",
      "1728 [D loss: 0.556550, acc.: 78.12%] [G loss: 0.987744]\n",
      "1729 [D loss: 0.703807, acc.: 53.12%] [G loss: 0.930340]\n",
      "1730 [D loss: 0.530965, acc.: 75.00%] [G loss: 0.884045]\n",
      "1731 [D loss: 0.524720, acc.: 81.25%] [G loss: 0.947726]\n",
      "1732 [D loss: 0.611602, acc.: 71.88%] [G loss: 0.884344]\n",
      "1733 [D loss: 0.545053, acc.: 71.88%] [G loss: 0.937556]\n",
      "1734 [D loss: 0.561965, acc.: 71.88%] [G loss: 1.002453]\n",
      "1735 [D loss: 0.573521, acc.: 75.00%] [G loss: 0.955941]\n",
      "1736 [D loss: 0.607422, acc.: 59.38%] [G loss: 0.788011]\n",
      "1737 [D loss: 0.577423, acc.: 68.75%] [G loss: 0.904837]\n",
      "1738 [D loss: 0.633723, acc.: 71.88%] [G loss: 0.956742]\n",
      "1739 [D loss: 0.557230, acc.: 71.88%] [G loss: 0.942080]\n",
      "1740 [D loss: 0.694027, acc.: 59.38%] [G loss: 0.921425]\n",
      "1741 [D loss: 0.475884, acc.: 81.25%] [G loss: 1.020345]\n",
      "1742 [D loss: 0.647191, acc.: 62.50%] [G loss: 0.904253]\n",
      "1743 [D loss: 0.605821, acc.: 59.38%] [G loss: 0.835018]\n",
      "1744 [D loss: 0.592164, acc.: 62.50%] [G loss: 0.831445]\n",
      "1745 [D loss: 0.611624, acc.: 68.75%] [G loss: 0.883191]\n",
      "1746 [D loss: 0.560598, acc.: 68.75%] [G loss: 0.938540]\n",
      "1747 [D loss: 0.567656, acc.: 75.00%] [G loss: 0.979846]\n",
      "1748 [D loss: 0.524192, acc.: 78.12%] [G loss: 0.918367]\n",
      "1749 [D loss: 0.703583, acc.: 50.00%] [G loss: 0.920437]\n",
      "1750 [D loss: 0.596376, acc.: 68.75%] [G loss: 0.916614]\n",
      "1751 [D loss: 0.598909, acc.: 68.75%] [G loss: 0.874865]\n",
      "1752 [D loss: 0.618908, acc.: 59.38%] [G loss: 0.859226]\n",
      "1753 [D loss: 0.601949, acc.: 71.88%] [G loss: 0.926107]\n",
      "1754 [D loss: 0.572666, acc.: 78.12%] [G loss: 0.890183]\n",
      "1755 [D loss: 0.561343, acc.: 62.50%] [G loss: 0.927560]\n",
      "1756 [D loss: 0.571636, acc.: 68.75%] [G loss: 0.979788]\n",
      "1757 [D loss: 0.716006, acc.: 53.12%] [G loss: 0.878196]\n",
      "1758 [D loss: 0.704145, acc.: 46.88%] [G loss: 0.812511]\n",
      "1759 [D loss: 0.595644, acc.: 75.00%] [G loss: 0.864835]\n",
      "1760 [D loss: 0.657097, acc.: 59.38%] [G loss: 0.882715]\n",
      "1761 [D loss: 0.620267, acc.: 59.38%] [G loss: 0.851983]\n",
      "1762 [D loss: 0.620341, acc.: 59.38%] [G loss: 0.909514]\n",
      "1763 [D loss: 0.662520, acc.: 53.12%] [G loss: 0.881209]\n",
      "1764 [D loss: 0.605154, acc.: 68.75%] [G loss: 0.924873]\n",
      "1765 [D loss: 0.661951, acc.: 65.62%] [G loss: 0.881549]\n",
      "1766 [D loss: 0.589364, acc.: 68.75%] [G loss: 0.904005]\n",
      "1767 [D loss: 0.540307, acc.: 78.12%] [G loss: 0.946216]\n",
      "1768 [D loss: 0.730320, acc.: 50.00%] [G loss: 0.934759]\n",
      "1769 [D loss: 0.592636, acc.: 65.62%] [G loss: 0.911628]\n",
      "1770 [D loss: 0.588941, acc.: 78.12%] [G loss: 0.872490]\n",
      "1771 [D loss: 0.608082, acc.: 71.88%] [G loss: 0.883695]\n",
      "1772 [D loss: 0.592714, acc.: 68.75%] [G loss: 0.907494]\n",
      "1773 [D loss: 0.658742, acc.: 71.88%] [G loss: 0.895449]\n",
      "1774 [D loss: 0.615759, acc.: 65.62%] [G loss: 0.885267]\n",
      "1775 [D loss: 0.665772, acc.: 68.75%] [G loss: 0.862079]\n",
      "1776 [D loss: 0.626071, acc.: 65.62%] [G loss: 0.956907]\n",
      "1777 [D loss: 0.537532, acc.: 81.25%] [G loss: 0.935468]\n",
      "1778 [D loss: 0.542570, acc.: 71.88%] [G loss: 0.932236]\n",
      "1779 [D loss: 0.654101, acc.: 59.38%] [G loss: 0.935820]\n",
      "1780 [D loss: 0.600691, acc.: 65.62%] [G loss: 0.961334]\n",
      "1781 [D loss: 0.606433, acc.: 68.75%] [G loss: 0.946163]\n",
      "1782 [D loss: 0.554087, acc.: 75.00%] [G loss: 0.931561]\n",
      "1783 [D loss: 0.569437, acc.: 81.25%] [G loss: 0.866526]\n",
      "1784 [D loss: 0.584196, acc.: 68.75%] [G loss: 0.933078]\n",
      "1785 [D loss: 0.626797, acc.: 68.75%] [G loss: 0.926931]\n",
      "1786 [D loss: 0.693529, acc.: 62.50%] [G loss: 0.893319]\n",
      "1787 [D loss: 0.660612, acc.: 59.38%] [G loss: 0.836244]\n",
      "1788 [D loss: 0.573245, acc.: 75.00%] [G loss: 0.918941]\n",
      "1789 [D loss: 0.602403, acc.: 68.75%] [G loss: 0.949652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790 [D loss: 0.640128, acc.: 59.38%] [G loss: 0.897268]\n",
      "1791 [D loss: 0.734235, acc.: 59.38%] [G loss: 0.879382]\n",
      "1792 [D loss: 0.582735, acc.: 78.12%] [G loss: 0.838215]\n",
      "1793 [D loss: 0.707495, acc.: 56.25%] [G loss: 0.831426]\n",
      "1794 [D loss: 0.612499, acc.: 53.12%] [G loss: 0.895595]\n",
      "1795 [D loss: 0.627078, acc.: 59.38%] [G loss: 0.919266]\n",
      "1796 [D loss: 0.616253, acc.: 68.75%] [G loss: 0.888455]\n",
      "1797 [D loss: 0.623384, acc.: 62.50%] [G loss: 0.927192]\n",
      "1798 [D loss: 0.682719, acc.: 62.50%] [G loss: 0.937706]\n",
      "1799 [D loss: 0.695001, acc.: 46.88%] [G loss: 0.954051]\n",
      "1800 [D loss: 0.614876, acc.: 59.38%] [G loss: 0.976202]\n",
      "1801 [D loss: 0.631057, acc.: 62.50%] [G loss: 0.978026]\n",
      "1802 [D loss: 0.537752, acc.: 78.12%] [G loss: 0.895275]\n",
      "1803 [D loss: 0.536722, acc.: 81.25%] [G loss: 0.910120]\n",
      "1804 [D loss: 0.609169, acc.: 68.75%] [G loss: 0.866862]\n",
      "1805 [D loss: 0.587890, acc.: 71.88%] [G loss: 0.937867]\n",
      "1806 [D loss: 0.609505, acc.: 78.12%] [G loss: 0.911285]\n",
      "1807 [D loss: 0.658531, acc.: 62.50%] [G loss: 0.819496]\n",
      "1808 [D loss: 0.581909, acc.: 68.75%] [G loss: 0.803821]\n",
      "1809 [D loss: 0.608741, acc.: 71.88%] [G loss: 0.882104]\n",
      "1810 [D loss: 0.611969, acc.: 75.00%] [G loss: 0.891168]\n",
      "1811 [D loss: 0.547295, acc.: 71.88%] [G loss: 0.892908]\n",
      "1812 [D loss: 0.652372, acc.: 62.50%] [G loss: 0.884891]\n",
      "1813 [D loss: 0.592399, acc.: 75.00%] [G loss: 0.882097]\n",
      "1814 [D loss: 0.612598, acc.: 81.25%] [G loss: 0.849664]\n",
      "1815 [D loss: 0.558354, acc.: 71.88%] [G loss: 0.887508]\n",
      "1816 [D loss: 0.652245, acc.: 59.38%] [G loss: 0.894544]\n",
      "1817 [D loss: 0.720808, acc.: 53.12%] [G loss: 0.897191]\n",
      "1818 [D loss: 0.671443, acc.: 62.50%] [G loss: 0.844308]\n",
      "1819 [D loss: 0.543094, acc.: 78.12%] [G loss: 0.865887]\n",
      "1820 [D loss: 0.619336, acc.: 71.88%] [G loss: 0.841515]\n",
      "1821 [D loss: 0.703422, acc.: 56.25%] [G loss: 0.896530]\n",
      "1822 [D loss: 0.659584, acc.: 71.88%] [G loss: 0.900180]\n",
      "1823 [D loss: 0.659116, acc.: 59.38%] [G loss: 0.911875]\n",
      "1824 [D loss: 0.694622, acc.: 56.25%] [G loss: 0.846379]\n",
      "1825 [D loss: 0.646013, acc.: 53.12%] [G loss: 0.910468]\n",
      "1826 [D loss: 0.601801, acc.: 62.50%] [G loss: 0.923253]\n",
      "1827 [D loss: 0.646841, acc.: 65.62%] [G loss: 0.983575]\n",
      "1828 [D loss: 0.586803, acc.: 71.88%] [G loss: 1.041596]\n",
      "1829 [D loss: 0.697736, acc.: 62.50%] [G loss: 0.879387]\n",
      "1830 [D loss: 0.657118, acc.: 59.38%] [G loss: 0.862249]\n",
      "1831 [D loss: 0.546188, acc.: 81.25%] [G loss: 0.855815]\n",
      "1832 [D loss: 0.570613, acc.: 78.12%] [G loss: 0.795340]\n",
      "1833 [D loss: 0.643543, acc.: 65.62%] [G loss: 0.878571]\n",
      "1834 [D loss: 0.602391, acc.: 65.62%] [G loss: 0.938007]\n",
      "1835 [D loss: 0.593578, acc.: 62.50%] [G loss: 1.018386]\n",
      "1836 [D loss: 0.753925, acc.: 46.88%] [G loss: 0.954772]\n",
      "1837 [D loss: 0.638140, acc.: 78.12%] [G loss: 0.879451]\n",
      "1838 [D loss: 0.641613, acc.: 65.62%] [G loss: 0.959843]\n",
      "1839 [D loss: 0.633617, acc.: 65.62%] [G loss: 0.941806]\n",
      "1840 [D loss: 0.626649, acc.: 68.75%] [G loss: 0.886765]\n",
      "1841 [D loss: 0.618633, acc.: 78.12%] [G loss: 0.949553]\n",
      "1842 [D loss: 0.645410, acc.: 53.12%] [G loss: 0.884376]\n",
      "1843 [D loss: 0.642933, acc.: 59.38%] [G loss: 0.914632]\n",
      "1844 [D loss: 0.623126, acc.: 68.75%] [G loss: 0.908943]\n",
      "1845 [D loss: 0.574853, acc.: 75.00%] [G loss: 0.953222]\n",
      "1846 [D loss: 0.724452, acc.: 53.12%] [G loss: 0.855806]\n",
      "1847 [D loss: 0.565024, acc.: 68.75%] [G loss: 0.914382]\n",
      "1848 [D loss: 0.636852, acc.: 71.88%] [G loss: 0.955543]\n",
      "1849 [D loss: 0.601684, acc.: 71.88%] [G loss: 0.893979]\n",
      "1850 [D loss: 0.595838, acc.: 68.75%] [G loss: 0.890904]\n",
      "1851 [D loss: 0.610186, acc.: 65.62%] [G loss: 0.914539]\n",
      "1852 [D loss: 0.658727, acc.: 62.50%] [G loss: 0.934122]\n",
      "1853 [D loss: 0.609349, acc.: 78.12%] [G loss: 0.962627]\n",
      "1854 [D loss: 0.567664, acc.: 81.25%] [G loss: 0.887499]\n",
      "1855 [D loss: 0.572974, acc.: 71.88%] [G loss: 0.890017]\n",
      "1856 [D loss: 0.613326, acc.: 68.75%] [G loss: 0.893467]\n",
      "1857 [D loss: 0.573157, acc.: 71.88%] [G loss: 0.876236]\n",
      "1858 [D loss: 0.599319, acc.: 71.88%] [G loss: 0.866581]\n",
      "1859 [D loss: 0.608052, acc.: 65.62%] [G loss: 0.887883]\n",
      "1860 [D loss: 0.605657, acc.: 59.38%] [G loss: 0.920165]\n",
      "1861 [D loss: 0.633558, acc.: 62.50%] [G loss: 0.857198]\n",
      "1862 [D loss: 0.556546, acc.: 75.00%] [G loss: 0.974969]\n",
      "1863 [D loss: 0.671344, acc.: 53.12%] [G loss: 0.895593]\n",
      "1864 [D loss: 0.636519, acc.: 68.75%] [G loss: 0.896967]\n",
      "1865 [D loss: 0.609534, acc.: 62.50%] [G loss: 0.958483]\n",
      "1866 [D loss: 0.553789, acc.: 78.12%] [G loss: 0.909122]\n",
      "1867 [D loss: 0.573898, acc.: 71.88%] [G loss: 0.867659]\n",
      "1868 [D loss: 0.567585, acc.: 75.00%] [G loss: 0.858222]\n",
      "1869 [D loss: 0.600384, acc.: 65.62%] [G loss: 0.872281]\n",
      "1870 [D loss: 0.546706, acc.: 78.12%] [G loss: 0.918485]\n",
      "1871 [D loss: 0.535610, acc.: 75.00%] [G loss: 0.901051]\n",
      "1872 [D loss: 0.589551, acc.: 71.88%] [G loss: 0.958570]\n",
      "1873 [D loss: 0.653955, acc.: 65.62%] [G loss: 0.972894]\n",
      "1874 [D loss: 0.583391, acc.: 75.00%] [G loss: 0.891062]\n",
      "1875 [D loss: 0.615272, acc.: 56.25%] [G loss: 0.886688]\n",
      "1876 [D loss: 0.571400, acc.: 75.00%] [G loss: 0.932329]\n",
      "1877 [D loss: 0.577189, acc.: 71.88%] [G loss: 0.967762]\n",
      "1878 [D loss: 0.679361, acc.: 56.25%] [G loss: 1.007620]\n",
      "1879 [D loss: 0.614180, acc.: 68.75%] [G loss: 0.948879]\n",
      "1880 [D loss: 0.634378, acc.: 71.88%] [G loss: 0.897429]\n",
      "1881 [D loss: 0.723495, acc.: 46.88%] [G loss: 0.865450]\n",
      "1882 [D loss: 0.578594, acc.: 75.00%] [G loss: 0.810586]\n",
      "1883 [D loss: 0.565095, acc.: 84.38%] [G loss: 0.831894]\n",
      "1884 [D loss: 0.623989, acc.: 62.50%] [G loss: 0.833067]\n",
      "1885 [D loss: 0.626410, acc.: 65.62%] [G loss: 0.810195]\n",
      "1886 [D loss: 0.640712, acc.: 59.38%] [G loss: 0.869016]\n",
      "1887 [D loss: 0.621138, acc.: 68.75%] [G loss: 0.864995]\n",
      "1888 [D loss: 0.636874, acc.: 65.62%] [G loss: 0.869609]\n",
      "1889 [D loss: 0.651813, acc.: 65.62%] [G loss: 0.846344]\n",
      "1890 [D loss: 0.630100, acc.: 65.62%] [G loss: 0.884451]\n",
      "1891 [D loss: 0.641892, acc.: 59.38%] [G loss: 0.918965]\n",
      "1892 [D loss: 0.667198, acc.: 62.50%] [G loss: 0.921916]\n",
      "1893 [D loss: 0.583400, acc.: 78.12%] [G loss: 0.854854]\n",
      "1894 [D loss: 0.596237, acc.: 71.88%] [G loss: 0.897246]\n",
      "1895 [D loss: 0.617876, acc.: 62.50%] [G loss: 0.870331]\n",
      "1896 [D loss: 0.729364, acc.: 53.12%] [G loss: 0.865167]\n",
      "1897 [D loss: 0.684054, acc.: 53.12%] [G loss: 0.839842]\n",
      "1898 [D loss: 0.554693, acc.: 78.12%] [G loss: 0.845952]\n",
      "1899 [D loss: 0.568984, acc.: 75.00%] [G loss: 0.911378]\n",
      "1900 [D loss: 0.480221, acc.: 81.25%] [G loss: 0.905416]\n",
      "1901 [D loss: 0.731412, acc.: 46.88%] [G loss: 0.896075]\n",
      "1902 [D loss: 0.582167, acc.: 68.75%] [G loss: 0.999095]\n",
      "1903 [D loss: 0.597081, acc.: 62.50%] [G loss: 0.992295]\n",
      "1904 [D loss: 0.605275, acc.: 62.50%] [G loss: 0.934665]\n",
      "1905 [D loss: 0.569594, acc.: 68.75%] [G loss: 0.945361]\n",
      "1906 [D loss: 0.717565, acc.: 56.25%] [G loss: 1.015283]\n",
      "1907 [D loss: 0.576225, acc.: 75.00%] [G loss: 0.910102]\n",
      "1908 [D loss: 0.637807, acc.: 59.38%] [G loss: 0.902547]\n",
      "1909 [D loss: 0.646578, acc.: 68.75%] [G loss: 0.904691]\n",
      "1910 [D loss: 0.642098, acc.: 50.00%] [G loss: 0.938445]\n",
      "1911 [D loss: 0.641295, acc.: 59.38%] [G loss: 0.837656]\n",
      "1912 [D loss: 0.561409, acc.: 75.00%] [G loss: 0.844557]\n",
      "1913 [D loss: 0.640278, acc.: 59.38%] [G loss: 0.843903]\n",
      "1914 [D loss: 0.571434, acc.: 75.00%] [G loss: 0.849273]\n",
      "1915 [D loss: 0.650184, acc.: 59.38%] [G loss: 0.850007]\n",
      "1916 [D loss: 0.665555, acc.: 56.25%] [G loss: 0.892023]\n",
      "1917 [D loss: 0.555156, acc.: 81.25%] [G loss: 0.903932]\n",
      "1918 [D loss: 0.616372, acc.: 68.75%] [G loss: 0.810046]\n",
      "1919 [D loss: 0.665371, acc.: 53.12%] [G loss: 0.812960]\n",
      "1920 [D loss: 0.604844, acc.: 65.62%] [G loss: 0.937536]\n",
      "1921 [D loss: 0.599023, acc.: 71.88%] [G loss: 0.821234]\n",
      "1922 [D loss: 0.712979, acc.: 59.38%] [G loss: 0.861122]\n",
      "1923 [D loss: 0.695553, acc.: 53.12%] [G loss: 0.822317]\n",
      "1924 [D loss: 0.595278, acc.: 65.62%] [G loss: 0.808457]\n",
      "1925 [D loss: 0.649999, acc.: 59.38%] [G loss: 0.860112]\n",
      "1926 [D loss: 0.598481, acc.: 68.75%] [G loss: 0.886346]\n",
      "1927 [D loss: 0.602899, acc.: 71.88%] [G loss: 0.855661]\n",
      "1928 [D loss: 0.561263, acc.: 78.12%] [G loss: 0.913990]\n",
      "1929 [D loss: 0.633053, acc.: 62.50%] [G loss: 0.915369]\n",
      "1930 [D loss: 0.665894, acc.: 59.38%] [G loss: 0.970475]\n",
      "1931 [D loss: 0.684808, acc.: 50.00%] [G loss: 0.923917]\n",
      "1932 [D loss: 0.641089, acc.: 71.88%] [G loss: 0.953449]\n",
      "1933 [D loss: 0.645011, acc.: 59.38%] [G loss: 0.879914]\n",
      "1934 [D loss: 0.624940, acc.: 68.75%] [G loss: 0.879327]\n",
      "1935 [D loss: 0.592047, acc.: 68.75%] [G loss: 0.862729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936 [D loss: 0.640405, acc.: 65.62%] [G loss: 0.863423]\n",
      "1937 [D loss: 0.569167, acc.: 78.12%] [G loss: 0.968541]\n",
      "1938 [D loss: 0.625518, acc.: 65.62%] [G loss: 0.877207]\n",
      "1939 [D loss: 0.562211, acc.: 81.25%] [G loss: 0.882701]\n",
      "1940 [D loss: 0.611265, acc.: 65.62%] [G loss: 0.965581]\n",
      "1941 [D loss: 0.609404, acc.: 78.12%] [G loss: 0.975199]\n",
      "1942 [D loss: 0.595459, acc.: 71.88%] [G loss: 0.900711]\n",
      "1943 [D loss: 0.553974, acc.: 71.88%] [G loss: 0.906136]\n",
      "1944 [D loss: 0.614659, acc.: 62.50%] [G loss: 0.889685]\n",
      "1945 [D loss: 0.621278, acc.: 75.00%] [G loss: 0.843567]\n",
      "1946 [D loss: 0.737562, acc.: 53.12%] [G loss: 0.976425]\n",
      "1947 [D loss: 0.545599, acc.: 78.12%] [G loss: 0.915859]\n",
      "1948 [D loss: 0.679619, acc.: 50.00%] [G loss: 0.892793]\n",
      "1949 [D loss: 0.537403, acc.: 81.25%] [G loss: 0.847890]\n",
      "1950 [D loss: 0.626977, acc.: 68.75%] [G loss: 0.886673]\n",
      "1951 [D loss: 0.604288, acc.: 71.88%] [G loss: 0.873451]\n",
      "1952 [D loss: 0.560823, acc.: 68.75%] [G loss: 0.993155]\n",
      "1953 [D loss: 0.675728, acc.: 62.50%] [G loss: 0.933228]\n",
      "1954 [D loss: 0.626399, acc.: 68.75%] [G loss: 0.891536]\n",
      "1955 [D loss: 0.600343, acc.: 71.88%] [G loss: 0.835854]\n",
      "1956 [D loss: 0.601481, acc.: 68.75%] [G loss: 0.788228]\n",
      "1957 [D loss: 0.553816, acc.: 75.00%] [G loss: 0.895848]\n",
      "1958 [D loss: 0.541142, acc.: 81.25%] [G loss: 0.924761]\n",
      "1959 [D loss: 0.608581, acc.: 68.75%] [G loss: 0.937971]\n",
      "1960 [D loss: 0.596442, acc.: 65.62%] [G loss: 0.887640]\n",
      "1961 [D loss: 0.625678, acc.: 68.75%] [G loss: 1.020676]\n",
      "1962 [D loss: 0.591392, acc.: 78.12%] [G loss: 0.913850]\n",
      "1963 [D loss: 0.621107, acc.: 71.88%] [G loss: 0.868061]\n",
      "1964 [D loss: 0.552444, acc.: 81.25%] [G loss: 0.934272]\n",
      "1965 [D loss: 0.564434, acc.: 81.25%] [G loss: 0.932664]\n",
      "1966 [D loss: 0.538463, acc.: 71.88%] [G loss: 1.005178]\n",
      "1967 [D loss: 0.608199, acc.: 65.62%] [G loss: 0.988392]\n",
      "1968 [D loss: 0.592611, acc.: 75.00%] [G loss: 0.946476]\n",
      "1969 [D loss: 0.631446, acc.: 62.50%] [G loss: 0.905353]\n",
      "1970 [D loss: 0.665357, acc.: 65.62%] [G loss: 0.991540]\n",
      "1971 [D loss: 0.585996, acc.: 68.75%] [G loss: 0.879144]\n",
      "1972 [D loss: 0.537068, acc.: 84.38%] [G loss: 0.876093]\n",
      "1973 [D loss: 0.523991, acc.: 84.38%] [G loss: 0.896749]\n",
      "1974 [D loss: 0.581685, acc.: 71.88%] [G loss: 0.856915]\n",
      "1975 [D loss: 0.723100, acc.: 53.12%] [G loss: 0.869718]\n",
      "1976 [D loss: 0.575255, acc.: 75.00%] [G loss: 0.900016]\n",
      "1977 [D loss: 0.643730, acc.: 62.50%] [G loss: 1.009168]\n",
      "1978 [D loss: 0.614953, acc.: 62.50%] [G loss: 1.001988]\n",
      "1979 [D loss: 0.656976, acc.: 62.50%] [G loss: 0.965777]\n",
      "1980 [D loss: 0.684868, acc.: 50.00%] [G loss: 0.867413]\n",
      "1981 [D loss: 0.665852, acc.: 56.25%] [G loss: 0.811641]\n",
      "1982 [D loss: 0.583292, acc.: 71.88%] [G loss: 0.821847]\n",
      "1983 [D loss: 0.695110, acc.: 56.25%] [G loss: 0.821954]\n",
      "1984 [D loss: 0.691371, acc.: 59.38%] [G loss: 0.856495]\n",
      "1985 [D loss: 0.492687, acc.: 75.00%] [G loss: 0.916849]\n",
      "1986 [D loss: 0.791009, acc.: 37.50%] [G loss: 0.963706]\n",
      "1987 [D loss: 0.609105, acc.: 65.62%] [G loss: 1.021874]\n",
      "1988 [D loss: 0.609304, acc.: 71.88%] [G loss: 1.016298]\n",
      "1989 [D loss: 0.630974, acc.: 71.88%] [G loss: 0.906028]\n",
      "1990 [D loss: 0.634083, acc.: 71.88%] [G loss: 0.813852]\n",
      "1991 [D loss: 0.560662, acc.: 71.88%] [G loss: 0.904053]\n",
      "1992 [D loss: 0.679844, acc.: 56.25%] [G loss: 0.923004]\n",
      "1993 [D loss: 0.580640, acc.: 68.75%] [G loss: 0.952465]\n",
      "1994 [D loss: 0.725457, acc.: 50.00%] [G loss: 0.899301]\n",
      "1995 [D loss: 0.639407, acc.: 59.38%] [G loss: 0.864573]\n",
      "1996 [D loss: 0.570911, acc.: 84.38%] [G loss: 0.901395]\n",
      "1997 [D loss: 0.704432, acc.: 46.88%] [G loss: 0.930574]\n",
      "1998 [D loss: 0.584930, acc.: 59.38%] [G loss: 0.915933]\n",
      "1999 [D loss: 0.575057, acc.: 68.75%] [G loss: 0.913457]\n",
      "2000 [D loss: 0.576613, acc.: 75.00%] [G loss: 0.968757]\n",
      "2001 [D loss: 0.632927, acc.: 62.50%] [G loss: 0.921705]\n",
      "2002 [D loss: 0.627885, acc.: 71.88%] [G loss: 0.886794]\n",
      "2003 [D loss: 0.664430, acc.: 62.50%] [G loss: 0.865563]\n",
      "2004 [D loss: 0.654738, acc.: 62.50%] [G loss: 0.867239]\n",
      "2005 [D loss: 0.621248, acc.: 65.62%] [G loss: 0.929587]\n",
      "2006 [D loss: 0.637138, acc.: 59.38%] [G loss: 0.834976]\n",
      "2007 [D loss: 0.589806, acc.: 75.00%] [G loss: 0.851249]\n",
      "2008 [D loss: 0.646760, acc.: 68.75%] [G loss: 0.855139]\n",
      "2009 [D loss: 0.571092, acc.: 78.12%] [G loss: 0.859328]\n",
      "2010 [D loss: 0.591389, acc.: 71.88%] [G loss: 0.877241]\n",
      "2011 [D loss: 0.572444, acc.: 71.88%] [G loss: 0.878477]\n",
      "2012 [D loss: 0.669038, acc.: 46.88%] [G loss: 0.876040]\n",
      "2013 [D loss: 0.571910, acc.: 75.00%] [G loss: 0.893782]\n",
      "2014 [D loss: 0.625504, acc.: 68.75%] [G loss: 0.907042]\n",
      "2015 [D loss: 0.697046, acc.: 50.00%] [G loss: 0.839484]\n",
      "2016 [D loss: 0.498241, acc.: 81.25%] [G loss: 0.822937]\n",
      "2017 [D loss: 0.531895, acc.: 87.50%] [G loss: 0.841771]\n",
      "2018 [D loss: 0.648001, acc.: 56.25%] [G loss: 0.873322]\n",
      "2019 [D loss: 0.543182, acc.: 78.12%] [G loss: 0.814626]\n",
      "2020 [D loss: 0.657295, acc.: 46.88%] [G loss: 0.898768]\n",
      "2021 [D loss: 0.668264, acc.: 56.25%] [G loss: 0.943012]\n",
      "2022 [D loss: 0.681725, acc.: 50.00%] [G loss: 0.940784]\n",
      "2023 [D loss: 0.638385, acc.: 65.62%] [G loss: 0.914415]\n",
      "2024 [D loss: 0.611250, acc.: 65.62%] [G loss: 0.992664]\n",
      "2025 [D loss: 0.657261, acc.: 65.62%] [G loss: 0.966763]\n",
      "2026 [D loss: 0.702857, acc.: 46.88%] [G loss: 0.839268]\n",
      "2027 [D loss: 0.601967, acc.: 68.75%] [G loss: 0.938066]\n",
      "2028 [D loss: 0.617679, acc.: 68.75%] [G loss: 0.886126]\n",
      "2029 [D loss: 0.682325, acc.: 59.38%] [G loss: 0.883863]\n",
      "2030 [D loss: 0.683437, acc.: 56.25%] [G loss: 0.938325]\n",
      "2031 [D loss: 0.565478, acc.: 81.25%] [G loss: 0.929968]\n",
      "2032 [D loss: 0.701119, acc.: 50.00%] [G loss: 0.917539]\n",
      "2033 [D loss: 0.573689, acc.: 71.88%] [G loss: 0.906300]\n",
      "2034 [D loss: 0.737591, acc.: 46.88%] [G loss: 0.832496]\n",
      "2035 [D loss: 0.588977, acc.: 81.25%] [G loss: 0.829300]\n",
      "2036 [D loss: 0.676346, acc.: 56.25%] [G loss: 0.916962]\n",
      "2037 [D loss: 0.610178, acc.: 65.62%] [G loss: 0.886321]\n",
      "2038 [D loss: 0.547452, acc.: 75.00%] [G loss: 0.871852]\n",
      "2039 [D loss: 0.695730, acc.: 59.38%] [G loss: 0.897067]\n",
      "2040 [D loss: 0.616497, acc.: 68.75%] [G loss: 0.896992]\n",
      "2041 [D loss: 0.630849, acc.: 56.25%] [G loss: 0.876238]\n",
      "2042 [D loss: 0.655652, acc.: 68.75%] [G loss: 0.842046]\n",
      "2043 [D loss: 0.626549, acc.: 50.00%] [G loss: 0.983249]\n",
      "2044 [D loss: 0.676738, acc.: 62.50%] [G loss: 0.936750]\n",
      "2045 [D loss: 0.617735, acc.: 65.62%] [G loss: 1.004297]\n",
      "2046 [D loss: 0.510161, acc.: 87.50%] [G loss: 1.025899]\n",
      "2047 [D loss: 0.661897, acc.: 59.38%] [G loss: 0.902468]\n",
      "2048 [D loss: 0.665759, acc.: 62.50%] [G loss: 0.868701]\n",
      "2049 [D loss: 0.588322, acc.: 68.75%] [G loss: 0.996491]\n",
      "2050 [D loss: 0.745666, acc.: 53.12%] [G loss: 0.914083]\n",
      "2051 [D loss: 0.542555, acc.: 68.75%] [G loss: 0.951645]\n",
      "2052 [D loss: 0.750432, acc.: 46.88%] [G loss: 0.976209]\n",
      "2053 [D loss: 0.654390, acc.: 56.25%] [G loss: 1.015330]\n",
      "2054 [D loss: 0.622178, acc.: 71.88%] [G loss: 0.932634]\n",
      "2055 [D loss: 0.575233, acc.: 81.25%] [G loss: 0.857632]\n",
      "2056 [D loss: 0.633580, acc.: 62.50%] [G loss: 0.923833]\n",
      "2057 [D loss: 0.692818, acc.: 65.62%] [G loss: 0.956968]\n",
      "2058 [D loss: 0.671809, acc.: 56.25%] [G loss: 0.909629]\n",
      "2059 [D loss: 0.641610, acc.: 59.38%] [G loss: 0.885618]\n",
      "2060 [D loss: 0.627690, acc.: 68.75%] [G loss: 0.965007]\n",
      "2061 [D loss: 0.694244, acc.: 53.12%] [G loss: 0.867610]\n",
      "2062 [D loss: 0.608880, acc.: 62.50%] [G loss: 0.902599]\n",
      "2063 [D loss: 0.594757, acc.: 71.88%] [G loss: 0.950491]\n",
      "2064 [D loss: 0.662407, acc.: 65.62%] [G loss: 0.935120]\n",
      "2065 [D loss: 0.654545, acc.: 65.62%] [G loss: 0.825994]\n",
      "2066 [D loss: 0.607350, acc.: 75.00%] [G loss: 0.853786]\n",
      "2067 [D loss: 0.602414, acc.: 71.88%] [G loss: 0.858382]\n",
      "2068 [D loss: 0.609669, acc.: 68.75%] [G loss: 0.870935]\n",
      "2069 [D loss: 0.708091, acc.: 50.00%] [G loss: 0.855551]\n",
      "2070 [D loss: 0.641951, acc.: 62.50%] [G loss: 0.870762]\n",
      "2071 [D loss: 0.630451, acc.: 71.88%] [G loss: 0.840700]\n",
      "2072 [D loss: 0.640029, acc.: 62.50%] [G loss: 0.924287]\n",
      "2073 [D loss: 0.633107, acc.: 59.38%] [G loss: 0.794761]\n",
      "2074 [D loss: 0.692542, acc.: 46.88%] [G loss: 0.857807]\n",
      "2075 [D loss: 0.751179, acc.: 43.75%] [G loss: 0.930446]\n",
      "2076 [D loss: 0.568807, acc.: 68.75%] [G loss: 0.842846]\n",
      "2077 [D loss: 0.562166, acc.: 81.25%] [G loss: 0.849015]\n",
      "2078 [D loss: 0.540164, acc.: 78.12%] [G loss: 0.863809]\n",
      "2079 [D loss: 0.659993, acc.: 59.38%] [G loss: 0.862067]\n",
      "2080 [D loss: 0.540270, acc.: 78.12%] [G loss: 0.888759]\n",
      "2081 [D loss: 0.740924, acc.: 46.88%] [G loss: 0.823779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2082 [D loss: 0.593361, acc.: 78.12%] [G loss: 0.829005]\n",
      "2083 [D loss: 0.625886, acc.: 65.62%] [G loss: 0.837716]\n",
      "2084 [D loss: 0.685695, acc.: 56.25%] [G loss: 0.957183]\n",
      "2085 [D loss: 0.656632, acc.: 65.62%] [G loss: 0.972683]\n",
      "2086 [D loss: 0.639257, acc.: 56.25%] [G loss: 0.986431]\n",
      "2087 [D loss: 0.678646, acc.: 53.12%] [G loss: 0.886014]\n",
      "2088 [D loss: 0.672206, acc.: 68.75%] [G loss: 0.970030]\n",
      "2089 [D loss: 0.661833, acc.: 62.50%] [G loss: 0.959307]\n",
      "2090 [D loss: 0.642403, acc.: 62.50%] [G loss: 1.021876]\n",
      "2091 [D loss: 0.599044, acc.: 75.00%] [G loss: 1.013395]\n",
      "2092 [D loss: 0.658942, acc.: 53.12%] [G loss: 0.933584]\n",
      "2093 [D loss: 0.580368, acc.: 78.12%] [G loss: 0.828557]\n",
      "2094 [D loss: 0.639374, acc.: 59.38%] [G loss: 0.848397]\n",
      "2095 [D loss: 0.666416, acc.: 50.00%] [G loss: 0.899922]\n",
      "2096 [D loss: 0.694544, acc.: 59.38%] [G loss: 0.912295]\n",
      "2097 [D loss: 0.679181, acc.: 59.38%] [G loss: 0.943382]\n",
      "2098 [D loss: 0.577658, acc.: 68.75%] [G loss: 0.975879]\n",
      "2099 [D loss: 0.637067, acc.: 62.50%] [G loss: 0.934788]\n",
      "2100 [D loss: 0.583070, acc.: 68.75%] [G loss: 0.932160]\n",
      "2101 [D loss: 0.631661, acc.: 65.62%] [G loss: 0.962478]\n",
      "2102 [D loss: 0.598888, acc.: 65.62%] [G loss: 0.927224]\n",
      "2103 [D loss: 0.667460, acc.: 56.25%] [G loss: 0.940452]\n",
      "2104 [D loss: 0.678843, acc.: 56.25%] [G loss: 0.883285]\n",
      "2105 [D loss: 0.659358, acc.: 62.50%] [G loss: 0.901477]\n",
      "2106 [D loss: 0.573417, acc.: 71.88%] [G loss: 0.901547]\n",
      "2107 [D loss: 0.605699, acc.: 68.75%] [G loss: 0.955690]\n",
      "2108 [D loss: 0.559970, acc.: 78.12%] [G loss: 0.870019]\n",
      "2109 [D loss: 0.605327, acc.: 78.12%] [G loss: 0.843617]\n",
      "2110 [D loss: 0.629424, acc.: 65.62%] [G loss: 0.897254]\n",
      "2111 [D loss: 0.632947, acc.: 59.38%] [G loss: 0.862990]\n",
      "2112 [D loss: 0.638364, acc.: 68.75%] [G loss: 0.878829]\n",
      "2113 [D loss: 0.711332, acc.: 46.88%] [G loss: 0.857711]\n",
      "2114 [D loss: 0.672575, acc.: 46.88%] [G loss: 0.812261]\n",
      "2115 [D loss: 0.572743, acc.: 75.00%] [G loss: 0.839390]\n",
      "2116 [D loss: 0.548682, acc.: 81.25%] [G loss: 0.849700]\n",
      "2117 [D loss: 0.666141, acc.: 62.50%] [G loss: 0.915535]\n",
      "2118 [D loss: 0.594466, acc.: 68.75%] [G loss: 0.820935]\n",
      "2119 [D loss: 0.564260, acc.: 71.88%] [G loss: 0.921375]\n",
      "2120 [D loss: 0.551023, acc.: 75.00%] [G loss: 0.932657]\n",
      "2121 [D loss: 0.609954, acc.: 62.50%] [G loss: 0.890858]\n",
      "2122 [D loss: 0.708890, acc.: 59.38%] [G loss: 0.927976]\n",
      "2123 [D loss: 0.762408, acc.: 50.00%] [G loss: 0.889958]\n",
      "2124 [D loss: 0.733470, acc.: 56.25%] [G loss: 0.858029]\n",
      "2125 [D loss: 0.633742, acc.: 68.75%] [G loss: 0.938123]\n",
      "2126 [D loss: 0.577773, acc.: 68.75%] [G loss: 0.876846]\n",
      "2127 [D loss: 0.626811, acc.: 59.38%] [G loss: 0.850893]\n",
      "2128 [D loss: 0.637224, acc.: 65.62%] [G loss: 0.899952]\n",
      "2129 [D loss: 0.492408, acc.: 81.25%] [G loss: 0.944236]\n",
      "2130 [D loss: 0.675191, acc.: 62.50%] [G loss: 0.930196]\n",
      "2131 [D loss: 0.586042, acc.: 65.62%] [G loss: 1.068066]\n",
      "2132 [D loss: 0.579515, acc.: 68.75%] [G loss: 1.064405]\n",
      "2133 [D loss: 0.549963, acc.: 71.88%] [G loss: 0.992923]\n",
      "2134 [D loss: 0.719391, acc.: 62.50%] [G loss: 0.960188]\n",
      "2135 [D loss: 0.566039, acc.: 65.62%] [G loss: 0.903562]\n",
      "2136 [D loss: 0.629896, acc.: 68.75%] [G loss: 0.915725]\n",
      "2137 [D loss: 0.596964, acc.: 68.75%] [G loss: 0.895383]\n",
      "2138 [D loss: 0.501014, acc.: 87.50%] [G loss: 0.878827]\n",
      "2139 [D loss: 0.616249, acc.: 75.00%] [G loss: 0.989814]\n",
      "2140 [D loss: 0.631374, acc.: 62.50%] [G loss: 0.832080]\n",
      "2141 [D loss: 0.698759, acc.: 46.88%] [G loss: 0.932038]\n",
      "2142 [D loss: 0.579945, acc.: 78.12%] [G loss: 0.871835]\n",
      "2143 [D loss: 0.649561, acc.: 62.50%] [G loss: 0.813312]\n",
      "2144 [D loss: 0.659592, acc.: 56.25%] [G loss: 0.831954]\n",
      "2145 [D loss: 0.625705, acc.: 75.00%] [G loss: 0.868814]\n",
      "2146 [D loss: 0.580539, acc.: 71.88%] [G loss: 0.928845]\n",
      "2147 [D loss: 0.620889, acc.: 65.62%] [G loss: 0.899880]\n",
      "2148 [D loss: 0.678350, acc.: 59.38%] [G loss: 0.894768]\n",
      "2149 [D loss: 0.572748, acc.: 71.88%] [G loss: 0.947978]\n",
      "2150 [D loss: 0.646671, acc.: 68.75%] [G loss: 0.904286]\n",
      "2151 [D loss: 0.685917, acc.: 53.12%] [G loss: 0.841146]\n",
      "2152 [D loss: 0.615034, acc.: 81.25%] [G loss: 0.808342]\n",
      "2153 [D loss: 0.597936, acc.: 68.75%] [G loss: 0.891572]\n",
      "2154 [D loss: 0.579190, acc.: 78.12%] [G loss: 0.841356]\n",
      "2155 [D loss: 0.660286, acc.: 62.50%] [G loss: 0.888012]\n",
      "2156 [D loss: 0.641089, acc.: 53.12%] [G loss: 0.809248]\n",
      "2157 [D loss: 0.603802, acc.: 62.50%] [G loss: 0.896723]\n",
      "2158 [D loss: 0.596372, acc.: 68.75%] [G loss: 0.826699]\n",
      "2159 [D loss: 0.607205, acc.: 75.00%] [G loss: 0.812701]\n",
      "2160 [D loss: 0.527376, acc.: 81.25%] [G loss: 0.885497]\n",
      "2161 [D loss: 0.624158, acc.: 78.12%] [G loss: 0.913151]\n",
      "2162 [D loss: 0.616207, acc.: 71.88%] [G loss: 0.957593]\n",
      "2163 [D loss: 0.602555, acc.: 75.00%] [G loss: 0.867658]\n",
      "2164 [D loss: 0.664271, acc.: 65.62%] [G loss: 0.870627]\n",
      "2165 [D loss: 0.656898, acc.: 62.50%] [G loss: 0.864715]\n",
      "2166 [D loss: 0.607632, acc.: 71.88%] [G loss: 0.884162]\n",
      "2167 [D loss: 0.573778, acc.: 81.25%] [G loss: 0.872873]\n",
      "2168 [D loss: 0.695959, acc.: 53.12%] [G loss: 0.947608]\n",
      "2169 [D loss: 0.661583, acc.: 56.25%] [G loss: 0.978193]\n",
      "2170 [D loss: 0.558339, acc.: 75.00%] [G loss: 0.966129]\n",
      "2171 [D loss: 0.566605, acc.: 75.00%] [G loss: 0.954565]\n",
      "2172 [D loss: 0.742627, acc.: 40.62%] [G loss: 0.881596]\n",
      "2173 [D loss: 0.572436, acc.: 68.75%] [G loss: 0.883731]\n",
      "2174 [D loss: 0.610574, acc.: 68.75%] [G loss: 0.943240]\n",
      "2175 [D loss: 0.563393, acc.: 87.50%] [G loss: 0.903667]\n",
      "2176 [D loss: 0.627431, acc.: 65.62%] [G loss: 0.857448]\n",
      "2177 [D loss: 0.640590, acc.: 65.62%] [G loss: 0.864042]\n",
      "2178 [D loss: 0.607961, acc.: 75.00%] [G loss: 0.902986]\n",
      "2179 [D loss: 0.546277, acc.: 75.00%] [G loss: 0.819295]\n",
      "2180 [D loss: 0.610748, acc.: 68.75%] [G loss: 0.873873]\n",
      "2181 [D loss: 0.656284, acc.: 65.62%] [G loss: 0.904459]\n",
      "2182 [D loss: 0.585808, acc.: 78.12%] [G loss: 0.863467]\n",
      "2183 [D loss: 0.595803, acc.: 65.62%] [G loss: 0.932759]\n",
      "2184 [D loss: 0.619321, acc.: 71.88%] [G loss: 0.945778]\n",
      "2185 [D loss: 0.653047, acc.: 56.25%] [G loss: 0.863380]\n",
      "2186 [D loss: 0.600221, acc.: 71.88%] [G loss: 0.849319]\n",
      "2187 [D loss: 0.566464, acc.: 75.00%] [G loss: 0.879605]\n",
      "2188 [D loss: 0.576581, acc.: 62.50%] [G loss: 0.857105]\n",
      "2189 [D loss: 0.626167, acc.: 59.38%] [G loss: 0.963492]\n",
      "2190 [D loss: 0.582518, acc.: 68.75%] [G loss: 0.849107]\n",
      "2191 [D loss: 0.626848, acc.: 68.75%] [G loss: 0.865029]\n",
      "2192 [D loss: 0.685563, acc.: 59.38%] [G loss: 0.870905]\n",
      "2193 [D loss: 0.723953, acc.: 40.62%] [G loss: 0.847977]\n",
      "2194 [D loss: 0.634716, acc.: 62.50%] [G loss: 0.884126]\n",
      "2195 [D loss: 0.643586, acc.: 65.62%] [G loss: 0.868888]\n",
      "2196 [D loss: 0.712871, acc.: 46.88%] [G loss: 0.787132]\n",
      "2197 [D loss: 0.643057, acc.: 71.88%] [G loss: 0.857365]\n",
      "2198 [D loss: 0.686103, acc.: 62.50%] [G loss: 0.838922]\n",
      "2199 [D loss: 0.638805, acc.: 71.88%] [G loss: 0.897661]\n",
      "2200 [D loss: 0.673582, acc.: 46.88%] [G loss: 0.874234]\n",
      "2201 [D loss: 0.638585, acc.: 68.75%] [G loss: 0.866962]\n",
      "2202 [D loss: 0.626576, acc.: 65.62%] [G loss: 0.913342]\n",
      "2203 [D loss: 0.683057, acc.: 56.25%] [G loss: 0.921504]\n",
      "2204 [D loss: 0.649679, acc.: 59.38%] [G loss: 0.922688]\n",
      "2205 [D loss: 0.646534, acc.: 62.50%] [G loss: 0.918514]\n",
      "2206 [D loss: 0.661893, acc.: 65.62%] [G loss: 0.882227]\n",
      "2207 [D loss: 0.556239, acc.: 75.00%] [G loss: 0.843324]\n",
      "2208 [D loss: 0.618209, acc.: 68.75%] [G loss: 0.857367]\n",
      "2209 [D loss: 0.522431, acc.: 84.38%] [G loss: 0.863570]\n",
      "2210 [D loss: 0.633058, acc.: 78.12%] [G loss: 0.844297]\n",
      "2211 [D loss: 0.580051, acc.: 75.00%] [G loss: 0.867416]\n",
      "2212 [D loss: 0.573903, acc.: 78.12%] [G loss: 0.895211]\n",
      "2213 [D loss: 0.652996, acc.: 50.00%] [G loss: 0.878824]\n",
      "2214 [D loss: 0.649423, acc.: 56.25%] [G loss: 0.887557]\n",
      "2215 [D loss: 0.643939, acc.: 59.38%] [G loss: 0.907303]\n",
      "2216 [D loss: 0.612835, acc.: 65.62%] [G loss: 0.847932]\n",
      "2217 [D loss: 0.626118, acc.: 62.50%] [G loss: 0.892184]\n",
      "2218 [D loss: 0.569660, acc.: 84.38%] [G loss: 0.928205]\n",
      "2219 [D loss: 0.594271, acc.: 71.88%] [G loss: 0.932614]\n",
      "2220 [D loss: 0.648506, acc.: 65.62%] [G loss: 0.893064]\n",
      "2221 [D loss: 0.566148, acc.: 84.38%] [G loss: 0.833135]\n",
      "2222 [D loss: 0.698200, acc.: 56.25%] [G loss: 0.864304]\n",
      "2223 [D loss: 0.533308, acc.: 84.38%] [G loss: 0.895298]\n",
      "2224 [D loss: 0.685817, acc.: 56.25%] [G loss: 0.966407]\n",
      "2225 [D loss: 0.623483, acc.: 71.88%] [G loss: 0.946133]\n",
      "2226 [D loss: 0.561538, acc.: 71.88%] [G loss: 0.921647]\n",
      "2227 [D loss: 0.617616, acc.: 65.62%] [G loss: 0.954039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2228 [D loss: 0.761015, acc.: 50.00%] [G loss: 0.892628]\n",
      "2229 [D loss: 0.694536, acc.: 59.38%] [G loss: 0.867577]\n",
      "2230 [D loss: 0.627463, acc.: 62.50%] [G loss: 0.930133]\n",
      "2231 [D loss: 0.526475, acc.: 81.25%] [G loss: 0.951850]\n",
      "2232 [D loss: 0.586533, acc.: 68.75%] [G loss: 0.898801]\n",
      "2233 [D loss: 0.665134, acc.: 62.50%] [G loss: 0.868216]\n",
      "2234 [D loss: 0.540430, acc.: 71.88%] [G loss: 0.939308]\n",
      "2235 [D loss: 0.593637, acc.: 71.88%] [G loss: 0.899598]\n",
      "2236 [D loss: 0.662740, acc.: 59.38%] [G loss: 0.896388]\n",
      "2237 [D loss: 0.654896, acc.: 59.38%] [G loss: 0.883611]\n",
      "2238 [D loss: 0.621176, acc.: 62.50%] [G loss: 0.890836]\n",
      "2239 [D loss: 0.612767, acc.: 65.62%] [G loss: 0.889599]\n",
      "2240 [D loss: 0.568673, acc.: 71.88%] [G loss: 0.884173]\n",
      "2241 [D loss: 0.627711, acc.: 59.38%] [G loss: 0.895841]\n",
      "2242 [D loss: 0.654523, acc.: 50.00%] [G loss: 0.901097]\n",
      "2243 [D loss: 0.588020, acc.: 65.62%] [G loss: 1.012257]\n",
      "2244 [D loss: 0.690269, acc.: 59.38%] [G loss: 0.952129]\n",
      "2245 [D loss: 0.644555, acc.: 56.25%] [G loss: 0.877392]\n",
      "2246 [D loss: 0.592632, acc.: 68.75%] [G loss: 0.882827]\n",
      "2247 [D loss: 0.620252, acc.: 56.25%] [G loss: 0.890726]\n",
      "2248 [D loss: 0.550903, acc.: 78.12%] [G loss: 0.905055]\n",
      "2249 [D loss: 0.625877, acc.: 53.12%] [G loss: 0.892372]\n",
      "2250 [D loss: 0.698920, acc.: 50.00%] [G loss: 0.942584]\n",
      "2251 [D loss: 0.663934, acc.: 56.25%] [G loss: 0.937926]\n",
      "2252 [D loss: 0.559628, acc.: 78.12%] [G loss: 0.971486]\n",
      "2253 [D loss: 0.573413, acc.: 62.50%] [G loss: 0.961773]\n",
      "2254 [D loss: 0.717128, acc.: 43.75%] [G loss: 0.973608]\n",
      "2255 [D loss: 0.646146, acc.: 62.50%] [G loss: 0.861789]\n",
      "2256 [D loss: 0.541657, acc.: 68.75%] [G loss: 0.913574]\n",
      "2257 [D loss: 0.626354, acc.: 62.50%] [G loss: 0.870602]\n",
      "2258 [D loss: 0.589184, acc.: 68.75%] [G loss: 0.902572]\n",
      "2259 [D loss: 0.645575, acc.: 68.75%] [G loss: 0.929873]\n",
      "2260 [D loss: 0.629518, acc.: 68.75%] [G loss: 0.834819]\n",
      "2261 [D loss: 0.621236, acc.: 68.75%] [G loss: 0.862187]\n",
      "2262 [D loss: 0.678562, acc.: 56.25%] [G loss: 0.843077]\n",
      "2263 [D loss: 0.626130, acc.: 62.50%] [G loss: 0.915644]\n",
      "2264 [D loss: 0.670371, acc.: 65.62%] [G loss: 0.876962]\n",
      "2265 [D loss: 0.677937, acc.: 53.12%] [G loss: 0.803449]\n",
      "2266 [D loss: 0.618245, acc.: 68.75%] [G loss: 0.939354]\n",
      "2267 [D loss: 0.654568, acc.: 65.62%] [G loss: 0.842110]\n",
      "2268 [D loss: 0.597522, acc.: 71.88%] [G loss: 0.938126]\n",
      "2269 [D loss: 0.601518, acc.: 65.62%] [G loss: 0.919589]\n",
      "2270 [D loss: 0.598343, acc.: 68.75%] [G loss: 0.909619]\n",
      "2271 [D loss: 0.561130, acc.: 81.25%] [G loss: 0.932816]\n",
      "2272 [D loss: 0.567007, acc.: 68.75%] [G loss: 0.882745]\n",
      "2273 [D loss: 0.704570, acc.: 40.62%] [G loss: 0.838425]\n",
      "2274 [D loss: 0.643515, acc.: 62.50%] [G loss: 0.906823]\n",
      "2275 [D loss: 0.576477, acc.: 68.75%] [G loss: 0.913590]\n",
      "2276 [D loss: 0.625709, acc.: 68.75%] [G loss: 0.938004]\n",
      "2277 [D loss: 0.644014, acc.: 59.38%] [G loss: 0.997843]\n",
      "2278 [D loss: 0.619895, acc.: 62.50%] [G loss: 0.866245]\n",
      "2279 [D loss: 0.598262, acc.: 68.75%] [G loss: 0.914322]\n",
      "2280 [D loss: 0.632308, acc.: 65.62%] [G loss: 0.805820]\n",
      "2281 [D loss: 0.572445, acc.: 75.00%] [G loss: 0.863754]\n",
      "2282 [D loss: 0.639392, acc.: 62.50%] [G loss: 0.911131]\n",
      "2283 [D loss: 0.651704, acc.: 56.25%] [G loss: 0.996405]\n",
      "2284 [D loss: 0.595507, acc.: 75.00%] [G loss: 0.963227]\n",
      "2285 [D loss: 0.609179, acc.: 65.62%] [G loss: 0.943989]\n",
      "2286 [D loss: 0.616111, acc.: 68.75%] [G loss: 0.836241]\n",
      "2287 [D loss: 0.654707, acc.: 68.75%] [G loss: 0.813895]\n",
      "2288 [D loss: 0.648876, acc.: 71.88%] [G loss: 0.792703]\n",
      "2289 [D loss: 0.685303, acc.: 65.62%] [G loss: 0.894277]\n",
      "2290 [D loss: 0.673916, acc.: 56.25%] [G loss: 0.886782]\n",
      "2291 [D loss: 0.695108, acc.: 53.12%] [G loss: 0.888886]\n",
      "2292 [D loss: 0.633113, acc.: 68.75%] [G loss: 0.873233]\n",
      "2293 [D loss: 0.612994, acc.: 71.88%] [G loss: 0.810936]\n",
      "2294 [D loss: 0.661916, acc.: 59.38%] [G loss: 0.960790]\n",
      "2295 [D loss: 0.473965, acc.: 93.75%] [G loss: 0.851790]\n",
      "2296 [D loss: 0.611758, acc.: 59.38%] [G loss: 0.863473]\n",
      "2297 [D loss: 0.658387, acc.: 59.38%] [G loss: 0.941626]\n",
      "2298 [D loss: 0.524127, acc.: 65.62%] [G loss: 0.944093]\n",
      "2299 [D loss: 0.653339, acc.: 62.50%] [G loss: 0.990415]\n",
      "2300 [D loss: 0.736725, acc.: 53.12%] [G loss: 0.974109]\n",
      "2301 [D loss: 0.659336, acc.: 71.88%] [G loss: 0.907391]\n",
      "2302 [D loss: 0.590518, acc.: 68.75%] [G loss: 0.976373]\n",
      "2303 [D loss: 0.667804, acc.: 59.38%] [G loss: 0.966375]\n",
      "2304 [D loss: 0.665360, acc.: 62.50%] [G loss: 0.853355]\n",
      "2305 [D loss: 0.491714, acc.: 81.25%] [G loss: 0.850818]\n",
      "2306 [D loss: 0.681652, acc.: 56.25%] [G loss: 0.797064]\n",
      "2307 [D loss: 0.651895, acc.: 56.25%] [G loss: 0.917930]\n",
      "2308 [D loss: 0.636708, acc.: 65.62%] [G loss: 0.904233]\n",
      "2309 [D loss: 0.664555, acc.: 59.38%] [G loss: 0.838179]\n",
      "2310 [D loss: 0.677566, acc.: 59.38%] [G loss: 0.889622]\n",
      "2311 [D loss: 0.599736, acc.: 65.62%] [G loss: 0.891567]\n",
      "2312 [D loss: 0.684871, acc.: 59.38%] [G loss: 0.805983]\n",
      "2313 [D loss: 0.590781, acc.: 75.00%] [G loss: 0.926514]\n",
      "2314 [D loss: 0.588126, acc.: 78.12%] [G loss: 0.972588]\n",
      "2315 [D loss: 0.601080, acc.: 65.62%] [G loss: 0.938035]\n",
      "2316 [D loss: 0.577039, acc.: 78.12%] [G loss: 0.995227]\n",
      "2317 [D loss: 0.583974, acc.: 78.12%] [G loss: 1.008727]\n",
      "2318 [D loss: 0.662258, acc.: 59.38%] [G loss: 0.857968]\n",
      "2319 [D loss: 0.596345, acc.: 78.12%] [G loss: 0.833834]\n",
      "2320 [D loss: 0.590117, acc.: 75.00%] [G loss: 0.857619]\n",
      "2321 [D loss: 0.598421, acc.: 78.12%] [G loss: 0.824484]\n",
      "2322 [D loss: 0.635939, acc.: 62.50%] [G loss: 0.882199]\n",
      "2323 [D loss: 0.626721, acc.: 62.50%] [G loss: 0.947473]\n",
      "2324 [D loss: 0.665642, acc.: 62.50%] [G loss: 0.921413]\n",
      "2325 [D loss: 0.709591, acc.: 43.75%] [G loss: 0.988755]\n",
      "2326 [D loss: 0.635170, acc.: 71.88%] [G loss: 0.928714]\n",
      "2327 [D loss: 0.574843, acc.: 71.88%] [G loss: 0.937894]\n",
      "2328 [D loss: 0.621462, acc.: 62.50%] [G loss: 0.958301]\n",
      "2329 [D loss: 0.627825, acc.: 78.12%] [G loss: 0.945323]\n",
      "2330 [D loss: 0.617640, acc.: 68.75%] [G loss: 0.931474]\n",
      "2331 [D loss: 0.649718, acc.: 68.75%] [G loss: 0.957700]\n",
      "2332 [D loss: 0.585930, acc.: 78.12%] [G loss: 0.965136]\n",
      "2333 [D loss: 0.606371, acc.: 78.12%] [G loss: 0.937271]\n",
      "2334 [D loss: 0.604502, acc.: 71.88%] [G loss: 0.870114]\n",
      "2335 [D loss: 0.623930, acc.: 62.50%] [G loss: 0.934041]\n",
      "2336 [D loss: 0.615301, acc.: 68.75%] [G loss: 0.873876]\n",
      "2337 [D loss: 0.611254, acc.: 78.12%] [G loss: 0.910798]\n",
      "2338 [D loss: 0.636551, acc.: 59.38%] [G loss: 0.831995]\n",
      "2339 [D loss: 0.606972, acc.: 65.62%] [G loss: 0.877181]\n",
      "2340 [D loss: 0.583244, acc.: 68.75%] [G loss: 0.927594]\n",
      "2341 [D loss: 0.617759, acc.: 62.50%] [G loss: 0.915381]\n",
      "2342 [D loss: 0.669359, acc.: 65.62%] [G loss: 0.887043]\n",
      "2343 [D loss: 0.659652, acc.: 62.50%] [G loss: 0.944101]\n",
      "2344 [D loss: 0.640747, acc.: 65.62%] [G loss: 0.887828]\n",
      "2345 [D loss: 0.637252, acc.: 62.50%] [G loss: 0.916726]\n",
      "2346 [D loss: 0.591368, acc.: 68.75%] [G loss: 1.022829]\n",
      "2347 [D loss: 0.680759, acc.: 59.38%] [G loss: 0.951018]\n",
      "2348 [D loss: 0.607747, acc.: 68.75%] [G loss: 0.826755]\n",
      "2349 [D loss: 0.717178, acc.: 56.25%] [G loss: 0.985964]\n",
      "2350 [D loss: 0.642028, acc.: 59.38%] [G loss: 0.934678]\n",
      "2351 [D loss: 0.638013, acc.: 59.38%] [G loss: 0.954699]\n",
      "2352 [D loss: 0.530503, acc.: 78.12%] [G loss: 1.054929]\n",
      "2353 [D loss: 0.639897, acc.: 62.50%] [G loss: 0.963976]\n",
      "2354 [D loss: 0.686038, acc.: 53.12%] [G loss: 0.875606]\n",
      "2355 [D loss: 0.666100, acc.: 53.12%] [G loss: 0.827405]\n",
      "2356 [D loss: 0.651444, acc.: 65.62%] [G loss: 0.945314]\n",
      "2357 [D loss: 0.641636, acc.: 68.75%] [G loss: 0.902294]\n",
      "2358 [D loss: 0.607584, acc.: 68.75%] [G loss: 0.897170]\n",
      "2359 [D loss: 0.575458, acc.: 71.88%] [G loss: 0.953444]\n",
      "2360 [D loss: 0.631035, acc.: 59.38%] [G loss: 0.875446]\n",
      "2361 [D loss: 0.596538, acc.: 71.88%] [G loss: 0.889947]\n",
      "2362 [D loss: 0.553668, acc.: 68.75%] [G loss: 0.961126]\n",
      "2363 [D loss: 0.652514, acc.: 59.38%] [G loss: 0.864646]\n",
      "2364 [D loss: 0.644185, acc.: 62.50%] [G loss: 0.844217]\n",
      "2365 [D loss: 0.537296, acc.: 81.25%] [G loss: 0.937309]\n",
      "2366 [D loss: 0.675708, acc.: 62.50%] [G loss: 0.812813]\n",
      "2367 [D loss: 0.599616, acc.: 75.00%] [G loss: 0.826780]\n",
      "2368 [D loss: 0.651851, acc.: 78.12%] [G loss: 0.899761]\n",
      "2369 [D loss: 0.572547, acc.: 68.75%] [G loss: 0.928297]\n",
      "2370 [D loss: 0.651873, acc.: 62.50%] [G loss: 0.843969]\n",
      "2371 [D loss: 0.715938, acc.: 46.88%] [G loss: 0.847116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2372 [D loss: 0.589545, acc.: 71.88%] [G loss: 0.929190]\n",
      "2373 [D loss: 0.621730, acc.: 65.62%] [G loss: 1.016606]\n",
      "2374 [D loss: 0.685328, acc.: 59.38%] [G loss: 0.900406]\n",
      "2375 [D loss: 0.657644, acc.: 53.12%] [G loss: 0.931305]\n",
      "2376 [D loss: 0.571496, acc.: 87.50%] [G loss: 0.889722]\n",
      "2377 [D loss: 0.661041, acc.: 71.88%] [G loss: 0.852003]\n",
      "2378 [D loss: 0.645310, acc.: 56.25%] [G loss: 0.855261]\n",
      "2379 [D loss: 0.500486, acc.: 84.38%] [G loss: 0.897883]\n",
      "2380 [D loss: 0.637788, acc.: 65.62%] [G loss: 0.893272]\n",
      "2381 [D loss: 0.549784, acc.: 71.88%] [G loss: 0.869120]\n",
      "2382 [D loss: 0.620271, acc.: 56.25%] [G loss: 0.947924]\n",
      "2383 [D loss: 0.560507, acc.: 84.38%] [G loss: 0.879036]\n",
      "2384 [D loss: 0.682135, acc.: 62.50%] [G loss: 0.879397]\n",
      "2385 [D loss: 0.699663, acc.: 53.12%] [G loss: 0.936735]\n",
      "2386 [D loss: 0.665189, acc.: 53.12%] [G loss: 0.930156]\n",
      "2387 [D loss: 0.561836, acc.: 71.88%] [G loss: 0.997840]\n",
      "2388 [D loss: 0.529847, acc.: 75.00%] [G loss: 0.945413]\n",
      "2389 [D loss: 0.637894, acc.: 62.50%] [G loss: 0.921147]\n",
      "2390 [D loss: 0.597288, acc.: 71.88%] [G loss: 0.901873]\n",
      "2391 [D loss: 0.550943, acc.: 84.38%] [G loss: 0.888558]\n",
      "2392 [D loss: 0.587209, acc.: 65.62%] [G loss: 0.955633]\n",
      "2393 [D loss: 0.644819, acc.: 62.50%] [G loss: 0.819656]\n",
      "2394 [D loss: 0.651602, acc.: 53.12%] [G loss: 0.893462]\n",
      "2395 [D loss: 0.674971, acc.: 65.62%] [G loss: 0.937926]\n",
      "2396 [D loss: 0.647002, acc.: 62.50%] [G loss: 0.883381]\n",
      "2397 [D loss: 0.664712, acc.: 50.00%] [G loss: 0.938263]\n",
      "2398 [D loss: 0.589467, acc.: 68.75%] [G loss: 0.982033]\n",
      "2399 [D loss: 0.615273, acc.: 75.00%] [G loss: 0.957704]\n",
      "2400 [D loss: 0.597560, acc.: 75.00%] [G loss: 0.992254]\n",
      "2401 [D loss: 0.641962, acc.: 65.62%] [G loss: 0.878371]\n",
      "2402 [D loss: 0.555922, acc.: 84.38%] [G loss: 0.913774]\n",
      "2403 [D loss: 0.593484, acc.: 71.88%] [G loss: 0.887061]\n",
      "2404 [D loss: 0.661478, acc.: 53.12%] [G loss: 0.841472]\n",
      "2405 [D loss: 0.706476, acc.: 59.38%] [G loss: 0.879627]\n",
      "2406 [D loss: 0.635103, acc.: 75.00%] [G loss: 0.880882]\n",
      "2407 [D loss: 0.566810, acc.: 71.88%] [G loss: 0.899035]\n",
      "2408 [D loss: 0.708875, acc.: 46.88%] [G loss: 0.876566]\n",
      "2409 [D loss: 0.558353, acc.: 75.00%] [G loss: 0.847054]\n",
      "2410 [D loss: 0.616892, acc.: 65.62%] [G loss: 0.877999]\n",
      "2411 [D loss: 0.633207, acc.: 65.62%] [G loss: 0.928090]\n",
      "2412 [D loss: 0.673861, acc.: 68.75%] [G loss: 0.879798]\n",
      "2413 [D loss: 0.607581, acc.: 62.50%] [G loss: 0.887537]\n",
      "2414 [D loss: 0.649022, acc.: 62.50%] [G loss: 0.917693]\n",
      "2415 [D loss: 0.613217, acc.: 71.88%] [G loss: 0.855872]\n",
      "2416 [D loss: 0.557313, acc.: 81.25%] [G loss: 0.881120]\n",
      "2417 [D loss: 0.602431, acc.: 65.62%] [G loss: 0.833325]\n",
      "2418 [D loss: 0.634699, acc.: 62.50%] [G loss: 0.860585]\n",
      "2419 [D loss: 0.620458, acc.: 68.75%] [G loss: 0.924442]\n",
      "2420 [D loss: 0.643536, acc.: 59.38%] [G loss: 0.905406]\n",
      "2421 [D loss: 0.569096, acc.: 75.00%] [G loss: 0.919975]\n",
      "2422 [D loss: 0.607675, acc.: 53.12%] [G loss: 0.924797]\n",
      "2423 [D loss: 0.584242, acc.: 71.88%] [G loss: 0.936447]\n",
      "2424 [D loss: 0.581859, acc.: 62.50%] [G loss: 0.872710]\n",
      "2425 [D loss: 0.596226, acc.: 71.88%] [G loss: 0.886138]\n",
      "2426 [D loss: 0.853478, acc.: 28.12%] [G loss: 0.849630]\n",
      "2427 [D loss: 0.598819, acc.: 71.88%] [G loss: 0.875107]\n",
      "2428 [D loss: 0.663607, acc.: 56.25%] [G loss: 0.929156]\n",
      "2429 [D loss: 0.561451, acc.: 87.50%] [G loss: 0.955614]\n",
      "2430 [D loss: 0.652140, acc.: 56.25%] [G loss: 0.941549]\n",
      "2431 [D loss: 0.643354, acc.: 68.75%] [G loss: 0.896050]\n",
      "2432 [D loss: 0.636454, acc.: 59.38%] [G loss: 0.911208]\n",
      "2433 [D loss: 0.652178, acc.: 59.38%] [G loss: 0.943455]\n",
      "2434 [D loss: 0.635962, acc.: 59.38%] [G loss: 0.934287]\n",
      "2435 [D loss: 0.639906, acc.: 68.75%] [G loss: 0.940472]\n",
      "2436 [D loss: 0.717574, acc.: 56.25%] [G loss: 0.870352]\n",
      "2437 [D loss: 0.593807, acc.: 75.00%] [G loss: 0.900109]\n",
      "2438 [D loss: 0.652705, acc.: 59.38%] [G loss: 0.911327]\n",
      "2439 [D loss: 0.668769, acc.: 53.12%] [G loss: 0.801482]\n",
      "2440 [D loss: 0.632135, acc.: 59.38%] [G loss: 0.931580]\n",
      "2441 [D loss: 0.606111, acc.: 71.88%] [G loss: 0.871766]\n",
      "2442 [D loss: 0.570490, acc.: 78.12%] [G loss: 0.954465]\n",
      "2443 [D loss: 0.603086, acc.: 65.62%] [G loss: 0.898217]\n",
      "2444 [D loss: 0.683813, acc.: 53.12%] [G loss: 0.871236]\n",
      "2445 [D loss: 0.575227, acc.: 78.12%] [G loss: 0.933213]\n",
      "2446 [D loss: 0.647116, acc.: 56.25%] [G loss: 0.919129]\n",
      "2447 [D loss: 0.573050, acc.: 75.00%] [G loss: 0.930351]\n",
      "2448 [D loss: 0.602113, acc.: 59.38%] [G loss: 0.834953]\n",
      "2449 [D loss: 0.653183, acc.: 68.75%] [G loss: 0.895306]\n",
      "2450 [D loss: 0.635108, acc.: 62.50%] [G loss: 0.883039]\n",
      "2451 [D loss: 0.660268, acc.: 62.50%] [G loss: 0.941993]\n",
      "2452 [D loss: 0.605731, acc.: 65.62%] [G loss: 1.006094]\n",
      "2453 [D loss: 0.633635, acc.: 59.38%] [G loss: 0.932974]\n",
      "2454 [D loss: 0.668306, acc.: 53.12%] [G loss: 0.908463]\n",
      "2455 [D loss: 0.705549, acc.: 62.50%] [G loss: 1.015668]\n",
      "2456 [D loss: 0.637742, acc.: 59.38%] [G loss: 0.963899]\n",
      "2457 [D loss: 0.631021, acc.: 56.25%] [G loss: 0.937078]\n",
      "2458 [D loss: 0.641631, acc.: 50.00%] [G loss: 0.922105]\n",
      "2459 [D loss: 0.695755, acc.: 56.25%] [G loss: 0.905207]\n",
      "2460 [D loss: 0.676554, acc.: 56.25%] [G loss: 0.899103]\n",
      "2461 [D loss: 0.610587, acc.: 71.88%] [G loss: 0.964899]\n",
      "2462 [D loss: 0.553030, acc.: 75.00%] [G loss: 0.910642]\n",
      "2463 [D loss: 0.572607, acc.: 71.88%] [G loss: 1.027825]\n",
      "2464 [D loss: 0.645566, acc.: 50.00%] [G loss: 0.913170]\n",
      "2465 [D loss: 0.638987, acc.: 59.38%] [G loss: 0.959570]\n",
      "2466 [D loss: 0.603765, acc.: 68.75%] [G loss: 0.903135]\n",
      "2467 [D loss: 0.680472, acc.: 50.00%] [G loss: 0.859595]\n",
      "2468 [D loss: 0.698169, acc.: 43.75%] [G loss: 0.892912]\n",
      "2469 [D loss: 0.660367, acc.: 59.38%] [G loss: 0.893161]\n",
      "2470 [D loss: 0.665532, acc.: 53.12%] [G loss: 0.986600]\n",
      "2471 [D loss: 0.656424, acc.: 62.50%] [G loss: 0.890536]\n",
      "2472 [D loss: 0.566139, acc.: 81.25%] [G loss: 0.972486]\n",
      "2473 [D loss: 0.664782, acc.: 68.75%] [G loss: 0.898986]\n",
      "2474 [D loss: 0.647489, acc.: 65.62%] [G loss: 0.884743]\n",
      "2475 [D loss: 0.576931, acc.: 71.88%] [G loss: 0.884436]\n",
      "2476 [D loss: 0.588446, acc.: 71.88%] [G loss: 0.927201]\n",
      "2477 [D loss: 0.633156, acc.: 62.50%] [G loss: 0.852886]\n",
      "2478 [D loss: 0.666096, acc.: 62.50%] [G loss: 0.846170]\n",
      "2479 [D loss: 0.619358, acc.: 65.62%] [G loss: 0.877982]\n",
      "2480 [D loss: 0.663487, acc.: 59.38%] [G loss: 0.826419]\n",
      "2481 [D loss: 0.655284, acc.: 65.62%] [G loss: 0.892908]\n",
      "2482 [D loss: 0.628010, acc.: 71.88%] [G loss: 0.879161]\n",
      "2483 [D loss: 0.588571, acc.: 68.75%] [G loss: 0.995384]\n",
      "2484 [D loss: 0.653174, acc.: 62.50%] [G loss: 0.991813]\n",
      "2485 [D loss: 0.692192, acc.: 50.00%] [G loss: 0.911177]\n",
      "2486 [D loss: 0.530325, acc.: 75.00%] [G loss: 0.949152]\n",
      "2487 [D loss: 0.587924, acc.: 78.12%] [G loss: 0.929532]\n",
      "2488 [D loss: 0.671882, acc.: 56.25%] [G loss: 0.860702]\n",
      "2489 [D loss: 0.625088, acc.: 62.50%] [G loss: 0.907372]\n",
      "2490 [D loss: 0.601974, acc.: 68.75%] [G loss: 0.916854]\n",
      "2491 [D loss: 0.650932, acc.: 56.25%] [G loss: 0.924481]\n",
      "2492 [D loss: 0.624608, acc.: 75.00%] [G loss: 0.979587]\n",
      "2493 [D loss: 0.604000, acc.: 65.62%] [G loss: 0.868977]\n",
      "2494 [D loss: 0.596011, acc.: 68.75%] [G loss: 0.909824]\n",
      "2495 [D loss: 0.594894, acc.: 68.75%] [G loss: 0.898364]\n",
      "2496 [D loss: 0.648087, acc.: 56.25%] [G loss: 0.873865]\n",
      "2497 [D loss: 0.654021, acc.: 56.25%] [G loss: 0.921467]\n",
      "2498 [D loss: 0.541623, acc.: 71.88%] [G loss: 0.982132]\n",
      "2499 [D loss: 0.687629, acc.: 62.50%] [G loss: 0.877408]\n",
      "2500 [D loss: 0.583627, acc.: 75.00%] [G loss: 0.902629]\n",
      "2501 [D loss: 0.676316, acc.: 59.38%] [G loss: 0.935616]\n",
      "2502 [D loss: 0.620340, acc.: 59.38%] [G loss: 0.946920]\n",
      "2503 [D loss: 0.617776, acc.: 68.75%] [G loss: 0.944873]\n",
      "2504 [D loss: 0.619142, acc.: 75.00%] [G loss: 0.920216]\n",
      "2505 [D loss: 0.533910, acc.: 81.25%] [G loss: 0.899158]\n",
      "2506 [D loss: 0.634433, acc.: 68.75%] [G loss: 0.907305]\n",
      "2507 [D loss: 0.676473, acc.: 59.38%] [G loss: 0.932987]\n",
      "2508 [D loss: 0.640315, acc.: 53.12%] [G loss: 0.944051]\n",
      "2509 [D loss: 0.579946, acc.: 75.00%] [G loss: 0.977216]\n",
      "2510 [D loss: 0.605714, acc.: 68.75%] [G loss: 0.837689]\n",
      "2511 [D loss: 0.587320, acc.: 71.88%] [G loss: 0.940367]\n",
      "2512 [D loss: 0.629093, acc.: 65.62%] [G loss: 0.927940]\n",
      "2513 [D loss: 0.663941, acc.: 46.88%] [G loss: 0.921378]\n",
      "2514 [D loss: 0.592433, acc.: 68.75%] [G loss: 0.966484]\n",
      "2515 [D loss: 0.591964, acc.: 65.62%] [G loss: 0.929861]\n",
      "2516 [D loss: 0.600723, acc.: 68.75%] [G loss: 0.903693]\n",
      "2517 [D loss: 0.544451, acc.: 75.00%] [G loss: 0.927831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518 [D loss: 0.595537, acc.: 65.62%] [G loss: 0.907844]\n",
      "2519 [D loss: 0.661734, acc.: 53.12%] [G loss: 0.847878]\n",
      "2520 [D loss: 0.585541, acc.: 65.62%] [G loss: 0.875866]\n",
      "2521 [D loss: 0.665950, acc.: 62.50%] [G loss: 0.866771]\n",
      "2522 [D loss: 0.632018, acc.: 65.62%] [G loss: 0.859649]\n",
      "2523 [D loss: 0.630829, acc.: 62.50%] [G loss: 0.885716]\n",
      "2524 [D loss: 0.641610, acc.: 59.38%] [G loss: 0.975556]\n",
      "2525 [D loss: 0.662536, acc.: 53.12%] [G loss: 0.896569]\n",
      "2526 [D loss: 0.667402, acc.: 68.75%] [G loss: 0.879824]\n",
      "2527 [D loss: 0.597508, acc.: 78.12%] [G loss: 0.888207]\n",
      "2528 [D loss: 0.643063, acc.: 62.50%] [G loss: 0.889867]\n",
      "2529 [D loss: 0.674436, acc.: 62.50%] [G loss: 0.847686]\n",
      "2530 [D loss: 0.637042, acc.: 62.50%] [G loss: 0.920706]\n",
      "2531 [D loss: 0.576627, acc.: 75.00%] [G loss: 0.887016]\n",
      "2532 [D loss: 0.549050, acc.: 68.75%] [G loss: 0.913895]\n",
      "2533 [D loss: 0.624322, acc.: 59.38%] [G loss: 0.877469]\n",
      "2534 [D loss: 0.646203, acc.: 62.50%] [G loss: 0.883294]\n",
      "2535 [D loss: 0.626097, acc.: 65.62%] [G loss: 0.883274]\n",
      "2536 [D loss: 0.682697, acc.: 50.00%] [G loss: 0.867393]\n",
      "2537 [D loss: 0.606123, acc.: 71.88%] [G loss: 0.985041]\n",
      "2538 [D loss: 0.661853, acc.: 56.25%] [G loss: 0.923608]\n",
      "2539 [D loss: 0.632895, acc.: 56.25%] [G loss: 0.968708]\n",
      "2540 [D loss: 0.544899, acc.: 75.00%] [G loss: 0.900195]\n",
      "2541 [D loss: 0.634210, acc.: 59.38%] [G loss: 1.011219]\n",
      "2542 [D loss: 0.631704, acc.: 59.38%] [G loss: 0.856165]\n",
      "2543 [D loss: 0.617506, acc.: 65.62%] [G loss: 0.885503]\n",
      "2544 [D loss: 0.589870, acc.: 65.62%] [G loss: 0.812088]\n",
      "2545 [D loss: 0.642766, acc.: 62.50%] [G loss: 0.925028]\n",
      "2546 [D loss: 0.662662, acc.: 65.62%] [G loss: 0.832991]\n",
      "2547 [D loss: 0.588930, acc.: 75.00%] [G loss: 0.900195]\n",
      "2548 [D loss: 0.721052, acc.: 53.12%] [G loss: 0.825081]\n",
      "2549 [D loss: 0.571813, acc.: 75.00%] [G loss: 0.901844]\n",
      "2550 [D loss: 0.637942, acc.: 62.50%] [G loss: 0.920162]\n",
      "2551 [D loss: 0.643358, acc.: 65.62%] [G loss: 0.893505]\n",
      "2552 [D loss: 0.538174, acc.: 81.25%] [G loss: 0.885086]\n",
      "2553 [D loss: 0.676960, acc.: 56.25%] [G loss: 0.819445]\n",
      "2554 [D loss: 0.660870, acc.: 65.62%] [G loss: 0.838522]\n",
      "2555 [D loss: 0.639872, acc.: 59.38%] [G loss: 0.883135]\n",
      "2556 [D loss: 0.544262, acc.: 78.12%] [G loss: 0.963826]\n",
      "2557 [D loss: 0.631074, acc.: 68.75%] [G loss: 0.943845]\n",
      "2558 [D loss: 0.552110, acc.: 68.75%] [G loss: 0.899606]\n",
      "2559 [D loss: 0.550372, acc.: 71.88%] [G loss: 0.907841]\n",
      "2560 [D loss: 0.590079, acc.: 68.75%] [G loss: 0.993675]\n",
      "2561 [D loss: 0.623652, acc.: 68.75%] [G loss: 0.939841]\n",
      "2562 [D loss: 0.677049, acc.: 53.12%] [G loss: 0.957380]\n",
      "2563 [D loss: 0.610879, acc.: 65.62%] [G loss: 0.980370]\n",
      "2564 [D loss: 0.661858, acc.: 62.50%] [G loss: 0.917863]\n",
      "2565 [D loss: 0.640181, acc.: 59.38%] [G loss: 0.953464]\n",
      "2566 [D loss: 0.573026, acc.: 71.88%] [G loss: 0.910856]\n",
      "2567 [D loss: 0.589608, acc.: 71.88%] [G loss: 0.970652]\n",
      "2568 [D loss: 0.599185, acc.: 71.88%] [G loss: 0.938060]\n",
      "2569 [D loss: 0.577129, acc.: 71.88%] [G loss: 1.003962]\n",
      "2570 [D loss: 0.676690, acc.: 62.50%] [G loss: 0.829823]\n",
      "2571 [D loss: 0.711390, acc.: 43.75%] [G loss: 0.850919]\n",
      "2572 [D loss: 0.576656, acc.: 71.88%] [G loss: 0.924791]\n",
      "2573 [D loss: 0.711129, acc.: 56.25%] [G loss: 0.906650]\n",
      "2574 [D loss: 0.632510, acc.: 71.88%] [G loss: 0.930897]\n",
      "2575 [D loss: 0.522936, acc.: 81.25%] [G loss: 0.919174]\n",
      "2576 [D loss: 0.673902, acc.: 59.38%] [G loss: 0.916065]\n",
      "2577 [D loss: 0.620568, acc.: 65.62%] [G loss: 0.880084]\n",
      "2578 [D loss: 0.590947, acc.: 68.75%] [G loss: 0.852540]\n",
      "2579 [D loss: 0.704144, acc.: 53.12%] [G loss: 0.865871]\n",
      "2580 [D loss: 0.690903, acc.: 53.12%] [G loss: 0.947933]\n",
      "2581 [D loss: 0.490062, acc.: 84.38%] [G loss: 0.923044]\n",
      "2582 [D loss: 0.646434, acc.: 62.50%] [G loss: 0.885830]\n",
      "2583 [D loss: 0.625983, acc.: 65.62%] [G loss: 0.807536]\n",
      "2584 [D loss: 0.681157, acc.: 62.50%] [G loss: 0.932679]\n",
      "2585 [D loss: 0.637369, acc.: 59.38%] [G loss: 0.902736]\n",
      "2586 [D loss: 0.671670, acc.: 59.38%] [G loss: 0.905199]\n",
      "2587 [D loss: 0.539615, acc.: 81.25%] [G loss: 0.921833]\n",
      "2588 [D loss: 0.578560, acc.: 71.88%] [G loss: 0.862093]\n",
      "2589 [D loss: 0.640956, acc.: 62.50%] [G loss: 0.915727]\n",
      "2590 [D loss: 0.589107, acc.: 75.00%] [G loss: 0.939040]\n",
      "2591 [D loss: 0.626722, acc.: 65.62%] [G loss: 0.845042]\n",
      "2592 [D loss: 0.567614, acc.: 68.75%] [G loss: 0.888811]\n",
      "2593 [D loss: 0.591824, acc.: 65.62%] [G loss: 0.930187]\n",
      "2594 [D loss: 0.608468, acc.: 68.75%] [G loss: 0.934284]\n",
      "2595 [D loss: 0.579269, acc.: 75.00%] [G loss: 0.959341]\n",
      "2596 [D loss: 0.590686, acc.: 75.00%] [G loss: 0.944101]\n",
      "2597 [D loss: 0.574211, acc.: 78.12%] [G loss: 1.001270]\n",
      "2598 [D loss: 0.677552, acc.: 65.62%] [G loss: 0.890437]\n",
      "2599 [D loss: 0.597834, acc.: 71.88%] [G loss: 0.869600]\n",
      "2600 [D loss: 0.600413, acc.: 68.75%] [G loss: 0.923508]\n",
      "2601 [D loss: 0.608712, acc.: 71.88%] [G loss: 0.925474]\n",
      "2602 [D loss: 0.616504, acc.: 68.75%] [G loss: 0.833034]\n",
      "2603 [D loss: 0.639363, acc.: 59.38%] [G loss: 0.899815]\n",
      "2604 [D loss: 0.551408, acc.: 87.50%] [G loss: 1.046415]\n",
      "2605 [D loss: 0.685361, acc.: 68.75%] [G loss: 0.958238]\n",
      "2606 [D loss: 0.615574, acc.: 71.88%] [G loss: 0.945288]\n",
      "2607 [D loss: 0.599662, acc.: 56.25%] [G loss: 0.926015]\n",
      "2608 [D loss: 0.660346, acc.: 62.50%] [G loss: 0.959562]\n",
      "2609 [D loss: 0.650053, acc.: 59.38%] [G loss: 0.895990]\n",
      "2610 [D loss: 0.603260, acc.: 59.38%] [G loss: 0.979664]\n",
      "2611 [D loss: 0.629004, acc.: 68.75%] [G loss: 0.921859]\n",
      "2612 [D loss: 0.535370, acc.: 75.00%] [G loss: 0.943451]\n",
      "2613 [D loss: 0.643681, acc.: 59.38%] [G loss: 0.935483]\n",
      "2614 [D loss: 0.639083, acc.: 59.38%] [G loss: 0.844782]\n",
      "2615 [D loss: 0.620416, acc.: 65.62%] [G loss: 0.819486]\n",
      "2616 [D loss: 0.736469, acc.: 50.00%] [G loss: 0.858392]\n",
      "2617 [D loss: 0.602798, acc.: 65.62%] [G loss: 0.924146]\n",
      "2618 [D loss: 0.733037, acc.: 46.88%] [G loss: 0.829620]\n",
      "2619 [D loss: 0.626495, acc.: 68.75%] [G loss: 0.859872]\n",
      "2620 [D loss: 0.655366, acc.: 56.25%] [G loss: 0.837669]\n",
      "2621 [D loss: 0.622142, acc.: 71.88%] [G loss: 0.900213]\n",
      "2622 [D loss: 0.612078, acc.: 68.75%] [G loss: 0.909443]\n",
      "2623 [D loss: 0.602884, acc.: 68.75%] [G loss: 0.885646]\n",
      "2624 [D loss: 0.711659, acc.: 59.38%] [G loss: 0.847815]\n",
      "2625 [D loss: 0.574835, acc.: 71.88%] [G loss: 0.922468]\n",
      "2626 [D loss: 0.702913, acc.: 50.00%] [G loss: 0.843194]\n",
      "2627 [D loss: 0.689206, acc.: 56.25%] [G loss: 0.902345]\n",
      "2628 [D loss: 0.558650, acc.: 71.88%] [G loss: 0.825755]\n",
      "2629 [D loss: 0.632167, acc.: 65.62%] [G loss: 0.824042]\n",
      "2630 [D loss: 0.614506, acc.: 71.88%] [G loss: 0.851465]\n",
      "2631 [D loss: 0.642389, acc.: 62.50%] [G loss: 0.865886]\n",
      "2632 [D loss: 0.647836, acc.: 56.25%] [G loss: 0.841607]\n",
      "2633 [D loss: 0.645421, acc.: 62.50%] [G loss: 1.020048]\n",
      "2634 [D loss: 0.656645, acc.: 71.88%] [G loss: 0.880789]\n",
      "2635 [D loss: 0.600330, acc.: 78.12%] [G loss: 0.898023]\n",
      "2636 [D loss: 0.676389, acc.: 56.25%] [G loss: 0.871876]\n",
      "2637 [D loss: 0.604721, acc.: 65.62%] [G loss: 0.859369]\n",
      "2638 [D loss: 0.687278, acc.: 56.25%] [G loss: 0.828074]\n",
      "2639 [D loss: 0.603352, acc.: 62.50%] [G loss: 0.898300]\n",
      "2640 [D loss: 0.570860, acc.: 68.75%] [G loss: 1.030869]\n",
      "2641 [D loss: 0.575094, acc.: 78.12%] [G loss: 0.998079]\n",
      "2642 [D loss: 0.692936, acc.: 65.62%] [G loss: 0.937517]\n",
      "2643 [D loss: 0.668813, acc.: 59.38%] [G loss: 0.973173]\n",
      "2644 [D loss: 0.724526, acc.: 53.12%] [G loss: 0.936586]\n",
      "2645 [D loss: 0.552988, acc.: 68.75%] [G loss: 0.912544]\n",
      "2646 [D loss: 0.662900, acc.: 56.25%] [G loss: 0.791143]\n",
      "2647 [D loss: 0.664684, acc.: 50.00%] [G loss: 0.869467]\n",
      "2648 [D loss: 0.694360, acc.: 56.25%] [G loss: 0.848172]\n",
      "2649 [D loss: 0.598098, acc.: 68.75%] [G loss: 0.897586]\n",
      "2650 [D loss: 0.595244, acc.: 71.88%] [G loss: 0.883774]\n",
      "2651 [D loss: 0.672079, acc.: 59.38%] [G loss: 0.889182]\n",
      "2652 [D loss: 0.549491, acc.: 68.75%] [G loss: 0.896313]\n",
      "2653 [D loss: 0.671478, acc.: 59.38%] [G loss: 0.905849]\n",
      "2654 [D loss: 0.625919, acc.: 65.62%] [G loss: 0.891836]\n",
      "2655 [D loss: 0.588927, acc.: 75.00%] [G loss: 0.877000]\n",
      "2656 [D loss: 0.602595, acc.: 81.25%] [G loss: 0.874842]\n",
      "2657 [D loss: 0.660083, acc.: 59.38%] [G loss: 0.837738]\n",
      "2658 [D loss: 0.614405, acc.: 68.75%] [G loss: 0.881738]\n",
      "2659 [D loss: 0.573677, acc.: 71.88%] [G loss: 0.830497]\n",
      "2660 [D loss: 0.623614, acc.: 71.88%] [G loss: 0.897609]\n",
      "2661 [D loss: 0.686782, acc.: 56.25%] [G loss: 0.880166]\n",
      "2662 [D loss: 0.647366, acc.: 65.62%] [G loss: 0.829772]\n",
      "2663 [D loss: 0.682040, acc.: 56.25%] [G loss: 0.886189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2664 [D loss: 0.597034, acc.: 68.75%] [G loss: 0.859065]\n",
      "2665 [D loss: 0.650945, acc.: 56.25%] [G loss: 0.904519]\n",
      "2666 [D loss: 0.547655, acc.: 78.12%] [G loss: 1.013736]\n",
      "2667 [D loss: 0.612726, acc.: 59.38%] [G loss: 0.993373]\n",
      "2668 [D loss: 0.626565, acc.: 56.25%] [G loss: 0.877640]\n",
      "2669 [D loss: 0.629745, acc.: 59.38%] [G loss: 0.840606]\n",
      "2670 [D loss: 0.624989, acc.: 65.62%] [G loss: 0.821482]\n",
      "2671 [D loss: 0.688053, acc.: 56.25%] [G loss: 0.873698]\n",
      "2672 [D loss: 0.666322, acc.: 59.38%] [G loss: 0.887687]\n",
      "2673 [D loss: 0.710957, acc.: 56.25%] [G loss: 0.854757]\n",
      "2674 [D loss: 0.588949, acc.: 65.62%] [G loss: 0.907205]\n",
      "2675 [D loss: 0.595155, acc.: 81.25%] [G loss: 0.880713]\n",
      "2676 [D loss: 0.596223, acc.: 81.25%] [G loss: 0.943895]\n",
      "2677 [D loss: 0.675071, acc.: 56.25%] [G loss: 0.878033]\n",
      "2678 [D loss: 0.686406, acc.: 53.12%] [G loss: 0.859938]\n",
      "2679 [D loss: 0.617078, acc.: 56.25%] [G loss: 0.955237]\n",
      "2680 [D loss: 0.629077, acc.: 68.75%] [G loss: 0.961428]\n",
      "2681 [D loss: 0.608148, acc.: 68.75%] [G loss: 0.923465]\n",
      "2682 [D loss: 0.623415, acc.: 68.75%] [G loss: 0.905402]\n",
      "2683 [D loss: 0.618477, acc.: 56.25%] [G loss: 0.867080]\n",
      "2684 [D loss: 0.542615, acc.: 78.12%] [G loss: 0.888171]\n",
      "2685 [D loss: 0.545583, acc.: 75.00%] [G loss: 0.918113]\n",
      "2686 [D loss: 0.658874, acc.: 62.50%] [G loss: 0.919113]\n",
      "2687 [D loss: 0.607266, acc.: 65.62%] [G loss: 0.918127]\n",
      "2688 [D loss: 0.722756, acc.: 46.88%] [G loss: 0.880512]\n",
      "2689 [D loss: 0.613515, acc.: 71.88%] [G loss: 0.865478]\n",
      "2690 [D loss: 0.704057, acc.: 53.12%] [G loss: 0.879471]\n",
      "2691 [D loss: 0.589279, acc.: 71.88%] [G loss: 0.848200]\n",
      "2692 [D loss: 0.602342, acc.: 71.88%] [G loss: 0.894337]\n",
      "2693 [D loss: 0.595226, acc.: 68.75%] [G loss: 0.892910]\n",
      "2694 [D loss: 0.689749, acc.: 46.88%] [G loss: 0.868745]\n",
      "2695 [D loss: 0.566103, acc.: 71.88%] [G loss: 0.897559]\n",
      "2696 [D loss: 0.622977, acc.: 62.50%] [G loss: 0.888908]\n",
      "2697 [D loss: 0.562884, acc.: 78.12%] [G loss: 0.861729]\n",
      "2698 [D loss: 0.604919, acc.: 65.62%] [G loss: 0.892531]\n",
      "2699 [D loss: 0.557510, acc.: 78.12%] [G loss: 0.867112]\n",
      "2700 [D loss: 0.639459, acc.: 59.38%] [G loss: 1.000045]\n",
      "2701 [D loss: 0.628788, acc.: 59.38%] [G loss: 0.970897]\n",
      "2702 [D loss: 0.584603, acc.: 71.88%] [G loss: 0.969470]\n",
      "2703 [D loss: 0.595715, acc.: 65.62%] [G loss: 0.992606]\n",
      "2704 [D loss: 0.557579, acc.: 75.00%] [G loss: 0.929602]\n",
      "2705 [D loss: 0.622608, acc.: 65.62%] [G loss: 0.973694]\n",
      "2706 [D loss: 0.758972, acc.: 53.12%] [G loss: 0.931476]\n",
      "2707 [D loss: 0.604678, acc.: 68.75%] [G loss: 1.049096]\n",
      "2708 [D loss: 0.678046, acc.: 62.50%] [G loss: 0.956506]\n",
      "2709 [D loss: 0.576361, acc.: 71.88%] [G loss: 0.990768]\n",
      "2710 [D loss: 0.665006, acc.: 53.12%] [G loss: 0.974945]\n",
      "2711 [D loss: 0.588747, acc.: 71.88%] [G loss: 0.991881]\n",
      "2712 [D loss: 0.710325, acc.: 56.25%] [G loss: 0.894423]\n",
      "2713 [D loss: 0.552089, acc.: 78.12%] [G loss: 0.947349]\n",
      "2714 [D loss: 0.609040, acc.: 65.62%] [G loss: 1.031934]\n",
      "2715 [D loss: 0.641083, acc.: 71.88%] [G loss: 0.991062]\n",
      "2716 [D loss: 0.593233, acc.: 75.00%] [G loss: 0.903085]\n",
      "2717 [D loss: 0.660211, acc.: 68.75%] [G loss: 0.893177]\n",
      "2718 [D loss: 0.660242, acc.: 65.62%] [G loss: 0.872447]\n",
      "2719 [D loss: 0.572151, acc.: 75.00%] [G loss: 0.859323]\n",
      "2720 [D loss: 0.653406, acc.: 56.25%] [G loss: 0.896048]\n",
      "2721 [D loss: 0.597783, acc.: 71.88%] [G loss: 0.878550]\n",
      "2722 [D loss: 0.525556, acc.: 84.38%] [G loss: 0.865723]\n",
      "2723 [D loss: 0.657278, acc.: 59.38%] [G loss: 0.889724]\n",
      "2724 [D loss: 0.643604, acc.: 50.00%] [G loss: 0.903375]\n",
      "2725 [D loss: 0.651172, acc.: 68.75%] [G loss: 0.911857]\n",
      "2726 [D loss: 0.589780, acc.: 68.75%] [G loss: 0.953436]\n",
      "2727 [D loss: 0.554155, acc.: 78.12%] [G loss: 1.008707]\n",
      "2728 [D loss: 0.566737, acc.: 68.75%] [G loss: 0.968863]\n",
      "2729 [D loss: 0.627195, acc.: 71.88%] [G loss: 0.978975]\n",
      "2730 [D loss: 0.658342, acc.: 62.50%] [G loss: 0.922569]\n",
      "2731 [D loss: 0.640341, acc.: 56.25%] [G loss: 0.931434]\n",
      "2732 [D loss: 0.562614, acc.: 68.75%] [G loss: 0.967326]\n",
      "2733 [D loss: 0.535887, acc.: 78.12%] [G loss: 1.023421]\n",
      "2734 [D loss: 0.588373, acc.: 68.75%] [G loss: 0.984361]\n",
      "2735 [D loss: 0.552558, acc.: 75.00%] [G loss: 0.986138]\n",
      "2736 [D loss: 0.589455, acc.: 68.75%] [G loss: 0.858057]\n",
      "2737 [D loss: 0.560524, acc.: 68.75%] [G loss: 0.887610]\n",
      "2738 [D loss: 0.623338, acc.: 68.75%] [G loss: 0.840911]\n",
      "2739 [D loss: 0.508966, acc.: 71.88%] [G loss: 0.933285]\n",
      "2740 [D loss: 0.588847, acc.: 71.88%] [G loss: 0.953810]\n",
      "2741 [D loss: 0.710509, acc.: 59.38%] [G loss: 0.919560]\n",
      "2742 [D loss: 0.658996, acc.: 59.38%] [G loss: 0.868330]\n",
      "2743 [D loss: 0.569461, acc.: 71.88%] [G loss: 0.917167]\n",
      "2744 [D loss: 0.664991, acc.: 71.88%] [G loss: 0.909905]\n",
      "2745 [D loss: 0.622531, acc.: 65.62%] [G loss: 0.899395]\n",
      "2746 [D loss: 0.660418, acc.: 59.38%] [G loss: 0.976394]\n",
      "2747 [D loss: 0.621909, acc.: 65.62%] [G loss: 1.060236]\n",
      "2748 [D loss: 0.506526, acc.: 90.62%] [G loss: 1.066119]\n",
      "2749 [D loss: 0.657134, acc.: 56.25%] [G loss: 0.958139]\n",
      "2750 [D loss: 0.663445, acc.: 59.38%] [G loss: 0.955863]\n",
      "2751 [D loss: 0.615957, acc.: 56.25%] [G loss: 0.923816]\n",
      "2752 [D loss: 0.525386, acc.: 84.38%] [G loss: 0.997792]\n",
      "2753 [D loss: 0.572661, acc.: 78.12%] [G loss: 0.994915]\n",
      "2754 [D loss: 0.645470, acc.: 68.75%] [G loss: 0.887284]\n",
      "2755 [D loss: 0.603323, acc.: 68.75%] [G loss: 0.925762]\n",
      "2756 [D loss: 0.590893, acc.: 71.88%] [G loss: 0.895826]\n",
      "2757 [D loss: 0.547905, acc.: 68.75%] [G loss: 0.875111]\n",
      "2758 [D loss: 0.580407, acc.: 71.88%] [G loss: 0.909723]\n",
      "2759 [D loss: 0.556742, acc.: 71.88%] [G loss: 0.843472]\n",
      "2760 [D loss: 0.587342, acc.: 65.62%] [G loss: 0.872369]\n",
      "2761 [D loss: 0.645393, acc.: 59.38%] [G loss: 0.871632]\n",
      "2762 [D loss: 0.668158, acc.: 56.25%] [G loss: 0.890902]\n",
      "2763 [D loss: 0.572157, acc.: 78.12%] [G loss: 0.945504]\n",
      "2764 [D loss: 0.547501, acc.: 87.50%] [G loss: 0.894544]\n",
      "2765 [D loss: 0.550325, acc.: 75.00%] [G loss: 0.966959]\n",
      "2766 [D loss: 0.587455, acc.: 65.62%] [G loss: 1.060377]\n",
      "2767 [D loss: 0.673949, acc.: 50.00%] [G loss: 0.941126]\n",
      "2768 [D loss: 0.742590, acc.: 53.12%] [G loss: 0.948565]\n",
      "2769 [D loss: 0.667745, acc.: 56.25%] [G loss: 0.926546]\n",
      "2770 [D loss: 0.643808, acc.: 59.38%] [G loss: 0.944905]\n",
      "2771 [D loss: 0.601385, acc.: 68.75%] [G loss: 0.932323]\n",
      "2772 [D loss: 0.571324, acc.: 81.25%] [G loss: 0.911567]\n",
      "2773 [D loss: 0.700294, acc.: 65.62%] [G loss: 0.907944]\n",
      "2774 [D loss: 0.590944, acc.: 71.88%] [G loss: 0.940229]\n",
      "2775 [D loss: 0.603841, acc.: 71.88%] [G loss: 0.888039]\n",
      "2776 [D loss: 0.543132, acc.: 75.00%] [G loss: 0.932703]\n",
      "2777 [D loss: 0.620071, acc.: 65.62%] [G loss: 0.930058]\n",
      "2778 [D loss: 0.621102, acc.: 65.62%] [G loss: 0.917738]\n",
      "2779 [D loss: 0.549046, acc.: 78.12%] [G loss: 0.881045]\n",
      "2780 [D loss: 0.545106, acc.: 71.88%] [G loss: 0.985032]\n",
      "2781 [D loss: 0.598281, acc.: 68.75%] [G loss: 0.903232]\n",
      "2782 [D loss: 0.598347, acc.: 59.38%] [G loss: 0.943391]\n",
      "2783 [D loss: 0.593660, acc.: 78.12%] [G loss: 0.829842]\n",
      "2784 [D loss: 0.670238, acc.: 62.50%] [G loss: 0.980226]\n",
      "2785 [D loss: 0.596798, acc.: 71.88%] [G loss: 1.085815]\n",
      "2786 [D loss: 0.700508, acc.: 50.00%] [G loss: 1.084615]\n",
      "2787 [D loss: 0.707250, acc.: 53.12%] [G loss: 1.014646]\n",
      "2788 [D loss: 0.837409, acc.: 40.62%] [G loss: 0.836151]\n",
      "2789 [D loss: 0.624230, acc.: 56.25%] [G loss: 0.895694]\n",
      "2790 [D loss: 0.612268, acc.: 65.62%] [G loss: 0.889099]\n",
      "2791 [D loss: 0.652053, acc.: 56.25%] [G loss: 0.872538]\n",
      "2792 [D loss: 0.540851, acc.: 71.88%] [G loss: 0.980470]\n",
      "2793 [D loss: 0.606304, acc.: 62.50%] [G loss: 0.918693]\n",
      "2794 [D loss: 0.716313, acc.: 68.75%] [G loss: 0.893428]\n",
      "2795 [D loss: 0.547655, acc.: 62.50%] [G loss: 0.959477]\n",
      "2796 [D loss: 0.738762, acc.: 53.12%] [G loss: 0.977265]\n",
      "2797 [D loss: 0.577196, acc.: 68.75%] [G loss: 0.922687]\n",
      "2798 [D loss: 0.588137, acc.: 68.75%] [G loss: 0.959880]\n",
      "2799 [D loss: 0.659476, acc.: 62.50%] [G loss: 0.877239]\n",
      "2800 [D loss: 0.678817, acc.: 50.00%] [G loss: 0.868109]\n",
      "2801 [D loss: 0.650174, acc.: 46.88%] [G loss: 0.930632]\n",
      "2802 [D loss: 0.661317, acc.: 62.50%] [G loss: 0.915506]\n",
      "2803 [D loss: 0.697959, acc.: 59.38%] [G loss: 1.025628]\n",
      "2804 [D loss: 0.549571, acc.: 68.75%] [G loss: 1.122023]\n",
      "2805 [D loss: 0.622675, acc.: 68.75%] [G loss: 0.933211]\n",
      "2806 [D loss: 0.599884, acc.: 78.12%] [G loss: 0.824237]\n",
      "2807 [D loss: 0.557414, acc.: 59.38%] [G loss: 0.894608]\n",
      "2808 [D loss: 0.651559, acc.: 68.75%] [G loss: 0.879148]\n",
      "2809 [D loss: 0.640055, acc.: 53.12%] [G loss: 0.867914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2810 [D loss: 0.702547, acc.: 43.75%] [G loss: 0.877789]\n",
      "2811 [D loss: 0.558782, acc.: 75.00%] [G loss: 0.986653]\n",
      "2812 [D loss: 0.625730, acc.: 68.75%] [G loss: 0.910650]\n",
      "2813 [D loss: 0.645617, acc.: 62.50%] [G loss: 0.905778]\n",
      "2814 [D loss: 0.568965, acc.: 68.75%] [G loss: 0.832354]\n",
      "2815 [D loss: 0.635385, acc.: 62.50%] [G loss: 0.841865]\n",
      "2816 [D loss: 0.608237, acc.: 71.88%] [G loss: 0.845917]\n",
      "2817 [D loss: 0.762347, acc.: 46.88%] [G loss: 0.907431]\n",
      "2818 [D loss: 0.591847, acc.: 71.88%] [G loss: 0.869910]\n",
      "2819 [D loss: 0.565363, acc.: 71.88%] [G loss: 0.862088]\n",
      "2820 [D loss: 0.640890, acc.: 68.75%] [G loss: 0.953002]\n",
      "2821 [D loss: 0.695135, acc.: 43.75%] [G loss: 0.837663]\n",
      "2822 [D loss: 0.592689, acc.: 71.88%] [G loss: 0.864092]\n",
      "2823 [D loss: 0.572324, acc.: 78.12%] [G loss: 0.904207]\n",
      "2824 [D loss: 0.709591, acc.: 46.88%] [G loss: 0.855234]\n",
      "2825 [D loss: 0.652691, acc.: 68.75%] [G loss: 0.891176]\n",
      "2826 [D loss: 0.585524, acc.: 65.62%] [G loss: 0.837102]\n",
      "2827 [D loss: 0.640376, acc.: 59.38%] [G loss: 0.833897]\n",
      "2828 [D loss: 0.561893, acc.: 78.12%] [G loss: 0.930205]\n",
      "2829 [D loss: 0.567664, acc.: 81.25%] [G loss: 0.886182]\n",
      "2830 [D loss: 0.584590, acc.: 65.62%] [G loss: 0.955817]\n",
      "2831 [D loss: 0.564324, acc.: 78.12%] [G loss: 0.941350]\n",
      "2832 [D loss: 0.641408, acc.: 59.38%] [G loss: 0.965134]\n",
      "2833 [D loss: 0.722556, acc.: 46.88%] [G loss: 0.924085]\n",
      "2834 [D loss: 0.619750, acc.: 62.50%] [G loss: 0.987641]\n",
      "2835 [D loss: 0.654882, acc.: 59.38%] [G loss: 0.966137]\n",
      "2836 [D loss: 0.651688, acc.: 65.62%] [G loss: 0.899795]\n",
      "2837 [D loss: 0.648087, acc.: 62.50%] [G loss: 1.000060]\n",
      "2838 [D loss: 0.678672, acc.: 59.38%] [G loss: 1.001816]\n",
      "2839 [D loss: 0.638330, acc.: 68.75%] [G loss: 0.988277]\n",
      "2840 [D loss: 0.581803, acc.: 71.88%] [G loss: 0.939433]\n",
      "2841 [D loss: 0.664194, acc.: 46.88%] [G loss: 0.954303]\n",
      "2842 [D loss: 0.674203, acc.: 62.50%] [G loss: 0.997324]\n",
      "2843 [D loss: 0.654963, acc.: 71.88%] [G loss: 0.922948]\n",
      "2844 [D loss: 0.592041, acc.: 71.88%] [G loss: 0.927355]\n",
      "2845 [D loss: 0.691631, acc.: 65.62%] [G loss: 0.842988]\n",
      "2846 [D loss: 0.607290, acc.: 65.62%] [G loss: 0.825264]\n",
      "2847 [D loss: 0.550617, acc.: 68.75%] [G loss: 0.874771]\n",
      "2848 [D loss: 0.561981, acc.: 78.12%] [G loss: 0.887013]\n",
      "2849 [D loss: 0.567912, acc.: 68.75%] [G loss: 0.900019]\n",
      "2850 [D loss: 0.617163, acc.: 71.88%] [G loss: 0.956206]\n",
      "2851 [D loss: 0.551307, acc.: 71.88%] [G loss: 0.854583]\n",
      "2852 [D loss: 0.598175, acc.: 62.50%] [G loss: 0.854462]\n",
      "2853 [D loss: 0.670198, acc.: 56.25%] [G loss: 0.941768]\n",
      "2854 [D loss: 0.590926, acc.: 65.62%] [G loss: 0.848281]\n",
      "2855 [D loss: 0.608550, acc.: 68.75%] [G loss: 0.931575]\n",
      "2856 [D loss: 0.641520, acc.: 65.62%] [G loss: 0.926175]\n",
      "2857 [D loss: 0.603006, acc.: 68.75%] [G loss: 0.860385]\n",
      "2858 [D loss: 0.661402, acc.: 53.12%] [G loss: 0.897821]\n",
      "2859 [D loss: 0.678935, acc.: 53.12%] [G loss: 0.965269]\n",
      "2860 [D loss: 0.612959, acc.: 62.50%] [G loss: 0.946563]\n",
      "2861 [D loss: 0.707479, acc.: 59.38%] [G loss: 0.802080]\n",
      "2862 [D loss: 0.694340, acc.: 53.12%] [G loss: 0.847842]\n",
      "2863 [D loss: 0.631494, acc.: 65.62%] [G loss: 0.882330]\n",
      "2864 [D loss: 0.673143, acc.: 53.12%] [G loss: 0.882828]\n",
      "2865 [D loss: 0.740498, acc.: 50.00%] [G loss: 0.843644]\n",
      "2866 [D loss: 0.584266, acc.: 75.00%] [G loss: 0.975454]\n",
      "2867 [D loss: 0.626241, acc.: 71.88%] [G loss: 0.966555]\n",
      "2868 [D loss: 0.619163, acc.: 68.75%] [G loss: 0.904494]\n",
      "2869 [D loss: 0.616263, acc.: 62.50%] [G loss: 0.928532]\n",
      "2870 [D loss: 0.516954, acc.: 84.38%] [G loss: 0.899097]\n",
      "2871 [D loss: 0.667819, acc.: 62.50%] [G loss: 0.945347]\n",
      "2872 [D loss: 0.612076, acc.: 75.00%] [G loss: 0.932471]\n",
      "2873 [D loss: 0.652907, acc.: 59.38%] [G loss: 0.921475]\n",
      "2874 [D loss: 0.602991, acc.: 68.75%] [G loss: 0.874826]\n",
      "2875 [D loss: 0.710883, acc.: 53.12%] [G loss: 0.926557]\n",
      "2876 [D loss: 0.591950, acc.: 71.88%] [G loss: 0.904232]\n",
      "2877 [D loss: 0.600883, acc.: 59.38%] [G loss: 0.923302]\n",
      "2878 [D loss: 0.573427, acc.: 68.75%] [G loss: 0.863620]\n",
      "2879 [D loss: 0.653796, acc.: 56.25%] [G loss: 0.882771]\n",
      "2880 [D loss: 0.677214, acc.: 59.38%] [G loss: 0.831674]\n",
      "2881 [D loss: 0.616857, acc.: 68.75%] [G loss: 0.940615]\n",
      "2882 [D loss: 0.728594, acc.: 40.62%] [G loss: 0.960326]\n",
      "2883 [D loss: 0.638483, acc.: 62.50%] [G loss: 0.949574]\n",
      "2884 [D loss: 0.647108, acc.: 59.38%] [G loss: 0.940018]\n",
      "2885 [D loss: 0.630659, acc.: 68.75%] [G loss: 0.915076]\n",
      "2886 [D loss: 0.692976, acc.: 56.25%] [G loss: 0.886546]\n",
      "2887 [D loss: 0.595668, acc.: 71.88%] [G loss: 0.948161]\n",
      "2888 [D loss: 0.701722, acc.: 56.25%] [G loss: 0.894815]\n",
      "2889 [D loss: 0.652883, acc.: 62.50%] [G loss: 0.876486]\n",
      "2890 [D loss: 0.622210, acc.: 65.62%] [G loss: 0.918161]\n",
      "2891 [D loss: 0.683842, acc.: 46.88%] [G loss: 0.854207]\n",
      "2892 [D loss: 0.520385, acc.: 75.00%] [G loss: 0.875031]\n",
      "2893 [D loss: 0.549925, acc.: 78.12%] [G loss: 0.909348]\n",
      "2894 [D loss: 0.657659, acc.: 59.38%] [G loss: 0.860181]\n",
      "2895 [D loss: 0.573776, acc.: 71.88%] [G loss: 0.868683]\n",
      "2896 [D loss: 0.627655, acc.: 71.88%] [G loss: 0.912975]\n",
      "2897 [D loss: 0.610622, acc.: 68.75%] [G loss: 0.886278]\n",
      "2898 [D loss: 0.557567, acc.: 81.25%] [G loss: 0.920818]\n",
      "2899 [D loss: 0.607970, acc.: 62.50%] [G loss: 0.923834]\n",
      "2900 [D loss: 0.602952, acc.: 68.75%] [G loss: 0.892931]\n",
      "2901 [D loss: 0.641129, acc.: 65.62%] [G loss: 0.873132]\n",
      "2902 [D loss: 0.700565, acc.: 50.00%] [G loss: 0.840674]\n",
      "2903 [D loss: 0.737633, acc.: 50.00%] [G loss: 0.809868]\n",
      "2904 [D loss: 0.732904, acc.: 62.50%] [G loss: 0.884815]\n",
      "2905 [D loss: 0.640459, acc.: 65.62%] [G loss: 0.848510]\n",
      "2906 [D loss: 0.627755, acc.: 65.62%] [G loss: 0.856858]\n",
      "2907 [D loss: 0.588447, acc.: 78.12%] [G loss: 0.968879]\n",
      "2908 [D loss: 0.534282, acc.: 81.25%] [G loss: 0.910626]\n",
      "2909 [D loss: 0.679345, acc.: 56.25%] [G loss: 0.917331]\n",
      "2910 [D loss: 0.576140, acc.: 71.88%] [G loss: 0.930497]\n",
      "2911 [D loss: 0.663126, acc.: 62.50%] [G loss: 0.872387]\n",
      "2912 [D loss: 0.622576, acc.: 53.12%] [G loss: 0.892177]\n",
      "2913 [D loss: 0.667438, acc.: 59.38%] [G loss: 0.979547]\n",
      "2914 [D loss: 0.589118, acc.: 65.62%] [G loss: 0.973951]\n",
      "2915 [D loss: 0.679770, acc.: 59.38%] [G loss: 0.863290]\n",
      "2916 [D loss: 0.585240, acc.: 68.75%] [G loss: 0.846611]\n",
      "2917 [D loss: 0.638561, acc.: 65.62%] [G loss: 0.883873]\n",
      "2918 [D loss: 0.609403, acc.: 62.50%] [G loss: 0.928817]\n",
      "2919 [D loss: 0.594192, acc.: 71.88%] [G loss: 0.857915]\n",
      "2920 [D loss: 0.639579, acc.: 59.38%] [G loss: 0.911711]\n",
      "2921 [D loss: 0.636264, acc.: 59.38%] [G loss: 0.890091]\n",
      "2922 [D loss: 0.587523, acc.: 71.88%] [G loss: 0.905477]\n",
      "2923 [D loss: 0.597526, acc.: 68.75%] [G loss: 0.881568]\n",
      "2924 [D loss: 0.622116, acc.: 75.00%] [G loss: 0.951383]\n",
      "2925 [D loss: 0.583799, acc.: 62.50%] [G loss: 0.952011]\n",
      "2926 [D loss: 0.684909, acc.: 56.25%] [G loss: 0.959743]\n",
      "2927 [D loss: 0.620744, acc.: 65.62%] [G loss: 0.863918]\n",
      "2928 [D loss: 0.566828, acc.: 68.75%] [G loss: 1.033494]\n",
      "2929 [D loss: 0.684151, acc.: 56.25%] [G loss: 0.860374]\n",
      "2930 [D loss: 0.639219, acc.: 56.25%] [G loss: 0.814624]\n",
      "2931 [D loss: 0.623122, acc.: 53.12%] [G loss: 0.937720]\n",
      "2932 [D loss: 0.568990, acc.: 75.00%] [G loss: 0.927508]\n",
      "2933 [D loss: 0.613527, acc.: 65.62%] [G loss: 0.912198]\n",
      "2934 [D loss: 0.588670, acc.: 71.88%] [G loss: 0.953482]\n",
      "2935 [D loss: 0.686441, acc.: 68.75%] [G loss: 0.940068]\n",
      "2936 [D loss: 0.616814, acc.: 62.50%] [G loss: 0.916697]\n",
      "2937 [D loss: 0.619376, acc.: 59.38%] [G loss: 0.926398]\n",
      "2938 [D loss: 0.604617, acc.: 65.62%] [G loss: 0.930352]\n",
      "2939 [D loss: 0.621382, acc.: 65.62%] [G loss: 0.945854]\n",
      "2940 [D loss: 0.615280, acc.: 62.50%] [G loss: 0.893449]\n",
      "2941 [D loss: 0.610906, acc.: 65.62%] [G loss: 0.976553]\n",
      "2942 [D loss: 0.656836, acc.: 62.50%] [G loss: 0.918808]\n",
      "2943 [D loss: 0.728259, acc.: 50.00%] [G loss: 0.967987]\n",
      "2944 [D loss: 0.684735, acc.: 56.25%] [G loss: 0.897956]\n",
      "2945 [D loss: 0.617856, acc.: 62.50%] [G loss: 0.882364]\n",
      "2946 [D loss: 0.628827, acc.: 59.38%] [G loss: 0.841157]\n",
      "2947 [D loss: 0.601424, acc.: 68.75%] [G loss: 0.924617]\n",
      "2948 [D loss: 0.601822, acc.: 68.75%] [G loss: 0.836297]\n",
      "2949 [D loss: 0.598922, acc.: 65.62%] [G loss: 0.896315]\n",
      "2950 [D loss: 0.694236, acc.: 56.25%] [G loss: 0.931288]\n",
      "2951 [D loss: 0.617191, acc.: 68.75%] [G loss: 0.934501]\n",
      "2952 [D loss: 0.611615, acc.: 65.62%] [G loss: 0.901053]\n",
      "2953 [D loss: 0.673725, acc.: 50.00%] [G loss: 0.877199]\n",
      "2954 [D loss: 0.573211, acc.: 81.25%] [G loss: 0.964798]\n",
      "2955 [D loss: 0.604893, acc.: 59.38%] [G loss: 0.883369]\n",
      "2956 [D loss: 0.640640, acc.: 59.38%] [G loss: 0.849011]\n",
      "2957 [D loss: 0.568817, acc.: 75.00%] [G loss: 0.914200]\n",
      "2958 [D loss: 0.574614, acc.: 81.25%] [G loss: 0.856351]\n",
      "2959 [D loss: 0.586591, acc.: 65.62%] [G loss: 0.942817]\n",
      "2960 [D loss: 0.665754, acc.: 53.12%] [G loss: 0.916244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2961 [D loss: 0.517068, acc.: 81.25%] [G loss: 0.858410]\n",
      "2962 [D loss: 0.699135, acc.: 62.50%] [G loss: 0.789833]\n",
      "2963 [D loss: 0.648203, acc.: 53.12%] [G loss: 0.856984]\n",
      "2964 [D loss: 0.610125, acc.: 65.62%] [G loss: 0.883918]\n",
      "2965 [D loss: 0.600340, acc.: 68.75%] [G loss: 0.869594]\n",
      "2966 [D loss: 0.655970, acc.: 62.50%] [G loss: 0.954459]\n",
      "2967 [D loss: 0.547997, acc.: 71.88%] [G loss: 0.897354]\n",
      "2968 [D loss: 0.641716, acc.: 59.38%] [G loss: 0.923550]\n",
      "2969 [D loss: 0.684999, acc.: 53.12%] [G loss: 0.785826]\n",
      "2970 [D loss: 0.636080, acc.: 59.38%] [G loss: 0.816068]\n",
      "2971 [D loss: 0.713145, acc.: 50.00%] [G loss: 0.815067]\n",
      "2972 [D loss: 0.583922, acc.: 71.88%] [G loss: 0.888017]\n",
      "2973 [D loss: 0.630006, acc.: 56.25%] [G loss: 1.004663]\n",
      "2974 [D loss: 0.785996, acc.: 40.62%] [G loss: 0.896857]\n",
      "2975 [D loss: 0.621620, acc.: 68.75%] [G loss: 0.952830]\n",
      "2976 [D loss: 0.589351, acc.: 71.88%] [G loss: 0.988347]\n",
      "2977 [D loss: 0.684867, acc.: 56.25%] [G loss: 0.956341]\n",
      "2978 [D loss: 0.583722, acc.: 75.00%] [G loss: 1.019038]\n",
      "2979 [D loss: 0.610252, acc.: 71.88%] [G loss: 0.915732]\n",
      "2980 [D loss: 0.673033, acc.: 50.00%] [G loss: 0.895179]\n",
      "2981 [D loss: 0.645110, acc.: 71.88%] [G loss: 0.933429]\n",
      "2982 [D loss: 0.704887, acc.: 53.12%] [G loss: 0.885257]\n",
      "2983 [D loss: 0.642606, acc.: 62.50%] [G loss: 0.883949]\n",
      "2984 [D loss: 0.701757, acc.: 62.50%] [G loss: 0.875044]\n",
      "2985 [D loss: 0.516547, acc.: 71.88%] [G loss: 1.040950]\n",
      "2986 [D loss: 0.600975, acc.: 62.50%] [G loss: 1.061008]\n",
      "2987 [D loss: 0.605016, acc.: 65.62%] [G loss: 0.945247]\n",
      "2988 [D loss: 0.729392, acc.: 50.00%] [G loss: 0.968748]\n",
      "2989 [D loss: 0.643114, acc.: 71.88%] [G loss: 0.969018]\n",
      "2990 [D loss: 0.610511, acc.: 68.75%] [G loss: 0.934451]\n",
      "2991 [D loss: 0.631556, acc.: 62.50%] [G loss: 0.888195]\n",
      "2992 [D loss: 0.659976, acc.: 56.25%] [G loss: 0.940097]\n",
      "2993 [D loss: 0.601627, acc.: 62.50%] [G loss: 0.985335]\n",
      "2994 [D loss: 0.502733, acc.: 78.12%] [G loss: 0.971556]\n",
      "2995 [D loss: 0.631039, acc.: 50.00%] [G loss: 0.926188]\n",
      "2996 [D loss: 0.650997, acc.: 62.50%] [G loss: 0.867896]\n",
      "2997 [D loss: 0.667789, acc.: 59.38%] [G loss: 0.778262]\n",
      "2998 [D loss: 0.662589, acc.: 68.75%] [G loss: 0.870164]\n",
      "2999 [D loss: 0.555450, acc.: 71.88%] [G loss: 0.974547]\n",
      "3000 [D loss: 0.616408, acc.: 68.75%] [G loss: 1.000086]\n",
      "3001 [D loss: 0.688554, acc.: 56.25%] [G loss: 0.924447]\n",
      "3002 [D loss: 0.592972, acc.: 75.00%] [G loss: 0.870904]\n",
      "3003 [D loss: 0.622405, acc.: 62.50%] [G loss: 0.906497]\n",
      "3004 [D loss: 0.677564, acc.: 62.50%] [G loss: 0.891372]\n",
      "3005 [D loss: 0.672075, acc.: 56.25%] [G loss: 0.921720]\n",
      "3006 [D loss: 0.545055, acc.: 78.12%] [G loss: 0.804595]\n",
      "3007 [D loss: 0.608361, acc.: 65.62%] [G loss: 0.881262]\n",
      "3008 [D loss: 0.706021, acc.: 43.75%] [G loss: 0.758082]\n",
      "3009 [D loss: 0.613836, acc.: 53.12%] [G loss: 0.880461]\n",
      "3010 [D loss: 0.613141, acc.: 68.75%] [G loss: 0.860627]\n",
      "3011 [D loss: 0.599140, acc.: 62.50%] [G loss: 0.959233]\n",
      "3012 [D loss: 0.621513, acc.: 68.75%] [G loss: 0.951838]\n",
      "3013 [D loss: 0.626779, acc.: 68.75%] [G loss: 0.926468]\n",
      "3014 [D loss: 0.604771, acc.: 65.62%] [G loss: 0.879636]\n",
      "3015 [D loss: 0.657127, acc.: 56.25%] [G loss: 0.936723]\n",
      "3016 [D loss: 0.705689, acc.: 56.25%] [G loss: 0.996377]\n",
      "3017 [D loss: 0.679317, acc.: 46.88%] [G loss: 0.889157]\n",
      "3018 [D loss: 0.776243, acc.: 46.88%] [G loss: 0.863247]\n",
      "3019 [D loss: 0.631938, acc.: 62.50%] [G loss: 0.904321]\n",
      "3020 [D loss: 0.578733, acc.: 78.12%] [G loss: 0.913139]\n",
      "3021 [D loss: 0.612259, acc.: 56.25%] [G loss: 0.872626]\n",
      "3022 [D loss: 0.694483, acc.: 46.88%] [G loss: 0.859944]\n",
      "3023 [D loss: 0.647573, acc.: 62.50%] [G loss: 0.793549]\n",
      "3024 [D loss: 0.621676, acc.: 62.50%] [G loss: 0.856938]\n",
      "3025 [D loss: 0.685387, acc.: 53.12%] [G loss: 0.824170]\n",
      "3026 [D loss: 0.646546, acc.: 62.50%] [G loss: 0.858758]\n",
      "3027 [D loss: 0.638510, acc.: 75.00%] [G loss: 0.860317]\n",
      "3028 [D loss: 0.610974, acc.: 68.75%] [G loss: 0.899647]\n",
      "3029 [D loss: 0.622686, acc.: 68.75%] [G loss: 0.840967]\n",
      "3030 [D loss: 0.649423, acc.: 56.25%] [G loss: 0.842868]\n",
      "3031 [D loss: 0.627742, acc.: 59.38%] [G loss: 0.817692]\n",
      "3032 [D loss: 0.734016, acc.: 56.25%] [G loss: 0.880258]\n",
      "3033 [D loss: 0.643621, acc.: 71.88%] [G loss: 0.876541]\n",
      "3034 [D loss: 0.614683, acc.: 68.75%] [G loss: 0.874981]\n",
      "3035 [D loss: 0.655462, acc.: 59.38%] [G loss: 0.913689]\n",
      "3036 [D loss: 0.696760, acc.: 46.88%] [G loss: 0.826797]\n",
      "3037 [D loss: 0.660184, acc.: 56.25%] [G loss: 0.834976]\n",
      "3038 [D loss: 0.607249, acc.: 75.00%] [G loss: 0.886001]\n",
      "3039 [D loss: 0.611937, acc.: 68.75%] [G loss: 0.962310]\n",
      "3040 [D loss: 0.648622, acc.: 59.38%] [G loss: 0.944580]\n",
      "3041 [D loss: 0.602362, acc.: 75.00%] [G loss: 0.869695]\n",
      "3042 [D loss: 0.671106, acc.: 59.38%] [G loss: 0.967456]\n",
      "3043 [D loss: 0.670865, acc.: 59.38%] [G loss: 0.808875]\n",
      "3044 [D loss: 0.670574, acc.: 65.62%] [G loss: 0.929690]\n",
      "3045 [D loss: 0.538784, acc.: 84.38%] [G loss: 0.955176]\n",
      "3046 [D loss: 0.566880, acc.: 78.12%] [G loss: 0.986505]\n",
      "3047 [D loss: 0.566458, acc.: 68.75%] [G loss: 0.892671]\n",
      "3048 [D loss: 0.656429, acc.: 56.25%] [G loss: 0.976351]\n",
      "3049 [D loss: 0.679039, acc.: 50.00%] [G loss: 0.906317]\n",
      "3050 [D loss: 0.672452, acc.: 59.38%] [G loss: 0.926211]\n",
      "3051 [D loss: 0.651993, acc.: 62.50%] [G loss: 0.891311]\n",
      "3052 [D loss: 0.661050, acc.: 53.12%] [G loss: 0.963694]\n",
      "3053 [D loss: 0.616486, acc.: 59.38%] [G loss: 0.926752]\n",
      "3054 [D loss: 0.562848, acc.: 78.12%] [G loss: 0.896148]\n",
      "3055 [D loss: 0.694744, acc.: 46.88%] [G loss: 0.857193]\n",
      "3056 [D loss: 0.625824, acc.: 62.50%] [G loss: 0.937675]\n",
      "3057 [D loss: 0.631020, acc.: 59.38%] [G loss: 0.885015]\n",
      "3058 [D loss: 0.677629, acc.: 65.62%] [G loss: 0.883942]\n",
      "3059 [D loss: 0.672618, acc.: 62.50%] [G loss: 0.933622]\n",
      "3060 [D loss: 0.663024, acc.: 50.00%] [G loss: 0.952382]\n",
      "3061 [D loss: 0.680300, acc.: 53.12%] [G loss: 0.981694]\n",
      "3062 [D loss: 0.678738, acc.: 56.25%] [G loss: 0.890672]\n",
      "3063 [D loss: 0.589605, acc.: 75.00%] [G loss: 0.929868]\n",
      "3064 [D loss: 0.645766, acc.: 65.62%] [G loss: 0.960440]\n",
      "3065 [D loss: 0.664224, acc.: 65.62%] [G loss: 0.970414]\n",
      "3066 [D loss: 0.656635, acc.: 59.38%] [G loss: 0.902921]\n",
      "3067 [D loss: 0.687172, acc.: 59.38%] [G loss: 0.907937]\n",
      "3068 [D loss: 0.567915, acc.: 78.12%] [G loss: 0.933163]\n",
      "3069 [D loss: 0.806670, acc.: 37.50%] [G loss: 0.855334]\n",
      "3070 [D loss: 0.537838, acc.: 71.88%] [G loss: 0.924017]\n",
      "3071 [D loss: 0.627275, acc.: 65.62%] [G loss: 0.934815]\n",
      "3072 [D loss: 0.560637, acc.: 71.88%] [G loss: 0.924346]\n",
      "3073 [D loss: 0.592881, acc.: 65.62%] [G loss: 0.958103]\n",
      "3074 [D loss: 0.564790, acc.: 68.75%] [G loss: 0.880393]\n",
      "3075 [D loss: 0.655608, acc.: 59.38%] [G loss: 0.829597]\n",
      "3076 [D loss: 0.621410, acc.: 68.75%] [G loss: 0.842483]\n",
      "3077 [D loss: 0.614265, acc.: 65.62%] [G loss: 0.861821]\n",
      "3078 [D loss: 0.639514, acc.: 68.75%] [G loss: 0.887958]\n",
      "3079 [D loss: 0.607193, acc.: 65.62%] [G loss: 0.972559]\n",
      "3080 [D loss: 0.567081, acc.: 62.50%] [G loss: 0.963340]\n",
      "3081 [D loss: 0.671900, acc.: 50.00%] [G loss: 0.928240]\n",
      "3082 [D loss: 0.603716, acc.: 78.12%] [G loss: 0.876715]\n",
      "3083 [D loss: 0.507718, acc.: 78.12%] [G loss: 0.882010]\n",
      "3084 [D loss: 0.662706, acc.: 62.50%] [G loss: 0.877985]\n",
      "3085 [D loss: 0.656998, acc.: 65.62%] [G loss: 0.989531]\n",
      "3086 [D loss: 0.619908, acc.: 65.62%] [G loss: 0.944887]\n",
      "3087 [D loss: 0.559328, acc.: 68.75%] [G loss: 0.903326]\n",
      "3088 [D loss: 0.576506, acc.: 75.00%] [G loss: 0.917359]\n",
      "3089 [D loss: 0.701207, acc.: 46.88%] [G loss: 0.952533]\n",
      "3090 [D loss: 0.676777, acc.: 62.50%] [G loss: 0.860227]\n",
      "3091 [D loss: 0.743131, acc.: 50.00%] [G loss: 0.909945]\n",
      "3092 [D loss: 0.607019, acc.: 68.75%] [G loss: 0.897758]\n",
      "3093 [D loss: 0.628386, acc.: 71.88%] [G loss: 0.949806]\n",
      "3094 [D loss: 0.662535, acc.: 65.62%] [G loss: 0.910257]\n",
      "3095 [D loss: 0.702302, acc.: 56.25%] [G loss: 0.913124]\n",
      "3096 [D loss: 0.645923, acc.: 62.50%] [G loss: 0.912530]\n",
      "3097 [D loss: 0.664279, acc.: 65.62%] [G loss: 0.888862]\n",
      "3098 [D loss: 0.648124, acc.: 68.75%] [G loss: 0.825836]\n",
      "3099 [D loss: 0.596721, acc.: 75.00%] [G loss: 0.832710]\n",
      "3100 [D loss: 0.598459, acc.: 65.62%] [G loss: 0.796951]\n",
      "3101 [D loss: 0.625783, acc.: 59.38%] [G loss: 0.867228]\n",
      "3102 [D loss: 0.663143, acc.: 62.50%] [G loss: 0.885466]\n",
      "3103 [D loss: 0.657668, acc.: 53.12%] [G loss: 0.886981]\n",
      "3104 [D loss: 0.672912, acc.: 59.38%] [G loss: 0.849813]\n",
      "3105 [D loss: 0.520041, acc.: 81.25%] [G loss: 0.903736]\n",
      "3106 [D loss: 0.658844, acc.: 65.62%] [G loss: 0.796589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3107 [D loss: 0.681614, acc.: 53.12%] [G loss: 0.817867]\n",
      "3108 [D loss: 0.673375, acc.: 59.38%] [G loss: 0.880795]\n",
      "3109 [D loss: 0.655029, acc.: 53.12%] [G loss: 0.849276]\n",
      "3110 [D loss: 0.640638, acc.: 59.38%] [G loss: 0.916442]\n",
      "3111 [D loss: 0.621501, acc.: 65.62%] [G loss: 0.872500]\n",
      "3112 [D loss: 0.643687, acc.: 62.50%] [G loss: 0.854018]\n",
      "3113 [D loss: 0.637205, acc.: 62.50%] [G loss: 0.863801]\n",
      "3114 [D loss: 0.606370, acc.: 68.75%] [G loss: 0.881547]\n",
      "3115 [D loss: 0.677784, acc.: 59.38%] [G loss: 0.842774]\n",
      "3116 [D loss: 0.609399, acc.: 68.75%] [G loss: 0.910461]\n",
      "3117 [D loss: 0.633587, acc.: 68.75%] [G loss: 0.907873]\n",
      "3118 [D loss: 0.594394, acc.: 68.75%] [G loss: 0.995811]\n",
      "3119 [D loss: 0.578591, acc.: 81.25%] [G loss: 0.876378]\n",
      "3120 [D loss: 0.642251, acc.: 65.62%] [G loss: 0.919773]\n",
      "3121 [D loss: 0.705155, acc.: 56.25%] [G loss: 0.944405]\n",
      "3122 [D loss: 0.612147, acc.: 75.00%] [G loss: 0.849734]\n",
      "3123 [D loss: 0.649213, acc.: 65.62%] [G loss: 0.831574]\n",
      "3124 [D loss: 0.642802, acc.: 59.38%] [G loss: 0.771448]\n",
      "3125 [D loss: 0.651001, acc.: 65.62%] [G loss: 0.708713]\n",
      "3126 [D loss: 0.626838, acc.: 68.75%] [G loss: 0.802725]\n",
      "3127 [D loss: 0.709653, acc.: 37.50%] [G loss: 0.737110]\n",
      "3128 [D loss: 0.607843, acc.: 65.62%] [G loss: 0.796010]\n",
      "3129 [D loss: 0.715206, acc.: 62.50%] [G loss: 0.946590]\n",
      "3130 [D loss: 0.657091, acc.: 68.75%] [G loss: 0.924005]\n",
      "3131 [D loss: 0.570256, acc.: 78.12%] [G loss: 0.888893]\n",
      "3132 [D loss: 0.623443, acc.: 78.12%] [G loss: 0.947917]\n",
      "3133 [D loss: 0.679254, acc.: 62.50%] [G loss: 0.875239]\n",
      "3134 [D loss: 0.672631, acc.: 56.25%] [G loss: 0.919779]\n",
      "3135 [D loss: 0.631938, acc.: 65.62%] [G loss: 0.869951]\n",
      "3136 [D loss: 0.694382, acc.: 56.25%] [G loss: 0.808417]\n",
      "3137 [D loss: 0.666338, acc.: 65.62%] [G loss: 0.767677]\n",
      "3138 [D loss: 0.619131, acc.: 65.62%] [G loss: 0.903714]\n",
      "3139 [D loss: 0.575557, acc.: 62.50%] [G loss: 0.921723]\n",
      "3140 [D loss: 0.641086, acc.: 62.50%] [G loss: 0.923606]\n",
      "3141 [D loss: 0.695888, acc.: 62.50%] [G loss: 0.925863]\n",
      "3142 [D loss: 0.679494, acc.: 59.38%] [G loss: 0.923425]\n",
      "3143 [D loss: 0.555868, acc.: 71.88%] [G loss: 0.927141]\n",
      "3144 [D loss: 0.615124, acc.: 68.75%] [G loss: 0.894575]\n",
      "3145 [D loss: 0.733591, acc.: 56.25%] [G loss: 0.930778]\n",
      "3146 [D loss: 0.630014, acc.: 65.62%] [G loss: 0.804753]\n",
      "3147 [D loss: 0.627485, acc.: 71.88%] [G loss: 0.838864]\n",
      "3148 [D loss: 0.590341, acc.: 75.00%] [G loss: 0.879677]\n",
      "3149 [D loss: 0.734083, acc.: 50.00%] [G loss: 0.886239]\n",
      "3150 [D loss: 0.678700, acc.: 53.12%] [G loss: 0.871413]\n",
      "3151 [D loss: 0.702100, acc.: 59.38%] [G loss: 0.918812]\n",
      "3152 [D loss: 0.580891, acc.: 78.12%] [G loss: 0.831515]\n",
      "3153 [D loss: 0.606256, acc.: 71.88%] [G loss: 0.890520]\n",
      "3154 [D loss: 0.730647, acc.: 59.38%] [G loss: 0.900542]\n",
      "3155 [D loss: 0.637756, acc.: 68.75%] [G loss: 0.887950]\n",
      "3156 [D loss: 0.697034, acc.: 56.25%] [G loss: 1.020118]\n",
      "3157 [D loss: 0.692746, acc.: 62.50%] [G loss: 0.961159]\n",
      "3158 [D loss: 0.717809, acc.: 50.00%] [G loss: 0.939534]\n",
      "3159 [D loss: 0.594023, acc.: 68.75%] [G loss: 0.965103]\n",
      "3160 [D loss: 0.677070, acc.: 56.25%] [G loss: 0.949583]\n",
      "3161 [D loss: 0.615119, acc.: 68.75%] [G loss: 0.899059]\n",
      "3162 [D loss: 0.623158, acc.: 65.62%] [G loss: 1.004135]\n",
      "3163 [D loss: 0.660751, acc.: 71.88%] [G loss: 0.912964]\n",
      "3164 [D loss: 0.580024, acc.: 65.62%] [G loss: 0.882739]\n",
      "3165 [D loss: 0.576314, acc.: 75.00%] [G loss: 0.939906]\n",
      "3166 [D loss: 0.610686, acc.: 68.75%] [G loss: 0.879781]\n",
      "3167 [D loss: 0.569330, acc.: 71.88%] [G loss: 0.877214]\n",
      "3168 [D loss: 0.668242, acc.: 56.25%] [G loss: 0.912264]\n",
      "3169 [D loss: 0.650808, acc.: 46.88%] [G loss: 0.891573]\n",
      "3170 [D loss: 0.646006, acc.: 65.62%] [G loss: 0.852751]\n",
      "3171 [D loss: 0.592511, acc.: 62.50%] [G loss: 0.918456]\n",
      "3172 [D loss: 0.579280, acc.: 71.88%] [G loss: 0.857443]\n",
      "3173 [D loss: 0.709301, acc.: 53.12%] [G loss: 0.831622]\n",
      "3174 [D loss: 0.592024, acc.: 68.75%] [G loss: 0.936904]\n",
      "3175 [D loss: 0.611284, acc.: 68.75%] [G loss: 0.901901]\n",
      "3176 [D loss: 0.611910, acc.: 65.62%] [G loss: 0.926106]\n",
      "3177 [D loss: 0.610817, acc.: 68.75%] [G loss: 0.916872]\n",
      "3178 [D loss: 0.664460, acc.: 59.38%] [G loss: 0.829859]\n",
      "3179 [D loss: 0.641565, acc.: 59.38%] [G loss: 0.857009]\n",
      "3180 [D loss: 0.636213, acc.: 65.62%] [G loss: 0.880215]\n",
      "3181 [D loss: 0.590565, acc.: 71.88%] [G loss: 0.873000]\n",
      "3182 [D loss: 0.635011, acc.: 59.38%] [G loss: 0.843562]\n",
      "3183 [D loss: 0.637522, acc.: 56.25%] [G loss: 0.899413]\n",
      "3184 [D loss: 0.642804, acc.: 50.00%] [G loss: 0.884134]\n",
      "3185 [D loss: 0.684681, acc.: 50.00%] [G loss: 0.868957]\n",
      "3186 [D loss: 0.681105, acc.: 65.62%] [G loss: 0.883691]\n",
      "3187 [D loss: 0.530225, acc.: 84.38%] [G loss: 0.910277]\n",
      "3188 [D loss: 0.585393, acc.: 75.00%] [G loss: 1.007106]\n",
      "3189 [D loss: 0.650136, acc.: 59.38%] [G loss: 0.892538]\n",
      "3190 [D loss: 0.657379, acc.: 56.25%] [G loss: 0.841714]\n",
      "3191 [D loss: 0.632387, acc.: 65.62%] [G loss: 0.892348]\n",
      "3192 [D loss: 0.783200, acc.: 50.00%] [G loss: 0.836771]\n",
      "3193 [D loss: 0.632565, acc.: 62.50%] [G loss: 0.901766]\n",
      "3194 [D loss: 0.633715, acc.: 68.75%] [G loss: 0.838881]\n",
      "3195 [D loss: 0.752535, acc.: 43.75%] [G loss: 0.836343]\n",
      "3196 [D loss: 0.622241, acc.: 56.25%] [G loss: 0.988827]\n",
      "3197 [D loss: 0.591900, acc.: 65.62%] [G loss: 0.923401]\n",
      "3198 [D loss: 0.632964, acc.: 62.50%] [G loss: 0.894525]\n",
      "3199 [D loss: 0.748677, acc.: 46.88%] [G loss: 0.969270]\n",
      "3200 [D loss: 0.613774, acc.: 75.00%] [G loss: 0.875730]\n",
      "3201 [D loss: 0.640335, acc.: 53.12%] [G loss: 0.909003]\n",
      "3202 [D loss: 0.622206, acc.: 75.00%] [G loss: 0.905924]\n",
      "3203 [D loss: 0.654697, acc.: 59.38%] [G loss: 0.873162]\n",
      "3204 [D loss: 0.499660, acc.: 87.50%] [G loss: 0.842009]\n",
      "3205 [D loss: 0.517874, acc.: 81.25%] [G loss: 0.975653]\n",
      "3206 [D loss: 0.680673, acc.: 62.50%] [G loss: 0.843885]\n",
      "3207 [D loss: 0.552556, acc.: 75.00%] [G loss: 0.878785]\n",
      "3208 [D loss: 0.665648, acc.: 65.62%] [G loss: 0.867129]\n",
      "3209 [D loss: 0.655208, acc.: 62.50%] [G loss: 0.879139]\n",
      "3210 [D loss: 0.736502, acc.: 56.25%] [G loss: 0.882093]\n",
      "3211 [D loss: 0.644674, acc.: 53.12%] [G loss: 0.847243]\n",
      "3212 [D loss: 0.610118, acc.: 62.50%] [G loss: 0.821836]\n",
      "3213 [D loss: 0.656363, acc.: 56.25%] [G loss: 0.836628]\n",
      "3214 [D loss: 0.622270, acc.: 62.50%] [G loss: 0.877646]\n",
      "3215 [D loss: 0.645531, acc.: 59.38%] [G loss: 0.895189]\n",
      "3216 [D loss: 0.599364, acc.: 68.75%] [G loss: 0.856194]\n",
      "3217 [D loss: 0.636193, acc.: 68.75%] [G loss: 0.871381]\n",
      "3218 [D loss: 0.698565, acc.: 59.38%] [G loss: 0.789153]\n",
      "3219 [D loss: 0.629716, acc.: 68.75%] [G loss: 0.819216]\n",
      "3220 [D loss: 0.668039, acc.: 53.12%] [G loss: 0.991732]\n",
      "3221 [D loss: 0.613986, acc.: 71.88%] [G loss: 1.030198]\n",
      "3222 [D loss: 0.569660, acc.: 75.00%] [G loss: 0.915760]\n",
      "3223 [D loss: 0.714584, acc.: 59.38%] [G loss: 0.898431]\n",
      "3224 [D loss: 0.724123, acc.: 53.12%] [G loss: 0.840304]\n",
      "3225 [D loss: 0.593026, acc.: 78.12%] [G loss: 0.908078]\n",
      "3226 [D loss: 0.629335, acc.: 59.38%] [G loss: 0.989200]\n",
      "3227 [D loss: 0.603471, acc.: 68.75%] [G loss: 0.908798]\n",
      "3228 [D loss: 0.589148, acc.: 71.88%] [G loss: 0.936422]\n",
      "3229 [D loss: 0.619082, acc.: 68.75%] [G loss: 0.855354]\n",
      "3230 [D loss: 0.570043, acc.: 71.88%] [G loss: 0.877360]\n",
      "3231 [D loss: 0.564084, acc.: 68.75%] [G loss: 0.995520]\n",
      "3232 [D loss: 0.715044, acc.: 50.00%] [G loss: 0.867582]\n",
      "3233 [D loss: 0.623379, acc.: 62.50%] [G loss: 0.922835]\n",
      "3234 [D loss: 0.644220, acc.: 56.25%] [G loss: 0.939258]\n",
      "3235 [D loss: 0.615009, acc.: 68.75%] [G loss: 0.869930]\n",
      "3236 [D loss: 0.621260, acc.: 68.75%] [G loss: 0.825356]\n",
      "3237 [D loss: 0.671958, acc.: 46.88%] [G loss: 0.912746]\n",
      "3238 [D loss: 0.691434, acc.: 56.25%] [G loss: 0.884064]\n",
      "3239 [D loss: 0.658887, acc.: 62.50%] [G loss: 0.846695]\n",
      "3240 [D loss: 0.658349, acc.: 68.75%] [G loss: 0.872766]\n",
      "3241 [D loss: 0.717083, acc.: 46.88%] [G loss: 0.922655]\n",
      "3242 [D loss: 0.673985, acc.: 56.25%] [G loss: 0.867499]\n",
      "3243 [D loss: 0.678984, acc.: 68.75%] [G loss: 0.801875]\n",
      "3244 [D loss: 0.638400, acc.: 62.50%] [G loss: 0.823641]\n",
      "3245 [D loss: 0.671867, acc.: 65.62%] [G loss: 0.860345]\n",
      "3246 [D loss: 0.615604, acc.: 65.62%] [G loss: 0.856118]\n",
      "3247 [D loss: 0.562052, acc.: 81.25%] [G loss: 0.874465]\n",
      "3248 [D loss: 0.599689, acc.: 65.62%] [G loss: 0.906187]\n",
      "3249 [D loss: 0.591688, acc.: 81.25%] [G loss: 0.930236]\n",
      "3250 [D loss: 0.648719, acc.: 50.00%] [G loss: 0.863090]\n",
      "3251 [D loss: 0.559524, acc.: 71.88%] [G loss: 0.875250]\n",
      "3252 [D loss: 0.674047, acc.: 59.38%] [G loss: 0.856277]\n",
      "3253 [D loss: 0.611198, acc.: 62.50%] [G loss: 0.882177]\n",
      "3254 [D loss: 0.596088, acc.: 68.75%] [G loss: 0.926341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3255 [D loss: 0.590860, acc.: 68.75%] [G loss: 0.857324]\n",
      "3256 [D loss: 0.572957, acc.: 75.00%] [G loss: 0.906096]\n",
      "3257 [D loss: 0.659228, acc.: 62.50%] [G loss: 0.920480]\n",
      "3258 [D loss: 0.697020, acc.: 53.12%] [G loss: 0.893176]\n",
      "3259 [D loss: 0.661694, acc.: 62.50%] [G loss: 0.963479]\n",
      "3260 [D loss: 0.728239, acc.: 53.12%] [G loss: 0.946749]\n",
      "3261 [D loss: 0.628099, acc.: 68.75%] [G loss: 0.832191]\n",
      "3262 [D loss: 0.622316, acc.: 68.75%] [G loss: 0.884400]\n",
      "3263 [D loss: 0.595815, acc.: 65.62%] [G loss: 0.879786]\n",
      "3264 [D loss: 0.683156, acc.: 59.38%] [G loss: 0.910319]\n",
      "3265 [D loss: 0.598502, acc.: 65.62%] [G loss: 0.877365]\n",
      "3266 [D loss: 0.691395, acc.: 53.12%] [G loss: 0.901974]\n",
      "3267 [D loss: 0.611361, acc.: 65.62%] [G loss: 0.894439]\n",
      "3268 [D loss: 0.565465, acc.: 75.00%] [G loss: 0.839697]\n",
      "3269 [D loss: 0.648410, acc.: 62.50%] [G loss: 0.844144]\n",
      "3270 [D loss: 0.621174, acc.: 62.50%] [G loss: 0.943153]\n",
      "3271 [D loss: 0.678064, acc.: 56.25%] [G loss: 0.923988]\n",
      "3272 [D loss: 0.600529, acc.: 68.75%] [G loss: 0.938672]\n",
      "3273 [D loss: 0.659754, acc.: 65.62%] [G loss: 0.919869]\n",
      "3274 [D loss: 0.622428, acc.: 62.50%] [G loss: 0.901562]\n",
      "3275 [D loss: 0.621626, acc.: 62.50%] [G loss: 0.946911]\n",
      "3276 [D loss: 0.627504, acc.: 59.38%] [G loss: 0.951317]\n",
      "3277 [D loss: 0.737654, acc.: 40.62%] [G loss: 0.946672]\n",
      "3278 [D loss: 0.666593, acc.: 59.38%] [G loss: 0.896794]\n",
      "3279 [D loss: 0.623951, acc.: 68.75%] [G loss: 0.952714]\n",
      "3280 [D loss: 0.610691, acc.: 62.50%] [G loss: 0.912703]\n",
      "3281 [D loss: 0.693408, acc.: 50.00%] [G loss: 0.859446]\n",
      "3282 [D loss: 0.628463, acc.: 62.50%] [G loss: 0.936964]\n",
      "3283 [D loss: 0.675704, acc.: 50.00%] [G loss: 0.901333]\n",
      "3284 [D loss: 0.629079, acc.: 71.88%] [G loss: 0.962692]\n",
      "3285 [D loss: 0.654077, acc.: 56.25%] [G loss: 0.892685]\n",
      "3286 [D loss: 0.633805, acc.: 56.25%] [G loss: 0.895859]\n",
      "3287 [D loss: 0.619642, acc.: 71.88%] [G loss: 0.867977]\n",
      "3288 [D loss: 0.568941, acc.: 81.25%] [G loss: 0.844668]\n",
      "3289 [D loss: 0.648160, acc.: 59.38%] [G loss: 0.888837]\n",
      "3290 [D loss: 0.574022, acc.: 75.00%] [G loss: 0.982906]\n",
      "3291 [D loss: 0.674840, acc.: 59.38%] [G loss: 0.937248]\n",
      "3292 [D loss: 0.631665, acc.: 59.38%] [G loss: 0.900313]\n",
      "3293 [D loss: 0.564597, acc.: 71.88%] [G loss: 0.957871]\n",
      "3294 [D loss: 0.621081, acc.: 65.62%] [G loss: 0.925676]\n",
      "3295 [D loss: 0.638443, acc.: 62.50%] [G loss: 0.964551]\n",
      "3296 [D loss: 0.700985, acc.: 59.38%] [G loss: 0.939011]\n",
      "3297 [D loss: 0.609652, acc.: 62.50%] [G loss: 0.889841]\n",
      "3298 [D loss: 0.695414, acc.: 50.00%] [G loss: 0.850031]\n",
      "3299 [D loss: 0.638073, acc.: 78.12%] [G loss: 0.884737]\n",
      "3300 [D loss: 0.609657, acc.: 71.88%] [G loss: 0.816164]\n",
      "3301 [D loss: 0.660029, acc.: 68.75%] [G loss: 0.868113]\n",
      "3302 [D loss: 0.740337, acc.: 53.12%] [G loss: 0.953086]\n",
      "3303 [D loss: 0.617846, acc.: 68.75%] [G loss: 0.877923]\n",
      "3304 [D loss: 0.625229, acc.: 62.50%] [G loss: 0.865606]\n",
      "3305 [D loss: 0.554267, acc.: 75.00%] [G loss: 0.815949]\n",
      "3306 [D loss: 0.645849, acc.: 50.00%] [G loss: 0.892898]\n",
      "3307 [D loss: 0.624319, acc.: 68.75%] [G loss: 0.942277]\n",
      "3308 [D loss: 0.592800, acc.: 68.75%] [G loss: 0.859945]\n",
      "3309 [D loss: 0.623405, acc.: 65.62%] [G loss: 0.866333]\n",
      "3310 [D loss: 0.630667, acc.: 68.75%] [G loss: 0.932325]\n",
      "3311 [D loss: 0.620373, acc.: 65.62%] [G loss: 0.940978]\n",
      "3312 [D loss: 0.659214, acc.: 46.88%] [G loss: 0.811989]\n",
      "3313 [D loss: 0.633255, acc.: 68.75%] [G loss: 0.968763]\n",
      "3314 [D loss: 0.627627, acc.: 62.50%] [G loss: 0.938500]\n",
      "3315 [D loss: 0.606765, acc.: 65.62%] [G loss: 0.861013]\n",
      "3316 [D loss: 0.668244, acc.: 56.25%] [G loss: 0.857810]\n",
      "3317 [D loss: 0.588508, acc.: 78.12%] [G loss: 0.917437]\n",
      "3318 [D loss: 0.671750, acc.: 56.25%] [G loss: 0.855637]\n",
      "3319 [D loss: 0.587855, acc.: 65.62%] [G loss: 0.941507]\n",
      "3320 [D loss: 0.626343, acc.: 75.00%] [G loss: 0.879517]\n",
      "3321 [D loss: 0.712825, acc.: 46.88%] [G loss: 0.890625]\n",
      "3322 [D loss: 0.600709, acc.: 65.62%] [G loss: 0.860556]\n",
      "3323 [D loss: 0.651293, acc.: 75.00%] [G loss: 0.893911]\n",
      "3324 [D loss: 0.634590, acc.: 65.62%] [G loss: 0.902939]\n",
      "3325 [D loss: 0.671122, acc.: 62.50%] [G loss: 0.858806]\n",
      "3326 [D loss: 0.629296, acc.: 65.62%] [G loss: 0.878036]\n",
      "3327 [D loss: 0.628949, acc.: 65.62%] [G loss: 0.928326]\n",
      "3328 [D loss: 0.599723, acc.: 78.12%] [G loss: 0.911469]\n",
      "3329 [D loss: 0.588770, acc.: 75.00%] [G loss: 0.905511]\n",
      "3330 [D loss: 0.580262, acc.: 71.88%] [G loss: 0.829325]\n",
      "3331 [D loss: 0.653929, acc.: 56.25%] [G loss: 0.869651]\n",
      "3332 [D loss: 0.531465, acc.: 78.12%] [G loss: 0.889605]\n",
      "3333 [D loss: 0.567336, acc.: 71.88%] [G loss: 0.946120]\n",
      "3334 [D loss: 0.667878, acc.: 53.12%] [G loss: 0.922066]\n",
      "3335 [D loss: 0.580469, acc.: 78.12%] [G loss: 0.933526]\n",
      "3336 [D loss: 0.648821, acc.: 59.38%] [G loss: 0.823074]\n",
      "3337 [D loss: 0.575607, acc.: 71.88%] [G loss: 0.862201]\n",
      "3338 [D loss: 0.666320, acc.: 59.38%] [G loss: 0.797494]\n",
      "3339 [D loss: 0.645578, acc.: 75.00%] [G loss: 0.823178]\n",
      "3340 [D loss: 0.661858, acc.: 53.12%] [G loss: 0.913425]\n",
      "3341 [D loss: 0.553398, acc.: 75.00%] [G loss: 0.840854]\n",
      "3342 [D loss: 0.593501, acc.: 75.00%] [G loss: 0.927936]\n",
      "3343 [D loss: 0.671895, acc.: 50.00%] [G loss: 0.881059]\n",
      "3344 [D loss: 0.625859, acc.: 68.75%] [G loss: 0.881126]\n",
      "3345 [D loss: 0.625715, acc.: 68.75%] [G loss: 0.950486]\n",
      "3346 [D loss: 0.609283, acc.: 71.88%] [G loss: 0.946401]\n",
      "3347 [D loss: 0.585415, acc.: 71.88%] [G loss: 0.846079]\n",
      "3348 [D loss: 0.561203, acc.: 81.25%] [G loss: 0.905766]\n",
      "3349 [D loss: 0.683276, acc.: 59.38%] [G loss: 0.936461]\n",
      "3350 [D loss: 0.631311, acc.: 65.62%] [G loss: 0.943550]\n",
      "3351 [D loss: 0.669731, acc.: 59.38%] [G loss: 0.925557]\n",
      "3352 [D loss: 0.643472, acc.: 71.88%] [G loss: 0.926033]\n",
      "3353 [D loss: 0.585345, acc.: 68.75%] [G loss: 0.926500]\n",
      "3354 [D loss: 0.653069, acc.: 65.62%] [G loss: 0.947364]\n",
      "3355 [D loss: 0.542104, acc.: 78.12%] [G loss: 0.974897]\n",
      "3356 [D loss: 0.661425, acc.: 46.88%] [G loss: 0.935140]\n",
      "3357 [D loss: 0.588698, acc.: 68.75%] [G loss: 0.854306]\n",
      "3358 [D loss: 0.618249, acc.: 59.38%] [G loss: 0.888075]\n",
      "3359 [D loss: 0.582005, acc.: 71.88%] [G loss: 0.866508]\n",
      "3360 [D loss: 0.620531, acc.: 71.88%] [G loss: 0.943477]\n",
      "3361 [D loss: 0.590937, acc.: 65.62%] [G loss: 0.929154]\n",
      "3362 [D loss: 0.522716, acc.: 84.38%] [G loss: 0.943728]\n",
      "3363 [D loss: 0.652164, acc.: 68.75%] [G loss: 0.869835]\n",
      "3364 [D loss: 0.580531, acc.: 68.75%] [G loss: 0.836777]\n",
      "3365 [D loss: 0.599836, acc.: 71.88%] [G loss: 1.029727]\n",
      "3366 [D loss: 0.700077, acc.: 56.25%] [G loss: 0.974943]\n",
      "3367 [D loss: 0.630668, acc.: 68.75%] [G loss: 0.964439]\n",
      "3368 [D loss: 0.686930, acc.: 56.25%] [G loss: 1.004639]\n",
      "3369 [D loss: 0.614036, acc.: 75.00%] [G loss: 0.958107]\n",
      "3370 [D loss: 0.645965, acc.: 65.62%] [G loss: 0.998324]\n",
      "3371 [D loss: 0.658041, acc.: 65.62%] [G loss: 0.971768]\n",
      "3372 [D loss: 0.632715, acc.: 56.25%] [G loss: 0.904847]\n",
      "3373 [D loss: 0.626534, acc.: 62.50%] [G loss: 0.981852]\n",
      "3374 [D loss: 0.617559, acc.: 71.88%] [G loss: 0.954809]\n",
      "3375 [D loss: 0.602729, acc.: 68.75%] [G loss: 0.923349]\n",
      "3376 [D loss: 0.621338, acc.: 62.50%] [G loss: 0.884205]\n",
      "3377 [D loss: 0.651363, acc.: 59.38%] [G loss: 0.862923]\n",
      "3378 [D loss: 0.629161, acc.: 68.75%] [G loss: 0.960990]\n",
      "3379 [D loss: 0.582787, acc.: 68.75%] [G loss: 1.000554]\n",
      "3380 [D loss: 0.631242, acc.: 68.75%] [G loss: 0.996789]\n",
      "3381 [D loss: 0.682867, acc.: 56.25%] [G loss: 0.862365]\n",
      "3382 [D loss: 0.660734, acc.: 62.50%] [G loss: 0.887204]\n",
      "3383 [D loss: 0.614866, acc.: 68.75%] [G loss: 0.927007]\n",
      "3384 [D loss: 0.564715, acc.: 71.88%] [G loss: 1.016805]\n",
      "3385 [D loss: 0.637937, acc.: 62.50%] [G loss: 0.901168]\n",
      "3386 [D loss: 0.620760, acc.: 78.12%] [G loss: 0.870234]\n",
      "3387 [D loss: 0.633908, acc.: 62.50%] [G loss: 0.893932]\n",
      "3388 [D loss: 0.672432, acc.: 56.25%] [G loss: 0.895714]\n",
      "3389 [D loss: 0.611286, acc.: 68.75%] [G loss: 0.868976]\n",
      "3390 [D loss: 0.666476, acc.: 56.25%] [G loss: 0.910123]\n",
      "3391 [D loss: 0.640342, acc.: 59.38%] [G loss: 0.873106]\n",
      "3392 [D loss: 0.633992, acc.: 68.75%] [G loss: 0.955164]\n",
      "3393 [D loss: 0.703569, acc.: 50.00%] [G loss: 0.939613]\n",
      "3394 [D loss: 0.621290, acc.: 65.62%] [G loss: 0.875439]\n",
      "3395 [D loss: 0.593007, acc.: 75.00%] [G loss: 0.964307]\n",
      "3396 [D loss: 0.571482, acc.: 65.62%] [G loss: 0.932316]\n",
      "3397 [D loss: 0.584227, acc.: 68.75%] [G loss: 1.007177]\n",
      "3398 [D loss: 0.670261, acc.: 62.50%] [G loss: 0.878513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3399 [D loss: 0.691807, acc.: 62.50%] [G loss: 0.928531]\n",
      "3400 [D loss: 0.571086, acc.: 68.75%] [G loss: 0.845192]\n",
      "3401 [D loss: 0.728923, acc.: 62.50%] [G loss: 0.832274]\n",
      "3402 [D loss: 0.650393, acc.: 65.62%] [G loss: 0.867264]\n",
      "3403 [D loss: 0.659917, acc.: 59.38%] [G loss: 0.902240]\n",
      "3404 [D loss: 0.591995, acc.: 59.38%] [G loss: 0.935879]\n",
      "3405 [D loss: 0.666918, acc.: 50.00%] [G loss: 0.842405]\n",
      "3406 [D loss: 0.673754, acc.: 62.50%] [G loss: 0.838294]\n",
      "3407 [D loss: 0.693049, acc.: 56.25%] [G loss: 0.927046]\n",
      "3408 [D loss: 0.604589, acc.: 62.50%] [G loss: 0.906020]\n",
      "3409 [D loss: 0.673553, acc.: 59.38%] [G loss: 0.887584]\n",
      "3410 [D loss: 0.693752, acc.: 56.25%] [G loss: 0.938096]\n",
      "3411 [D loss: 0.663446, acc.: 56.25%] [G loss: 0.900294]\n",
      "3412 [D loss: 0.609748, acc.: 75.00%] [G loss: 0.847300]\n",
      "3413 [D loss: 0.643631, acc.: 62.50%] [G loss: 0.921275]\n",
      "3414 [D loss: 0.589487, acc.: 75.00%] [G loss: 0.933644]\n",
      "3415 [D loss: 0.621848, acc.: 68.75%] [G loss: 0.865009]\n",
      "3416 [D loss: 0.603060, acc.: 78.12%] [G loss: 0.860084]\n",
      "3417 [D loss: 0.646108, acc.: 62.50%] [G loss: 0.885499]\n",
      "3418 [D loss: 0.650148, acc.: 65.62%] [G loss: 0.856980]\n",
      "3419 [D loss: 0.685302, acc.: 53.12%] [G loss: 0.905022]\n",
      "3420 [D loss: 0.646433, acc.: 68.75%] [G loss: 0.888564]\n",
      "3421 [D loss: 0.569003, acc.: 78.12%] [G loss: 0.864464]\n",
      "3422 [D loss: 0.554317, acc.: 65.62%] [G loss: 0.843932]\n",
      "3423 [D loss: 0.672978, acc.: 62.50%] [G loss: 0.815548]\n",
      "3424 [D loss: 0.643136, acc.: 68.75%] [G loss: 0.837516]\n",
      "3425 [D loss: 0.712755, acc.: 53.12%] [G loss: 0.848391]\n",
      "3426 [D loss: 0.608279, acc.: 68.75%] [G loss: 0.938000]\n",
      "3427 [D loss: 0.646148, acc.: 62.50%] [G loss: 0.898780]\n",
      "3428 [D loss: 0.641352, acc.: 68.75%] [G loss: 0.915747]\n",
      "3429 [D loss: 0.626065, acc.: 75.00%] [G loss: 0.933142]\n",
      "3430 [D loss: 0.645559, acc.: 65.62%] [G loss: 0.900173]\n",
      "3431 [D loss: 0.666094, acc.: 56.25%] [G loss: 0.924795]\n",
      "3432 [D loss: 0.641439, acc.: 59.38%] [G loss: 0.891186]\n",
      "3433 [D loss: 0.560143, acc.: 71.88%] [G loss: 0.914101]\n",
      "3434 [D loss: 0.646628, acc.: 62.50%] [G loss: 0.906209]\n",
      "3435 [D loss: 0.671827, acc.: 65.62%] [G loss: 0.898715]\n",
      "3436 [D loss: 0.562858, acc.: 78.12%] [G loss: 1.003436]\n",
      "3437 [D loss: 0.559291, acc.: 71.88%] [G loss: 0.985333]\n",
      "3438 [D loss: 0.639765, acc.: 62.50%] [G loss: 1.024336]\n",
      "3439 [D loss: 0.654412, acc.: 62.50%] [G loss: 0.873995]\n",
      "3440 [D loss: 0.712763, acc.: 56.25%] [G loss: 0.845454]\n",
      "3441 [D loss: 0.597690, acc.: 62.50%] [G loss: 0.916641]\n",
      "3442 [D loss: 0.637049, acc.: 68.75%] [G loss: 0.874740]\n",
      "3443 [D loss: 0.626461, acc.: 59.38%] [G loss: 0.953551]\n",
      "3444 [D loss: 0.648570, acc.: 65.62%] [G loss: 0.883888]\n",
      "3445 [D loss: 0.737792, acc.: 53.12%] [G loss: 0.829303]\n",
      "3446 [D loss: 0.590631, acc.: 81.25%] [G loss: 0.925132]\n",
      "3447 [D loss: 0.692914, acc.: 56.25%] [G loss: 0.924920]\n",
      "3448 [D loss: 0.661927, acc.: 56.25%] [G loss: 0.921281]\n",
      "3449 [D loss: 0.530281, acc.: 78.12%] [G loss: 0.945099]\n",
      "3450 [D loss: 0.640104, acc.: 62.50%] [G loss: 0.801117]\n",
      "3451 [D loss: 0.711711, acc.: 53.12%] [G loss: 0.880813]\n",
      "3452 [D loss: 0.565116, acc.: 71.88%] [G loss: 0.929964]\n",
      "3453 [D loss: 0.718739, acc.: 46.88%] [G loss: 0.926813]\n",
      "3454 [D loss: 0.624964, acc.: 62.50%] [G loss: 0.878141]\n",
      "3455 [D loss: 0.623331, acc.: 65.62%] [G loss: 0.958656]\n",
      "3456 [D loss: 0.583654, acc.: 75.00%] [G loss: 0.942122]\n",
      "3457 [D loss: 0.610986, acc.: 75.00%] [G loss: 0.911713]\n",
      "3458 [D loss: 0.721851, acc.: 50.00%] [G loss: 0.906508]\n",
      "3459 [D loss: 0.574571, acc.: 71.88%] [G loss: 0.897587]\n",
      "3460 [D loss: 0.672131, acc.: 50.00%] [G loss: 0.873578]\n",
      "3461 [D loss: 0.695740, acc.: 62.50%] [G loss: 0.874581]\n",
      "3462 [D loss: 0.443895, acc.: 90.62%] [G loss: 0.906730]\n",
      "3463 [D loss: 0.616297, acc.: 65.62%] [G loss: 0.940381]\n",
      "3464 [D loss: 0.805695, acc.: 46.88%] [G loss: 0.887041]\n",
      "3465 [D loss: 0.711986, acc.: 59.38%] [G loss: 0.897363]\n",
      "3466 [D loss: 0.622281, acc.: 62.50%] [G loss: 0.859853]\n",
      "3467 [D loss: 0.627276, acc.: 62.50%] [G loss: 0.860097]\n",
      "3468 [D loss: 0.660146, acc.: 62.50%] [G loss: 0.851231]\n",
      "3469 [D loss: 0.710342, acc.: 50.00%] [G loss: 0.945922]\n",
      "3470 [D loss: 0.620073, acc.: 68.75%] [G loss: 0.901102]\n",
      "3471 [D loss: 0.571366, acc.: 78.12%] [G loss: 1.019406]\n",
      "3472 [D loss: 0.704843, acc.: 46.88%] [G loss: 0.913009]\n",
      "3473 [D loss: 0.668217, acc.: 65.62%] [G loss: 0.930554]\n",
      "3474 [D loss: 0.587187, acc.: 71.88%] [G loss: 0.993880]\n",
      "3475 [D loss: 0.695919, acc.: 56.25%] [G loss: 0.873656]\n",
      "3476 [D loss: 0.637929, acc.: 65.62%] [G loss: 0.823873]\n",
      "3477 [D loss: 0.637584, acc.: 71.88%] [G loss: 0.847605]\n",
      "3478 [D loss: 0.635454, acc.: 65.62%] [G loss: 0.906402]\n",
      "3479 [D loss: 0.604963, acc.: 65.62%] [G loss: 0.896833]\n",
      "3480 [D loss: 0.620303, acc.: 71.88%] [G loss: 1.017990]\n",
      "3481 [D loss: 0.612982, acc.: 75.00%] [G loss: 0.937937]\n",
      "3482 [D loss: 0.689336, acc.: 53.12%] [G loss: 0.869589]\n",
      "3483 [D loss: 0.598669, acc.: 75.00%] [G loss: 0.823863]\n",
      "3484 [D loss: 0.710272, acc.: 59.38%] [G loss: 0.897375]\n",
      "3485 [D loss: 0.672462, acc.: 56.25%] [G loss: 0.931392]\n",
      "3486 [D loss: 0.594681, acc.: 68.75%] [G loss: 0.931451]\n",
      "3487 [D loss: 0.623297, acc.: 71.88%] [G loss: 1.012844]\n",
      "3488 [D loss: 0.618994, acc.: 59.38%] [G loss: 0.971249]\n",
      "3489 [D loss: 0.617363, acc.: 68.75%] [G loss: 0.911855]\n",
      "3490 [D loss: 0.707705, acc.: 56.25%] [G loss: 0.879416]\n",
      "3491 [D loss: 0.585119, acc.: 75.00%] [G loss: 0.966572]\n",
      "3492 [D loss: 0.658467, acc.: 59.38%] [G loss: 0.977291]\n",
      "3493 [D loss: 0.673862, acc.: 50.00%] [G loss: 0.960574]\n",
      "3494 [D loss: 0.630543, acc.: 59.38%] [G loss: 0.918321]\n",
      "3495 [D loss: 0.626797, acc.: 65.62%] [G loss: 0.928866]\n",
      "3496 [D loss: 0.640732, acc.: 62.50%] [G loss: 0.917760]\n",
      "3497 [D loss: 0.563263, acc.: 78.12%] [G loss: 0.956803]\n",
      "3498 [D loss: 0.555853, acc.: 81.25%] [G loss: 0.970486]\n",
      "3499 [D loss: 0.641356, acc.: 59.38%] [G loss: 1.080907]\n",
      "3500 [D loss: 0.671196, acc.: 59.38%] [G loss: 0.893847]\n",
      "3501 [D loss: 0.669201, acc.: 53.12%] [G loss: 0.839788]\n",
      "3502 [D loss: 0.721960, acc.: 56.25%] [G loss: 0.745520]\n",
      "3503 [D loss: 0.599709, acc.: 68.75%] [G loss: 0.960575]\n",
      "3504 [D loss: 0.666856, acc.: 62.50%] [G loss: 0.984671]\n",
      "3505 [D loss: 0.553969, acc.: 78.12%] [G loss: 0.927872]\n",
      "3506 [D loss: 0.621639, acc.: 65.62%] [G loss: 0.905939]\n",
      "3507 [D loss: 0.698849, acc.: 50.00%] [G loss: 0.919583]\n",
      "3508 [D loss: 0.615271, acc.: 71.88%] [G loss: 0.855275]\n",
      "3509 [D loss: 0.531164, acc.: 84.38%] [G loss: 1.010785]\n",
      "3510 [D loss: 0.623792, acc.: 65.62%] [G loss: 0.961786]\n",
      "3511 [D loss: 0.665471, acc.: 59.38%] [G loss: 0.912607]\n",
      "3512 [D loss: 0.602860, acc.: 65.62%] [G loss: 0.848013]\n",
      "3513 [D loss: 0.733187, acc.: 50.00%] [G loss: 0.873401]\n",
      "3514 [D loss: 0.676585, acc.: 59.38%] [G loss: 0.860566]\n",
      "3515 [D loss: 0.603586, acc.: 68.75%] [G loss: 0.970490]\n",
      "3516 [D loss: 0.592429, acc.: 59.38%] [G loss: 0.962372]\n",
      "3517 [D loss: 0.491529, acc.: 90.62%] [G loss: 0.996415]\n",
      "3518 [D loss: 0.656990, acc.: 56.25%] [G loss: 0.980959]\n",
      "3519 [D loss: 0.719479, acc.: 56.25%] [G loss: 0.822828]\n",
      "3520 [D loss: 0.608051, acc.: 62.50%] [G loss: 0.840363]\n",
      "3521 [D loss: 0.593419, acc.: 62.50%] [G loss: 0.958050]\n",
      "3522 [D loss: 0.617049, acc.: 68.75%] [G loss: 0.970795]\n",
      "3523 [D loss: 0.645835, acc.: 62.50%] [G loss: 0.942188]\n",
      "3524 [D loss: 0.645475, acc.: 62.50%] [G loss: 0.939354]\n",
      "3525 [D loss: 0.631235, acc.: 65.62%] [G loss: 0.880856]\n",
      "3526 [D loss: 0.630551, acc.: 59.38%] [G loss: 0.804706]\n",
      "3527 [D loss: 0.597299, acc.: 62.50%] [G loss: 0.886594]\n",
      "3528 [D loss: 0.592894, acc.: 78.12%] [G loss: 0.945167]\n",
      "3529 [D loss: 0.738804, acc.: 50.00%] [G loss: 0.906514]\n",
      "3530 [D loss: 0.581697, acc.: 75.00%] [G loss: 0.987450]\n",
      "3531 [D loss: 0.661952, acc.: 59.38%] [G loss: 0.889784]\n",
      "3532 [D loss: 0.640963, acc.: 71.88%] [G loss: 0.824978]\n",
      "3533 [D loss: 0.621932, acc.: 59.38%] [G loss: 0.829128]\n",
      "3534 [D loss: 0.776005, acc.: 43.75%] [G loss: 0.907614]\n",
      "3535 [D loss: 0.558728, acc.: 65.62%] [G loss: 0.913629]\n",
      "3536 [D loss: 0.711394, acc.: 62.50%] [G loss: 0.905919]\n",
      "3537 [D loss: 0.614275, acc.: 59.38%] [G loss: 0.885074]\n",
      "3538 [D loss: 0.536959, acc.: 75.00%] [G loss: 0.954051]\n",
      "3539 [D loss: 0.611474, acc.: 68.75%] [G loss: 0.956137]\n",
      "3540 [D loss: 0.691275, acc.: 50.00%] [G loss: 0.974842]\n",
      "3541 [D loss: 0.582361, acc.: 71.88%] [G loss: 0.932065]\n",
      "3542 [D loss: 0.623592, acc.: 62.50%] [G loss: 0.887545]\n",
      "3543 [D loss: 0.669322, acc.: 62.50%] [G loss: 0.954791]\n",
      "3544 [D loss: 0.650308, acc.: 65.62%] [G loss: 0.987954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3545 [D loss: 0.612954, acc.: 56.25%] [G loss: 0.963512]\n",
      "3546 [D loss: 0.645670, acc.: 68.75%] [G loss: 0.880135]\n",
      "3547 [D loss: 0.601911, acc.: 62.50%] [G loss: 0.888456]\n",
      "3548 [D loss: 0.638077, acc.: 56.25%] [G loss: 0.817453]\n",
      "3549 [D loss: 0.663278, acc.: 59.38%] [G loss: 0.863460]\n",
      "3550 [D loss: 0.627806, acc.: 68.75%] [G loss: 0.938792]\n",
      "3551 [D loss: 0.686637, acc.: 59.38%] [G loss: 0.975508]\n",
      "3552 [D loss: 0.647182, acc.: 62.50%] [G loss: 0.852106]\n",
      "3553 [D loss: 0.637485, acc.: 68.75%] [G loss: 0.919142]\n",
      "3554 [D loss: 0.599961, acc.: 68.75%] [G loss: 1.015468]\n",
      "3555 [D loss: 0.619343, acc.: 71.88%] [G loss: 0.886647]\n",
      "3556 [D loss: 0.641777, acc.: 71.88%] [G loss: 0.905089]\n",
      "3557 [D loss: 0.618676, acc.: 62.50%] [G loss: 0.840636]\n",
      "3558 [D loss: 0.711194, acc.: 62.50%] [G loss: 0.860499]\n",
      "3559 [D loss: 0.673664, acc.: 62.50%] [G loss: 0.856694]\n",
      "3560 [D loss: 0.606662, acc.: 62.50%] [G loss: 0.946712]\n",
      "3561 [D loss: 0.638850, acc.: 56.25%] [G loss: 0.988245]\n",
      "3562 [D loss: 0.693248, acc.: 56.25%] [G loss: 0.994112]\n",
      "3563 [D loss: 0.638441, acc.: 65.62%] [G loss: 0.916493]\n",
      "3564 [D loss: 0.559498, acc.: 75.00%] [G loss: 0.871585]\n",
      "3565 [D loss: 0.715287, acc.: 59.38%] [G loss: 0.850263]\n",
      "3566 [D loss: 0.666448, acc.: 56.25%] [G loss: 0.781515]\n",
      "3567 [D loss: 0.652144, acc.: 62.50%] [G loss: 0.792211]\n",
      "3568 [D loss: 0.618000, acc.: 68.75%] [G loss: 0.808027]\n",
      "3569 [D loss: 0.696504, acc.: 46.88%] [G loss: 0.836464]\n",
      "3570 [D loss: 0.528737, acc.: 81.25%] [G loss: 0.879313]\n",
      "3571 [D loss: 0.737723, acc.: 56.25%] [G loss: 0.857111]\n",
      "3572 [D loss: 0.681273, acc.: 56.25%] [G loss: 0.825278]\n",
      "3573 [D loss: 0.663861, acc.: 71.88%] [G loss: 0.896440]\n",
      "3574 [D loss: 0.654829, acc.: 53.12%] [G loss: 0.874629]\n",
      "3575 [D loss: 0.568481, acc.: 87.50%] [G loss: 0.826597]\n",
      "3576 [D loss: 0.608440, acc.: 78.12%] [G loss: 0.903789]\n",
      "3577 [D loss: 0.611928, acc.: 65.62%] [G loss: 0.812892]\n",
      "3578 [D loss: 0.675443, acc.: 56.25%] [G loss: 0.892876]\n",
      "3579 [D loss: 0.646101, acc.: 53.12%] [G loss: 0.982210]\n",
      "3580 [D loss: 0.623479, acc.: 59.38%] [G loss: 1.035574]\n",
      "3581 [D loss: 0.699480, acc.: 56.25%] [G loss: 0.942456]\n",
      "3582 [D loss: 0.691463, acc.: 59.38%] [G loss: 0.879318]\n",
      "3583 [D loss: 0.686192, acc.: 62.50%] [G loss: 0.930632]\n",
      "3584 [D loss: 0.660010, acc.: 65.62%] [G loss: 0.924404]\n",
      "3585 [D loss: 0.640096, acc.: 56.25%] [G loss: 0.918376]\n",
      "3586 [D loss: 0.620276, acc.: 62.50%] [G loss: 0.919282]\n",
      "3587 [D loss: 0.647767, acc.: 68.75%] [G loss: 0.847441]\n",
      "3588 [D loss: 0.580048, acc.: 71.88%] [G loss: 0.883959]\n",
      "3589 [D loss: 0.581643, acc.: 68.75%] [G loss: 0.872097]\n",
      "3590 [D loss: 0.519549, acc.: 81.25%] [G loss: 0.839296]\n",
      "3591 [D loss: 0.648875, acc.: 65.62%] [G loss: 0.825850]\n",
      "3592 [D loss: 0.617723, acc.: 71.88%] [G loss: 0.860262]\n",
      "3593 [D loss: 0.774370, acc.: 43.75%] [G loss: 0.854494]\n",
      "3594 [D loss: 0.578475, acc.: 68.75%] [G loss: 0.916488]\n",
      "3595 [D loss: 0.566704, acc.: 68.75%] [G loss: 0.888375]\n",
      "3596 [D loss: 0.656485, acc.: 68.75%] [G loss: 0.870050]\n",
      "3597 [D loss: 0.581569, acc.: 71.88%] [G loss: 0.872417]\n",
      "3598 [D loss: 0.641183, acc.: 50.00%] [G loss: 0.882205]\n",
      "3599 [D loss: 0.625234, acc.: 71.88%] [G loss: 1.001255]\n",
      "3600 [D loss: 0.681856, acc.: 59.38%] [G loss: 0.883810]\n",
      "3601 [D loss: 0.635433, acc.: 62.50%] [G loss: 0.909244]\n",
      "3602 [D loss: 0.690268, acc.: 62.50%] [G loss: 0.895527]\n",
      "3603 [D loss: 0.664093, acc.: 50.00%] [G loss: 0.856303]\n",
      "3604 [D loss: 0.670763, acc.: 59.38%] [G loss: 0.803291]\n",
      "3605 [D loss: 0.759355, acc.: 50.00%] [G loss: 0.794001]\n",
      "3606 [D loss: 0.670535, acc.: 62.50%] [G loss: 0.944899]\n",
      "3607 [D loss: 0.582652, acc.: 62.50%] [G loss: 0.838036]\n",
      "3608 [D loss: 0.638745, acc.: 68.75%] [G loss: 0.941492]\n",
      "3609 [D loss: 0.662841, acc.: 56.25%] [G loss: 0.837049]\n",
      "3610 [D loss: 0.704289, acc.: 62.50%] [G loss: 0.931858]\n",
      "3611 [D loss: 0.666360, acc.: 62.50%] [G loss: 0.956487]\n",
      "3612 [D loss: 0.565199, acc.: 75.00%] [G loss: 0.969578]\n",
      "3613 [D loss: 0.692385, acc.: 56.25%] [G loss: 0.970952]\n",
      "3614 [D loss: 0.640129, acc.: 71.88%] [G loss: 0.957167]\n",
      "3615 [D loss: 0.602386, acc.: 75.00%] [G loss: 0.859993]\n",
      "3616 [D loss: 0.606596, acc.: 65.62%] [G loss: 0.901337]\n",
      "3617 [D loss: 0.747300, acc.: 50.00%] [G loss: 0.831807]\n",
      "3618 [D loss: 0.587893, acc.: 71.88%] [G loss: 0.910961]\n",
      "3619 [D loss: 0.644739, acc.: 56.25%] [G loss: 0.866161]\n",
      "3620 [D loss: 0.574302, acc.: 75.00%] [G loss: 1.049126]\n",
      "3621 [D loss: 0.766252, acc.: 46.88%] [G loss: 0.867512]\n",
      "3622 [D loss: 0.702797, acc.: 50.00%] [G loss: 0.824474]\n",
      "3623 [D loss: 0.660849, acc.: 68.75%] [G loss: 0.885437]\n",
      "3624 [D loss: 0.673137, acc.: 56.25%] [G loss: 0.909320]\n",
      "3625 [D loss: 0.611758, acc.: 71.88%] [G loss: 0.873720]\n",
      "3626 [D loss: 0.666275, acc.: 59.38%] [G loss: 0.841652]\n",
      "3627 [D loss: 0.694513, acc.: 53.12%] [G loss: 0.895692]\n",
      "3628 [D loss: 0.620054, acc.: 65.62%] [G loss: 0.880159]\n",
      "3629 [D loss: 0.567128, acc.: 71.88%] [G loss: 0.841213]\n",
      "3630 [D loss: 0.678679, acc.: 56.25%] [G loss: 0.905597]\n",
      "3631 [D loss: 0.725289, acc.: 56.25%] [G loss: 0.923874]\n",
      "3632 [D loss: 0.609055, acc.: 62.50%] [G loss: 0.894947]\n",
      "3633 [D loss: 0.594075, acc.: 71.88%] [G loss: 0.844290]\n",
      "3634 [D loss: 0.733218, acc.: 40.62%] [G loss: 0.938456]\n",
      "3635 [D loss: 0.695190, acc.: 56.25%] [G loss: 0.868273]\n",
      "3636 [D loss: 0.608121, acc.: 62.50%] [G loss: 0.892316]\n",
      "3637 [D loss: 0.697738, acc.: 56.25%] [G loss: 0.978028]\n",
      "3638 [D loss: 0.540251, acc.: 75.00%] [G loss: 0.896572]\n",
      "3639 [D loss: 0.670725, acc.: 53.12%] [G loss: 0.825756]\n",
      "3640 [D loss: 0.619869, acc.: 59.38%] [G loss: 0.982819]\n",
      "3641 [D loss: 0.663130, acc.: 75.00%] [G loss: 0.933803]\n",
      "3642 [D loss: 0.706270, acc.: 46.88%] [G loss: 0.848112]\n",
      "3643 [D loss: 0.554054, acc.: 78.12%] [G loss: 0.901387]\n",
      "3644 [D loss: 0.610087, acc.: 71.88%] [G loss: 0.963704]\n",
      "3645 [D loss: 0.642764, acc.: 62.50%] [G loss: 0.887493]\n",
      "3646 [D loss: 0.620395, acc.: 65.62%] [G loss: 0.922172]\n",
      "3647 [D loss: 0.724095, acc.: 53.12%] [G loss: 0.910843]\n",
      "3648 [D loss: 0.597382, acc.: 65.62%] [G loss: 0.939327]\n",
      "3649 [D loss: 0.660117, acc.: 62.50%] [G loss: 0.968698]\n",
      "3650 [D loss: 0.625057, acc.: 65.62%] [G loss: 0.894463]\n",
      "3651 [D loss: 0.687479, acc.: 56.25%] [G loss: 0.863988]\n",
      "3652 [D loss: 0.585718, acc.: 81.25%] [G loss: 0.944964]\n",
      "3653 [D loss: 0.659458, acc.: 56.25%] [G loss: 0.876598]\n",
      "3654 [D loss: 0.722133, acc.: 53.12%] [G loss: 0.895944]\n",
      "3655 [D loss: 0.664213, acc.: 59.38%] [G loss: 0.851920]\n",
      "3656 [D loss: 0.689270, acc.: 68.75%] [G loss: 0.811242]\n",
      "3657 [D loss: 0.702414, acc.: 56.25%] [G loss: 0.908116]\n",
      "3658 [D loss: 0.634060, acc.: 65.62%] [G loss: 0.907844]\n",
      "3659 [D loss: 0.677784, acc.: 62.50%] [G loss: 0.831357]\n",
      "3660 [D loss: 0.578495, acc.: 62.50%] [G loss: 0.887304]\n",
      "3661 [D loss: 0.662761, acc.: 59.38%] [G loss: 0.804774]\n",
      "3662 [D loss: 0.582467, acc.: 78.12%] [G loss: 0.867422]\n",
      "3663 [D loss: 0.558206, acc.: 78.12%] [G loss: 0.855188]\n",
      "3664 [D loss: 0.709888, acc.: 56.25%] [G loss: 0.960783]\n",
      "3665 [D loss: 0.634504, acc.: 62.50%] [G loss: 0.941798]\n",
      "3666 [D loss: 0.676085, acc.: 46.88%] [G loss: 0.918749]\n",
      "3667 [D loss: 0.656446, acc.: 50.00%] [G loss: 0.911763]\n",
      "3668 [D loss: 0.665421, acc.: 65.62%] [G loss: 0.875881]\n",
      "3669 [D loss: 0.609912, acc.: 65.62%] [G loss: 0.891760]\n",
      "3670 [D loss: 0.655636, acc.: 62.50%] [G loss: 0.828924]\n",
      "3671 [D loss: 0.571275, acc.: 75.00%] [G loss: 0.849305]\n",
      "3672 [D loss: 0.622290, acc.: 59.38%] [G loss: 0.872245]\n",
      "3673 [D loss: 0.596805, acc.: 65.62%] [G loss: 1.026279]\n",
      "3674 [D loss: 0.627792, acc.: 62.50%] [G loss: 0.976773]\n",
      "3675 [D loss: 0.601214, acc.: 68.75%] [G loss: 0.895915]\n",
      "3676 [D loss: 0.550771, acc.: 75.00%] [G loss: 0.864063]\n",
      "3677 [D loss: 0.617757, acc.: 71.88%] [G loss: 0.914432]\n",
      "3678 [D loss: 0.560990, acc.: 75.00%] [G loss: 0.936481]\n",
      "3679 [D loss: 0.676595, acc.: 53.12%] [G loss: 0.905038]\n",
      "3680 [D loss: 0.666709, acc.: 56.25%] [G loss: 0.923073]\n",
      "3681 [D loss: 0.602833, acc.: 68.75%] [G loss: 0.912677]\n",
      "3682 [D loss: 0.608131, acc.: 75.00%] [G loss: 0.863399]\n",
      "3683 [D loss: 0.748831, acc.: 43.75%] [G loss: 0.846555]\n",
      "3684 [D loss: 0.627728, acc.: 65.62%] [G loss: 0.845970]\n",
      "3685 [D loss: 0.773238, acc.: 53.12%] [G loss: 0.789546]\n",
      "3686 [D loss: 0.542767, acc.: 71.88%] [G loss: 0.923896]\n",
      "3687 [D loss: 0.668006, acc.: 53.12%] [G loss: 0.944213]\n",
      "3688 [D loss: 0.661877, acc.: 59.38%] [G loss: 0.953077]\n",
      "3689 [D loss: 0.587606, acc.: 68.75%] [G loss: 0.986880]\n",
      "3690 [D loss: 0.654693, acc.: 53.12%] [G loss: 0.891465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691 [D loss: 0.601856, acc.: 65.62%] [G loss: 0.904115]\n",
      "3692 [D loss: 0.586548, acc.: 68.75%] [G loss: 0.906319]\n",
      "3693 [D loss: 0.498641, acc.: 81.25%] [G loss: 0.958845]\n",
      "3694 [D loss: 0.708717, acc.: 43.75%] [G loss: 0.906457]\n",
      "3695 [D loss: 0.550911, acc.: 71.88%] [G loss: 0.927374]\n",
      "3696 [D loss: 0.553714, acc.: 71.88%] [G loss: 0.973349]\n",
      "3697 [D loss: 0.690426, acc.: 50.00%] [G loss: 0.987901]\n",
      "3698 [D loss: 0.606898, acc.: 65.62%] [G loss: 1.018685]\n",
      "3699 [D loss: 0.503133, acc.: 84.38%] [G loss: 0.981403]\n",
      "3700 [D loss: 0.643874, acc.: 65.62%] [G loss: 0.825934]\n",
      "3701 [D loss: 0.646852, acc.: 71.88%] [G loss: 0.885587]\n",
      "3702 [D loss: 0.647977, acc.: 62.50%] [G loss: 0.886181]\n",
      "3703 [D loss: 0.652137, acc.: 65.62%] [G loss: 0.891055]\n",
      "3704 [D loss: 0.675083, acc.: 65.62%] [G loss: 0.921378]\n",
      "3705 [D loss: 0.623335, acc.: 71.88%] [G loss: 0.957987]\n",
      "3706 [D loss: 0.664776, acc.: 53.12%] [G loss: 0.880737]\n",
      "3707 [D loss: 0.589913, acc.: 75.00%] [G loss: 0.869255]\n",
      "3708 [D loss: 0.570672, acc.: 68.75%] [G loss: 0.847040]\n",
      "3709 [D loss: 0.606530, acc.: 68.75%] [G loss: 0.838410]\n",
      "3710 [D loss: 0.637003, acc.: 68.75%] [G loss: 0.860635]\n",
      "3711 [D loss: 0.617573, acc.: 65.62%] [G loss: 0.838462]\n",
      "3712 [D loss: 0.649550, acc.: 62.50%] [G loss: 0.914763]\n",
      "3713 [D loss: 0.505299, acc.: 81.25%] [G loss: 0.904979]\n",
      "3714 [D loss: 0.671719, acc.: 53.12%] [G loss: 0.916667]\n",
      "3715 [D loss: 0.627006, acc.: 53.12%] [G loss: 0.887919]\n",
      "3716 [D loss: 0.701055, acc.: 59.38%] [G loss: 0.894935]\n",
      "3717 [D loss: 0.742503, acc.: 56.25%] [G loss: 0.877388]\n",
      "3718 [D loss: 0.647303, acc.: 68.75%] [G loss: 1.004689]\n",
      "3719 [D loss: 0.719960, acc.: 59.38%] [G loss: 0.900965]\n",
      "3720 [D loss: 0.646570, acc.: 56.25%] [G loss: 0.877564]\n",
      "3721 [D loss: 0.582812, acc.: 75.00%] [G loss: 0.865279]\n",
      "3722 [D loss: 0.714313, acc.: 56.25%] [G loss: 0.916269]\n",
      "3723 [D loss: 0.629409, acc.: 68.75%] [G loss: 0.897403]\n",
      "3724 [D loss: 0.647016, acc.: 65.62%] [G loss: 0.924295]\n",
      "3725 [D loss: 0.656065, acc.: 56.25%] [G loss: 0.985782]\n",
      "3726 [D loss: 0.695983, acc.: 43.75%] [G loss: 0.867974]\n",
      "3727 [D loss: 0.601834, acc.: 71.88%] [G loss: 0.825786]\n",
      "3728 [D loss: 0.603026, acc.: 56.25%] [G loss: 0.871745]\n",
      "3729 [D loss: 0.605751, acc.: 75.00%] [G loss: 0.945937]\n",
      "3730 [D loss: 0.623919, acc.: 65.62%] [G loss: 0.895132]\n",
      "3731 [D loss: 0.611191, acc.: 68.75%] [G loss: 0.980913]\n",
      "3732 [D loss: 0.568877, acc.: 75.00%] [G loss: 0.869649]\n",
      "3733 [D loss: 0.675164, acc.: 62.50%] [G loss: 0.932497]\n",
      "3734 [D loss: 0.663082, acc.: 62.50%] [G loss: 0.887378]\n",
      "3735 [D loss: 0.654438, acc.: 56.25%] [G loss: 0.927512]\n",
      "3736 [D loss: 0.637498, acc.: 68.75%] [G loss: 1.001343]\n",
      "3737 [D loss: 0.731026, acc.: 53.12%] [G loss: 0.854128]\n",
      "3738 [D loss: 0.575624, acc.: 71.88%] [G loss: 0.952537]\n",
      "3739 [D loss: 0.545479, acc.: 84.38%] [G loss: 0.853487]\n",
      "3740 [D loss: 0.645894, acc.: 65.62%] [G loss: 0.872175]\n",
      "3741 [D loss: 0.648569, acc.: 68.75%] [G loss: 0.905579]\n",
      "3742 [D loss: 0.614379, acc.: 59.38%] [G loss: 0.897028]\n",
      "3743 [D loss: 0.642909, acc.: 68.75%] [G loss: 0.999604]\n",
      "3744 [D loss: 0.649006, acc.: 65.62%] [G loss: 0.948801]\n",
      "3745 [D loss: 0.654850, acc.: 65.62%] [G loss: 0.925124]\n",
      "3746 [D loss: 0.671161, acc.: 50.00%] [G loss: 0.974036]\n",
      "3747 [D loss: 0.589274, acc.: 68.75%] [G loss: 0.917825]\n",
      "3748 [D loss: 0.540403, acc.: 75.00%] [G loss: 1.034058]\n",
      "3749 [D loss: 0.668460, acc.: 65.62%] [G loss: 0.946732]\n",
      "3750 [D loss: 0.703969, acc.: 56.25%] [G loss: 0.831727]\n",
      "3751 [D loss: 0.633928, acc.: 62.50%] [G loss: 0.913085]\n",
      "3752 [D loss: 0.606140, acc.: 68.75%] [G loss: 0.859185]\n",
      "3753 [D loss: 0.518523, acc.: 78.12%] [G loss: 0.935471]\n",
      "3754 [D loss: 0.598243, acc.: 62.50%] [G loss: 0.951221]\n",
      "3755 [D loss: 0.607166, acc.: 68.75%] [G loss: 1.001869]\n",
      "3756 [D loss: 0.601834, acc.: 65.62%] [G loss: 1.016784]\n",
      "3757 [D loss: 0.585320, acc.: 65.62%] [G loss: 0.999264]\n",
      "3758 [D loss: 0.610875, acc.: 68.75%] [G loss: 0.954477]\n",
      "3759 [D loss: 0.671992, acc.: 56.25%] [G loss: 0.966911]\n",
      "3760 [D loss: 0.603498, acc.: 68.75%] [G loss: 0.963108]\n",
      "3761 [D loss: 0.708421, acc.: 56.25%] [G loss: 1.020266]\n",
      "3762 [D loss: 0.651073, acc.: 59.38%] [G loss: 1.039975]\n",
      "3763 [D loss: 0.543393, acc.: 68.75%] [G loss: 1.054472]\n",
      "3764 [D loss: 0.623641, acc.: 68.75%] [G loss: 1.017959]\n",
      "3765 [D loss: 0.767775, acc.: 46.88%] [G loss: 0.905856]\n",
      "3766 [D loss: 0.577793, acc.: 75.00%] [G loss: 0.891885]\n",
      "3767 [D loss: 0.722006, acc.: 43.75%] [G loss: 0.921689]\n",
      "3768 [D loss: 0.663018, acc.: 56.25%] [G loss: 0.928896]\n",
      "3769 [D loss: 0.660952, acc.: 65.62%] [G loss: 0.925969]\n",
      "3770 [D loss: 0.693542, acc.: 53.12%] [G loss: 0.884301]\n",
      "3771 [D loss: 0.710218, acc.: 59.38%] [G loss: 0.898367]\n",
      "3772 [D loss: 0.585658, acc.: 75.00%] [G loss: 0.887713]\n",
      "3773 [D loss: 0.555321, acc.: 75.00%] [G loss: 0.929579]\n",
      "3774 [D loss: 0.689139, acc.: 59.38%] [G loss: 0.946854]\n",
      "3775 [D loss: 0.547498, acc.: 75.00%] [G loss: 0.916568]\n",
      "3776 [D loss: 0.648020, acc.: 62.50%] [G loss: 1.031487]\n",
      "3777 [D loss: 0.671030, acc.: 56.25%] [G loss: 0.968679]\n",
      "3778 [D loss: 0.687032, acc.: 62.50%] [G loss: 0.867734]\n",
      "3779 [D loss: 0.688391, acc.: 53.12%] [G loss: 0.839848]\n",
      "3780 [D loss: 0.656767, acc.: 62.50%] [G loss: 0.923494]\n",
      "3781 [D loss: 0.586831, acc.: 68.75%] [G loss: 0.936817]\n",
      "3782 [D loss: 0.563614, acc.: 65.62%] [G loss: 1.058705]\n",
      "3783 [D loss: 0.695167, acc.: 59.38%] [G loss: 0.925123]\n",
      "3784 [D loss: 0.613854, acc.: 68.75%] [G loss: 0.901102]\n",
      "3785 [D loss: 0.569240, acc.: 78.12%] [G loss: 0.948941]\n",
      "3786 [D loss: 0.580968, acc.: 71.88%] [G loss: 0.894248]\n",
      "3787 [D loss: 0.648208, acc.: 59.38%] [G loss: 0.860794]\n",
      "3788 [D loss: 0.631064, acc.: 71.88%] [G loss: 0.936419]\n",
      "3789 [D loss: 0.644326, acc.: 62.50%] [G loss: 0.896228]\n",
      "3790 [D loss: 0.718529, acc.: 43.75%] [G loss: 0.906761]\n",
      "3791 [D loss: 0.661052, acc.: 59.38%] [G loss: 0.924522]\n",
      "3792 [D loss: 0.641104, acc.: 62.50%] [G loss: 0.955664]\n",
      "3793 [D loss: 0.604632, acc.: 75.00%] [G loss: 0.876979]\n",
      "3794 [D loss: 0.574951, acc.: 68.75%] [G loss: 0.887431]\n",
      "3795 [D loss: 0.576979, acc.: 75.00%] [G loss: 0.834596]\n",
      "3796 [D loss: 0.628342, acc.: 62.50%] [G loss: 0.839945]\n",
      "3797 [D loss: 0.746051, acc.: 43.75%] [G loss: 0.837315]\n",
      "3798 [D loss: 0.570529, acc.: 81.25%] [G loss: 0.872843]\n",
      "3799 [D loss: 0.627486, acc.: 59.38%] [G loss: 0.876105]\n",
      "3800 [D loss: 0.600684, acc.: 71.88%] [G loss: 0.898481]\n",
      "3801 [D loss: 0.676731, acc.: 59.38%] [G loss: 0.833063]\n",
      "3802 [D loss: 0.638792, acc.: 65.62%] [G loss: 0.847413]\n",
      "3803 [D loss: 0.627800, acc.: 75.00%] [G loss: 0.841111]\n",
      "3804 [D loss: 0.639617, acc.: 62.50%] [G loss: 0.906349]\n",
      "3805 [D loss: 0.687917, acc.: 56.25%] [G loss: 0.837837]\n",
      "3806 [D loss: 0.654183, acc.: 71.88%] [G loss: 0.868092]\n",
      "3807 [D loss: 0.719536, acc.: 62.50%] [G loss: 0.948818]\n",
      "3808 [D loss: 0.591732, acc.: 71.88%] [G loss: 0.902618]\n",
      "3809 [D loss: 0.566528, acc.: 81.25%] [G loss: 0.930945]\n",
      "3810 [D loss: 0.498935, acc.: 84.38%] [G loss: 0.928519]\n",
      "3811 [D loss: 0.507527, acc.: 75.00%] [G loss: 0.916144]\n",
      "3812 [D loss: 0.745781, acc.: 53.12%] [G loss: 0.974776]\n",
      "3813 [D loss: 0.679761, acc.: 56.25%] [G loss: 0.869468]\n",
      "3814 [D loss: 0.590978, acc.: 78.12%] [G loss: 0.958341]\n",
      "3815 [D loss: 0.657700, acc.: 56.25%] [G loss: 0.906732]\n",
      "3816 [D loss: 0.676946, acc.: 53.12%] [G loss: 0.915963]\n",
      "3817 [D loss: 0.605250, acc.: 65.62%] [G loss: 0.894193]\n",
      "3818 [D loss: 0.692123, acc.: 53.12%] [G loss: 0.944305]\n",
      "3819 [D loss: 0.569118, acc.: 71.88%] [G loss: 0.898105]\n",
      "3820 [D loss: 0.606481, acc.: 68.75%] [G loss: 0.992792]\n",
      "3821 [D loss: 0.559421, acc.: 75.00%] [G loss: 0.870381]\n",
      "3822 [D loss: 0.631350, acc.: 65.62%] [G loss: 0.968200]\n",
      "3823 [D loss: 0.669235, acc.: 62.50%] [G loss: 0.904346]\n",
      "3824 [D loss: 0.733592, acc.: 53.12%] [G loss: 0.892845]\n",
      "3825 [D loss: 0.638206, acc.: 53.12%] [G loss: 0.844733]\n",
      "3826 [D loss: 0.627181, acc.: 71.88%] [G loss: 0.807638]\n",
      "3827 [D loss: 0.596760, acc.: 68.75%] [G loss: 0.955496]\n",
      "3828 [D loss: 0.733414, acc.: 53.12%] [G loss: 0.885617]\n",
      "3829 [D loss: 0.591226, acc.: 75.00%] [G loss: 0.974397]\n",
      "3830 [D loss: 0.600354, acc.: 65.62%] [G loss: 0.967761]\n",
      "3831 [D loss: 0.664953, acc.: 56.25%] [G loss: 0.937554]\n",
      "3832 [D loss: 0.633082, acc.: 68.75%] [G loss: 0.902619]\n",
      "3833 [D loss: 0.596150, acc.: 71.88%] [G loss: 0.842809]\n",
      "3834 [D loss: 0.697639, acc.: 50.00%] [G loss: 0.867844]\n",
      "3835 [D loss: 0.639255, acc.: 59.38%] [G loss: 0.886462]\n",
      "3836 [D loss: 0.532033, acc.: 81.25%] [G loss: 0.874359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3837 [D loss: 0.601775, acc.: 62.50%] [G loss: 0.893256]\n",
      "3838 [D loss: 0.522208, acc.: 78.12%] [G loss: 0.954397]\n",
      "3839 [D loss: 0.736635, acc.: 46.88%] [G loss: 0.886446]\n",
      "3840 [D loss: 0.574943, acc.: 78.12%] [G loss: 0.922121]\n",
      "3841 [D loss: 0.596241, acc.: 62.50%] [G loss: 1.013548]\n",
      "3842 [D loss: 0.678692, acc.: 53.12%] [G loss: 0.966513]\n",
      "3843 [D loss: 0.703858, acc.: 50.00%] [G loss: 0.974713]\n",
      "3844 [D loss: 0.651863, acc.: 56.25%] [G loss: 0.931978]\n",
      "3845 [D loss: 0.561182, acc.: 81.25%] [G loss: 0.936365]\n",
      "3846 [D loss: 0.786731, acc.: 53.12%] [G loss: 0.897328]\n",
      "3847 [D loss: 0.647981, acc.: 62.50%] [G loss: 0.946986]\n",
      "3848 [D loss: 0.551237, acc.: 78.12%] [G loss: 1.028349]\n",
      "3849 [D loss: 0.728986, acc.: 56.25%] [G loss: 0.898540]\n",
      "3850 [D loss: 0.726257, acc.: 50.00%] [G loss: 0.932245]\n",
      "3851 [D loss: 0.598997, acc.: 75.00%] [G loss: 0.898942]\n",
      "3852 [D loss: 0.641137, acc.: 53.12%] [G loss: 0.858409]\n",
      "3853 [D loss: 0.715271, acc.: 50.00%] [G loss: 0.846148]\n",
      "3854 [D loss: 0.605452, acc.: 65.62%] [G loss: 0.885499]\n",
      "3855 [D loss: 0.672542, acc.: 62.50%] [G loss: 0.898471]\n",
      "3856 [D loss: 0.656969, acc.: 50.00%] [G loss: 0.942037]\n",
      "3857 [D loss: 0.629481, acc.: 75.00%] [G loss: 0.904613]\n",
      "3858 [D loss: 0.756462, acc.: 53.12%] [G loss: 0.859007]\n",
      "3859 [D loss: 0.541315, acc.: 81.25%] [G loss: 0.888127]\n",
      "3860 [D loss: 0.656239, acc.: 59.38%] [G loss: 0.853567]\n",
      "3861 [D loss: 0.588557, acc.: 75.00%] [G loss: 0.964315]\n",
      "3862 [D loss: 0.568685, acc.: 84.38%] [G loss: 0.980173]\n",
      "3863 [D loss: 0.566178, acc.: 75.00%] [G loss: 0.923574]\n",
      "3864 [D loss: 0.581262, acc.: 62.50%] [G loss: 1.028159]\n",
      "3865 [D loss: 0.658371, acc.: 62.50%] [G loss: 0.845812]\n",
      "3866 [D loss: 0.596712, acc.: 68.75%] [G loss: 0.864580]\n",
      "3867 [D loss: 0.649486, acc.: 75.00%] [G loss: 0.877686]\n",
      "3868 [D loss: 0.613497, acc.: 68.75%] [G loss: 0.873911]\n",
      "3869 [D loss: 0.642537, acc.: 62.50%] [G loss: 0.807235]\n",
      "3870 [D loss: 0.763860, acc.: 56.25%] [G loss: 0.821833]\n",
      "3871 [D loss: 0.685219, acc.: 46.88%] [G loss: 0.863155]\n",
      "3872 [D loss: 0.620283, acc.: 71.88%] [G loss: 0.905647]\n",
      "3873 [D loss: 0.558888, acc.: 75.00%] [G loss: 0.881952]\n",
      "3874 [D loss: 0.655155, acc.: 65.62%] [G loss: 0.925774]\n",
      "3875 [D loss: 0.621040, acc.: 65.62%] [G loss: 0.974011]\n",
      "3876 [D loss: 0.677596, acc.: 59.38%] [G loss: 0.888528]\n",
      "3877 [D loss: 0.654509, acc.: 62.50%] [G loss: 0.868359]\n",
      "3878 [D loss: 0.614969, acc.: 65.62%] [G loss: 0.981260]\n",
      "3879 [D loss: 0.570392, acc.: 71.88%] [G loss: 1.063428]\n",
      "3880 [D loss: 0.776287, acc.: 56.25%] [G loss: 0.890266]\n",
      "3881 [D loss: 0.571088, acc.: 71.88%] [G loss: 0.861787]\n",
      "3882 [D loss: 0.585521, acc.: 65.62%] [G loss: 0.912776]\n",
      "3883 [D loss: 0.632806, acc.: 65.62%] [G loss: 0.953669]\n",
      "3884 [D loss: 0.606742, acc.: 78.12%] [G loss: 0.932099]\n",
      "3885 [D loss: 0.591323, acc.: 68.75%] [G loss: 0.862890]\n",
      "3886 [D loss: 0.650876, acc.: 65.62%] [G loss: 0.917072]\n",
      "3887 [D loss: 0.605476, acc.: 78.12%] [G loss: 0.855148]\n",
      "3888 [D loss: 0.601202, acc.: 62.50%] [G loss: 0.901629]\n",
      "3889 [D loss: 0.535530, acc.: 78.12%] [G loss: 0.963142]\n",
      "3890 [D loss: 0.674651, acc.: 65.62%] [G loss: 0.989762]\n",
      "3891 [D loss: 0.703245, acc.: 56.25%] [G loss: 0.924649]\n",
      "3892 [D loss: 0.687282, acc.: 56.25%] [G loss: 0.928907]\n",
      "3893 [D loss: 0.602136, acc.: 71.88%] [G loss: 0.928217]\n",
      "3894 [D loss: 0.610558, acc.: 68.75%] [G loss: 0.920160]\n",
      "3895 [D loss: 0.664220, acc.: 50.00%] [G loss: 0.909190]\n",
      "3896 [D loss: 0.627809, acc.: 68.75%] [G loss: 0.916340]\n",
      "3897 [D loss: 0.658898, acc.: 65.62%] [G loss: 0.857599]\n",
      "3898 [D loss: 0.687645, acc.: 53.12%] [G loss: 0.926881]\n",
      "3899 [D loss: 0.654782, acc.: 59.38%] [G loss: 0.908233]\n",
      "3900 [D loss: 0.608204, acc.: 62.50%] [G loss: 0.920899]\n",
      "3901 [D loss: 0.725399, acc.: 46.88%] [G loss: 0.884637]\n",
      "3902 [D loss: 0.590845, acc.: 65.62%] [G loss: 0.877006]\n",
      "3903 [D loss: 0.572684, acc.: 62.50%] [G loss: 0.838561]\n",
      "3904 [D loss: 0.689041, acc.: 50.00%] [G loss: 0.958971]\n",
      "3905 [D loss: 0.752940, acc.: 43.75%] [G loss: 0.942199]\n",
      "3906 [D loss: 0.605432, acc.: 65.62%] [G loss: 0.913869]\n",
      "3907 [D loss: 0.752392, acc.: 43.75%] [G loss: 0.907522]\n",
      "3908 [D loss: 0.613623, acc.: 68.75%] [G loss: 0.935473]\n",
      "3909 [D loss: 0.611139, acc.: 75.00%] [G loss: 0.937477]\n",
      "3910 [D loss: 0.647159, acc.: 59.38%] [G loss: 0.862589]\n",
      "3911 [D loss: 0.621973, acc.: 68.75%] [G loss: 0.803622]\n",
      "3912 [D loss: 0.581570, acc.: 81.25%] [G loss: 0.957135]\n",
      "3913 [D loss: 0.560560, acc.: 62.50%] [G loss: 0.923300]\n",
      "3914 [D loss: 0.678557, acc.: 56.25%] [G loss: 0.874291]\n",
      "3915 [D loss: 0.573674, acc.: 68.75%] [G loss: 0.838484]\n",
      "3916 [D loss: 0.590116, acc.: 65.62%] [G loss: 0.816616]\n",
      "3917 [D loss: 0.666865, acc.: 59.38%] [G loss: 0.835270]\n",
      "3918 [D loss: 0.604712, acc.: 75.00%] [G loss: 0.849077]\n",
      "3919 [D loss: 0.642210, acc.: 71.88%] [G loss: 0.867693]\n",
      "3920 [D loss: 0.635688, acc.: 56.25%] [G loss: 0.863665]\n",
      "3921 [D loss: 0.746564, acc.: 43.75%] [G loss: 0.846997]\n",
      "3922 [D loss: 0.601035, acc.: 75.00%] [G loss: 0.928264]\n",
      "3923 [D loss: 0.711006, acc.: 53.12%] [G loss: 0.943174]\n",
      "3924 [D loss: 0.609779, acc.: 71.88%] [G loss: 0.957245]\n",
      "3925 [D loss: 0.630938, acc.: 71.88%] [G loss: 0.886378]\n",
      "3926 [D loss: 0.554288, acc.: 71.88%] [G loss: 0.901689]\n",
      "3927 [D loss: 0.647206, acc.: 62.50%] [G loss: 0.801090]\n",
      "3928 [D loss: 0.815119, acc.: 34.38%] [G loss: 0.813061]\n",
      "3929 [D loss: 0.600309, acc.: 75.00%] [G loss: 0.920743]\n",
      "3930 [D loss: 0.598661, acc.: 71.88%] [G loss: 0.970408]\n",
      "3931 [D loss: 0.617398, acc.: 68.75%] [G loss: 0.890985]\n",
      "3932 [D loss: 0.667580, acc.: 59.38%] [G loss: 0.914536]\n",
      "3933 [D loss: 0.557206, acc.: 78.12%] [G loss: 0.930269]\n",
      "3934 [D loss: 0.613693, acc.: 68.75%] [G loss: 0.969006]\n",
      "3935 [D loss: 0.689918, acc.: 56.25%] [G loss: 0.938146]\n",
      "3936 [D loss: 0.571762, acc.: 78.12%] [G loss: 0.967617]\n",
      "3937 [D loss: 0.598850, acc.: 68.75%] [G loss: 0.960769]\n",
      "3938 [D loss: 0.608555, acc.: 68.75%] [G loss: 0.968075]\n",
      "3939 [D loss: 0.474321, acc.: 87.50%] [G loss: 0.934927]\n",
      "3940 [D loss: 0.634966, acc.: 59.38%] [G loss: 0.934657]\n",
      "3941 [D loss: 0.714828, acc.: 43.75%] [G loss: 0.908663]\n",
      "3942 [D loss: 0.670156, acc.: 62.50%] [G loss: 0.890331]\n",
      "3943 [D loss: 0.578468, acc.: 75.00%] [G loss: 0.923457]\n",
      "3944 [D loss: 0.601663, acc.: 65.62%] [G loss: 0.874882]\n",
      "3945 [D loss: 0.593921, acc.: 62.50%] [G loss: 0.972340]\n",
      "3946 [D loss: 0.610820, acc.: 68.75%] [G loss: 0.930298]\n",
      "3947 [D loss: 0.759359, acc.: 53.12%] [G loss: 0.873575]\n",
      "3948 [D loss: 0.589534, acc.: 68.75%] [G loss: 0.909180]\n",
      "3949 [D loss: 0.631522, acc.: 56.25%] [G loss: 0.893213]\n",
      "3950 [D loss: 0.705930, acc.: 43.75%] [G loss: 0.875136]\n",
      "3951 [D loss: 0.588829, acc.: 65.62%] [G loss: 0.909668]\n",
      "3952 [D loss: 0.579132, acc.: 78.12%] [G loss: 0.939152]\n",
      "3953 [D loss: 0.722685, acc.: 46.88%] [G loss: 0.852687]\n",
      "3954 [D loss: 0.596832, acc.: 71.88%] [G loss: 0.864569]\n",
      "3955 [D loss: 0.597705, acc.: 68.75%] [G loss: 0.927834]\n",
      "3956 [D loss: 0.516777, acc.: 78.12%] [G loss: 0.888965]\n",
      "3957 [D loss: 0.685128, acc.: 68.75%] [G loss: 0.800732]\n",
      "3958 [D loss: 0.658720, acc.: 65.62%] [G loss: 0.866365]\n",
      "3959 [D loss: 0.672751, acc.: 59.38%] [G loss: 0.966294]\n",
      "3960 [D loss: 0.721106, acc.: 46.88%] [G loss: 0.940319]\n",
      "3961 [D loss: 0.670916, acc.: 53.12%] [G loss: 0.944711]\n",
      "3962 [D loss: 0.636696, acc.: 62.50%] [G loss: 0.978894]\n",
      "3963 [D loss: 0.568293, acc.: 68.75%] [G loss: 0.950946]\n",
      "3964 [D loss: 0.693309, acc.: 59.38%] [G loss: 1.028464]\n",
      "3965 [D loss: 0.509276, acc.: 84.38%] [G loss: 1.062513]\n",
      "3966 [D loss: 0.750084, acc.: 50.00%] [G loss: 0.875902]\n",
      "3967 [D loss: 0.557340, acc.: 78.12%] [G loss: 0.901048]\n",
      "3968 [D loss: 0.694404, acc.: 53.12%] [G loss: 0.924967]\n",
      "3969 [D loss: 0.634864, acc.: 62.50%] [G loss: 0.835550]\n",
      "3970 [D loss: 0.696900, acc.: 46.88%] [G loss: 0.985630]\n",
      "3971 [D loss: 0.614968, acc.: 71.88%] [G loss: 0.929356]\n",
      "3972 [D loss: 0.575170, acc.: 75.00%] [G loss: 0.958978]\n",
      "3973 [D loss: 0.676400, acc.: 56.25%] [G loss: 0.963390]\n",
      "3974 [D loss: 0.605685, acc.: 68.75%] [G loss: 0.979693]\n",
      "3975 [D loss: 0.528655, acc.: 81.25%] [G loss: 0.973312]\n",
      "3976 [D loss: 0.645413, acc.: 71.88%] [G loss: 0.980479]\n",
      "3977 [D loss: 0.631372, acc.: 75.00%] [G loss: 0.938441]\n",
      "3978 [D loss: 0.795077, acc.: 31.25%] [G loss: 0.904801]\n",
      "3979 [D loss: 0.619133, acc.: 62.50%] [G loss: 0.914134]\n",
      "3980 [D loss: 0.711455, acc.: 50.00%] [G loss: 0.901620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3981 [D loss: 0.545189, acc.: 78.12%] [G loss: 1.019690]\n",
      "3982 [D loss: 0.632294, acc.: 59.38%] [G loss: 0.980104]\n",
      "3983 [D loss: 0.602316, acc.: 62.50%] [G loss: 1.010729]\n",
      "3984 [D loss: 0.635533, acc.: 71.88%] [G loss: 0.935210]\n",
      "3985 [D loss: 0.665924, acc.: 56.25%] [G loss: 0.890423]\n",
      "3986 [D loss: 0.608710, acc.: 68.75%] [G loss: 0.954508]\n",
      "3987 [D loss: 0.679442, acc.: 59.38%] [G loss: 0.850460]\n",
      "3988 [D loss: 0.600970, acc.: 68.75%] [G loss: 0.914739]\n",
      "3989 [D loss: 0.550278, acc.: 75.00%] [G loss: 0.987166]\n",
      "3990 [D loss: 0.576418, acc.: 65.62%] [G loss: 0.912907]\n",
      "3991 [D loss: 0.572906, acc.: 62.50%] [G loss: 0.818457]\n",
      "3992 [D loss: 0.742238, acc.: 50.00%] [G loss: 0.899138]\n",
      "3993 [D loss: 0.595674, acc.: 78.12%] [G loss: 0.957325]\n",
      "3994 [D loss: 0.631695, acc.: 65.62%] [G loss: 0.961903]\n",
      "3995 [D loss: 0.739029, acc.: 53.12%] [G loss: 0.961581]\n",
      "3996 [D loss: 0.652642, acc.: 65.62%] [G loss: 0.913794]\n",
      "3997 [D loss: 0.681628, acc.: 43.75%] [G loss: 0.937282]\n",
      "3998 [D loss: 0.679256, acc.: 56.25%] [G loss: 0.843359]\n",
      "3999 [D loss: 0.597935, acc.: 62.50%] [G loss: 0.894200]\n",
      "4000 [D loss: 0.616664, acc.: 68.75%] [G loss: 0.982044]\n",
      "4001 [D loss: 0.636108, acc.: 59.38%] [G loss: 0.929753]\n",
      "4002 [D loss: 0.608919, acc.: 65.62%] [G loss: 0.904529]\n",
      "4003 [D loss: 0.581096, acc.: 75.00%] [G loss: 0.923766]\n",
      "4004 [D loss: 0.645289, acc.: 65.62%] [G loss: 0.884553]\n",
      "4005 [D loss: 0.768190, acc.: 40.62%] [G loss: 0.864889]\n",
      "4006 [D loss: 0.668525, acc.: 59.38%] [G loss: 0.971806]\n",
      "4007 [D loss: 0.722372, acc.: 50.00%] [G loss: 0.953452]\n",
      "4008 [D loss: 0.624797, acc.: 59.38%] [G loss: 0.880639]\n",
      "4009 [D loss: 0.653221, acc.: 62.50%] [G loss: 0.919324]\n",
      "4010 [D loss: 0.527642, acc.: 87.50%] [G loss: 1.025778]\n",
      "4011 [D loss: 0.719370, acc.: 56.25%] [G loss: 0.925389]\n",
      "4012 [D loss: 0.733304, acc.: 50.00%] [G loss: 0.893231]\n",
      "4013 [D loss: 0.709380, acc.: 56.25%] [G loss: 0.851407]\n",
      "4014 [D loss: 0.691913, acc.: 53.12%] [G loss: 0.947032]\n",
      "4015 [D loss: 0.650297, acc.: 56.25%] [G loss: 0.916487]\n",
      "4016 [D loss: 0.616751, acc.: 65.62%] [G loss: 0.896861]\n",
      "4017 [D loss: 0.536904, acc.: 78.12%] [G loss: 0.932158]\n",
      "4018 [D loss: 0.541198, acc.: 75.00%] [G loss: 0.954316]\n",
      "4019 [D loss: 0.573376, acc.: 75.00%] [G loss: 0.877114]\n",
      "4020 [D loss: 0.695127, acc.: 53.12%] [G loss: 0.910746]\n",
      "4021 [D loss: 0.636772, acc.: 65.62%] [G loss: 0.861394]\n",
      "4022 [D loss: 0.676254, acc.: 59.38%] [G loss: 0.850635]\n",
      "4023 [D loss: 0.642681, acc.: 65.62%] [G loss: 0.957441]\n",
      "4024 [D loss: 0.676874, acc.: 56.25%] [G loss: 0.987484]\n",
      "4025 [D loss: 0.638014, acc.: 59.38%] [G loss: 0.824969]\n",
      "4026 [D loss: 0.601363, acc.: 75.00%] [G loss: 0.918343]\n",
      "4027 [D loss: 0.576643, acc.: 81.25%] [G loss: 0.875719]\n",
      "4028 [D loss: 0.601102, acc.: 75.00%] [G loss: 0.916721]\n",
      "4029 [D loss: 0.605413, acc.: 71.88%] [G loss: 0.955022]\n",
      "4030 [D loss: 0.678025, acc.: 53.12%] [G loss: 0.868336]\n",
      "4031 [D loss: 0.622097, acc.: 81.25%] [G loss: 0.939584]\n",
      "4032 [D loss: 0.601267, acc.: 71.88%] [G loss: 1.009990]\n",
      "4033 [D loss: 0.594756, acc.: 71.88%] [G loss: 0.887642]\n",
      "4034 [D loss: 0.677376, acc.: 50.00%] [G loss: 0.910173]\n",
      "4035 [D loss: 0.573526, acc.: 68.75%] [G loss: 0.899559]\n",
      "4036 [D loss: 0.724618, acc.: 56.25%] [G loss: 0.900374]\n",
      "4037 [D loss: 0.697219, acc.: 50.00%] [G loss: 0.806483]\n",
      "4038 [D loss: 0.601579, acc.: 71.88%] [G loss: 0.885787]\n",
      "4039 [D loss: 0.632923, acc.: 56.25%] [G loss: 0.914082]\n",
      "4040 [D loss: 0.713866, acc.: 46.88%] [G loss: 0.840938]\n",
      "4041 [D loss: 0.633917, acc.: 62.50%] [G loss: 0.753763]\n",
      "4042 [D loss: 0.591568, acc.: 75.00%] [G loss: 0.859430]\n",
      "4043 [D loss: 0.639994, acc.: 65.62%] [G loss: 0.890295]\n",
      "4044 [D loss: 0.684178, acc.: 56.25%] [G loss: 0.806046]\n",
      "4045 [D loss: 0.669460, acc.: 62.50%] [G loss: 0.834978]\n",
      "4046 [D loss: 0.607023, acc.: 65.62%] [G loss: 0.872085]\n",
      "4047 [D loss: 0.680832, acc.: 65.62%] [G loss: 0.892951]\n",
      "4048 [D loss: 0.614168, acc.: 68.75%] [G loss: 0.858579]\n",
      "4049 [D loss: 0.640489, acc.: 71.88%] [G loss: 0.889273]\n",
      "4050 [D loss: 0.580938, acc.: 65.62%] [G loss: 0.909130]\n",
      "4051 [D loss: 0.669965, acc.: 62.50%] [G loss: 0.924095]\n",
      "4052 [D loss: 0.727718, acc.: 46.88%] [G loss: 0.863902]\n",
      "4053 [D loss: 0.580550, acc.: 71.88%] [G loss: 0.917141]\n",
      "4054 [D loss: 0.637702, acc.: 65.62%] [G loss: 0.895472]\n",
      "4055 [D loss: 0.679302, acc.: 56.25%] [G loss: 0.948117]\n",
      "4056 [D loss: 0.729806, acc.: 56.25%] [G loss: 0.928618]\n",
      "4057 [D loss: 0.649515, acc.: 65.62%] [G loss: 0.900608]\n",
      "4058 [D loss: 0.681906, acc.: 53.12%] [G loss: 0.894820]\n",
      "4059 [D loss: 0.590280, acc.: 68.75%] [G loss: 0.988286]\n",
      "4060 [D loss: 0.630142, acc.: 62.50%] [G loss: 0.917274]\n",
      "4061 [D loss: 0.629340, acc.: 71.88%] [G loss: 0.940411]\n",
      "4062 [D loss: 0.686020, acc.: 62.50%] [G loss: 0.918090]\n",
      "4063 [D loss: 0.627446, acc.: 62.50%] [G loss: 0.917400]\n",
      "4064 [D loss: 0.653877, acc.: 65.62%] [G loss: 0.957097]\n",
      "4065 [D loss: 0.707790, acc.: 59.38%] [G loss: 0.838509]\n",
      "4066 [D loss: 0.671649, acc.: 56.25%] [G loss: 0.907276]\n",
      "4067 [D loss: 0.594618, acc.: 68.75%] [G loss: 0.845014]\n",
      "4068 [D loss: 0.593263, acc.: 71.88%] [G loss: 0.851810]\n",
      "4069 [D loss: 0.652428, acc.: 68.75%] [G loss: 0.888575]\n",
      "4070 [D loss: 0.703921, acc.: 50.00%] [G loss: 0.927387]\n",
      "4071 [D loss: 0.589158, acc.: 78.12%] [G loss: 0.987672]\n",
      "4072 [D loss: 0.624289, acc.: 62.50%] [G loss: 0.962081]\n",
      "4073 [D loss: 0.536631, acc.: 65.62%] [G loss: 0.942667]\n",
      "4074 [D loss: 0.633697, acc.: 62.50%] [G loss: 0.869678]\n",
      "4075 [D loss: 0.719853, acc.: 50.00%] [G loss: 1.085741]\n",
      "4076 [D loss: 0.596916, acc.: 71.88%] [G loss: 1.029504]\n",
      "4077 [D loss: 0.557920, acc.: 81.25%] [G loss: 1.032098]\n",
      "4078 [D loss: 0.679759, acc.: 59.38%] [G loss: 0.944600]\n",
      "4079 [D loss: 0.664298, acc.: 59.38%] [G loss: 0.922552]\n",
      "4080 [D loss: 0.613078, acc.: 68.75%] [G loss: 0.926214]\n",
      "4081 [D loss: 0.672034, acc.: 56.25%] [G loss: 0.973279]\n",
      "4082 [D loss: 0.555807, acc.: 75.00%] [G loss: 0.890195]\n",
      "4083 [D loss: 0.621752, acc.: 75.00%] [G loss: 0.956750]\n",
      "4084 [D loss: 0.578907, acc.: 65.62%] [G loss: 0.930629]\n",
      "4085 [D loss: 0.594070, acc.: 65.62%] [G loss: 0.919114]\n",
      "4086 [D loss: 0.605963, acc.: 65.62%] [G loss: 0.964345]\n",
      "4087 [D loss: 0.612034, acc.: 65.62%] [G loss: 0.900617]\n",
      "4088 [D loss: 0.600490, acc.: 68.75%] [G loss: 0.930150]\n",
      "4089 [D loss: 0.520057, acc.: 68.75%] [G loss: 0.935692]\n",
      "4090 [D loss: 0.689537, acc.: 46.88%] [G loss: 0.908567]\n",
      "4091 [D loss: 0.559038, acc.: 78.12%] [G loss: 0.853467]\n",
      "4092 [D loss: 0.728572, acc.: 53.12%] [G loss: 0.846261]\n",
      "4093 [D loss: 0.678982, acc.: 46.88%] [G loss: 0.894158]\n",
      "4094 [D loss: 0.639743, acc.: 65.62%] [G loss: 0.884641]\n",
      "4095 [D loss: 0.622304, acc.: 50.00%] [G loss: 0.914223]\n",
      "4096 [D loss: 0.649476, acc.: 65.62%] [G loss: 0.925696]\n",
      "4097 [D loss: 0.622815, acc.: 71.88%] [G loss: 0.883647]\n",
      "4098 [D loss: 0.579115, acc.: 75.00%] [G loss: 0.994546]\n",
      "4099 [D loss: 0.549681, acc.: 68.75%] [G loss: 0.979770]\n",
      "4100 [D loss: 0.616853, acc.: 68.75%] [G loss: 0.959681]\n",
      "4101 [D loss: 0.586132, acc.: 75.00%] [G loss: 0.905625]\n",
      "4102 [D loss: 0.720703, acc.: 43.75%] [G loss: 1.015731]\n",
      "4103 [D loss: 0.746418, acc.: 46.88%] [G loss: 0.937965]\n",
      "4104 [D loss: 0.638330, acc.: 68.75%] [G loss: 0.936887]\n",
      "4105 [D loss: 0.688957, acc.: 59.38%] [G loss: 0.859858]\n",
      "4106 [D loss: 0.647506, acc.: 65.62%] [G loss: 0.928156]\n",
      "4107 [D loss: 0.636561, acc.: 53.12%] [G loss: 0.913886]\n",
      "4108 [D loss: 0.640861, acc.: 71.88%] [G loss: 0.912607]\n",
      "4109 [D loss: 0.619425, acc.: 68.75%] [G loss: 0.948395]\n",
      "4110 [D loss: 0.666190, acc.: 59.38%] [G loss: 0.880170]\n",
      "4111 [D loss: 0.671178, acc.: 62.50%] [G loss: 0.878671]\n",
      "4112 [D loss: 0.681398, acc.: 65.62%] [G loss: 0.872895]\n",
      "4113 [D loss: 0.723853, acc.: 65.62%] [G loss: 0.846190]\n",
      "4114 [D loss: 0.606175, acc.: 71.88%] [G loss: 0.942268]\n",
      "4115 [D loss: 0.713768, acc.: 50.00%] [G loss: 0.786023]\n",
      "4116 [D loss: 0.621862, acc.: 56.25%] [G loss: 0.779952]\n",
      "4117 [D loss: 0.533199, acc.: 78.12%] [G loss: 0.857146]\n",
      "4118 [D loss: 0.750144, acc.: 50.00%] [G loss: 0.925306]\n",
      "4119 [D loss: 0.627492, acc.: 62.50%] [G loss: 0.931786]\n",
      "4120 [D loss: 0.619026, acc.: 65.62%] [G loss: 0.843857]\n",
      "4121 [D loss: 0.539820, acc.: 78.12%] [G loss: 0.826615]\n",
      "4122 [D loss: 0.597308, acc.: 75.00%] [G loss: 0.912809]\n",
      "4123 [D loss: 0.789540, acc.: 43.75%] [G loss: 0.918957]\n",
      "4124 [D loss: 0.670685, acc.: 46.88%] [G loss: 0.763562]\n",
      "4125 [D loss: 0.694884, acc.: 56.25%] [G loss: 0.923151]\n",
      "4126 [D loss: 0.564200, acc.: 81.25%] [G loss: 0.890185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4127 [D loss: 0.777227, acc.: 56.25%] [G loss: 0.910884]\n",
      "4128 [D loss: 0.653250, acc.: 59.38%] [G loss: 1.006362]\n",
      "4129 [D loss: 0.642503, acc.: 59.38%] [G loss: 0.923497]\n",
      "4130 [D loss: 0.577199, acc.: 71.88%] [G loss: 0.983931]\n",
      "4131 [D loss: 0.692070, acc.: 65.62%] [G loss: 0.973853]\n",
      "4132 [D loss: 0.641986, acc.: 65.62%] [G loss: 0.940412]\n",
      "4133 [D loss: 0.679714, acc.: 65.62%] [G loss: 0.919921]\n",
      "4134 [D loss: 0.630332, acc.: 68.75%] [G loss: 0.860000]\n",
      "4135 [D loss: 0.708561, acc.: 56.25%] [G loss: 0.836214]\n",
      "4136 [D loss: 0.699554, acc.: 68.75%] [G loss: 0.833402]\n",
      "4137 [D loss: 0.596660, acc.: 68.75%] [G loss: 0.904345]\n",
      "4138 [D loss: 0.630406, acc.: 68.75%] [G loss: 0.889419]\n",
      "4139 [D loss: 0.683368, acc.: 56.25%] [G loss: 0.839975]\n",
      "4140 [D loss: 0.566851, acc.: 65.62%] [G loss: 0.895464]\n",
      "4141 [D loss: 0.668741, acc.: 53.12%] [G loss: 0.931122]\n",
      "4142 [D loss: 0.598166, acc.: 68.75%] [G loss: 0.975520]\n",
      "4143 [D loss: 0.660033, acc.: 53.12%] [G loss: 0.926425]\n",
      "4144 [D loss: 0.567611, acc.: 75.00%] [G loss: 0.996909]\n",
      "4145 [D loss: 0.744034, acc.: 46.88%] [G loss: 0.931453]\n",
      "4146 [D loss: 0.592333, acc.: 68.75%] [G loss: 0.901034]\n",
      "4147 [D loss: 0.587566, acc.: 81.25%] [G loss: 0.893051]\n",
      "4148 [D loss: 0.629514, acc.: 65.62%] [G loss: 0.938623]\n",
      "4149 [D loss: 0.604311, acc.: 75.00%] [G loss: 0.946532]\n",
      "4150 [D loss: 0.598854, acc.: 71.88%] [G loss: 1.102228]\n",
      "4151 [D loss: 0.695754, acc.: 59.38%] [G loss: 0.978130]\n",
      "4152 [D loss: 0.688807, acc.: 53.12%] [G loss: 0.976567]\n",
      "4153 [D loss: 0.584628, acc.: 75.00%] [G loss: 0.901248]\n",
      "4154 [D loss: 0.695470, acc.: 50.00%] [G loss: 0.859671]\n",
      "4155 [D loss: 0.693693, acc.: 56.25%] [G loss: 0.866098]\n",
      "4156 [D loss: 0.657157, acc.: 62.50%] [G loss: 0.945643]\n",
      "4157 [D loss: 0.664916, acc.: 53.12%] [G loss: 0.867360]\n",
      "4158 [D loss: 0.662309, acc.: 62.50%] [G loss: 0.814584]\n",
      "4159 [D loss: 0.618987, acc.: 71.88%] [G loss: 0.970089]\n",
      "4160 [D loss: 0.621318, acc.: 71.88%] [G loss: 0.869570]\n",
      "4161 [D loss: 0.668435, acc.: 59.38%] [G loss: 0.996476]\n",
      "4162 [D loss: 0.649769, acc.: 62.50%] [G loss: 0.967793]\n",
      "4163 [D loss: 0.653560, acc.: 68.75%] [G loss: 0.869611]\n",
      "4164 [D loss: 0.678164, acc.: 62.50%] [G loss: 0.833201]\n",
      "4165 [D loss: 0.598906, acc.: 75.00%] [G loss: 0.885803]\n",
      "4166 [D loss: 0.609501, acc.: 65.62%] [G loss: 0.926811]\n",
      "4167 [D loss: 0.682737, acc.: 53.12%] [G loss: 0.904797]\n",
      "4168 [D loss: 0.662764, acc.: 59.38%] [G loss: 0.885424]\n",
      "4169 [D loss: 0.596466, acc.: 71.88%] [G loss: 0.916906]\n",
      "4170 [D loss: 0.655507, acc.: 59.38%] [G loss: 0.831014]\n",
      "4171 [D loss: 0.610990, acc.: 59.38%] [G loss: 0.894786]\n",
      "4172 [D loss: 0.625477, acc.: 62.50%] [G loss: 0.981396]\n",
      "4173 [D loss: 0.701705, acc.: 53.12%] [G loss: 0.898823]\n",
      "4174 [D loss: 0.576690, acc.: 71.88%] [G loss: 0.899303]\n",
      "4175 [D loss: 0.751148, acc.: 43.75%] [G loss: 0.930796]\n",
      "4176 [D loss: 0.600451, acc.: 71.88%] [G loss: 0.913427]\n",
      "4177 [D loss: 0.744031, acc.: 53.12%] [G loss: 0.911423]\n",
      "4178 [D loss: 0.610180, acc.: 59.38%] [G loss: 1.020409]\n",
      "4179 [D loss: 0.579295, acc.: 71.88%] [G loss: 0.883397]\n",
      "4180 [D loss: 0.595891, acc.: 68.75%] [G loss: 0.930815]\n",
      "4181 [D loss: 0.602570, acc.: 71.88%] [G loss: 0.849068]\n",
      "4182 [D loss: 0.654015, acc.: 68.75%] [G loss: 0.881757]\n",
      "4183 [D loss: 0.634634, acc.: 65.62%] [G loss: 0.935421]\n",
      "4184 [D loss: 0.618325, acc.: 68.75%] [G loss: 0.896387]\n",
      "4185 [D loss: 0.635129, acc.: 65.62%] [G loss: 0.886916]\n",
      "4186 [D loss: 0.677732, acc.: 62.50%] [G loss: 0.987435]\n",
      "4187 [D loss: 0.724153, acc.: 50.00%] [G loss: 0.904019]\n",
      "4188 [D loss: 0.738115, acc.: 46.88%] [G loss: 0.835415]\n",
      "4189 [D loss: 0.664700, acc.: 59.38%] [G loss: 0.813336]\n",
      "4190 [D loss: 0.608726, acc.: 68.75%] [G loss: 0.875693]\n",
      "4191 [D loss: 0.559803, acc.: 78.12%] [G loss: 0.848536]\n",
      "4192 [D loss: 0.654406, acc.: 50.00%] [G loss: 0.878843]\n",
      "4193 [D loss: 0.594100, acc.: 65.62%] [G loss: 0.938305]\n",
      "4194 [D loss: 0.586213, acc.: 65.62%] [G loss: 1.017693]\n",
      "4195 [D loss: 0.566292, acc.: 68.75%] [G loss: 0.922989]\n",
      "4196 [D loss: 0.680999, acc.: 68.75%] [G loss: 0.856244]\n",
      "4197 [D loss: 0.598190, acc.: 78.12%] [G loss: 0.845485]\n",
      "4198 [D loss: 0.770985, acc.: 50.00%] [G loss: 0.874045]\n",
      "4199 [D loss: 0.596600, acc.: 65.62%] [G loss: 1.015092]\n",
      "4200 [D loss: 0.521321, acc.: 84.38%] [G loss: 1.005217]\n",
      "4201 [D loss: 0.611717, acc.: 68.75%] [G loss: 0.872539]\n",
      "4202 [D loss: 0.755897, acc.: 53.12%] [G loss: 0.756482]\n",
      "4203 [D loss: 0.660989, acc.: 68.75%] [G loss: 0.779141]\n",
      "4204 [D loss: 0.652171, acc.: 62.50%] [G loss: 0.772267]\n",
      "4205 [D loss: 0.647236, acc.: 59.38%] [G loss: 0.852147]\n",
      "4206 [D loss: 0.678098, acc.: 62.50%] [G loss: 0.843579]\n",
      "4207 [D loss: 0.538817, acc.: 68.75%] [G loss: 0.972601]\n",
      "4208 [D loss: 0.654619, acc.: 62.50%] [G loss: 0.946945]\n",
      "4209 [D loss: 0.618128, acc.: 62.50%] [G loss: 0.896363]\n",
      "4210 [D loss: 0.643411, acc.: 56.25%] [G loss: 0.973225]\n",
      "4211 [D loss: 0.712656, acc.: 56.25%] [G loss: 0.889635]\n",
      "4212 [D loss: 0.585945, acc.: 71.88%] [G loss: 0.906166]\n",
      "4213 [D loss: 0.536008, acc.: 78.12%] [G loss: 0.996706]\n",
      "4214 [D loss: 0.530476, acc.: 84.38%] [G loss: 0.919972]\n",
      "4215 [D loss: 0.642261, acc.: 65.62%] [G loss: 0.923452]\n",
      "4216 [D loss: 0.600297, acc.: 71.88%] [G loss: 0.949861]\n",
      "4217 [D loss: 0.622268, acc.: 68.75%] [G loss: 0.997372]\n",
      "4218 [D loss: 0.626175, acc.: 56.25%] [G loss: 0.997820]\n",
      "4219 [D loss: 0.686181, acc.: 56.25%] [G loss: 0.940077]\n",
      "4220 [D loss: 0.705853, acc.: 46.88%] [G loss: 0.970239]\n",
      "4221 [D loss: 0.678910, acc.: 50.00%] [G loss: 0.858949]\n",
      "4222 [D loss: 0.655246, acc.: 65.62%] [G loss: 0.886904]\n",
      "4223 [D loss: 0.654562, acc.: 65.62%] [G loss: 0.845218]\n",
      "4224 [D loss: 0.689218, acc.: 59.38%] [G loss: 0.818027]\n",
      "4225 [D loss: 0.724376, acc.: 53.12%] [G loss: 0.870581]\n",
      "4226 [D loss: 0.560983, acc.: 75.00%] [G loss: 0.925112]\n",
      "4227 [D loss: 0.589053, acc.: 75.00%] [G loss: 0.885661]\n",
      "4228 [D loss: 0.635386, acc.: 62.50%] [G loss: 0.895369]\n",
      "4229 [D loss: 0.593979, acc.: 68.75%] [G loss: 0.880815]\n",
      "4230 [D loss: 0.754707, acc.: 40.62%] [G loss: 0.928346]\n",
      "4231 [D loss: 0.711954, acc.: 46.88%] [G loss: 0.880347]\n",
      "4232 [D loss: 0.656673, acc.: 56.25%] [G loss: 0.871498]\n",
      "4233 [D loss: 0.580039, acc.: 78.12%] [G loss: 0.852672]\n",
      "4234 [D loss: 0.625846, acc.: 59.38%] [G loss: 0.943228]\n",
      "4235 [D loss: 0.625458, acc.: 68.75%] [G loss: 0.854661]\n",
      "4236 [D loss: 0.670930, acc.: 65.62%] [G loss: 0.891673]\n",
      "4237 [D loss: 0.626569, acc.: 65.62%] [G loss: 0.982793]\n",
      "4238 [D loss: 0.593453, acc.: 71.88%] [G loss: 0.955591]\n",
      "4239 [D loss: 0.535601, acc.: 75.00%] [G loss: 0.916797]\n",
      "4240 [D loss: 0.625648, acc.: 56.25%] [G loss: 0.950055]\n",
      "4241 [D loss: 0.702752, acc.: 56.25%] [G loss: 0.808662]\n",
      "4242 [D loss: 0.612002, acc.: 59.38%] [G loss: 0.930825]\n",
      "4243 [D loss: 0.631906, acc.: 59.38%] [G loss: 0.978794]\n",
      "4244 [D loss: 0.691937, acc.: 56.25%] [G loss: 0.959836]\n",
      "4245 [D loss: 0.671223, acc.: 56.25%] [G loss: 0.941103]\n",
      "4246 [D loss: 0.727513, acc.: 43.75%] [G loss: 0.923666]\n",
      "4247 [D loss: 0.665424, acc.: 56.25%] [G loss: 0.898505]\n",
      "4248 [D loss: 0.580949, acc.: 68.75%] [G loss: 0.909989]\n",
      "4249 [D loss: 0.702350, acc.: 59.38%] [G loss: 0.955140]\n",
      "4250 [D loss: 0.754148, acc.: 56.25%] [G loss: 0.892657]\n",
      "4251 [D loss: 0.747867, acc.: 46.88%] [G loss: 0.955985]\n",
      "4252 [D loss: 0.635280, acc.: 62.50%] [G loss: 1.004657]\n",
      "4253 [D loss: 0.685222, acc.: 50.00%] [G loss: 0.920092]\n",
      "4254 [D loss: 0.630576, acc.: 62.50%] [G loss: 0.861604]\n",
      "4255 [D loss: 0.574379, acc.: 75.00%] [G loss: 0.807899]\n",
      "4256 [D loss: 0.670546, acc.: 68.75%] [G loss: 0.917121]\n",
      "4257 [D loss: 0.552608, acc.: 78.12%] [G loss: 0.878556]\n",
      "4258 [D loss: 0.634174, acc.: 71.88%] [G loss: 0.903286]\n",
      "4259 [D loss: 0.696601, acc.: 56.25%] [G loss: 0.849109]\n",
      "4260 [D loss: 0.711964, acc.: 53.12%] [G loss: 0.925240]\n",
      "4261 [D loss: 0.664559, acc.: 59.38%] [G loss: 0.888700]\n",
      "4262 [D loss: 0.684310, acc.: 50.00%] [G loss: 0.972501]\n",
      "4263 [D loss: 0.623416, acc.: 56.25%] [G loss: 1.002951]\n",
      "4264 [D loss: 0.646390, acc.: 56.25%] [G loss: 1.041551]\n",
      "4265 [D loss: 0.755940, acc.: 50.00%] [G loss: 1.066348]\n",
      "4266 [D loss: 0.642755, acc.: 71.88%] [G loss: 1.029710]\n",
      "4267 [D loss: 0.716847, acc.: 46.88%] [G loss: 0.901312]\n",
      "4268 [D loss: 0.652514, acc.: 62.50%] [G loss: 0.986350]\n",
      "4269 [D loss: 0.712791, acc.: 50.00%] [G loss: 1.017163]\n",
      "4270 [D loss: 0.615773, acc.: 68.75%] [G loss: 0.909312]\n",
      "4271 [D loss: 0.665426, acc.: 62.50%] [G loss: 0.799073]\n",
      "4272 [D loss: 0.678392, acc.: 56.25%] [G loss: 0.892850]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4273 [D loss: 0.709778, acc.: 59.38%] [G loss: 0.964244]\n",
      "4274 [D loss: 0.638534, acc.: 68.75%] [G loss: 0.925104]\n",
      "4275 [D loss: 0.576419, acc.: 65.62%] [G loss: 0.886455]\n",
      "4276 [D loss: 0.540895, acc.: 78.12%] [G loss: 0.957128]\n",
      "4277 [D loss: 0.555711, acc.: 71.88%] [G loss: 0.882439]\n",
      "4278 [D loss: 0.649897, acc.: 62.50%] [G loss: 0.924806]\n",
      "4279 [D loss: 0.711430, acc.: 56.25%] [G loss: 0.861922]\n",
      "4280 [D loss: 0.584417, acc.: 71.88%] [G loss: 0.905609]\n",
      "4281 [D loss: 0.690513, acc.: 50.00%] [G loss: 0.895904]\n",
      "4282 [D loss: 0.648556, acc.: 62.50%] [G loss: 0.905573]\n",
      "4283 [D loss: 0.556324, acc.: 68.75%] [G loss: 0.865270]\n",
      "4284 [D loss: 0.676985, acc.: 50.00%] [G loss: 0.951866]\n",
      "4285 [D loss: 0.599208, acc.: 71.88%] [G loss: 0.915632]\n",
      "4286 [D loss: 0.656684, acc.: 65.62%] [G loss: 0.928517]\n",
      "4287 [D loss: 0.616977, acc.: 59.38%] [G loss: 0.933055]\n",
      "4288 [D loss: 0.633549, acc.: 65.62%] [G loss: 0.870775]\n",
      "4289 [D loss: 0.624577, acc.: 59.38%] [G loss: 0.930277]\n",
      "4290 [D loss: 0.565702, acc.: 78.12%] [G loss: 0.885345]\n",
      "4291 [D loss: 0.771168, acc.: 53.12%] [G loss: 0.926142]\n",
      "4292 [D loss: 0.649442, acc.: 59.38%] [G loss: 0.945942]\n",
      "4293 [D loss: 0.654744, acc.: 56.25%] [G loss: 0.939408]\n",
      "4294 [D loss: 0.713186, acc.: 50.00%] [G loss: 0.882788]\n",
      "4295 [D loss: 0.604875, acc.: 68.75%] [G loss: 0.868071]\n",
      "4296 [D loss: 0.596904, acc.: 68.75%] [G loss: 0.798222]\n",
      "4297 [D loss: 0.618300, acc.: 53.12%] [G loss: 0.841042]\n",
      "4298 [D loss: 0.622510, acc.: 62.50%] [G loss: 0.932643]\n",
      "4299 [D loss: 0.684646, acc.: 62.50%] [G loss: 0.816222]\n",
      "4300 [D loss: 0.638167, acc.: 78.12%] [G loss: 0.868449]\n",
      "4301 [D loss: 0.671654, acc.: 53.12%] [G loss: 0.931626]\n",
      "4302 [D loss: 0.532558, acc.: 81.25%] [G loss: 0.952615]\n",
      "4303 [D loss: 0.610081, acc.: 56.25%] [G loss: 0.972675]\n",
      "4304 [D loss: 0.620570, acc.: 68.75%] [G loss: 0.961483]\n",
      "4305 [D loss: 0.659019, acc.: 65.62%] [G loss: 0.831487]\n",
      "4306 [D loss: 0.625185, acc.: 68.75%] [G loss: 0.932722]\n",
      "4307 [D loss: 0.684361, acc.: 59.38%] [G loss: 0.958024]\n",
      "4308 [D loss: 0.617876, acc.: 71.88%] [G loss: 0.911620]\n",
      "4309 [D loss: 0.625034, acc.: 62.50%] [G loss: 0.963980]\n",
      "4310 [D loss: 0.606674, acc.: 71.88%] [G loss: 0.866480]\n",
      "4311 [D loss: 0.646450, acc.: 59.38%] [G loss: 1.031252]\n",
      "4312 [D loss: 0.709464, acc.: 53.12%] [G loss: 0.851447]\n",
      "4313 [D loss: 0.637181, acc.: 62.50%] [G loss: 0.941798]\n",
      "4314 [D loss: 0.581511, acc.: 71.88%] [G loss: 0.866188]\n",
      "4315 [D loss: 0.646744, acc.: 59.38%] [G loss: 0.887640]\n",
      "4316 [D loss: 0.627355, acc.: 75.00%] [G loss: 0.896207]\n",
      "4317 [D loss: 0.651814, acc.: 59.38%] [G loss: 0.929630]\n",
      "4318 [D loss: 0.696391, acc.: 53.12%] [G loss: 0.932213]\n",
      "4319 [D loss: 0.691756, acc.: 59.38%] [G loss: 0.763947]\n",
      "4320 [D loss: 0.713940, acc.: 46.88%] [G loss: 0.877280]\n",
      "4321 [D loss: 0.588884, acc.: 75.00%] [G loss: 0.881799]\n",
      "4322 [D loss: 0.617478, acc.: 65.62%] [G loss: 0.843880]\n",
      "4323 [D loss: 0.675242, acc.: 62.50%] [G loss: 0.850441]\n",
      "4324 [D loss: 0.670289, acc.: 62.50%] [G loss: 0.870385]\n",
      "4325 [D loss: 0.639315, acc.: 68.75%] [G loss: 0.897059]\n",
      "4326 [D loss: 0.627561, acc.: 50.00%] [G loss: 0.911666]\n",
      "4327 [D loss: 0.577694, acc.: 65.62%] [G loss: 0.921126]\n",
      "4328 [D loss: 0.589926, acc.: 62.50%] [G loss: 0.941948]\n",
      "4329 [D loss: 0.636154, acc.: 59.38%] [G loss: 0.832162]\n",
      "4330 [D loss: 0.702099, acc.: 50.00%] [G loss: 0.869408]\n",
      "4331 [D loss: 0.626877, acc.: 65.62%] [G loss: 0.954860]\n",
      "4332 [D loss: 0.581951, acc.: 78.12%] [G loss: 0.872509]\n",
      "4333 [D loss: 0.602146, acc.: 71.88%] [G loss: 0.848696]\n",
      "4334 [D loss: 0.646912, acc.: 59.38%] [G loss: 0.858899]\n",
      "4335 [D loss: 0.674774, acc.: 62.50%] [G loss: 0.931556]\n",
      "4336 [D loss: 0.637885, acc.: 53.12%] [G loss: 0.896849]\n",
      "4337 [D loss: 0.583970, acc.: 68.75%] [G loss: 0.867892]\n",
      "4338 [D loss: 0.655675, acc.: 71.88%] [G loss: 0.879347]\n",
      "4339 [D loss: 0.528518, acc.: 75.00%] [G loss: 0.843884]\n",
      "4340 [D loss: 0.607584, acc.: 62.50%] [G loss: 0.792148]\n",
      "4341 [D loss: 0.610918, acc.: 71.88%] [G loss: 0.940654]\n",
      "4342 [D loss: 0.618021, acc.: 68.75%] [G loss: 0.985870]\n",
      "4343 [D loss: 0.761306, acc.: 56.25%] [G loss: 1.016848]\n",
      "4344 [D loss: 0.666079, acc.: 62.50%] [G loss: 0.896547]\n",
      "4345 [D loss: 0.596471, acc.: 71.88%] [G loss: 0.942290]\n",
      "4346 [D loss: 0.639717, acc.: 65.62%] [G loss: 0.870905]\n",
      "4347 [D loss: 0.668411, acc.: 50.00%] [G loss: 0.798068]\n",
      "4348 [D loss: 0.549609, acc.: 75.00%] [G loss: 0.800551]\n",
      "4349 [D loss: 0.672279, acc.: 53.12%] [G loss: 0.844379]\n",
      "4350 [D loss: 0.620054, acc.: 65.62%] [G loss: 0.779465]\n",
      "4351 [D loss: 0.670163, acc.: 59.38%] [G loss: 0.845428]\n",
      "4352 [D loss: 0.730640, acc.: 46.88%] [G loss: 0.776169]\n",
      "4353 [D loss: 0.596973, acc.: 65.62%] [G loss: 0.840176]\n",
      "4354 [D loss: 0.613289, acc.: 62.50%] [G loss: 0.866000]\n",
      "4355 [D loss: 0.652556, acc.: 65.62%] [G loss: 1.028176]\n",
      "4356 [D loss: 0.593668, acc.: 68.75%] [G loss: 1.062821]\n",
      "4357 [D loss: 0.692478, acc.: 62.50%] [G loss: 0.995794]\n",
      "4358 [D loss: 0.606685, acc.: 71.88%] [G loss: 0.977813]\n",
      "4359 [D loss: 0.635581, acc.: 59.38%] [G loss: 0.921375]\n",
      "4360 [D loss: 0.608802, acc.: 75.00%] [G loss: 0.816367]\n",
      "4361 [D loss: 0.605245, acc.: 71.88%] [G loss: 0.791495]\n",
      "4362 [D loss: 0.621353, acc.: 75.00%] [G loss: 0.868428]\n",
      "4363 [D loss: 0.601219, acc.: 81.25%] [G loss: 0.893835]\n",
      "4364 [D loss: 0.662555, acc.: 56.25%] [G loss: 0.919568]\n",
      "4365 [D loss: 0.664720, acc.: 56.25%] [G loss: 0.973300]\n",
      "4366 [D loss: 0.754159, acc.: 40.62%] [G loss: 0.945627]\n",
      "4367 [D loss: 0.639765, acc.: 68.75%] [G loss: 0.854076]\n",
      "4368 [D loss: 0.595358, acc.: 71.88%] [G loss: 0.832973]\n",
      "4369 [D loss: 0.621422, acc.: 62.50%] [G loss: 0.914560]\n",
      "4370 [D loss: 0.629647, acc.: 68.75%] [G loss: 0.908565]\n",
      "4371 [D loss: 0.714065, acc.: 68.75%] [G loss: 0.998367]\n",
      "4372 [D loss: 0.653439, acc.: 53.12%] [G loss: 0.956727]\n",
      "4373 [D loss: 0.549151, acc.: 71.88%] [G loss: 0.906074]\n",
      "4374 [D loss: 0.661615, acc.: 59.38%] [G loss: 0.904988]\n",
      "4375 [D loss: 0.688809, acc.: 53.12%] [G loss: 0.840094]\n",
      "4376 [D loss: 0.622146, acc.: 62.50%] [G loss: 0.953742]\n",
      "4377 [D loss: 0.649021, acc.: 56.25%] [G loss: 0.896423]\n",
      "4378 [D loss: 0.598613, acc.: 71.88%] [G loss: 0.968510]\n",
      "4379 [D loss: 0.634477, acc.: 65.62%] [G loss: 0.979758]\n",
      "4380 [D loss: 0.590294, acc.: 65.62%] [G loss: 0.974542]\n",
      "4381 [D loss: 0.644316, acc.: 59.38%] [G loss: 0.951474]\n",
      "4382 [D loss: 0.533502, acc.: 81.25%] [G loss: 0.976961]\n",
      "4383 [D loss: 0.634637, acc.: 62.50%] [G loss: 0.975299]\n",
      "4384 [D loss: 0.602769, acc.: 65.62%] [G loss: 0.846403]\n",
      "4385 [D loss: 0.762233, acc.: 53.12%] [G loss: 0.872673]\n",
      "4386 [D loss: 0.601263, acc.: 75.00%] [G loss: 0.826816]\n",
      "4387 [D loss: 0.696608, acc.: 56.25%] [G loss: 0.856183]\n",
      "4388 [D loss: 0.670850, acc.: 59.38%] [G loss: 0.926472]\n",
      "4389 [D loss: 0.605823, acc.: 71.88%] [G loss: 0.961747]\n",
      "4390 [D loss: 0.572053, acc.: 78.12%] [G loss: 0.866700]\n",
      "4391 [D loss: 0.596709, acc.: 65.62%] [G loss: 0.835892]\n",
      "4392 [D loss: 0.713475, acc.: 56.25%] [G loss: 0.807424]\n",
      "4393 [D loss: 0.622854, acc.: 68.75%] [G loss: 0.892330]\n",
      "4394 [D loss: 0.608538, acc.: 71.88%] [G loss: 0.838440]\n",
      "4395 [D loss: 0.605359, acc.: 71.88%] [G loss: 0.890040]\n",
      "4396 [D loss: 0.626263, acc.: 68.75%] [G loss: 0.821695]\n",
      "4397 [D loss: 0.628703, acc.: 71.88%] [G loss: 0.908166]\n",
      "4398 [D loss: 0.624053, acc.: 68.75%] [G loss: 0.930377]\n",
      "4399 [D loss: 0.573739, acc.: 71.88%] [G loss: 0.888657]\n",
      "4400 [D loss: 0.635220, acc.: 62.50%] [G loss: 0.832819]\n",
      "4401 [D loss: 0.661214, acc.: 53.12%] [G loss: 0.913448]\n",
      "4402 [D loss: 0.584181, acc.: 68.75%] [G loss: 0.798444]\n",
      "4403 [D loss: 0.579679, acc.: 71.88%] [G loss: 0.852672]\n",
      "4404 [D loss: 0.570678, acc.: 68.75%] [G loss: 0.855019]\n",
      "4405 [D loss: 0.643509, acc.: 59.38%] [G loss: 0.891002]\n",
      "4406 [D loss: 0.623967, acc.: 65.62%] [G loss: 0.887408]\n",
      "4407 [D loss: 0.792011, acc.: 43.75%] [G loss: 0.881669]\n",
      "4408 [D loss: 0.556959, acc.: 65.62%] [G loss: 0.919001]\n",
      "4409 [D loss: 0.612735, acc.: 71.88%] [G loss: 1.021998]\n",
      "4410 [D loss: 0.712697, acc.: 56.25%] [G loss: 0.920312]\n",
      "4411 [D loss: 0.697123, acc.: 56.25%] [G loss: 0.874486]\n",
      "4412 [D loss: 0.638623, acc.: 62.50%] [G loss: 0.832586]\n",
      "4413 [D loss: 0.556972, acc.: 75.00%] [G loss: 0.923105]\n",
      "4414 [D loss: 0.590771, acc.: 62.50%] [G loss: 0.900169]\n",
      "4415 [D loss: 0.601130, acc.: 65.62%] [G loss: 0.890397]\n",
      "4416 [D loss: 0.639112, acc.: 50.00%] [G loss: 0.929688]\n",
      "4417 [D loss: 0.643696, acc.: 59.38%] [G loss: 0.903768]\n",
      "4418 [D loss: 0.566830, acc.: 81.25%] [G loss: 0.968661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4419 [D loss: 0.611991, acc.: 75.00%] [G loss: 0.879788]\n",
      "4420 [D loss: 0.695744, acc.: 56.25%] [G loss: 0.891873]\n",
      "4421 [D loss: 0.583884, acc.: 68.75%] [G loss: 0.939381]\n",
      "4422 [D loss: 0.706619, acc.: 59.38%] [G loss: 0.928839]\n",
      "4423 [D loss: 0.661062, acc.: 65.62%] [G loss: 0.926992]\n",
      "4424 [D loss: 0.578004, acc.: 75.00%] [G loss: 0.927189]\n",
      "4425 [D loss: 0.536569, acc.: 81.25%] [G loss: 1.020165]\n",
      "4426 [D loss: 0.676092, acc.: 59.38%] [G loss: 1.030588]\n",
      "4427 [D loss: 0.622786, acc.: 59.38%] [G loss: 0.991346]\n",
      "4428 [D loss: 0.643225, acc.: 65.62%] [G loss: 1.064088]\n",
      "4429 [D loss: 0.626560, acc.: 68.75%] [G loss: 0.981795]\n",
      "4430 [D loss: 0.678886, acc.: 59.38%] [G loss: 0.974196]\n",
      "4431 [D loss: 0.623731, acc.: 71.88%] [G loss: 0.939069]\n",
      "4432 [D loss: 0.646902, acc.: 59.38%] [G loss: 0.883730]\n",
      "4433 [D loss: 0.558530, acc.: 75.00%] [G loss: 0.908334]\n",
      "4434 [D loss: 0.608213, acc.: 65.62%] [G loss: 0.894227]\n",
      "4435 [D loss: 0.625491, acc.: 71.88%] [G loss: 0.937603]\n",
      "4436 [D loss: 0.597316, acc.: 62.50%] [G loss: 0.871754]\n",
      "4437 [D loss: 0.622662, acc.: 65.62%] [G loss: 0.857960]\n",
      "4438 [D loss: 0.731730, acc.: 46.88%] [G loss: 0.942748]\n",
      "4439 [D loss: 0.554963, acc.: 71.88%] [G loss: 0.935639]\n",
      "4440 [D loss: 0.600370, acc.: 71.88%] [G loss: 0.876980]\n",
      "4441 [D loss: 0.621894, acc.: 62.50%] [G loss: 0.958695]\n",
      "4442 [D loss: 0.633645, acc.: 59.38%] [G loss: 1.007598]\n",
      "4443 [D loss: 0.608730, acc.: 62.50%] [G loss: 1.014789]\n",
      "4444 [D loss: 0.597423, acc.: 68.75%] [G loss: 0.978080]\n",
      "4445 [D loss: 0.677949, acc.: 65.62%] [G loss: 0.882732]\n",
      "4446 [D loss: 0.702168, acc.: 37.50%] [G loss: 0.874955]\n",
      "4447 [D loss: 0.666239, acc.: 62.50%] [G loss: 0.879660]\n",
      "4448 [D loss: 0.616977, acc.: 65.62%] [G loss: 0.994442]\n",
      "4449 [D loss: 0.568905, acc.: 62.50%] [G loss: 0.871938]\n",
      "4450 [D loss: 0.536937, acc.: 75.00%] [G loss: 0.891004]\n",
      "4451 [D loss: 0.583802, acc.: 75.00%] [G loss: 0.959502]\n",
      "4452 [D loss: 0.546377, acc.: 78.12%] [G loss: 0.991025]\n",
      "4453 [D loss: 0.688463, acc.: 59.38%] [G loss: 0.844837]\n",
      "4454 [D loss: 0.662787, acc.: 62.50%] [G loss: 0.921641]\n",
      "4455 [D loss: 0.581461, acc.: 78.12%] [G loss: 0.900994]\n",
      "4456 [D loss: 0.589389, acc.: 81.25%] [G loss: 1.025070]\n",
      "4457 [D loss: 0.671172, acc.: 56.25%] [G loss: 1.013211]\n",
      "4458 [D loss: 0.732011, acc.: 53.12%] [G loss: 1.063735]\n",
      "4459 [D loss: 0.654800, acc.: 68.75%] [G loss: 1.034062]\n",
      "4460 [D loss: 0.617592, acc.: 62.50%] [G loss: 1.004718]\n",
      "4461 [D loss: 0.732144, acc.: 53.12%] [G loss: 0.911058]\n",
      "4462 [D loss: 0.693486, acc.: 50.00%] [G loss: 0.991941]\n",
      "4463 [D loss: 0.631618, acc.: 68.75%] [G loss: 1.080345]\n",
      "4464 [D loss: 0.626166, acc.: 62.50%] [G loss: 0.981161]\n",
      "4465 [D loss: 0.703430, acc.: 50.00%] [G loss: 0.936710]\n",
      "4466 [D loss: 0.579691, acc.: 65.62%] [G loss: 0.838928]\n",
      "4467 [D loss: 0.639026, acc.: 62.50%] [G loss: 0.908499]\n",
      "4468 [D loss: 0.671739, acc.: 62.50%] [G loss: 0.899461]\n",
      "4469 [D loss: 0.655035, acc.: 62.50%] [G loss: 0.928827]\n",
      "4470 [D loss: 0.639746, acc.: 56.25%] [G loss: 0.989818]\n",
      "4471 [D loss: 0.621904, acc.: 78.12%] [G loss: 1.008010]\n",
      "4472 [D loss: 0.689198, acc.: 53.12%] [G loss: 0.860941]\n",
      "4473 [D loss: 0.546433, acc.: 81.25%] [G loss: 0.945397]\n",
      "4474 [D loss: 0.674575, acc.: 59.38%] [G loss: 0.840493]\n",
      "4475 [D loss: 0.632015, acc.: 53.12%] [G loss: 0.866839]\n",
      "4476 [D loss: 0.577513, acc.: 62.50%] [G loss: 0.878604]\n",
      "4477 [D loss: 0.699300, acc.: 50.00%] [G loss: 0.885530]\n",
      "4478 [D loss: 0.666752, acc.: 65.62%] [G loss: 0.914562]\n",
      "4479 [D loss: 0.716087, acc.: 53.12%] [G loss: 0.779977]\n",
      "4480 [D loss: 0.687609, acc.: 56.25%] [G loss: 0.858578]\n",
      "4481 [D loss: 0.686346, acc.: 62.50%] [G loss: 0.851665]\n",
      "4482 [D loss: 0.579249, acc.: 71.88%] [G loss: 0.896761]\n",
      "4483 [D loss: 0.689565, acc.: 59.38%] [G loss: 0.898486]\n",
      "4484 [D loss: 0.707947, acc.: 59.38%] [G loss: 0.909634]\n",
      "4485 [D loss: 0.597284, acc.: 65.62%] [G loss: 0.931784]\n",
      "4486 [D loss: 0.587743, acc.: 68.75%] [G loss: 0.857102]\n",
      "4487 [D loss: 0.641406, acc.: 59.38%] [G loss: 0.876064]\n",
      "4488 [D loss: 0.637591, acc.: 46.88%] [G loss: 0.861840]\n",
      "4489 [D loss: 0.640022, acc.: 68.75%] [G loss: 0.860346]\n",
      "4490 [D loss: 0.766249, acc.: 40.62%] [G loss: 0.935936]\n",
      "4491 [D loss: 0.705949, acc.: 56.25%] [G loss: 0.977073]\n",
      "4492 [D loss: 0.672660, acc.: 59.38%] [G loss: 0.999499]\n",
      "4493 [D loss: 0.614994, acc.: 78.12%] [G loss: 0.944672]\n",
      "4494 [D loss: 0.741328, acc.: 59.38%] [G loss: 0.888793]\n",
      "4495 [D loss: 0.642257, acc.: 71.88%] [G loss: 0.955737]\n",
      "4496 [D loss: 0.651936, acc.: 62.50%] [G loss: 0.991615]\n",
      "4497 [D loss: 0.634265, acc.: 68.75%] [G loss: 0.926253]\n",
      "4498 [D loss: 0.557634, acc.: 78.12%] [G loss: 0.951653]\n",
      "4499 [D loss: 0.678526, acc.: 56.25%] [G loss: 0.919888]\n",
      "4500 [D loss: 0.596261, acc.: 71.88%] [G loss: 0.879947]\n",
      "4501 [D loss: 0.682335, acc.: 62.50%] [G loss: 0.942537]\n",
      "4502 [D loss: 0.626845, acc.: 68.75%] [G loss: 0.947096]\n",
      "4503 [D loss: 0.664345, acc.: 56.25%] [G loss: 0.915454]\n",
      "4504 [D loss: 0.623193, acc.: 62.50%] [G loss: 0.878053]\n",
      "4505 [D loss: 0.603788, acc.: 71.88%] [G loss: 0.838306]\n",
      "4506 [D loss: 0.592437, acc.: 75.00%] [G loss: 0.860047]\n",
      "4507 [D loss: 0.627266, acc.: 71.88%] [G loss: 0.880985]\n",
      "4508 [D loss: 0.727191, acc.: 56.25%] [G loss: 0.931798]\n",
      "4509 [D loss: 0.602313, acc.: 75.00%] [G loss: 0.975067]\n",
      "4510 [D loss: 0.680647, acc.: 59.38%] [G loss: 0.967279]\n",
      "4511 [D loss: 0.648704, acc.: 68.75%] [G loss: 0.894924]\n",
      "4512 [D loss: 0.567602, acc.: 71.88%] [G loss: 0.916902]\n",
      "4513 [D loss: 0.603104, acc.: 68.75%] [G loss: 0.957456]\n",
      "4514 [D loss: 0.707299, acc.: 62.50%] [G loss: 0.899818]\n",
      "4515 [D loss: 0.578359, acc.: 65.62%] [G loss: 0.986009]\n",
      "4516 [D loss: 0.662785, acc.: 68.75%] [G loss: 0.850251]\n",
      "4517 [D loss: 0.703712, acc.: 59.38%] [G loss: 0.954550]\n",
      "4518 [D loss: 0.642801, acc.: 53.12%] [G loss: 0.977596]\n",
      "4519 [D loss: 0.757095, acc.: 46.88%] [G loss: 0.960483]\n",
      "4520 [D loss: 0.628007, acc.: 71.88%] [G loss: 0.895153]\n",
      "4521 [D loss: 0.779227, acc.: 46.88%] [G loss: 0.849074]\n",
      "4522 [D loss: 0.613301, acc.: 78.12%] [G loss: 0.834177]\n",
      "4523 [D loss: 0.635806, acc.: 56.25%] [G loss: 0.781549]\n",
      "4524 [D loss: 0.711591, acc.: 53.12%] [G loss: 0.874455]\n",
      "4525 [D loss: 0.632570, acc.: 59.38%] [G loss: 0.778546]\n",
      "4526 [D loss: 0.657517, acc.: 62.50%] [G loss: 0.856976]\n",
      "4527 [D loss: 0.693194, acc.: 56.25%] [G loss: 0.863770]\n",
      "4528 [D loss: 0.555931, acc.: 68.75%] [G loss: 0.846900]\n",
      "4529 [D loss: 0.792994, acc.: 46.88%] [G loss: 0.862464]\n",
      "4530 [D loss: 0.688193, acc.: 53.12%] [G loss: 0.834715]\n",
      "4531 [D loss: 0.688872, acc.: 56.25%] [G loss: 0.764023]\n",
      "4532 [D loss: 0.629579, acc.: 71.88%] [G loss: 0.792209]\n",
      "4533 [D loss: 0.678387, acc.: 56.25%] [G loss: 0.873129]\n",
      "4534 [D loss: 0.657451, acc.: 65.62%] [G loss: 0.822610]\n",
      "4535 [D loss: 0.635389, acc.: 65.62%] [G loss: 0.944258]\n",
      "4536 [D loss: 0.625845, acc.: 68.75%] [G loss: 0.874681]\n",
      "4537 [D loss: 0.624905, acc.: 65.62%] [G loss: 0.989915]\n",
      "4538 [D loss: 0.551662, acc.: 75.00%] [G loss: 0.995517]\n",
      "4539 [D loss: 0.617023, acc.: 68.75%] [G loss: 0.923214]\n",
      "4540 [D loss: 0.664289, acc.: 59.38%] [G loss: 0.845770]\n",
      "4541 [D loss: 0.506101, acc.: 84.38%] [G loss: 0.917743]\n",
      "4542 [D loss: 0.529624, acc.: 81.25%] [G loss: 0.975888]\n",
      "4543 [D loss: 0.603235, acc.: 71.88%] [G loss: 0.903549]\n",
      "4544 [D loss: 0.591738, acc.: 68.75%] [G loss: 0.945596]\n",
      "4545 [D loss: 0.689463, acc.: 53.12%] [G loss: 0.837874]\n",
      "4546 [D loss: 0.605920, acc.: 71.88%] [G loss: 0.808244]\n",
      "4547 [D loss: 0.702561, acc.: 53.12%] [G loss: 0.884016]\n",
      "4548 [D loss: 0.622563, acc.: 68.75%] [G loss: 0.894095]\n",
      "4549 [D loss: 0.581191, acc.: 65.62%] [G loss: 0.905956]\n",
      "4550 [D loss: 0.552301, acc.: 78.12%] [G loss: 0.949215]\n",
      "4551 [D loss: 0.620581, acc.: 62.50%] [G loss: 0.940132]\n",
      "4552 [D loss: 0.674531, acc.: 46.88%] [G loss: 0.932864]\n",
      "4553 [D loss: 0.611287, acc.: 62.50%] [G loss: 0.902453]\n",
      "4554 [D loss: 0.625486, acc.: 75.00%] [G loss: 0.940054]\n",
      "4555 [D loss: 0.641556, acc.: 59.38%] [G loss: 0.988724]\n",
      "4556 [D loss: 0.763704, acc.: 46.88%] [G loss: 0.956957]\n",
      "4557 [D loss: 0.673495, acc.: 62.50%] [G loss: 0.900042]\n",
      "4558 [D loss: 0.532878, acc.: 78.12%] [G loss: 0.891437]\n",
      "4559 [D loss: 0.598406, acc.: 71.88%] [G loss: 0.852909]\n",
      "4560 [D loss: 0.596629, acc.: 62.50%] [G loss: 0.932026]\n",
      "4561 [D loss: 0.607192, acc.: 68.75%] [G loss: 1.018919]\n",
      "4562 [D loss: 0.553377, acc.: 81.25%] [G loss: 1.037811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4563 [D loss: 0.618614, acc.: 53.12%] [G loss: 0.912668]\n",
      "4564 [D loss: 0.693978, acc.: 56.25%] [G loss: 0.958993]\n",
      "4565 [D loss: 0.718888, acc.: 50.00%] [G loss: 0.831217]\n",
      "4566 [D loss: 0.695515, acc.: 62.50%] [G loss: 0.892787]\n",
      "4567 [D loss: 0.612169, acc.: 68.75%] [G loss: 0.872691]\n",
      "4568 [D loss: 0.635081, acc.: 62.50%] [G loss: 0.887309]\n",
      "4569 [D loss: 0.596051, acc.: 71.88%] [G loss: 0.899903]\n",
      "4570 [D loss: 0.730607, acc.: 53.12%] [G loss: 0.875680]\n",
      "4571 [D loss: 0.546064, acc.: 75.00%] [G loss: 0.900814]\n",
      "4572 [D loss: 0.619492, acc.: 71.88%] [G loss: 0.870957]\n",
      "4573 [D loss: 0.585016, acc.: 75.00%] [G loss: 0.853337]\n",
      "4574 [D loss: 0.589880, acc.: 71.88%] [G loss: 0.970523]\n",
      "4575 [D loss: 0.604978, acc.: 65.62%] [G loss: 0.875244]\n",
      "4576 [D loss: 0.594488, acc.: 75.00%] [G loss: 0.919575]\n",
      "4577 [D loss: 0.655910, acc.: 59.38%] [G loss: 0.918541]\n",
      "4578 [D loss: 0.640532, acc.: 59.38%] [G loss: 0.904955]\n",
      "4579 [D loss: 0.744944, acc.: 43.75%] [G loss: 0.968220]\n",
      "4580 [D loss: 0.677341, acc.: 62.50%] [G loss: 0.856133]\n",
      "4581 [D loss: 0.616947, acc.: 62.50%] [G loss: 0.952559]\n",
      "4582 [D loss: 0.535289, acc.: 81.25%] [G loss: 0.903977]\n",
      "4583 [D loss: 0.646811, acc.: 68.75%] [G loss: 0.956427]\n",
      "4584 [D loss: 0.648814, acc.: 62.50%] [G loss: 0.987646]\n",
      "4585 [D loss: 0.574270, acc.: 75.00%] [G loss: 0.952621]\n",
      "4586 [D loss: 0.583790, acc.: 71.88%] [G loss: 1.004099]\n",
      "4587 [D loss: 0.625176, acc.: 71.88%] [G loss: 1.041808]\n",
      "4588 [D loss: 0.699604, acc.: 53.12%] [G loss: 0.947810]\n",
      "4589 [D loss: 0.552078, acc.: 81.25%] [G loss: 0.936183]\n",
      "4590 [D loss: 0.651438, acc.: 59.38%] [G loss: 0.846406]\n",
      "4591 [D loss: 0.668170, acc.: 59.38%] [G loss: 0.878549]\n",
      "4592 [D loss: 0.597221, acc.: 65.62%] [G loss: 0.853303]\n",
      "4593 [D loss: 0.592397, acc.: 68.75%] [G loss: 0.908845]\n",
      "4594 [D loss: 0.744616, acc.: 53.12%] [G loss: 0.899870]\n",
      "4595 [D loss: 0.642059, acc.: 62.50%] [G loss: 0.974778]\n",
      "4596 [D loss: 0.622373, acc.: 68.75%] [G loss: 0.926755]\n",
      "4597 [D loss: 0.708923, acc.: 50.00%] [G loss: 0.893191]\n",
      "4598 [D loss: 0.535837, acc.: 78.12%] [G loss: 0.833174]\n",
      "4599 [D loss: 0.657282, acc.: 56.25%] [G loss: 0.932750]\n",
      "4600 [D loss: 0.626973, acc.: 62.50%] [G loss: 0.982878]\n",
      "4601 [D loss: 0.576541, acc.: 75.00%] [G loss: 0.931575]\n",
      "4602 [D loss: 0.532711, acc.: 84.38%] [G loss: 0.984536]\n",
      "4603 [D loss: 0.722820, acc.: 40.62%] [G loss: 0.959217]\n",
      "4604 [D loss: 0.663795, acc.: 59.38%] [G loss: 0.950757]\n",
      "4605 [D loss: 0.708034, acc.: 43.75%] [G loss: 0.951351]\n",
      "4606 [D loss: 0.561932, acc.: 68.75%] [G loss: 0.933221]\n",
      "4607 [D loss: 0.608018, acc.: 65.62%] [G loss: 0.986453]\n",
      "4608 [D loss: 0.622680, acc.: 56.25%] [G loss: 0.835508]\n",
      "4609 [D loss: 0.657843, acc.: 46.88%] [G loss: 0.893983]\n",
      "4610 [D loss: 0.630172, acc.: 65.62%] [G loss: 1.028986]\n",
      "4611 [D loss: 0.584449, acc.: 78.12%] [G loss: 1.034546]\n",
      "4612 [D loss: 0.612440, acc.: 68.75%] [G loss: 0.966923]\n",
      "4613 [D loss: 0.718538, acc.: 56.25%] [G loss: 1.022444]\n",
      "4614 [D loss: 0.683004, acc.: 62.50%] [G loss: 0.929726]\n",
      "4615 [D loss: 0.590196, acc.: 59.38%] [G loss: 0.956805]\n",
      "4616 [D loss: 0.641557, acc.: 71.88%] [G loss: 0.869685]\n",
      "4617 [D loss: 0.710248, acc.: 50.00%] [G loss: 0.859854]\n",
      "4618 [D loss: 0.708122, acc.: 50.00%] [G loss: 0.920658]\n",
      "4619 [D loss: 0.571480, acc.: 75.00%] [G loss: 0.941499]\n",
      "4620 [D loss: 0.716716, acc.: 59.38%] [G loss: 0.873341]\n",
      "4621 [D loss: 0.614603, acc.: 68.75%] [G loss: 0.915503]\n",
      "4622 [D loss: 0.627389, acc.: 62.50%] [G loss: 0.912256]\n",
      "4623 [D loss: 0.684848, acc.: 65.62%] [G loss: 0.893579]\n",
      "4624 [D loss: 0.693268, acc.: 56.25%] [G loss: 0.817537]\n",
      "4625 [D loss: 0.710356, acc.: 53.12%] [G loss: 0.944575]\n",
      "4626 [D loss: 0.682171, acc.: 50.00%] [G loss: 0.855055]\n",
      "4627 [D loss: 0.673494, acc.: 75.00%] [G loss: 0.924256]\n",
      "4628 [D loss: 0.636062, acc.: 59.38%] [G loss: 0.962463]\n",
      "4629 [D loss: 0.612716, acc.: 68.75%] [G loss: 0.929840]\n",
      "4630 [D loss: 0.615509, acc.: 65.62%] [G loss: 0.944000]\n",
      "4631 [D loss: 0.595396, acc.: 65.62%] [G loss: 0.915737]\n",
      "4632 [D loss: 0.590098, acc.: 65.62%] [G loss: 1.007233]\n",
      "4633 [D loss: 0.592745, acc.: 68.75%] [G loss: 1.054297]\n",
      "4634 [D loss: 0.686284, acc.: 56.25%] [G loss: 0.947034]\n",
      "4635 [D loss: 0.694133, acc.: 59.38%] [G loss: 0.950771]\n",
      "4636 [D loss: 0.593115, acc.: 68.75%] [G loss: 0.984187]\n",
      "4637 [D loss: 0.533604, acc.: 75.00%] [G loss: 1.012021]\n",
      "4638 [D loss: 0.785424, acc.: 46.88%] [G loss: 0.949667]\n",
      "4639 [D loss: 0.780832, acc.: 40.62%] [G loss: 0.876323]\n",
      "4640 [D loss: 0.661970, acc.: 59.38%] [G loss: 1.025177]\n",
      "4641 [D loss: 0.660771, acc.: 53.12%] [G loss: 0.926413]\n",
      "4642 [D loss: 0.715270, acc.: 59.38%] [G loss: 0.882805]\n",
      "4643 [D loss: 0.612638, acc.: 56.25%] [G loss: 0.963462]\n",
      "4644 [D loss: 0.647348, acc.: 65.62%] [G loss: 0.953136]\n",
      "4645 [D loss: 0.591609, acc.: 68.75%] [G loss: 0.955896]\n",
      "4646 [D loss: 0.693427, acc.: 56.25%] [G loss: 0.832937]\n",
      "4647 [D loss: 0.630043, acc.: 68.75%] [G loss: 0.989442]\n",
      "4648 [D loss: 0.727049, acc.: 56.25%] [G loss: 0.933635]\n",
      "4649 [D loss: 0.636504, acc.: 65.62%] [G loss: 0.918627]\n",
      "4650 [D loss: 0.643509, acc.: 62.50%] [G loss: 0.968228]\n",
      "4651 [D loss: 0.553732, acc.: 71.88%] [G loss: 0.961311]\n",
      "4652 [D loss: 0.635940, acc.: 62.50%] [G loss: 0.879797]\n",
      "4653 [D loss: 0.692518, acc.: 56.25%] [G loss: 0.832326]\n",
      "4654 [D loss: 0.617596, acc.: 68.75%] [G loss: 0.802035]\n",
      "4655 [D loss: 0.643520, acc.: 65.62%] [G loss: 0.856030]\n",
      "4656 [D loss: 0.615866, acc.: 65.62%] [G loss: 0.881102]\n",
      "4657 [D loss: 0.597026, acc.: 75.00%] [G loss: 0.862680]\n",
      "4658 [D loss: 0.656618, acc.: 59.38%] [G loss: 0.883421]\n",
      "4659 [D loss: 0.678971, acc.: 59.38%] [G loss: 0.833690]\n",
      "4660 [D loss: 0.776156, acc.: 50.00%] [G loss: 0.890862]\n",
      "4661 [D loss: 0.635967, acc.: 59.38%] [G loss: 0.945292]\n",
      "4662 [D loss: 0.571534, acc.: 78.12%] [G loss: 0.906349]\n",
      "4663 [D loss: 0.592620, acc.: 68.75%] [G loss: 0.930229]\n",
      "4664 [D loss: 0.657024, acc.: 62.50%] [G loss: 1.016953]\n",
      "4665 [D loss: 0.638702, acc.: 65.62%] [G loss: 0.955793]\n",
      "4666 [D loss: 0.653254, acc.: 59.38%] [G loss: 0.956682]\n",
      "4667 [D loss: 0.736620, acc.: 59.38%] [G loss: 0.844006]\n",
      "4668 [D loss: 0.716723, acc.: 62.50%] [G loss: 0.878856]\n",
      "4669 [D loss: 0.706895, acc.: 53.12%] [G loss: 0.929091]\n",
      "4670 [D loss: 0.670247, acc.: 50.00%] [G loss: 0.951274]\n",
      "4671 [D loss: 0.681562, acc.: 53.12%] [G loss: 0.863346]\n",
      "4672 [D loss: 0.594573, acc.: 68.75%] [G loss: 0.978180]\n",
      "4673 [D loss: 0.704004, acc.: 50.00%] [G loss: 0.957454]\n",
      "4674 [D loss: 0.741423, acc.: 43.75%] [G loss: 0.832734]\n",
      "4675 [D loss: 0.743913, acc.: 46.88%] [G loss: 0.900507]\n",
      "4676 [D loss: 0.679123, acc.: 59.38%] [G loss: 0.956271]\n",
      "4677 [D loss: 0.649426, acc.: 53.12%] [G loss: 0.965392]\n",
      "4678 [D loss: 0.648865, acc.: 68.75%] [G loss: 0.904185]\n",
      "4679 [D loss: 0.598346, acc.: 71.88%] [G loss: 1.028674]\n",
      "4680 [D loss: 0.641249, acc.: 68.75%] [G loss: 0.874355]\n",
      "4681 [D loss: 0.673179, acc.: 56.25%] [G loss: 0.849889]\n",
      "4682 [D loss: 0.699609, acc.: 68.75%] [G loss: 0.852888]\n",
      "4683 [D loss: 0.616106, acc.: 65.62%] [G loss: 0.915909]\n",
      "4684 [D loss: 0.550669, acc.: 71.88%] [G loss: 0.921446]\n",
      "4685 [D loss: 0.538614, acc.: 68.75%] [G loss: 0.879354]\n",
      "4686 [D loss: 0.637995, acc.: 53.12%] [G loss: 0.911964]\n",
      "4687 [D loss: 0.685306, acc.: 53.12%] [G loss: 0.912958]\n",
      "4688 [D loss: 0.577972, acc.: 75.00%] [G loss: 0.899280]\n",
      "4689 [D loss: 0.576104, acc.: 75.00%] [G loss: 0.906706]\n",
      "4690 [D loss: 0.631521, acc.: 59.38%] [G loss: 0.923188]\n",
      "4691 [D loss: 0.675859, acc.: 56.25%] [G loss: 1.055805]\n",
      "4692 [D loss: 0.675114, acc.: 62.50%] [G loss: 0.952556]\n",
      "4693 [D loss: 0.631814, acc.: 62.50%] [G loss: 0.941980]\n",
      "4694 [D loss: 0.667442, acc.: 68.75%] [G loss: 0.912651]\n",
      "4695 [D loss: 0.645908, acc.: 68.75%] [G loss: 0.912912]\n",
      "4696 [D loss: 0.614135, acc.: 71.88%] [G loss: 0.884523]\n",
      "4697 [D loss: 0.655797, acc.: 53.12%] [G loss: 0.916066]\n",
      "4698 [D loss: 0.694860, acc.: 56.25%] [G loss: 0.904314]\n",
      "4699 [D loss: 0.594626, acc.: 71.88%] [G loss: 0.976525]\n",
      "4700 [D loss: 0.617794, acc.: 75.00%] [G loss: 0.969355]\n",
      "4701 [D loss: 0.564030, acc.: 75.00%] [G loss: 0.921860]\n",
      "4702 [D loss: 0.630096, acc.: 59.38%] [G loss: 0.900526]\n",
      "4703 [D loss: 0.540673, acc.: 75.00%] [G loss: 0.899040]\n",
      "4704 [D loss: 0.668187, acc.: 53.12%] [G loss: 0.834952]\n",
      "4705 [D loss: 0.588928, acc.: 59.38%] [G loss: 0.863164]\n",
      "4706 [D loss: 0.680486, acc.: 53.12%] [G loss: 0.934232]\n",
      "4707 [D loss: 0.534538, acc.: 78.12%] [G loss: 0.961785]\n",
      "4708 [D loss: 0.615459, acc.: 68.75%] [G loss: 0.869336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4709 [D loss: 0.699046, acc.: 43.75%] [G loss: 0.895994]\n",
      "4710 [D loss: 0.630615, acc.: 65.62%] [G loss: 0.913486]\n",
      "4711 [D loss: 0.598121, acc.: 65.62%] [G loss: 0.950300]\n",
      "4712 [D loss: 0.754700, acc.: 53.12%] [G loss: 0.930670]\n",
      "4713 [D loss: 0.629991, acc.: 62.50%] [G loss: 1.018100]\n",
      "4714 [D loss: 0.666519, acc.: 62.50%] [G loss: 0.920066]\n",
      "4715 [D loss: 0.591395, acc.: 65.62%] [G loss: 0.975314]\n",
      "4716 [D loss: 0.663309, acc.: 68.75%] [G loss: 0.861347]\n",
      "4717 [D loss: 0.530298, acc.: 84.38%] [G loss: 0.925292]\n",
      "4718 [D loss: 0.654182, acc.: 65.62%] [G loss: 0.833596]\n",
      "4719 [D loss: 0.642240, acc.: 65.62%] [G loss: 0.904463]\n",
      "4720 [D loss: 0.721519, acc.: 56.25%] [G loss: 0.859197]\n",
      "4721 [D loss: 0.605191, acc.: 71.88%] [G loss: 0.966207]\n",
      "4722 [D loss: 0.656885, acc.: 65.62%] [G loss: 0.890858]\n",
      "4723 [D loss: 0.680599, acc.: 59.38%] [G loss: 0.881153]\n",
      "4724 [D loss: 0.646552, acc.: 62.50%] [G loss: 0.890718]\n",
      "4725 [D loss: 0.743119, acc.: 43.75%] [G loss: 0.845423]\n",
      "4726 [D loss: 0.663755, acc.: 65.62%] [G loss: 0.863443]\n",
      "4727 [D loss: 0.670940, acc.: 65.62%] [G loss: 0.904598]\n",
      "4728 [D loss: 0.597813, acc.: 71.88%] [G loss: 0.897424]\n",
      "4729 [D loss: 0.642555, acc.: 56.25%] [G loss: 0.961525]\n",
      "4730 [D loss: 0.640098, acc.: 62.50%] [G loss: 0.960905]\n",
      "4731 [D loss: 0.807646, acc.: 43.75%] [G loss: 0.865038]\n",
      "4732 [D loss: 0.631767, acc.: 59.38%] [G loss: 0.860453]\n",
      "4733 [D loss: 0.652470, acc.: 62.50%] [G loss: 0.891514]\n",
      "4734 [D loss: 0.723419, acc.: 53.12%] [G loss: 0.862141]\n",
      "4735 [D loss: 0.685103, acc.: 62.50%] [G loss: 0.861427]\n",
      "4736 [D loss: 0.588998, acc.: 65.62%] [G loss: 0.797107]\n",
      "4737 [D loss: 0.549394, acc.: 81.25%] [G loss: 0.830609]\n",
      "4738 [D loss: 0.717104, acc.: 50.00%] [G loss: 0.852885]\n",
      "4739 [D loss: 0.524233, acc.: 84.38%] [G loss: 0.954126]\n",
      "4740 [D loss: 0.634737, acc.: 68.75%] [G loss: 0.835491]\n",
      "4741 [D loss: 0.722681, acc.: 53.12%] [G loss: 0.881125]\n",
      "4742 [D loss: 0.647271, acc.: 59.38%] [G loss: 0.861581]\n",
      "4743 [D loss: 0.619757, acc.: 59.38%] [G loss: 0.939539]\n",
      "4744 [D loss: 0.666653, acc.: 59.38%] [G loss: 0.942505]\n",
      "4745 [D loss: 0.768440, acc.: 37.50%] [G loss: 0.826339]\n",
      "4746 [D loss: 0.624261, acc.: 59.38%] [G loss: 0.840813]\n",
      "4747 [D loss: 0.684013, acc.: 46.88%] [G loss: 0.912516]\n",
      "4748 [D loss: 0.653860, acc.: 68.75%] [G loss: 0.876738]\n",
      "4749 [D loss: 0.633912, acc.: 62.50%] [G loss: 0.988355]\n",
      "4750 [D loss: 0.590946, acc.: 75.00%] [G loss: 0.912938]\n",
      "4751 [D loss: 0.691840, acc.: 53.12%] [G loss: 0.923978]\n",
      "4752 [D loss: 0.567278, acc.: 71.88%] [G loss: 0.904990]\n",
      "4753 [D loss: 0.671821, acc.: 62.50%] [G loss: 0.958073]\n",
      "4754 [D loss: 0.590848, acc.: 68.75%] [G loss: 0.953610]\n",
      "4755 [D loss: 0.757376, acc.: 46.88%] [G loss: 0.871742]\n",
      "4756 [D loss: 0.573085, acc.: 78.12%] [G loss: 0.934977]\n",
      "4757 [D loss: 0.575871, acc.: 71.88%] [G loss: 0.918747]\n",
      "4758 [D loss: 0.629810, acc.: 56.25%] [G loss: 0.929071]\n",
      "4759 [D loss: 0.656326, acc.: 62.50%] [G loss: 0.884394]\n",
      "4760 [D loss: 0.625477, acc.: 59.38%] [G loss: 0.934206]\n",
      "4761 [D loss: 0.790860, acc.: 40.62%] [G loss: 0.923848]\n",
      "4762 [D loss: 0.557899, acc.: 71.88%] [G loss: 0.921270]\n",
      "4763 [D loss: 0.620009, acc.: 68.75%] [G loss: 1.003488]\n",
      "4764 [D loss: 0.624804, acc.: 65.62%] [G loss: 0.878417]\n",
      "4765 [D loss: 0.571583, acc.: 65.62%] [G loss: 0.997122]\n",
      "4766 [D loss: 0.610671, acc.: 62.50%] [G loss: 0.958284]\n",
      "4767 [D loss: 0.710312, acc.: 50.00%] [G loss: 0.882491]\n",
      "4768 [D loss: 0.668619, acc.: 50.00%] [G loss: 0.954392]\n",
      "4769 [D loss: 0.681455, acc.: 53.12%] [G loss: 0.992306]\n",
      "4770 [D loss: 0.724230, acc.: 50.00%] [G loss: 0.969509]\n",
      "4771 [D loss: 0.640637, acc.: 59.38%] [G loss: 0.919137]\n",
      "4772 [D loss: 0.594938, acc.: 65.62%] [G loss: 0.954305]\n",
      "4773 [D loss: 0.629869, acc.: 56.25%] [G loss: 0.882434]\n",
      "4774 [D loss: 0.724348, acc.: 53.12%] [G loss: 0.851022]\n",
      "4775 [D loss: 0.712598, acc.: 53.12%] [G loss: 0.830873]\n",
      "4776 [D loss: 0.686876, acc.: 50.00%] [G loss: 0.921507]\n",
      "4777 [D loss: 0.726680, acc.: 53.12%] [G loss: 0.816282]\n",
      "4778 [D loss: 0.711106, acc.: 50.00%] [G loss: 0.744938]\n",
      "4779 [D loss: 0.661136, acc.: 65.62%] [G loss: 0.849118]\n",
      "4780 [D loss: 0.609189, acc.: 65.62%] [G loss: 0.844777]\n",
      "4781 [D loss: 0.612344, acc.: 62.50%] [G loss: 0.937833]\n",
      "4782 [D loss: 0.724195, acc.: 56.25%] [G loss: 0.954722]\n",
      "4783 [D loss: 0.684959, acc.: 59.38%] [G loss: 0.856148]\n",
      "4784 [D loss: 0.671913, acc.: 53.12%] [G loss: 0.976517]\n",
      "4785 [D loss: 0.676126, acc.: 53.12%] [G loss: 0.969788]\n",
      "4786 [D loss: 0.634659, acc.: 62.50%] [G loss: 0.940266]\n",
      "4787 [D loss: 0.565794, acc.: 75.00%] [G loss: 0.864476]\n",
      "4788 [D loss: 0.620906, acc.: 65.62%] [G loss: 0.951246]\n",
      "4789 [D loss: 0.793332, acc.: 56.25%] [G loss: 0.868121]\n",
      "4790 [D loss: 0.607403, acc.: 65.62%] [G loss: 0.945218]\n",
      "4791 [D loss: 0.662837, acc.: 71.88%] [G loss: 0.840012]\n",
      "4792 [D loss: 0.610742, acc.: 65.62%] [G loss: 0.946905]\n",
      "4793 [D loss: 0.597644, acc.: 71.88%] [G loss: 0.969626]\n",
      "4794 [D loss: 0.671763, acc.: 65.62%] [G loss: 0.852862]\n",
      "4795 [D loss: 0.644730, acc.: 56.25%] [G loss: 0.885821]\n",
      "4796 [D loss: 0.792105, acc.: 50.00%] [G loss: 0.843832]\n",
      "4797 [D loss: 0.597478, acc.: 78.12%] [G loss: 0.910972]\n",
      "4798 [D loss: 0.620338, acc.: 62.50%] [G loss: 0.940426]\n",
      "4799 [D loss: 0.558986, acc.: 75.00%] [G loss: 0.981202]\n",
      "4800 [D loss: 0.677370, acc.: 59.38%] [G loss: 0.851745]\n",
      "4801 [D loss: 0.634268, acc.: 65.62%] [G loss: 0.804808]\n",
      "4802 [D loss: 0.634419, acc.: 62.50%] [G loss: 0.866480]\n",
      "4803 [D loss: 0.635095, acc.: 62.50%] [G loss: 0.884525]\n",
      "4804 [D loss: 0.561223, acc.: 75.00%] [G loss: 1.009797]\n",
      "4805 [D loss: 0.632930, acc.: 56.25%] [G loss: 0.990710]\n",
      "4806 [D loss: 0.577032, acc.: 81.25%] [G loss: 0.912161]\n",
      "4807 [D loss: 0.715241, acc.: 43.75%] [G loss: 0.953003]\n",
      "4808 [D loss: 0.641769, acc.: 65.62%] [G loss: 0.862991]\n",
      "4809 [D loss: 0.727665, acc.: 46.88%] [G loss: 0.928842]\n",
      "4810 [D loss: 0.639665, acc.: 62.50%] [G loss: 0.864969]\n",
      "4811 [D loss: 0.607514, acc.: 62.50%] [G loss: 0.907475]\n",
      "4812 [D loss: 0.632176, acc.: 71.88%] [G loss: 0.929364]\n",
      "4813 [D loss: 0.618882, acc.: 65.62%] [G loss: 0.836179]\n",
      "4814 [D loss: 0.667561, acc.: 62.50%] [G loss: 0.811904]\n",
      "4815 [D loss: 0.571835, acc.: 75.00%] [G loss: 0.911728]\n",
      "4816 [D loss: 0.672330, acc.: 53.12%] [G loss: 0.891583]\n",
      "4817 [D loss: 0.685177, acc.: 59.38%] [G loss: 0.872562]\n",
      "4818 [D loss: 0.579190, acc.: 68.75%] [G loss: 0.881708]\n",
      "4819 [D loss: 0.621450, acc.: 62.50%] [G loss: 0.925504]\n",
      "4820 [D loss: 0.660038, acc.: 53.12%] [G loss: 0.957372]\n",
      "4821 [D loss: 0.641220, acc.: 65.62%] [G loss: 0.867161]\n",
      "4822 [D loss: 0.727643, acc.: 53.12%] [G loss: 0.898549]\n",
      "4823 [D loss: 0.542935, acc.: 68.75%] [G loss: 0.962262]\n",
      "4824 [D loss: 0.639474, acc.: 68.75%] [G loss: 0.971250]\n",
      "4825 [D loss: 0.626983, acc.: 62.50%] [G loss: 0.954655]\n",
      "4826 [D loss: 0.638479, acc.: 62.50%] [G loss: 0.971444]\n",
      "4827 [D loss: 0.678484, acc.: 50.00%] [G loss: 0.902516]\n",
      "4828 [D loss: 0.659335, acc.: 53.12%] [G loss: 0.951047]\n",
      "4829 [D loss: 0.616865, acc.: 75.00%] [G loss: 1.005282]\n",
      "4830 [D loss: 0.598466, acc.: 78.12%] [G loss: 0.926360]\n",
      "4831 [D loss: 0.609336, acc.: 68.75%] [G loss: 0.954821]\n",
      "4832 [D loss: 0.594288, acc.: 62.50%] [G loss: 0.851836]\n",
      "4833 [D loss: 0.505393, acc.: 81.25%] [G loss: 0.893345]\n",
      "4834 [D loss: 0.529593, acc.: 75.00%] [G loss: 0.898887]\n",
      "4835 [D loss: 0.571053, acc.: 65.62%] [G loss: 0.953862]\n",
      "4836 [D loss: 0.809139, acc.: 40.62%] [G loss: 0.904098]\n",
      "4837 [D loss: 0.652509, acc.: 53.12%] [G loss: 0.994922]\n",
      "4838 [D loss: 0.679628, acc.: 65.62%] [G loss: 0.895038]\n",
      "4839 [D loss: 0.593259, acc.: 75.00%] [G loss: 1.005819]\n",
      "4840 [D loss: 0.550217, acc.: 78.12%] [G loss: 0.891078]\n",
      "4841 [D loss: 0.718738, acc.: 40.62%] [G loss: 0.822396]\n",
      "4842 [D loss: 0.634256, acc.: 62.50%] [G loss: 0.882022]\n",
      "4843 [D loss: 0.657843, acc.: 68.75%] [G loss: 0.945343]\n",
      "4844 [D loss: 0.712577, acc.: 53.12%] [G loss: 0.911436]\n",
      "4845 [D loss: 0.584339, acc.: 75.00%] [G loss: 0.880638]\n",
      "4846 [D loss: 0.597169, acc.: 59.38%] [G loss: 0.908471]\n",
      "4847 [D loss: 0.587735, acc.: 68.75%] [G loss: 0.888247]\n",
      "4848 [D loss: 0.609426, acc.: 62.50%] [G loss: 0.877251]\n",
      "4849 [D loss: 0.663773, acc.: 56.25%] [G loss: 0.878335]\n",
      "4850 [D loss: 0.654638, acc.: 68.75%] [G loss: 0.780892]\n",
      "4851 [D loss: 0.621377, acc.: 75.00%] [G loss: 0.825794]\n",
      "4852 [D loss: 0.737504, acc.: 56.25%] [G loss: 0.813770]\n",
      "4853 [D loss: 0.670048, acc.: 62.50%] [G loss: 0.900126]\n",
      "4854 [D loss: 0.616473, acc.: 59.38%] [G loss: 0.890287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855 [D loss: 0.642966, acc.: 65.62%] [G loss: 0.951690]\n",
      "4856 [D loss: 0.603750, acc.: 71.88%] [G loss: 1.047136]\n",
      "4857 [D loss: 0.797612, acc.: 56.25%] [G loss: 1.001185]\n",
      "4858 [D loss: 0.559565, acc.: 71.88%] [G loss: 1.044785]\n",
      "4859 [D loss: 0.648475, acc.: 62.50%] [G loss: 0.940322]\n",
      "4860 [D loss: 0.549615, acc.: 75.00%] [G loss: 0.880576]\n",
      "4861 [D loss: 0.776845, acc.: 50.00%] [G loss: 0.886933]\n",
      "4862 [D loss: 0.672733, acc.: 65.62%] [G loss: 0.894566]\n",
      "4863 [D loss: 0.718409, acc.: 56.25%] [G loss: 0.965536]\n",
      "4864 [D loss: 0.570216, acc.: 71.88%] [G loss: 0.929238]\n",
      "4865 [D loss: 0.723503, acc.: 62.50%] [G loss: 0.911310]\n",
      "4866 [D loss: 0.721647, acc.: 46.88%] [G loss: 0.816277]\n",
      "4867 [D loss: 0.723084, acc.: 43.75%] [G loss: 0.924807]\n",
      "4868 [D loss: 0.612056, acc.: 65.62%] [G loss: 0.901415]\n",
      "4869 [D loss: 0.685485, acc.: 59.38%] [G loss: 0.827926]\n",
      "4870 [D loss: 0.586899, acc.: 68.75%] [G loss: 0.872388]\n",
      "4871 [D loss: 0.714652, acc.: 50.00%] [G loss: 0.871870]\n",
      "4872 [D loss: 0.700011, acc.: 53.12%] [G loss: 0.948932]\n",
      "4873 [D loss: 0.760558, acc.: 50.00%] [G loss: 0.906056]\n",
      "4874 [D loss: 0.697070, acc.: 53.12%] [G loss: 0.883610]\n",
      "4875 [D loss: 0.584469, acc.: 68.75%] [G loss: 0.841818]\n",
      "4876 [D loss: 0.616074, acc.: 65.62%] [G loss: 0.942363]\n",
      "4877 [D loss: 0.661651, acc.: 59.38%] [G loss: 0.938268]\n",
      "4878 [D loss: 0.628634, acc.: 65.62%] [G loss: 0.884165]\n",
      "4879 [D loss: 0.610807, acc.: 71.88%] [G loss: 0.934178]\n",
      "4880 [D loss: 0.644354, acc.: 56.25%] [G loss: 0.942960]\n",
      "4881 [D loss: 0.584876, acc.: 65.62%] [G loss: 0.954922]\n",
      "4882 [D loss: 0.651123, acc.: 59.38%] [G loss: 0.871783]\n",
      "4883 [D loss: 0.632112, acc.: 59.38%] [G loss: 0.864522]\n",
      "4884 [D loss: 0.659102, acc.: 59.38%] [G loss: 0.844657]\n",
      "4885 [D loss: 0.648585, acc.: 59.38%] [G loss: 0.875414]\n",
      "4886 [D loss: 0.653924, acc.: 65.62%] [G loss: 0.874166]\n",
      "4887 [D loss: 0.650708, acc.: 71.88%] [G loss: 0.926529]\n",
      "4888 [D loss: 0.694643, acc.: 56.25%] [G loss: 0.880482]\n",
      "4889 [D loss: 0.590719, acc.: 75.00%] [G loss: 0.931079]\n",
      "4890 [D loss: 0.608220, acc.: 65.62%] [G loss: 1.025339]\n",
      "4891 [D loss: 0.677127, acc.: 59.38%] [G loss: 0.923802]\n",
      "4892 [D loss: 0.656860, acc.: 62.50%] [G loss: 0.940778]\n",
      "4893 [D loss: 0.657446, acc.: 56.25%] [G loss: 0.891062]\n",
      "4894 [D loss: 0.636019, acc.: 59.38%] [G loss: 0.863638]\n",
      "4895 [D loss: 0.639797, acc.: 65.62%] [G loss: 0.965632]\n",
      "4896 [D loss: 0.619815, acc.: 71.88%] [G loss: 0.926151]\n",
      "4897 [D loss: 0.649897, acc.: 62.50%] [G loss: 1.013663]\n",
      "4898 [D loss: 0.591065, acc.: 68.75%] [G loss: 0.956298]\n",
      "4899 [D loss: 0.620127, acc.: 68.75%] [G loss: 0.974556]\n",
      "4900 [D loss: 0.627088, acc.: 68.75%] [G loss: 0.917135]\n",
      "4901 [D loss: 0.670806, acc.: 59.38%] [G loss: 0.883296]\n",
      "4902 [D loss: 0.646289, acc.: 75.00%] [G loss: 0.825084]\n",
      "4903 [D loss: 0.617390, acc.: 68.75%] [G loss: 0.861759]\n",
      "4904 [D loss: 0.578555, acc.: 75.00%] [G loss: 0.855610]\n",
      "4905 [D loss: 0.629906, acc.: 65.62%] [G loss: 0.931046]\n",
      "4906 [D loss: 0.695477, acc.: 50.00%] [G loss: 0.939267]\n",
      "4907 [D loss: 0.673554, acc.: 56.25%] [G loss: 0.960603]\n",
      "4908 [D loss: 0.652702, acc.: 53.12%] [G loss: 0.963635]\n",
      "4909 [D loss: 0.656387, acc.: 65.62%] [G loss: 0.961341]\n",
      "4910 [D loss: 0.626364, acc.: 65.62%] [G loss: 0.909651]\n",
      "4911 [D loss: 0.620794, acc.: 75.00%] [G loss: 0.951480]\n",
      "4912 [D loss: 0.721959, acc.: 56.25%] [G loss: 0.901705]\n",
      "4913 [D loss: 0.695160, acc.: 53.12%] [G loss: 0.895039]\n",
      "4914 [D loss: 0.605834, acc.: 78.12%] [G loss: 0.938894]\n",
      "4915 [D loss: 0.622208, acc.: 68.75%] [G loss: 0.955388]\n",
      "4916 [D loss: 0.621280, acc.: 62.50%] [G loss: 0.935309]\n",
      "4917 [D loss: 0.610794, acc.: 71.88%] [G loss: 1.001024]\n",
      "4918 [D loss: 0.667041, acc.: 59.38%] [G loss: 0.872530]\n",
      "4919 [D loss: 0.671708, acc.: 56.25%] [G loss: 0.927160]\n",
      "4920 [D loss: 0.563435, acc.: 71.88%] [G loss: 0.918234]\n",
      "4921 [D loss: 0.656277, acc.: 56.25%] [G loss: 0.950086]\n",
      "4922 [D loss: 0.649617, acc.: 59.38%] [G loss: 0.848216]\n",
      "4923 [D loss: 0.748515, acc.: 43.75%] [G loss: 0.837452]\n",
      "4924 [D loss: 0.645506, acc.: 78.12%] [G loss: 0.954816]\n",
      "4925 [D loss: 0.563263, acc.: 75.00%] [G loss: 0.932400]\n",
      "4926 [D loss: 0.645643, acc.: 62.50%] [G loss: 0.881626]\n",
      "4927 [D loss: 0.623408, acc.: 62.50%] [G loss: 0.922343]\n",
      "4928 [D loss: 0.626129, acc.: 62.50%] [G loss: 0.895616]\n",
      "4929 [D loss: 0.667174, acc.: 53.12%] [G loss: 0.829979]\n",
      "4930 [D loss: 0.715609, acc.: 46.88%] [G loss: 0.974671]\n",
      "4931 [D loss: 0.620622, acc.: 62.50%] [G loss: 0.964103]\n",
      "4932 [D loss: 0.597401, acc.: 71.88%] [G loss: 0.939440]\n",
      "4933 [D loss: 0.635051, acc.: 62.50%] [G loss: 0.906231]\n",
      "4934 [D loss: 0.760809, acc.: 50.00%] [G loss: 0.865760]\n",
      "4935 [D loss: 0.666249, acc.: 56.25%] [G loss: 0.783624]\n",
      "4936 [D loss: 0.720038, acc.: 50.00%] [G loss: 0.849409]\n",
      "4937 [D loss: 0.593205, acc.: 71.88%] [G loss: 0.926492]\n",
      "4938 [D loss: 0.627901, acc.: 65.62%] [G loss: 0.929360]\n",
      "4939 [D loss: 0.682386, acc.: 59.38%] [G loss: 0.922491]\n",
      "4940 [D loss: 0.639586, acc.: 59.38%] [G loss: 0.911040]\n",
      "4941 [D loss: 0.681221, acc.: 56.25%] [G loss: 0.943375]\n",
      "4942 [D loss: 0.672864, acc.: 59.38%] [G loss: 0.900333]\n",
      "4943 [D loss: 0.610128, acc.: 71.88%] [G loss: 0.897694]\n",
      "4944 [D loss: 0.618953, acc.: 71.88%] [G loss: 0.965943]\n",
      "4945 [D loss: 0.624446, acc.: 65.62%] [G loss: 0.909052]\n",
      "4946 [D loss: 0.714838, acc.: 43.75%] [G loss: 0.801527]\n",
      "4947 [D loss: 0.642911, acc.: 65.62%] [G loss: 0.870470]\n",
      "4948 [D loss: 0.631696, acc.: 68.75%] [G loss: 0.907830]\n",
      "4949 [D loss: 0.652413, acc.: 62.50%] [G loss: 1.014246]\n",
      "4950 [D loss: 0.693924, acc.: 46.88%] [G loss: 0.945813]\n",
      "4951 [D loss: 0.777569, acc.: 46.88%] [G loss: 0.898604]\n",
      "4952 [D loss: 0.638972, acc.: 65.62%] [G loss: 0.810562]\n",
      "4953 [D loss: 0.622344, acc.: 75.00%] [G loss: 0.922366]\n",
      "4954 [D loss: 0.685046, acc.: 50.00%] [G loss: 0.899172]\n",
      "4955 [D loss: 0.653855, acc.: 50.00%] [G loss: 0.896717]\n",
      "4956 [D loss: 0.618579, acc.: 65.62%] [G loss: 0.893366]\n",
      "4957 [D loss: 0.627761, acc.: 71.88%] [G loss: 0.863763]\n",
      "4958 [D loss: 0.836081, acc.: 50.00%] [G loss: 0.871342]\n",
      "4959 [D loss: 0.692605, acc.: 53.12%] [G loss: 0.898335]\n",
      "4960 [D loss: 0.633357, acc.: 59.38%] [G loss: 0.886794]\n",
      "4961 [D loss: 0.712401, acc.: 56.25%] [G loss: 0.906044]\n",
      "4962 [D loss: 0.639940, acc.: 56.25%] [G loss: 0.875318]\n",
      "4963 [D loss: 0.643766, acc.: 62.50%] [G loss: 0.938159]\n",
      "4964 [D loss: 0.630888, acc.: 62.50%] [G loss: 0.832677]\n",
      "4965 [D loss: 0.649586, acc.: 62.50%] [G loss: 0.844080]\n",
      "4966 [D loss: 0.651682, acc.: 62.50%] [G loss: 0.808911]\n",
      "4967 [D loss: 0.634947, acc.: 62.50%] [G loss: 0.896838]\n",
      "4968 [D loss: 0.579631, acc.: 71.88%] [G loss: 0.876360]\n",
      "4969 [D loss: 0.633903, acc.: 62.50%] [G loss: 0.921019]\n",
      "4970 [D loss: 0.550625, acc.: 75.00%] [G loss: 0.871989]\n",
      "4971 [D loss: 0.777640, acc.: 43.75%] [G loss: 0.918163]\n",
      "4972 [D loss: 0.682366, acc.: 56.25%] [G loss: 0.895384]\n",
      "4973 [D loss: 0.627487, acc.: 62.50%] [G loss: 0.953292]\n",
      "4974 [D loss: 0.594322, acc.: 75.00%] [G loss: 0.958028]\n",
      "4975 [D loss: 0.571853, acc.: 71.88%] [G loss: 0.883235]\n",
      "4976 [D loss: 0.622228, acc.: 62.50%] [G loss: 0.925424]\n",
      "4977 [D loss: 0.715826, acc.: 53.12%] [G loss: 0.882721]\n",
      "4978 [D loss: 0.591209, acc.: 65.62%] [G loss: 0.867467]\n",
      "4979 [D loss: 0.528456, acc.: 84.38%] [G loss: 0.914189]\n",
      "4980 [D loss: 0.726574, acc.: 50.00%] [G loss: 0.840887]\n",
      "4981 [D loss: 0.660053, acc.: 62.50%] [G loss: 0.840820]\n",
      "4982 [D loss: 0.732295, acc.: 56.25%] [G loss: 0.862526]\n",
      "4983 [D loss: 0.673138, acc.: 62.50%] [G loss: 0.837449]\n",
      "4984 [D loss: 0.599436, acc.: 75.00%] [G loss: 0.872028]\n",
      "4985 [D loss: 0.631795, acc.: 59.38%] [G loss: 0.880587]\n",
      "4986 [D loss: 0.663635, acc.: 62.50%] [G loss: 0.888575]\n",
      "4987 [D loss: 0.646280, acc.: 50.00%] [G loss: 0.896388]\n",
      "4988 [D loss: 0.669830, acc.: 59.38%] [G loss: 0.960707]\n",
      "4989 [D loss: 0.604071, acc.: 71.88%] [G loss: 0.992755]\n",
      "4990 [D loss: 0.709921, acc.: 53.12%] [G loss: 0.988428]\n",
      "4991 [D loss: 0.628663, acc.: 68.75%] [G loss: 0.947075]\n",
      "4992 [D loss: 0.692755, acc.: 50.00%] [G loss: 0.895940]\n",
      "4993 [D loss: 0.599867, acc.: 75.00%] [G loss: 0.925552]\n",
      "4994 [D loss: 0.620695, acc.: 59.38%] [G loss: 0.967962]\n",
      "4995 [D loss: 0.548804, acc.: 75.00%] [G loss: 0.954514]\n",
      "4996 [D loss: 0.647838, acc.: 62.50%] [G loss: 0.914198]\n",
      "4997 [D loss: 0.657439, acc.: 53.12%] [G loss: 0.896712]\n",
      "4998 [D loss: 0.683295, acc.: 65.62%] [G loss: 0.870826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 [D loss: 0.715509, acc.: 46.88%] [G loss: 0.927723]\n",
      "5000 [D loss: 0.596880, acc.: 62.50%] [G loss: 0.853307]\n",
      "5001 [D loss: 0.668100, acc.: 65.62%] [G loss: 0.774648]\n",
      "5002 [D loss: 0.655316, acc.: 71.88%] [G loss: 0.866834]\n",
      "5003 [D loss: 0.576887, acc.: 71.88%] [G loss: 0.849869]\n",
      "5004 [D loss: 0.680119, acc.: 56.25%] [G loss: 0.900200]\n",
      "5005 [D loss: 0.675406, acc.: 59.38%] [G loss: 0.990140]\n",
      "5006 [D loss: 0.661812, acc.: 59.38%] [G loss: 0.906364]\n",
      "5007 [D loss: 0.610608, acc.: 65.62%] [G loss: 0.893432]\n",
      "5008 [D loss: 0.580526, acc.: 71.88%] [G loss: 0.835021]\n",
      "5009 [D loss: 0.633033, acc.: 56.25%] [G loss: 0.853228]\n",
      "5010 [D loss: 0.647061, acc.: 59.38%] [G loss: 0.871639]\n",
      "5011 [D loss: 0.650875, acc.: 68.75%] [G loss: 0.881948]\n",
      "5012 [D loss: 0.678794, acc.: 53.12%] [G loss: 0.910142]\n",
      "5013 [D loss: 0.610934, acc.: 59.38%] [G loss: 0.898553]\n",
      "5014 [D loss: 0.578548, acc.: 68.75%] [G loss: 0.934107]\n",
      "5015 [D loss: 0.566783, acc.: 75.00%] [G loss: 0.886614]\n",
      "5016 [D loss: 0.712357, acc.: 53.12%] [G loss: 0.890725]\n",
      "5017 [D loss: 0.738185, acc.: 46.88%] [G loss: 0.968856]\n",
      "5018 [D loss: 0.714167, acc.: 56.25%] [G loss: 0.980571]\n",
      "5019 [D loss: 0.643897, acc.: 62.50%] [G loss: 1.003145]\n",
      "5020 [D loss: 0.705821, acc.: 53.12%] [G loss: 0.941827]\n",
      "5021 [D loss: 0.637625, acc.: 59.38%] [G loss: 0.973237]\n",
      "5022 [D loss: 0.622608, acc.: 56.25%] [G loss: 0.979481]\n",
      "5023 [D loss: 0.756056, acc.: 53.12%] [G loss: 1.026138]\n",
      "5024 [D loss: 0.614479, acc.: 62.50%] [G loss: 0.946744]\n",
      "5025 [D loss: 0.687606, acc.: 56.25%] [G loss: 0.920780]\n",
      "5026 [D loss: 0.681880, acc.: 50.00%] [G loss: 1.009851]\n",
      "5027 [D loss: 0.652552, acc.: 59.38%] [G loss: 1.047011]\n",
      "5028 [D loss: 0.549895, acc.: 81.25%] [G loss: 0.972295]\n",
      "5029 [D loss: 0.649365, acc.: 59.38%] [G loss: 0.999550]\n",
      "5030 [D loss: 0.660972, acc.: 62.50%] [G loss: 0.913132]\n",
      "5031 [D loss: 0.681363, acc.: 53.12%] [G loss: 0.855090]\n",
      "5032 [D loss: 0.634333, acc.: 65.62%] [G loss: 0.846054]\n",
      "5033 [D loss: 0.756858, acc.: 46.88%] [G loss: 0.802304]\n",
      "5034 [D loss: 0.602403, acc.: 71.88%] [G loss: 0.865590]\n",
      "5035 [D loss: 0.674946, acc.: 59.38%] [G loss: 0.880229]\n",
      "5036 [D loss: 0.690529, acc.: 50.00%] [G loss: 0.870033]\n",
      "5037 [D loss: 0.636117, acc.: 65.62%] [G loss: 0.982986]\n",
      "5038 [D loss: 0.615503, acc.: 68.75%] [G loss: 1.047305]\n",
      "5039 [D loss: 0.661079, acc.: 62.50%] [G loss: 0.968017]\n",
      "5040 [D loss: 0.600786, acc.: 68.75%] [G loss: 0.905715]\n",
      "5041 [D loss: 0.704677, acc.: 53.12%] [G loss: 0.879111]\n",
      "5042 [D loss: 0.602234, acc.: 65.62%] [G loss: 0.919264]\n",
      "5043 [D loss: 0.652372, acc.: 53.12%] [G loss: 0.864831]\n",
      "5044 [D loss: 0.623058, acc.: 62.50%] [G loss: 0.908547]\n",
      "5045 [D loss: 0.634596, acc.: 56.25%] [G loss: 0.905818]\n",
      "5046 [D loss: 0.623510, acc.: 62.50%] [G loss: 0.866940]\n",
      "5047 [D loss: 0.689981, acc.: 59.38%] [G loss: 0.828365]\n",
      "5048 [D loss: 0.589879, acc.: 75.00%] [G loss: 0.896333]\n",
      "5049 [D loss: 0.628728, acc.: 68.75%] [G loss: 0.798618]\n",
      "5050 [D loss: 0.696786, acc.: 53.12%] [G loss: 0.929533]\n",
      "5051 [D loss: 0.716378, acc.: 62.50%] [G loss: 0.942633]\n",
      "5052 [D loss: 0.627240, acc.: 65.62%] [G loss: 0.926014]\n",
      "5053 [D loss: 0.645768, acc.: 71.88%] [G loss: 0.907858]\n",
      "5054 [D loss: 0.642723, acc.: 56.25%] [G loss: 0.907848]\n",
      "5055 [D loss: 0.687519, acc.: 53.12%] [G loss: 0.861658]\n",
      "5056 [D loss: 0.591574, acc.: 68.75%] [G loss: 0.865328]\n",
      "5057 [D loss: 0.579578, acc.: 71.88%] [G loss: 0.958336]\n",
      "5058 [D loss: 0.690676, acc.: 59.38%] [G loss: 0.940949]\n",
      "5059 [D loss: 0.646218, acc.: 65.62%] [G loss: 0.934349]\n",
      "5060 [D loss: 0.731316, acc.: 46.88%] [G loss: 0.954006]\n",
      "5061 [D loss: 0.632892, acc.: 53.12%] [G loss: 0.873564]\n",
      "5062 [D loss: 0.633821, acc.: 59.38%] [G loss: 0.806496]\n",
      "5063 [D loss: 0.596547, acc.: 78.12%] [G loss: 0.850862]\n",
      "5064 [D loss: 0.637839, acc.: 59.38%] [G loss: 0.782932]\n",
      "5065 [D loss: 0.626167, acc.: 62.50%] [G loss: 0.822995]\n",
      "5066 [D loss: 0.645652, acc.: 59.38%] [G loss: 0.835879]\n",
      "5067 [D loss: 0.705306, acc.: 56.25%] [G loss: 0.910390]\n",
      "5068 [D loss: 0.691420, acc.: 56.25%] [G loss: 0.967024]\n",
      "5069 [D loss: 0.709436, acc.: 53.12%] [G loss: 0.887835]\n",
      "5070 [D loss: 0.594768, acc.: 68.75%] [G loss: 0.909075]\n",
      "5071 [D loss: 0.561165, acc.: 75.00%] [G loss: 0.877295]\n",
      "5072 [D loss: 0.671224, acc.: 65.62%] [G loss: 0.890858]\n",
      "5073 [D loss: 0.594606, acc.: 71.88%] [G loss: 0.973409]\n",
      "5074 [D loss: 0.606312, acc.: 62.50%] [G loss: 0.823433]\n",
      "5075 [D loss: 0.602185, acc.: 65.62%] [G loss: 0.859526]\n",
      "5076 [D loss: 0.728151, acc.: 62.50%] [G loss: 0.880130]\n",
      "5077 [D loss: 0.607675, acc.: 81.25%] [G loss: 0.936084]\n",
      "5078 [D loss: 0.740356, acc.: 56.25%] [G loss: 0.886297]\n",
      "5079 [D loss: 0.596678, acc.: 56.25%] [G loss: 0.880601]\n",
      "5080 [D loss: 0.645389, acc.: 53.12%] [G loss: 0.857729]\n",
      "5081 [D loss: 0.559557, acc.: 81.25%] [G loss: 0.904331]\n",
      "5082 [D loss: 0.700313, acc.: 53.12%] [G loss: 0.905311]\n",
      "5083 [D loss: 0.557540, acc.: 65.62%] [G loss: 0.863268]\n",
      "5084 [D loss: 0.728635, acc.: 46.88%] [G loss: 0.800267]\n",
      "5085 [D loss: 0.638387, acc.: 65.62%] [G loss: 0.863314]\n",
      "5086 [D loss: 0.660325, acc.: 59.38%] [G loss: 0.889602]\n",
      "5087 [D loss: 0.704992, acc.: 56.25%] [G loss: 0.823215]\n",
      "5088 [D loss: 0.614363, acc.: 65.62%] [G loss: 0.966236]\n",
      "5089 [D loss: 0.730894, acc.: 50.00%] [G loss: 0.911728]\n",
      "5090 [D loss: 0.709701, acc.: 56.25%] [G loss: 0.862886]\n",
      "5091 [D loss: 0.684753, acc.: 50.00%] [G loss: 0.888057]\n",
      "5092 [D loss: 0.670857, acc.: 59.38%] [G loss: 0.884669]\n",
      "5093 [D loss: 0.683660, acc.: 53.12%] [G loss: 0.922261]\n",
      "5094 [D loss: 0.623969, acc.: 56.25%] [G loss: 0.911981]\n",
      "5095 [D loss: 0.653382, acc.: 59.38%] [G loss: 0.890497]\n",
      "5096 [D loss: 0.713418, acc.: 46.88%] [G loss: 0.878068]\n",
      "5097 [D loss: 0.724413, acc.: 62.50%] [G loss: 0.869306]\n",
      "5098 [D loss: 0.600836, acc.: 71.88%] [G loss: 0.799050]\n",
      "5099 [D loss: 0.705182, acc.: 59.38%] [G loss: 0.855656]\n",
      "5100 [D loss: 0.603509, acc.: 65.62%] [G loss: 0.910451]\n",
      "5101 [D loss: 0.670379, acc.: 62.50%] [G loss: 0.943822]\n",
      "5102 [D loss: 0.681923, acc.: 59.38%] [G loss: 0.905709]\n",
      "5103 [D loss: 0.583921, acc.: 71.88%] [G loss: 0.934649]\n",
      "5104 [D loss: 0.525505, acc.: 75.00%] [G loss: 0.847429]\n",
      "5105 [D loss: 0.630487, acc.: 65.62%] [G loss: 0.947173]\n",
      "5106 [D loss: 0.636995, acc.: 68.75%] [G loss: 0.877118]\n",
      "5107 [D loss: 0.638638, acc.: 68.75%] [G loss: 0.865348]\n",
      "5108 [D loss: 0.560400, acc.: 78.12%] [G loss: 0.849473]\n",
      "5109 [D loss: 0.669070, acc.: 56.25%] [G loss: 0.872772]\n",
      "5110 [D loss: 0.680937, acc.: 62.50%] [G loss: 0.971355]\n",
      "5111 [D loss: 0.747150, acc.: 46.88%] [G loss: 0.923104]\n",
      "5112 [D loss: 0.638663, acc.: 56.25%] [G loss: 0.943466]\n",
      "5113 [D loss: 0.719321, acc.: 46.88%] [G loss: 0.952583]\n",
      "5114 [D loss: 0.600355, acc.: 75.00%] [G loss: 0.920700]\n",
      "5115 [D loss: 0.631681, acc.: 56.25%] [G loss: 0.897608]\n",
      "5116 [D loss: 0.723840, acc.: 56.25%] [G loss: 0.842655]\n",
      "5117 [D loss: 0.612718, acc.: 59.38%] [G loss: 0.863632]\n",
      "5118 [D loss: 0.614496, acc.: 65.62%] [G loss: 0.893650]\n",
      "5119 [D loss: 0.624545, acc.: 62.50%] [G loss: 0.879812]\n",
      "5120 [D loss: 0.739122, acc.: 50.00%] [G loss: 0.968474]\n",
      "5121 [D loss: 0.588041, acc.: 75.00%] [G loss: 0.918190]\n",
      "5122 [D loss: 0.609829, acc.: 71.88%] [G loss: 0.915063]\n",
      "5123 [D loss: 0.580487, acc.: 68.75%] [G loss: 0.878188]\n",
      "5124 [D loss: 0.733079, acc.: 50.00%] [G loss: 0.894074]\n",
      "5125 [D loss: 0.696517, acc.: 56.25%] [G loss: 0.862608]\n",
      "5126 [D loss: 0.700796, acc.: 62.50%] [G loss: 0.873417]\n",
      "5127 [D loss: 0.591454, acc.: 59.38%] [G loss: 0.972350]\n",
      "5128 [D loss: 0.696404, acc.: 50.00%] [G loss: 0.947333]\n",
      "5129 [D loss: 0.576801, acc.: 75.00%] [G loss: 0.889973]\n",
      "5130 [D loss: 0.588766, acc.: 65.62%] [G loss: 0.917240]\n",
      "5131 [D loss: 0.638724, acc.: 65.62%] [G loss: 0.874039]\n",
      "5132 [D loss: 0.626353, acc.: 68.75%] [G loss: 0.871132]\n",
      "5133 [D loss: 0.616942, acc.: 75.00%] [G loss: 0.873590]\n",
      "5134 [D loss: 0.692599, acc.: 43.75%] [G loss: 0.889143]\n",
      "5135 [D loss: 0.567179, acc.: 75.00%] [G loss: 0.919427]\n",
      "5136 [D loss: 0.609485, acc.: 75.00%] [G loss: 0.919838]\n",
      "5137 [D loss: 0.520885, acc.: 84.38%] [G loss: 0.989538]\n",
      "5138 [D loss: 0.613677, acc.: 59.38%] [G loss: 0.868498]\n",
      "5139 [D loss: 0.678156, acc.: 50.00%] [G loss: 0.871280]\n",
      "5140 [D loss: 0.673092, acc.: 65.62%] [G loss: 0.815086]\n",
      "5141 [D loss: 0.557533, acc.: 81.25%] [G loss: 0.990394]\n",
      "5142 [D loss: 0.592750, acc.: 65.62%] [G loss: 0.851050]\n",
      "5143 [D loss: 0.739032, acc.: 59.38%] [G loss: 0.880714]\n",
      "5144 [D loss: 0.688457, acc.: 56.25%] [G loss: 0.934262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145 [D loss: 0.629189, acc.: 68.75%] [G loss: 0.938775]\n",
      "5146 [D loss: 0.595437, acc.: 68.75%] [G loss: 1.021376]\n",
      "5147 [D loss: 0.638737, acc.: 65.62%] [G loss: 0.979011]\n",
      "5148 [D loss: 0.543368, acc.: 78.12%] [G loss: 0.976221]\n",
      "5149 [D loss: 0.666362, acc.: 65.62%] [G loss: 0.924381]\n",
      "5150 [D loss: 0.583769, acc.: 65.62%] [G loss: 0.883987]\n",
      "5151 [D loss: 0.772300, acc.: 43.75%] [G loss: 0.941409]\n",
      "5152 [D loss: 0.558507, acc.: 71.88%] [G loss: 0.928523]\n",
      "5153 [D loss: 0.643999, acc.: 62.50%] [G loss: 0.891298]\n",
      "5154 [D loss: 0.768628, acc.: 43.75%] [G loss: 0.898484]\n",
      "5155 [D loss: 0.599889, acc.: 68.75%] [G loss: 0.948386]\n",
      "5156 [D loss: 0.699561, acc.: 56.25%] [G loss: 0.947280]\n",
      "5157 [D loss: 0.690639, acc.: 62.50%] [G loss: 0.934189]\n",
      "5158 [D loss: 0.649796, acc.: 56.25%] [G loss: 0.864256]\n",
      "5159 [D loss: 0.536409, acc.: 71.88%] [G loss: 0.949822]\n",
      "5160 [D loss: 0.702502, acc.: 43.75%] [G loss: 0.954624]\n",
      "5161 [D loss: 0.675822, acc.: 62.50%] [G loss: 0.961671]\n",
      "5162 [D loss: 0.573043, acc.: 78.12%] [G loss: 1.045480]\n",
      "5163 [D loss: 0.697060, acc.: 56.25%] [G loss: 0.908983]\n",
      "5164 [D loss: 0.642087, acc.: 59.38%] [G loss: 0.967418]\n",
      "5165 [D loss: 0.684837, acc.: 50.00%] [G loss: 0.950715]\n",
      "5166 [D loss: 0.627815, acc.: 71.88%] [G loss: 0.887009]\n",
      "5167 [D loss: 0.609207, acc.: 78.12%] [G loss: 0.907797]\n",
      "5168 [D loss: 0.605606, acc.: 71.88%] [G loss: 0.847338]\n",
      "5169 [D loss: 0.693509, acc.: 62.50%] [G loss: 0.868213]\n",
      "5170 [D loss: 0.724456, acc.: 50.00%] [G loss: 0.848605]\n",
      "5171 [D loss: 0.661022, acc.: 62.50%] [G loss: 0.883019]\n",
      "5172 [D loss: 0.667653, acc.: 56.25%] [G loss: 0.882980]\n",
      "5173 [D loss: 0.585142, acc.: 75.00%] [G loss: 0.883654]\n",
      "5174 [D loss: 0.669918, acc.: 68.75%] [G loss: 0.804790]\n",
      "5175 [D loss: 0.652589, acc.: 56.25%] [G loss: 0.825875]\n",
      "5176 [D loss: 0.705356, acc.: 62.50%] [G loss: 0.777270]\n",
      "5177 [D loss: 0.794170, acc.: 40.62%] [G loss: 0.822437]\n",
      "5178 [D loss: 0.716646, acc.: 56.25%] [G loss: 0.854384]\n",
      "5179 [D loss: 0.740135, acc.: 50.00%] [G loss: 0.912996]\n",
      "5180 [D loss: 0.638366, acc.: 62.50%] [G loss: 0.862056]\n",
      "5181 [D loss: 0.615901, acc.: 68.75%] [G loss: 0.814990]\n",
      "5182 [D loss: 0.697685, acc.: 56.25%] [G loss: 0.934658]\n",
      "5183 [D loss: 0.650078, acc.: 68.75%] [G loss: 0.880078]\n",
      "5184 [D loss: 0.738130, acc.: 53.12%] [G loss: 0.944790]\n",
      "5185 [D loss: 0.687006, acc.: 62.50%] [G loss: 0.863506]\n",
      "5186 [D loss: 0.743992, acc.: 50.00%] [G loss: 0.825438]\n",
      "5187 [D loss: 0.704479, acc.: 50.00%] [G loss: 0.904718]\n",
      "5188 [D loss: 0.631655, acc.: 62.50%] [G loss: 0.922888]\n",
      "5189 [D loss: 0.608355, acc.: 75.00%] [G loss: 0.982036]\n",
      "5190 [D loss: 0.626985, acc.: 65.62%] [G loss: 0.861496]\n",
      "5191 [D loss: 0.594663, acc.: 71.88%] [G loss: 0.883073]\n",
      "5192 [D loss: 0.625725, acc.: 68.75%] [G loss: 0.930971]\n",
      "5193 [D loss: 0.657868, acc.: 68.75%] [G loss: 0.865348]\n",
      "5194 [D loss: 0.589565, acc.: 71.88%] [G loss: 0.913269]\n",
      "5195 [D loss: 0.620709, acc.: 62.50%] [G loss: 1.057584]\n",
      "5196 [D loss: 0.718061, acc.: 56.25%] [G loss: 0.935968]\n",
      "5197 [D loss: 0.681647, acc.: 50.00%] [G loss: 0.893341]\n",
      "5198 [D loss: 0.683565, acc.: 56.25%] [G loss: 0.904441]\n",
      "5199 [D loss: 0.667142, acc.: 53.12%] [G loss: 0.831815]\n",
      "5200 [D loss: 0.690543, acc.: 62.50%] [G loss: 0.914612]\n",
      "5201 [D loss: 0.586924, acc.: 84.38%] [G loss: 0.923129]\n",
      "5202 [D loss: 0.529545, acc.: 75.00%] [G loss: 0.937809]\n",
      "5203 [D loss: 0.768272, acc.: 46.88%] [G loss: 0.863932]\n",
      "5204 [D loss: 0.693300, acc.: 56.25%] [G loss: 0.804586]\n",
      "5205 [D loss: 0.580917, acc.: 75.00%] [G loss: 0.923569]\n",
      "5206 [D loss: 0.620150, acc.: 56.25%] [G loss: 0.890257]\n",
      "5207 [D loss: 0.709955, acc.: 53.12%] [G loss: 0.842210]\n",
      "5208 [D loss: 0.540298, acc.: 75.00%] [G loss: 0.930791]\n",
      "5209 [D loss: 0.508417, acc.: 75.00%] [G loss: 0.915092]\n",
      "5210 [D loss: 0.650546, acc.: 53.12%] [G loss: 0.941569]\n",
      "5211 [D loss: 0.620199, acc.: 68.75%] [G loss: 0.831168]\n",
      "5212 [D loss: 0.626225, acc.: 59.38%] [G loss: 1.057691]\n",
      "5213 [D loss: 0.671584, acc.: 62.50%] [G loss: 0.931725]\n",
      "5214 [D loss: 0.743384, acc.: 46.88%] [G loss: 0.884958]\n",
      "5215 [D loss: 0.686035, acc.: 59.38%] [G loss: 0.832636]\n",
      "5216 [D loss: 0.628493, acc.: 65.62%] [G loss: 0.843869]\n",
      "5217 [D loss: 0.623776, acc.: 65.62%] [G loss: 0.895813]\n",
      "5218 [D loss: 0.613047, acc.: 71.88%] [G loss: 0.937788]\n",
      "5219 [D loss: 0.721867, acc.: 68.75%] [G loss: 0.905790]\n",
      "5220 [D loss: 0.586436, acc.: 65.62%] [G loss: 0.873642]\n",
      "5221 [D loss: 0.635348, acc.: 62.50%] [G loss: 0.898175]\n",
      "5222 [D loss: 0.699937, acc.: 59.38%] [G loss: 0.911479]\n",
      "5223 [D loss: 0.592675, acc.: 68.75%] [G loss: 0.898424]\n",
      "5224 [D loss: 0.667007, acc.: 59.38%] [G loss: 0.873064]\n",
      "5225 [D loss: 0.687998, acc.: 56.25%] [G loss: 0.980545]\n",
      "5226 [D loss: 0.712666, acc.: 40.62%] [G loss: 0.940511]\n",
      "5227 [D loss: 0.711418, acc.: 56.25%] [G loss: 0.810907]\n",
      "5228 [D loss: 0.612109, acc.: 71.88%] [G loss: 0.867940]\n",
      "5229 [D loss: 0.653870, acc.: 71.88%] [G loss: 0.877663]\n",
      "5230 [D loss: 0.633360, acc.: 68.75%] [G loss: 0.896601]\n",
      "5231 [D loss: 0.616218, acc.: 62.50%] [G loss: 0.892104]\n",
      "5232 [D loss: 0.709560, acc.: 59.38%] [G loss: 0.855164]\n",
      "5233 [D loss: 0.604116, acc.: 65.62%] [G loss: 0.838136]\n",
      "5234 [D loss: 0.617355, acc.: 68.75%] [G loss: 0.829193]\n",
      "5235 [D loss: 0.560204, acc.: 75.00%] [G loss: 0.829550]\n",
      "5236 [D loss: 0.727432, acc.: 56.25%] [G loss: 0.814398]\n",
      "5237 [D loss: 0.741208, acc.: 43.75%] [G loss: 0.884332]\n",
      "5238 [D loss: 0.682098, acc.: 53.12%] [G loss: 0.974647]\n",
      "5239 [D loss: 0.737229, acc.: 53.12%] [G loss: 0.946712]\n",
      "5240 [D loss: 0.635572, acc.: 65.62%] [G loss: 1.050542]\n",
      "5241 [D loss: 0.613253, acc.: 62.50%] [G loss: 1.011234]\n",
      "5242 [D loss: 0.641417, acc.: 65.62%] [G loss: 0.897850]\n",
      "5243 [D loss: 0.628859, acc.: 75.00%] [G loss: 0.827057]\n",
      "5244 [D loss: 0.654756, acc.: 50.00%] [G loss: 0.847902]\n",
      "5245 [D loss: 0.669640, acc.: 53.12%] [G loss: 0.889085]\n",
      "5246 [D loss: 0.681291, acc.: 53.12%] [G loss: 0.861759]\n",
      "5247 [D loss: 0.639893, acc.: 65.62%] [G loss: 0.892895]\n",
      "5248 [D loss: 0.614731, acc.: 68.75%] [G loss: 0.928680]\n",
      "5249 [D loss: 0.609693, acc.: 56.25%] [G loss: 0.979740]\n",
      "5250 [D loss: 0.686996, acc.: 62.50%] [G loss: 0.948286]\n",
      "5251 [D loss: 0.584020, acc.: 75.00%] [G loss: 0.995764]\n",
      "5252 [D loss: 0.586822, acc.: 65.62%] [G loss: 1.045803]\n",
      "5253 [D loss: 0.681883, acc.: 50.00%] [G loss: 0.963799]\n",
      "5254 [D loss: 0.528452, acc.: 81.25%] [G loss: 0.919986]\n",
      "5255 [D loss: 0.689328, acc.: 46.88%] [G loss: 0.793221]\n",
      "5256 [D loss: 0.593567, acc.: 68.75%] [G loss: 0.916199]\n",
      "5257 [D loss: 0.567894, acc.: 65.62%] [G loss: 1.003680]\n",
      "5258 [D loss: 0.635928, acc.: 65.62%] [G loss: 0.912381]\n",
      "5259 [D loss: 0.822111, acc.: 34.38%] [G loss: 0.856517]\n",
      "5260 [D loss: 0.567052, acc.: 81.25%] [G loss: 0.900389]\n",
      "5261 [D loss: 0.628181, acc.: 68.75%] [G loss: 0.926783]\n",
      "5262 [D loss: 0.696817, acc.: 56.25%] [G loss: 0.833804]\n",
      "5263 [D loss: 0.615910, acc.: 68.75%] [G loss: 0.994823]\n",
      "5264 [D loss: 0.660971, acc.: 65.62%] [G loss: 0.875484]\n",
      "5265 [D loss: 0.629490, acc.: 62.50%] [G loss: 0.844788]\n",
      "5266 [D loss: 0.652184, acc.: 59.38%] [G loss: 0.872149]\n",
      "5267 [D loss: 0.547631, acc.: 75.00%] [G loss: 0.986844]\n",
      "5268 [D loss: 0.643763, acc.: 62.50%] [G loss: 0.940584]\n",
      "5269 [D loss: 0.643847, acc.: 71.88%] [G loss: 0.895843]\n",
      "5270 [D loss: 0.582525, acc.: 78.12%] [G loss: 0.878215]\n",
      "5271 [D loss: 0.632804, acc.: 56.25%] [G loss: 0.865405]\n",
      "5272 [D loss: 0.710305, acc.: 53.12%] [G loss: 0.887266]\n",
      "5273 [D loss: 0.646690, acc.: 53.12%] [G loss: 0.864842]\n",
      "5274 [D loss: 0.666074, acc.: 59.38%] [G loss: 0.941731]\n",
      "5275 [D loss: 0.583465, acc.: 78.12%] [G loss: 0.910971]\n",
      "5276 [D loss: 0.587219, acc.: 75.00%] [G loss: 0.954507]\n",
      "5277 [D loss: 0.674764, acc.: 56.25%] [G loss: 0.891757]\n",
      "5278 [D loss: 0.775479, acc.: 40.62%] [G loss: 0.920621]\n",
      "5279 [D loss: 0.699699, acc.: 50.00%] [G loss: 0.881220]\n",
      "5280 [D loss: 0.612008, acc.: 68.75%] [G loss: 0.941564]\n",
      "5281 [D loss: 0.650163, acc.: 68.75%] [G loss: 0.985937]\n",
      "5282 [D loss: 0.672227, acc.: 56.25%] [G loss: 0.872550]\n",
      "5283 [D loss: 0.739666, acc.: 43.75%] [G loss: 0.858071]\n",
      "5284 [D loss: 0.630632, acc.: 59.38%] [G loss: 0.840833]\n",
      "5285 [D loss: 0.676857, acc.: 56.25%] [G loss: 0.986539]\n",
      "5286 [D loss: 0.630751, acc.: 62.50%] [G loss: 0.986162]\n",
      "5287 [D loss: 0.630276, acc.: 68.75%] [G loss: 0.945093]\n",
      "5288 [D loss: 0.610977, acc.: 62.50%] [G loss: 0.861133]\n",
      "5289 [D loss: 0.651661, acc.: 62.50%] [G loss: 0.930730]\n",
      "5290 [D loss: 0.646833, acc.: 62.50%] [G loss: 0.949711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5291 [D loss: 0.622728, acc.: 62.50%] [G loss: 0.894380]\n",
      "5292 [D loss: 0.637638, acc.: 71.88%] [G loss: 0.937794]\n",
      "5293 [D loss: 0.685323, acc.: 62.50%] [G loss: 1.007892]\n",
      "5294 [D loss: 0.632451, acc.: 59.38%] [G loss: 0.959156]\n",
      "5295 [D loss: 0.596743, acc.: 68.75%] [G loss: 0.944958]\n",
      "5296 [D loss: 0.615487, acc.: 65.62%] [G loss: 0.905709]\n",
      "5297 [D loss: 0.622397, acc.: 65.62%] [G loss: 0.933609]\n",
      "5298 [D loss: 0.702993, acc.: 56.25%] [G loss: 1.006571]\n",
      "5299 [D loss: 0.650023, acc.: 56.25%] [G loss: 0.942969]\n",
      "5300 [D loss: 0.736930, acc.: 50.00%] [G loss: 0.870389]\n",
      "5301 [D loss: 0.683639, acc.: 50.00%] [G loss: 0.927765]\n",
      "5302 [D loss: 0.603101, acc.: 65.62%] [G loss: 0.978650]\n",
      "5303 [D loss: 0.691548, acc.: 62.50%] [G loss: 0.917551]\n",
      "5304 [D loss: 0.617812, acc.: 56.25%] [G loss: 0.910262]\n",
      "5305 [D loss: 0.539651, acc.: 75.00%] [G loss: 0.937305]\n",
      "5306 [D loss: 0.773788, acc.: 43.75%] [G loss: 0.898673]\n",
      "5307 [D loss: 0.576708, acc.: 65.62%] [G loss: 0.882799]\n",
      "5308 [D loss: 0.547734, acc.: 71.88%] [G loss: 0.816113]\n",
      "5309 [D loss: 0.673327, acc.: 50.00%] [G loss: 0.810439]\n",
      "5310 [D loss: 0.530920, acc.: 75.00%] [G loss: 0.819614]\n",
      "5311 [D loss: 0.688687, acc.: 59.38%] [G loss: 0.801395]\n",
      "5312 [D loss: 0.638595, acc.: 59.38%] [G loss: 0.808788]\n",
      "5313 [D loss: 0.662559, acc.: 56.25%] [G loss: 0.877340]\n",
      "5314 [D loss: 0.710115, acc.: 65.62%] [G loss: 0.886788]\n",
      "5315 [D loss: 0.629054, acc.: 68.75%] [G loss: 1.023619]\n",
      "5316 [D loss: 0.548180, acc.: 78.12%] [G loss: 0.992829]\n",
      "5317 [D loss: 0.747598, acc.: 50.00%] [G loss: 0.870999]\n",
      "5318 [D loss: 0.571537, acc.: 75.00%] [G loss: 0.873404]\n",
      "5319 [D loss: 0.614361, acc.: 62.50%] [G loss: 0.954134]\n",
      "5320 [D loss: 0.597079, acc.: 71.88%] [G loss: 0.875147]\n",
      "5321 [D loss: 0.659534, acc.: 62.50%] [G loss: 0.862015]\n",
      "5322 [D loss: 0.638665, acc.: 68.75%] [G loss: 0.935147]\n",
      "5323 [D loss: 0.628971, acc.: 65.62%] [G loss: 0.916341]\n",
      "5324 [D loss: 0.628088, acc.: 62.50%] [G loss: 0.930738]\n",
      "5325 [D loss: 0.588401, acc.: 78.12%] [G loss: 0.896068]\n",
      "5326 [D loss: 0.681379, acc.: 62.50%] [G loss: 0.920708]\n",
      "5327 [D loss: 0.644123, acc.: 59.38%] [G loss: 0.940555]\n",
      "5328 [D loss: 0.588596, acc.: 71.88%] [G loss: 0.863085]\n",
      "5329 [D loss: 0.624844, acc.: 65.62%] [G loss: 0.881906]\n",
      "5330 [D loss: 0.625280, acc.: 65.62%] [G loss: 0.974198]\n",
      "5331 [D loss: 0.727082, acc.: 56.25%] [G loss: 0.942666]\n",
      "5332 [D loss: 0.674034, acc.: 62.50%] [G loss: 0.906768]\n",
      "5333 [D loss: 0.607604, acc.: 65.62%] [G loss: 0.877954]\n",
      "5334 [D loss: 0.617379, acc.: 65.62%] [G loss: 0.965370]\n",
      "5335 [D loss: 0.597349, acc.: 62.50%] [G loss: 0.919370]\n",
      "5336 [D loss: 0.693513, acc.: 46.88%] [G loss: 0.931101]\n",
      "5337 [D loss: 0.623166, acc.: 71.88%] [G loss: 0.884208]\n",
      "5338 [D loss: 0.564446, acc.: 65.62%] [G loss: 0.889550]\n",
      "5339 [D loss: 0.694236, acc.: 46.88%] [G loss: 0.928848]\n",
      "5340 [D loss: 0.565749, acc.: 68.75%] [G loss: 0.934702]\n",
      "5341 [D loss: 0.736876, acc.: 53.12%] [G loss: 0.864112]\n",
      "5342 [D loss: 0.611195, acc.: 68.75%] [G loss: 0.823405]\n",
      "5343 [D loss: 0.774091, acc.: 56.25%] [G loss: 0.912433]\n",
      "5344 [D loss: 0.693746, acc.: 46.88%] [G loss: 0.837825]\n",
      "5345 [D loss: 0.623799, acc.: 62.50%] [G loss: 0.929551]\n",
      "5346 [D loss: 0.678292, acc.: 59.38%] [G loss: 0.910984]\n",
      "5347 [D loss: 0.624536, acc.: 62.50%] [G loss: 0.958067]\n",
      "5348 [D loss: 0.650112, acc.: 53.12%] [G loss: 1.029273]\n",
      "5349 [D loss: 0.585276, acc.: 56.25%] [G loss: 0.965086]\n",
      "5350 [D loss: 0.676414, acc.: 56.25%] [G loss: 0.908805]\n",
      "5351 [D loss: 0.811991, acc.: 50.00%] [G loss: 0.915147]\n",
      "5352 [D loss: 0.598984, acc.: 62.50%] [G loss: 0.979508]\n",
      "5353 [D loss: 0.625759, acc.: 78.12%] [G loss: 0.884790]\n",
      "5354 [D loss: 0.617130, acc.: 62.50%] [G loss: 0.888517]\n",
      "5355 [D loss: 0.600617, acc.: 68.75%] [G loss: 0.829863]\n",
      "5356 [D loss: 0.636911, acc.: 56.25%] [G loss: 0.866990]\n",
      "5357 [D loss: 0.585421, acc.: 71.88%] [G loss: 0.990688]\n",
      "5358 [D loss: 0.576951, acc.: 81.25%] [G loss: 0.896598]\n",
      "5359 [D loss: 0.719933, acc.: 50.00%] [G loss: 0.986876]\n",
      "5360 [D loss: 0.575494, acc.: 75.00%] [G loss: 0.967909]\n",
      "5361 [D loss: 0.690745, acc.: 50.00%] [G loss: 0.932301]\n",
      "5362 [D loss: 0.711874, acc.: 53.12%] [G loss: 0.922530]\n",
      "5363 [D loss: 0.686779, acc.: 56.25%] [G loss: 0.872873]\n",
      "5364 [D loss: 0.655092, acc.: 65.62%] [G loss: 0.788001]\n",
      "5365 [D loss: 0.713905, acc.: 56.25%] [G loss: 0.801237]\n",
      "5366 [D loss: 0.576375, acc.: 71.88%] [G loss: 0.947644]\n",
      "5367 [D loss: 0.730788, acc.: 46.88%] [G loss: 1.013300]\n",
      "5368 [D loss: 0.590450, acc.: 75.00%] [G loss: 0.971112]\n",
      "5369 [D loss: 0.552468, acc.: 75.00%] [G loss: 1.000747]\n",
      "5370 [D loss: 0.648439, acc.: 68.75%] [G loss: 0.981645]\n",
      "5371 [D loss: 0.691552, acc.: 59.38%] [G loss: 0.891066]\n",
      "5372 [D loss: 0.667616, acc.: 62.50%] [G loss: 0.871249]\n",
      "5373 [D loss: 0.665596, acc.: 59.38%] [G loss: 0.889239]\n",
      "5374 [D loss: 0.728815, acc.: 46.88%] [G loss: 0.823590]\n",
      "5375 [D loss: 0.668873, acc.: 53.12%] [G loss: 0.750165]\n",
      "5376 [D loss: 0.656686, acc.: 59.38%] [G loss: 0.839250]\n",
      "5377 [D loss: 0.764333, acc.: 59.38%] [G loss: 0.898099]\n",
      "5378 [D loss: 0.623898, acc.: 65.62%] [G loss: 0.817552]\n",
      "5379 [D loss: 0.641270, acc.: 65.62%] [G loss: 0.855479]\n",
      "5380 [D loss: 0.652145, acc.: 62.50%] [G loss: 0.885684]\n",
      "5381 [D loss: 0.594418, acc.: 68.75%] [G loss: 0.848497]\n",
      "5382 [D loss: 0.668916, acc.: 50.00%] [G loss: 0.949121]\n",
      "5383 [D loss: 0.597427, acc.: 71.88%] [G loss: 0.928187]\n",
      "5384 [D loss: 0.627256, acc.: 65.62%] [G loss: 1.006604]\n",
      "5385 [D loss: 0.537346, acc.: 81.25%] [G loss: 0.969907]\n",
      "5386 [D loss: 0.620064, acc.: 62.50%] [G loss: 0.953322]\n",
      "5387 [D loss: 0.606258, acc.: 65.62%] [G loss: 0.933429]\n",
      "5388 [D loss: 0.619904, acc.: 65.62%] [G loss: 0.888555]\n",
      "5389 [D loss: 0.621451, acc.: 65.62%] [G loss: 0.962978]\n",
      "5390 [D loss: 0.600741, acc.: 65.62%] [G loss: 0.879548]\n",
      "5391 [D loss: 0.642922, acc.: 62.50%] [G loss: 0.966313]\n",
      "5392 [D loss: 0.757733, acc.: 43.75%] [G loss: 0.938093]\n",
      "5393 [D loss: 0.611638, acc.: 75.00%] [G loss: 0.965401]\n",
      "5394 [D loss: 0.624736, acc.: 68.75%] [G loss: 0.946278]\n",
      "5395 [D loss: 0.665468, acc.: 53.12%] [G loss: 0.961773]\n",
      "5396 [D loss: 0.692228, acc.: 62.50%] [G loss: 0.865511]\n",
      "5397 [D loss: 0.698205, acc.: 43.75%] [G loss: 0.873061]\n",
      "5398 [D loss: 0.669799, acc.: 50.00%] [G loss: 0.926557]\n",
      "5399 [D loss: 0.691985, acc.: 56.25%] [G loss: 0.865313]\n",
      "5400 [D loss: 0.604617, acc.: 62.50%] [G loss: 0.949422]\n",
      "5401 [D loss: 0.724691, acc.: 46.88%] [G loss: 0.907017]\n",
      "5402 [D loss: 0.670547, acc.: 59.38%] [G loss: 0.939842]\n",
      "5403 [D loss: 0.623355, acc.: 65.62%] [G loss: 0.852687]\n",
      "5404 [D loss: 0.643750, acc.: 56.25%] [G loss: 0.939163]\n",
      "5405 [D loss: 0.657897, acc.: 56.25%] [G loss: 0.941102]\n",
      "5406 [D loss: 0.672652, acc.: 59.38%] [G loss: 0.961479]\n",
      "5407 [D loss: 0.701462, acc.: 59.38%] [G loss: 0.899319]\n",
      "5408 [D loss: 0.689840, acc.: 65.62%] [G loss: 0.991702]\n",
      "5409 [D loss: 0.631727, acc.: 65.62%] [G loss: 0.925300]\n",
      "5410 [D loss: 0.638566, acc.: 68.75%] [G loss: 0.865413]\n",
      "5411 [D loss: 0.570061, acc.: 75.00%] [G loss: 0.839113]\n",
      "5412 [D loss: 0.719648, acc.: 46.88%] [G loss: 0.828488]\n",
      "5413 [D loss: 0.646126, acc.: 68.75%] [G loss: 0.902908]\n",
      "5414 [D loss: 0.686798, acc.: 59.38%] [G loss: 0.869411]\n",
      "5415 [D loss: 0.661660, acc.: 56.25%] [G loss: 0.930280]\n",
      "5416 [D loss: 0.678010, acc.: 59.38%] [G loss: 0.974635]\n",
      "5417 [D loss: 0.683605, acc.: 62.50%] [G loss: 0.846178]\n",
      "5418 [D loss: 0.689079, acc.: 53.12%] [G loss: 0.862892]\n",
      "5419 [D loss: 0.669975, acc.: 53.12%] [G loss: 0.900741]\n",
      "5420 [D loss: 0.622896, acc.: 65.62%] [G loss: 0.836701]\n",
      "5421 [D loss: 0.719471, acc.: 50.00%] [G loss: 0.914136]\n",
      "5422 [D loss: 0.590934, acc.: 65.62%] [G loss: 1.020332]\n",
      "5423 [D loss: 0.671907, acc.: 65.62%] [G loss: 0.922129]\n",
      "5424 [D loss: 0.708792, acc.: 46.88%] [G loss: 0.924968]\n",
      "5425 [D loss: 0.824456, acc.: 50.00%] [G loss: 0.877437]\n",
      "5426 [D loss: 0.623206, acc.: 62.50%] [G loss: 0.882756]\n",
      "5427 [D loss: 0.624331, acc.: 59.38%] [G loss: 0.916773]\n",
      "5428 [D loss: 0.648196, acc.: 68.75%] [G loss: 0.867471]\n",
      "5429 [D loss: 0.630529, acc.: 59.38%] [G loss: 0.912862]\n",
      "5430 [D loss: 0.823193, acc.: 34.38%] [G loss: 0.796490]\n",
      "5431 [D loss: 0.641986, acc.: 65.62%] [G loss: 0.930503]\n",
      "5432 [D loss: 0.634031, acc.: 56.25%] [G loss: 0.897643]\n",
      "5433 [D loss: 0.604853, acc.: 68.75%] [G loss: 0.888615]\n",
      "5434 [D loss: 0.641287, acc.: 59.38%] [G loss: 0.830637]\n",
      "5435 [D loss: 0.677208, acc.: 65.62%] [G loss: 0.860211]\n",
      "5436 [D loss: 0.682060, acc.: 56.25%] [G loss: 0.880620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5437 [D loss: 0.626662, acc.: 68.75%] [G loss: 0.911749]\n",
      "5438 [D loss: 0.645593, acc.: 62.50%] [G loss: 0.875476]\n",
      "5439 [D loss: 0.755396, acc.: 53.12%] [G loss: 0.878379]\n",
      "5440 [D loss: 0.638117, acc.: 59.38%] [G loss: 0.796522]\n",
      "5441 [D loss: 0.576780, acc.: 78.12%] [G loss: 0.942229]\n",
      "5442 [D loss: 0.619286, acc.: 65.62%] [G loss: 0.956543]\n",
      "5443 [D loss: 0.646831, acc.: 59.38%] [G loss: 0.868795]\n",
      "5444 [D loss: 0.720617, acc.: 50.00%] [G loss: 0.875557]\n",
      "5445 [D loss: 0.779553, acc.: 40.62%] [G loss: 0.902657]\n",
      "5446 [D loss: 0.727952, acc.: 59.38%] [G loss: 0.849789]\n",
      "5447 [D loss: 0.688531, acc.: 59.38%] [G loss: 0.906781]\n",
      "5448 [D loss: 0.648339, acc.: 53.12%] [G loss: 0.978020]\n",
      "5449 [D loss: 0.682162, acc.: 56.25%] [G loss: 0.903196]\n",
      "5450 [D loss: 0.674231, acc.: 50.00%] [G loss: 0.893810]\n",
      "5451 [D loss: 0.620751, acc.: 59.38%] [G loss: 1.047089]\n",
      "5452 [D loss: 0.702402, acc.: 56.25%] [G loss: 0.906713]\n",
      "5453 [D loss: 0.674852, acc.: 59.38%] [G loss: 0.829992]\n",
      "5454 [D loss: 0.564170, acc.: 68.75%] [G loss: 0.916431]\n",
      "5455 [D loss: 0.715656, acc.: 53.12%] [G loss: 0.894578]\n",
      "5456 [D loss: 0.669817, acc.: 62.50%] [G loss: 0.891436]\n",
      "5457 [D loss: 0.631691, acc.: 59.38%] [G loss: 0.951366]\n",
      "5458 [D loss: 0.631886, acc.: 62.50%] [G loss: 0.887821]\n",
      "5459 [D loss: 0.736796, acc.: 43.75%] [G loss: 0.813039]\n",
      "5460 [D loss: 0.673009, acc.: 53.12%] [G loss: 0.802071]\n",
      "5461 [D loss: 0.629818, acc.: 62.50%] [G loss: 0.844374]\n",
      "5462 [D loss: 0.617753, acc.: 75.00%] [G loss: 0.905691]\n",
      "5463 [D loss: 0.623601, acc.: 62.50%] [G loss: 0.817040]\n",
      "5464 [D loss: 0.611356, acc.: 68.75%] [G loss: 0.909474]\n",
      "5465 [D loss: 0.619398, acc.: 59.38%] [G loss: 0.971960]\n",
      "5466 [D loss: 0.740704, acc.: 56.25%] [G loss: 0.935066]\n",
      "5467 [D loss: 0.634247, acc.: 68.75%] [G loss: 1.058847]\n",
      "5468 [D loss: 0.600568, acc.: 75.00%] [G loss: 0.979002]\n",
      "5469 [D loss: 0.601468, acc.: 62.50%] [G loss: 0.920112]\n",
      "5470 [D loss: 0.626025, acc.: 65.62%] [G loss: 0.972471]\n",
      "5471 [D loss: 0.748478, acc.: 56.25%] [G loss: 0.858343]\n",
      "5472 [D loss: 0.658093, acc.: 75.00%] [G loss: 0.929839]\n",
      "5473 [D loss: 0.624103, acc.: 68.75%] [G loss: 0.891590]\n",
      "5474 [D loss: 0.657616, acc.: 59.38%] [G loss: 0.868700]\n",
      "5475 [D loss: 0.671585, acc.: 65.62%] [G loss: 0.794973]\n",
      "5476 [D loss: 0.598182, acc.: 75.00%] [G loss: 0.890729]\n",
      "5477 [D loss: 0.636185, acc.: 68.75%] [G loss: 0.877432]\n",
      "5478 [D loss: 0.781730, acc.: 37.50%] [G loss: 0.954342]\n",
      "5479 [D loss: 0.628985, acc.: 68.75%] [G loss: 0.954697]\n",
      "5480 [D loss: 0.638070, acc.: 65.62%] [G loss: 0.845772]\n",
      "5481 [D loss: 0.646105, acc.: 59.38%] [G loss: 0.858079]\n",
      "5482 [D loss: 0.679334, acc.: 62.50%] [G loss: 0.854142]\n",
      "5483 [D loss: 0.620073, acc.: 65.62%] [G loss: 0.913974]\n",
      "5484 [D loss: 0.570050, acc.: 68.75%] [G loss: 0.900138]\n",
      "5485 [D loss: 0.577105, acc.: 71.88%] [G loss: 0.908489]\n",
      "5486 [D loss: 0.692842, acc.: 43.75%] [G loss: 0.889022]\n",
      "5487 [D loss: 0.660527, acc.: 68.75%] [G loss: 0.917227]\n",
      "5488 [D loss: 0.580781, acc.: 78.12%] [G loss: 0.890954]\n",
      "5489 [D loss: 0.612586, acc.: 68.75%] [G loss: 0.903057]\n",
      "5490 [D loss: 0.669984, acc.: 59.38%] [G loss: 0.872758]\n",
      "5491 [D loss: 0.712111, acc.: 56.25%] [G loss: 0.892638]\n",
      "5492 [D loss: 0.643735, acc.: 65.62%] [G loss: 0.882585]\n",
      "5493 [D loss: 0.655361, acc.: 65.62%] [G loss: 0.927679]\n",
      "5494 [D loss: 0.659543, acc.: 56.25%] [G loss: 0.881725]\n",
      "5495 [D loss: 0.622390, acc.: 62.50%] [G loss: 1.036805]\n",
      "5496 [D loss: 0.665421, acc.: 71.88%] [G loss: 0.820373]\n",
      "5497 [D loss: 0.649602, acc.: 65.62%] [G loss: 0.954220]\n",
      "5498 [D loss: 0.665037, acc.: 62.50%] [G loss: 0.920092]\n",
      "5499 [D loss: 0.639410, acc.: 53.12%] [G loss: 0.936028]\n",
      "5500 [D loss: 0.664662, acc.: 56.25%] [G loss: 0.874088]\n",
      "5501 [D loss: 0.681338, acc.: 56.25%] [G loss: 0.874108]\n",
      "5502 [D loss: 0.677984, acc.: 65.62%] [G loss: 0.990431]\n",
      "5503 [D loss: 0.650959, acc.: 56.25%] [G loss: 0.955679]\n",
      "5504 [D loss: 0.642059, acc.: 75.00%] [G loss: 0.925695]\n",
      "5505 [D loss: 0.596128, acc.: 75.00%] [G loss: 0.935274]\n",
      "5506 [D loss: 0.663084, acc.: 62.50%] [G loss: 0.953575]\n",
      "5507 [D loss: 0.668297, acc.: 56.25%] [G loss: 0.956223]\n",
      "5508 [D loss: 0.570675, acc.: 71.88%] [G loss: 0.983095]\n",
      "5509 [D loss: 0.711686, acc.: 53.12%] [G loss: 0.862599]\n",
      "5510 [D loss: 0.672289, acc.: 53.12%] [G loss: 0.969510]\n",
      "5511 [D loss: 0.723283, acc.: 59.38%] [G loss: 0.870902]\n",
      "5512 [D loss: 0.610391, acc.: 71.88%] [G loss: 0.772062]\n",
      "5513 [D loss: 0.709416, acc.: 56.25%] [G loss: 0.828844]\n",
      "5514 [D loss: 0.635716, acc.: 59.38%] [G loss: 0.864555]\n",
      "5515 [D loss: 0.601451, acc.: 65.62%] [G loss: 0.976678]\n",
      "5516 [D loss: 0.685734, acc.: 53.12%] [G loss: 0.915135]\n",
      "5517 [D loss: 0.651005, acc.: 65.62%] [G loss: 0.894625]\n",
      "5518 [D loss: 0.734414, acc.: 56.25%] [G loss: 0.952898]\n",
      "5519 [D loss: 0.745175, acc.: 43.75%] [G loss: 0.933700]\n",
      "5520 [D loss: 0.613751, acc.: 65.62%] [G loss: 0.856752]\n",
      "5521 [D loss: 0.556374, acc.: 78.12%] [G loss: 0.842197]\n",
      "5522 [D loss: 0.641869, acc.: 59.38%] [G loss: 0.960389]\n",
      "5523 [D loss: 0.626085, acc.: 62.50%] [G loss: 0.880626]\n",
      "5524 [D loss: 0.665232, acc.: 53.12%] [G loss: 0.992623]\n",
      "5525 [D loss: 0.609104, acc.: 65.62%] [G loss: 0.904762]\n",
      "5526 [D loss: 0.723507, acc.: 53.12%] [G loss: 0.927502]\n",
      "5527 [D loss: 0.628612, acc.: 65.62%] [G loss: 0.877549]\n",
      "5528 [D loss: 0.726025, acc.: 43.75%] [G loss: 0.888535]\n",
      "5529 [D loss: 0.726766, acc.: 59.38%] [G loss: 0.777566]\n",
      "5530 [D loss: 0.598313, acc.: 75.00%] [G loss: 0.841085]\n",
      "5531 [D loss: 0.791124, acc.: 53.12%] [G loss: 0.892104]\n",
      "5532 [D loss: 0.620421, acc.: 62.50%] [G loss: 0.873237]\n",
      "5533 [D loss: 0.619983, acc.: 62.50%] [G loss: 0.955848]\n",
      "5534 [D loss: 0.626293, acc.: 68.75%] [G loss: 0.906855]\n",
      "5535 [D loss: 0.625136, acc.: 68.75%] [G loss: 0.955823]\n",
      "5536 [D loss: 0.571699, acc.: 75.00%] [G loss: 0.995246]\n",
      "5537 [D loss: 0.593707, acc.: 68.75%] [G loss: 0.931638]\n",
      "5538 [D loss: 0.700326, acc.: 65.62%] [G loss: 0.874925]\n",
      "5539 [D loss: 0.600803, acc.: 65.62%] [G loss: 0.960213]\n",
      "5540 [D loss: 0.601581, acc.: 65.62%] [G loss: 0.938342]\n",
      "5541 [D loss: 0.757914, acc.: 50.00%] [G loss: 0.941644]\n",
      "5542 [D loss: 0.691406, acc.: 62.50%] [G loss: 0.857499]\n",
      "5543 [D loss: 0.711391, acc.: 59.38%] [G loss: 0.817831]\n",
      "5544 [D loss: 0.584987, acc.: 65.62%] [G loss: 0.885837]\n",
      "5545 [D loss: 0.723526, acc.: 53.12%] [G loss: 1.001633]\n",
      "5546 [D loss: 0.626145, acc.: 65.62%] [G loss: 0.899514]\n",
      "5547 [D loss: 0.698675, acc.: 50.00%] [G loss: 0.900966]\n",
      "5548 [D loss: 0.577480, acc.: 68.75%] [G loss: 1.008498]\n",
      "5549 [D loss: 0.726139, acc.: 53.12%] [G loss: 0.897833]\n",
      "5550 [D loss: 0.692889, acc.: 50.00%] [G loss: 0.954141]\n",
      "5551 [D loss: 0.593475, acc.: 65.62%] [G loss: 0.944442]\n",
      "5552 [D loss: 0.606713, acc.: 62.50%] [G loss: 0.955227]\n",
      "5553 [D loss: 0.548268, acc.: 75.00%] [G loss: 0.975498]\n",
      "5554 [D loss: 0.661828, acc.: 65.62%] [G loss: 0.873062]\n",
      "5555 [D loss: 0.685649, acc.: 56.25%] [G loss: 0.915016]\n",
      "5556 [D loss: 0.660010, acc.: 56.25%] [G loss: 1.017460]\n",
      "5557 [D loss: 0.700117, acc.: 59.38%] [G loss: 1.000911]\n",
      "5558 [D loss: 0.618015, acc.: 65.62%] [G loss: 0.939089]\n",
      "5559 [D loss: 0.577281, acc.: 78.12%] [G loss: 0.990597]\n",
      "5560 [D loss: 0.684211, acc.: 56.25%] [G loss: 0.927594]\n",
      "5561 [D loss: 0.662131, acc.: 65.62%] [G loss: 0.864240]\n",
      "5562 [D loss: 0.693974, acc.: 53.12%] [G loss: 0.827900]\n",
      "5563 [D loss: 0.662756, acc.: 53.12%] [G loss: 0.924643]\n",
      "5564 [D loss: 0.605573, acc.: 68.75%] [G loss: 0.923018]\n",
      "5565 [D loss: 0.632011, acc.: 65.62%] [G loss: 0.969915]\n",
      "5566 [D loss: 0.566787, acc.: 75.00%] [G loss: 0.928977]\n",
      "5567 [D loss: 0.640762, acc.: 71.88%] [G loss: 0.976899]\n",
      "5568 [D loss: 0.795768, acc.: 40.62%] [G loss: 0.935371]\n",
      "5569 [D loss: 0.673505, acc.: 59.38%] [G loss: 0.867890]\n",
      "5570 [D loss: 0.586869, acc.: 78.12%] [G loss: 0.856288]\n",
      "5571 [D loss: 0.691406, acc.: 56.25%] [G loss: 0.905456]\n",
      "5572 [D loss: 0.707903, acc.: 43.75%] [G loss: 1.069320]\n",
      "5573 [D loss: 0.594707, acc.: 62.50%] [G loss: 0.951488]\n",
      "5574 [D loss: 0.701171, acc.: 50.00%] [G loss: 0.967510]\n",
      "5575 [D loss: 0.659511, acc.: 65.62%] [G loss: 0.949660]\n",
      "5576 [D loss: 0.760187, acc.: 46.88%] [G loss: 0.821252]\n",
      "5577 [D loss: 0.692056, acc.: 59.38%] [G loss: 0.941784]\n",
      "5578 [D loss: 0.659548, acc.: 43.75%] [G loss: 0.902027]\n",
      "5579 [D loss: 0.581436, acc.: 75.00%] [G loss: 1.008155]\n",
      "5580 [D loss: 0.575617, acc.: 81.25%] [G loss: 0.968089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5581 [D loss: 0.732643, acc.: 56.25%] [G loss: 0.876859]\n",
      "5582 [D loss: 0.698959, acc.: 59.38%] [G loss: 0.846795]\n",
      "5583 [D loss: 0.701392, acc.: 53.12%] [G loss: 0.843418]\n",
      "5584 [D loss: 0.698233, acc.: 53.12%] [G loss: 0.832879]\n",
      "5585 [D loss: 0.644971, acc.: 65.62%] [G loss: 0.916043]\n",
      "5586 [D loss: 0.616272, acc.: 65.62%] [G loss: 0.888845]\n",
      "5587 [D loss: 0.691048, acc.: 59.38%] [G loss: 0.867595]\n",
      "5588 [D loss: 0.714372, acc.: 56.25%] [G loss: 0.784009]\n",
      "5589 [D loss: 0.718776, acc.: 46.88%] [G loss: 0.857624]\n",
      "5590 [D loss: 0.707001, acc.: 59.38%] [G loss: 0.830807]\n",
      "5591 [D loss: 0.647576, acc.: 62.50%] [G loss: 0.900115]\n",
      "5592 [D loss: 0.656800, acc.: 62.50%] [G loss: 0.814329]\n",
      "5593 [D loss: 0.721523, acc.: 50.00%] [G loss: 0.812622]\n",
      "5594 [D loss: 0.725480, acc.: 46.88%] [G loss: 0.863085]\n",
      "5595 [D loss: 0.713951, acc.: 53.12%] [G loss: 0.922635]\n",
      "5596 [D loss: 0.637511, acc.: 68.75%] [G loss: 0.908455]\n",
      "5597 [D loss: 0.719325, acc.: 50.00%] [G loss: 0.783809]\n",
      "5598 [D loss: 0.576998, acc.: 81.25%] [G loss: 0.904462]\n",
      "5599 [D loss: 0.653701, acc.: 56.25%] [G loss: 0.905355]\n",
      "5600 [D loss: 0.614566, acc.: 68.75%] [G loss: 0.925626]\n",
      "5601 [D loss: 0.683351, acc.: 65.62%] [G loss: 0.863012]\n",
      "5602 [D loss: 0.845099, acc.: 46.88%] [G loss: 0.927774]\n",
      "5603 [D loss: 0.662816, acc.: 59.38%] [G loss: 0.910090]\n",
      "5604 [D loss: 0.615708, acc.: 68.75%] [G loss: 0.923274]\n",
      "5605 [D loss: 0.699335, acc.: 62.50%] [G loss: 0.866345]\n",
      "5606 [D loss: 0.685256, acc.: 59.38%] [G loss: 0.838599]\n",
      "5607 [D loss: 0.664261, acc.: 62.50%] [G loss: 0.847236]\n",
      "5608 [D loss: 0.581530, acc.: 71.88%] [G loss: 0.820101]\n",
      "5609 [D loss: 0.677770, acc.: 50.00%] [G loss: 0.867761]\n",
      "5610 [D loss: 0.684834, acc.: 65.62%] [G loss: 0.836277]\n",
      "5611 [D loss: 0.579663, acc.: 81.25%] [G loss: 0.886378]\n",
      "5612 [D loss: 0.626346, acc.: 68.75%] [G loss: 0.919990]\n",
      "5613 [D loss: 0.646762, acc.: 56.25%] [G loss: 0.952662]\n",
      "5614 [D loss: 0.707967, acc.: 53.12%] [G loss: 0.890864]\n",
      "5615 [D loss: 0.680114, acc.: 56.25%] [G loss: 0.835147]\n",
      "5616 [D loss: 0.654936, acc.: 59.38%] [G loss: 0.868716]\n",
      "5617 [D loss: 0.657236, acc.: 53.12%] [G loss: 0.868197]\n",
      "5618 [D loss: 0.572886, acc.: 81.25%] [G loss: 0.930480]\n",
      "5619 [D loss: 0.602407, acc.: 75.00%] [G loss: 0.943278]\n",
      "5620 [D loss: 0.737467, acc.: 53.12%] [G loss: 0.873370]\n",
      "5621 [D loss: 0.788150, acc.: 43.75%] [G loss: 0.850140]\n",
      "5622 [D loss: 0.574585, acc.: 81.25%] [G loss: 0.921850]\n",
      "5623 [D loss: 0.815267, acc.: 34.38%] [G loss: 0.910909]\n",
      "5624 [D loss: 0.644299, acc.: 56.25%] [G loss: 0.910748]\n",
      "5625 [D loss: 0.717875, acc.: 50.00%] [G loss: 0.907156]\n",
      "5626 [D loss: 0.664662, acc.: 53.12%] [G loss: 0.835003]\n",
      "5627 [D loss: 0.632905, acc.: 68.75%] [G loss: 0.886955]\n",
      "5628 [D loss: 0.691021, acc.: 46.88%] [G loss: 0.897671]\n",
      "5629 [D loss: 0.640043, acc.: 62.50%] [G loss: 0.839183]\n",
      "5630 [D loss: 0.705002, acc.: 59.38%] [G loss: 0.832960]\n",
      "5631 [D loss: 0.638648, acc.: 59.38%] [G loss: 0.941762]\n",
      "5632 [D loss: 0.619353, acc.: 65.62%] [G loss: 0.861742]\n",
      "5633 [D loss: 0.691325, acc.: 53.12%] [G loss: 0.817906]\n",
      "5634 [D loss: 0.662403, acc.: 59.38%] [G loss: 0.934957]\n",
      "5635 [D loss: 0.737122, acc.: 62.50%] [G loss: 0.909802]\n",
      "5636 [D loss: 0.757845, acc.: 34.38%] [G loss: 0.929286]\n",
      "5637 [D loss: 0.610071, acc.: 65.62%] [G loss: 0.895837]\n",
      "5638 [D loss: 0.571067, acc.: 75.00%] [G loss: 0.932382]\n",
      "5639 [D loss: 0.633363, acc.: 65.62%] [G loss: 0.982122]\n",
      "5640 [D loss: 0.640716, acc.: 68.75%] [G loss: 0.895563]\n",
      "5641 [D loss: 0.709306, acc.: 56.25%] [G loss: 0.942453]\n",
      "5642 [D loss: 0.668532, acc.: 62.50%] [G loss: 0.905106]\n",
      "5643 [D loss: 0.643434, acc.: 68.75%] [G loss: 0.772374]\n",
      "5644 [D loss: 0.644772, acc.: 65.62%] [G loss: 0.829609]\n",
      "5645 [D loss: 0.697051, acc.: 62.50%] [G loss: 0.864473]\n",
      "5646 [D loss: 0.742810, acc.: 46.88%] [G loss: 0.874872]\n",
      "5647 [D loss: 0.631926, acc.: 56.25%] [G loss: 0.823639]\n",
      "5648 [D loss: 0.598993, acc.: 71.88%] [G loss: 0.855973]\n",
      "5649 [D loss: 0.588172, acc.: 78.12%] [G loss: 0.851363]\n",
      "5650 [D loss: 0.611467, acc.: 62.50%] [G loss: 0.823419]\n",
      "5651 [D loss: 0.713940, acc.: 59.38%] [G loss: 0.913387]\n",
      "5652 [D loss: 0.686778, acc.: 43.75%] [G loss: 0.887817]\n",
      "5653 [D loss: 0.682595, acc.: 56.25%] [G loss: 0.914447]\n",
      "5654 [D loss: 0.660916, acc.: 62.50%] [G loss: 0.824115]\n",
      "5655 [D loss: 0.755064, acc.: 43.75%] [G loss: 0.836191]\n",
      "5656 [D loss: 0.735190, acc.: 37.50%] [G loss: 0.913963]\n",
      "5657 [D loss: 0.709123, acc.: 53.12%] [G loss: 0.919655]\n",
      "5658 [D loss: 0.569333, acc.: 62.50%] [G loss: 0.824839]\n",
      "5659 [D loss: 0.700958, acc.: 53.12%] [G loss: 0.876682]\n",
      "5660 [D loss: 0.653073, acc.: 56.25%] [G loss: 0.838029]\n",
      "5661 [D loss: 0.638518, acc.: 68.75%] [G loss: 0.854969]\n",
      "5662 [D loss: 0.644031, acc.: 59.38%] [G loss: 0.815925]\n",
      "5663 [D loss: 0.697666, acc.: 56.25%] [G loss: 0.824274]\n",
      "5664 [D loss: 0.611667, acc.: 68.75%] [G loss: 0.882054]\n",
      "5665 [D loss: 0.591824, acc.: 62.50%] [G loss: 0.867875]\n",
      "5666 [D loss: 0.662872, acc.: 62.50%] [G loss: 0.804862]\n",
      "5667 [D loss: 0.630018, acc.: 65.62%] [G loss: 0.804446]\n",
      "5668 [D loss: 0.657401, acc.: 65.62%] [G loss: 0.862500]\n",
      "5669 [D loss: 0.619331, acc.: 65.62%] [G loss: 0.835448]\n",
      "5670 [D loss: 0.612597, acc.: 71.88%] [G loss: 0.855826]\n",
      "5671 [D loss: 0.678427, acc.: 62.50%] [G loss: 0.903053]\n",
      "5672 [D loss: 0.646405, acc.: 62.50%] [G loss: 0.960738]\n",
      "5673 [D loss: 0.606187, acc.: 59.38%] [G loss: 0.846292]\n",
      "5674 [D loss: 0.609118, acc.: 75.00%] [G loss: 0.878231]\n",
      "5675 [D loss: 0.701068, acc.: 53.12%] [G loss: 0.892129]\n",
      "5676 [D loss: 0.650736, acc.: 68.75%] [G loss: 0.916937]\n",
      "5677 [D loss: 0.664328, acc.: 62.50%] [G loss: 0.891421]\n",
      "5678 [D loss: 0.606698, acc.: 68.75%] [G loss: 0.809983]\n",
      "5679 [D loss: 0.681426, acc.: 62.50%] [G loss: 0.822772]\n",
      "5680 [D loss: 0.692384, acc.: 59.38%] [G loss: 0.877014]\n",
      "5681 [D loss: 0.643304, acc.: 62.50%] [G loss: 0.797577]\n",
      "5682 [D loss: 0.621316, acc.: 56.25%] [G loss: 0.814771]\n",
      "5683 [D loss: 0.574422, acc.: 68.75%] [G loss: 0.893019]\n",
      "5684 [D loss: 0.583138, acc.: 78.12%] [G loss: 0.983653]\n",
      "5685 [D loss: 0.630043, acc.: 71.88%] [G loss: 1.018685]\n",
      "5686 [D loss: 0.680912, acc.: 62.50%] [G loss: 0.906406]\n",
      "5687 [D loss: 0.728923, acc.: 46.88%] [G loss: 0.873052]\n",
      "5688 [D loss: 0.726470, acc.: 46.88%] [G loss: 0.961601]\n",
      "5689 [D loss: 0.699593, acc.: 59.38%] [G loss: 0.962626]\n",
      "5690 [D loss: 0.720031, acc.: 40.62%] [G loss: 0.877580]\n",
      "5691 [D loss: 0.602180, acc.: 56.25%] [G loss: 0.909251]\n",
      "5692 [D loss: 0.642108, acc.: 62.50%] [G loss: 0.868065]\n",
      "5693 [D loss: 0.674733, acc.: 59.38%] [G loss: 0.752399]\n",
      "5694 [D loss: 0.651302, acc.: 59.38%] [G loss: 0.875497]\n",
      "5695 [D loss: 0.625955, acc.: 59.38%] [G loss: 0.802631]\n",
      "5696 [D loss: 0.648518, acc.: 56.25%] [G loss: 0.825993]\n",
      "5697 [D loss: 0.653277, acc.: 43.75%] [G loss: 0.833662]\n",
      "5698 [D loss: 0.630997, acc.: 68.75%] [G loss: 0.835241]\n",
      "5699 [D loss: 0.610507, acc.: 71.88%] [G loss: 0.910953]\n",
      "5700 [D loss: 0.688503, acc.: 53.12%] [G loss: 0.904108]\n",
      "5701 [D loss: 0.668772, acc.: 65.62%] [G loss: 0.872707]\n",
      "5702 [D loss: 0.640890, acc.: 59.38%] [G loss: 0.978033]\n",
      "5703 [D loss: 0.584364, acc.: 59.38%] [G loss: 0.945122]\n",
      "5704 [D loss: 0.737742, acc.: 50.00%] [G loss: 0.981839]\n",
      "5705 [D loss: 0.708922, acc.: 62.50%] [G loss: 0.889441]\n",
      "5706 [D loss: 0.668927, acc.: 53.12%] [G loss: 0.968343]\n",
      "5707 [D loss: 0.641563, acc.: 56.25%] [G loss: 0.965823]\n",
      "5708 [D loss: 0.700203, acc.: 59.38%] [G loss: 0.895926]\n",
      "5709 [D loss: 0.697708, acc.: 56.25%] [G loss: 0.905077]\n",
      "5710 [D loss: 0.745082, acc.: 50.00%] [G loss: 0.838536]\n",
      "5711 [D loss: 0.671352, acc.: 65.62%] [G loss: 0.819001]\n",
      "5712 [D loss: 0.684495, acc.: 62.50%] [G loss: 0.874889]\n",
      "5713 [D loss: 0.709724, acc.: 56.25%] [G loss: 0.868148]\n",
      "5714 [D loss: 0.642532, acc.: 65.62%] [G loss: 0.885464]\n",
      "5715 [D loss: 0.665506, acc.: 59.38%] [G loss: 0.926656]\n",
      "5716 [D loss: 0.644030, acc.: 68.75%] [G loss: 0.984211]\n",
      "5717 [D loss: 0.526598, acc.: 78.12%] [G loss: 1.072457]\n",
      "5718 [D loss: 0.677082, acc.: 53.12%] [G loss: 0.907731]\n",
      "5719 [D loss: 0.529252, acc.: 84.38%] [G loss: 0.901475]\n",
      "5720 [D loss: 0.709480, acc.: 62.50%] [G loss: 0.899570]\n",
      "5721 [D loss: 0.696358, acc.: 56.25%] [G loss: 0.862966]\n",
      "5722 [D loss: 0.753802, acc.: 43.75%] [G loss: 0.794816]\n",
      "5723 [D loss: 0.716420, acc.: 50.00%] [G loss: 0.860060]\n",
      "5724 [D loss: 0.693239, acc.: 56.25%] [G loss: 0.809067]\n",
      "5725 [D loss: 0.566736, acc.: 65.62%] [G loss: 0.881959]\n",
      "5726 [D loss: 0.663856, acc.: 50.00%] [G loss: 0.957994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5727 [D loss: 0.603605, acc.: 71.88%] [G loss: 0.954517]\n",
      "5728 [D loss: 0.591743, acc.: 71.88%] [G loss: 0.928442]\n",
      "5729 [D loss: 0.711032, acc.: 53.12%] [G loss: 0.896384]\n",
      "5730 [D loss: 0.691365, acc.: 56.25%] [G loss: 0.823235]\n",
      "5731 [D loss: 0.660512, acc.: 59.38%] [G loss: 0.783632]\n",
      "5732 [D loss: 0.657362, acc.: 62.50%] [G loss: 0.831340]\n",
      "5733 [D loss: 0.724247, acc.: 56.25%] [G loss: 0.820739]\n",
      "5734 [D loss: 0.667326, acc.: 50.00%] [G loss: 0.904671]\n",
      "5735 [D loss: 0.582981, acc.: 59.38%] [G loss: 0.832004]\n",
      "5736 [D loss: 0.627911, acc.: 62.50%] [G loss: 0.810681]\n",
      "5737 [D loss: 0.727493, acc.: 43.75%] [G loss: 0.873720]\n",
      "5738 [D loss: 0.696481, acc.: 59.38%] [G loss: 0.968984]\n",
      "5739 [D loss: 0.708865, acc.: 56.25%] [G loss: 0.915269]\n",
      "5740 [D loss: 0.581516, acc.: 75.00%] [G loss: 0.927732]\n",
      "5741 [D loss: 0.676604, acc.: 62.50%] [G loss: 0.870808]\n",
      "5742 [D loss: 0.675404, acc.: 46.88%] [G loss: 0.933680]\n",
      "5743 [D loss: 0.736491, acc.: 50.00%] [G loss: 1.001880]\n",
      "5744 [D loss: 0.576503, acc.: 71.88%] [G loss: 0.900928]\n",
      "5745 [D loss: 0.641556, acc.: 62.50%] [G loss: 0.884705]\n",
      "5746 [D loss: 0.650387, acc.: 59.38%] [G loss: 0.906865]\n",
      "5747 [D loss: 0.648205, acc.: 62.50%] [G loss: 0.918007]\n",
      "5748 [D loss: 0.609040, acc.: 75.00%] [G loss: 0.883844]\n",
      "5749 [D loss: 0.652242, acc.: 59.38%] [G loss: 0.925512]\n",
      "5750 [D loss: 0.640372, acc.: 56.25%] [G loss: 0.818142]\n",
      "5751 [D loss: 0.668524, acc.: 53.12%] [G loss: 0.864985]\n",
      "5752 [D loss: 0.627782, acc.: 71.88%] [G loss: 0.859355]\n",
      "5753 [D loss: 0.630468, acc.: 65.62%] [G loss: 0.782399]\n",
      "5754 [D loss: 0.633656, acc.: 68.75%] [G loss: 0.816581]\n",
      "5755 [D loss: 0.706415, acc.: 56.25%] [G loss: 0.879278]\n",
      "5756 [D loss: 0.647742, acc.: 65.62%] [G loss: 0.844102]\n",
      "5757 [D loss: 0.607589, acc.: 65.62%] [G loss: 0.897822]\n",
      "5758 [D loss: 0.668853, acc.: 43.75%] [G loss: 0.837564]\n",
      "5759 [D loss: 0.652426, acc.: 65.62%] [G loss: 0.888299]\n",
      "5760 [D loss: 0.726808, acc.: 50.00%] [G loss: 0.829694]\n",
      "5761 [D loss: 0.691549, acc.: 62.50%] [G loss: 0.863185]\n",
      "5762 [D loss: 0.672815, acc.: 50.00%] [G loss: 0.879334]\n",
      "5763 [D loss: 0.622793, acc.: 71.88%] [G loss: 0.881100]\n",
      "5764 [D loss: 0.597118, acc.: 68.75%] [G loss: 0.878125]\n",
      "5765 [D loss: 0.634493, acc.: 71.88%] [G loss: 0.841542]\n",
      "5766 [D loss: 0.702361, acc.: 56.25%] [G loss: 0.862181]\n",
      "5767 [D loss: 0.668479, acc.: 62.50%] [G loss: 0.809839]\n",
      "5768 [D loss: 0.616620, acc.: 68.75%] [G loss: 0.919150]\n",
      "5769 [D loss: 0.603132, acc.: 68.75%] [G loss: 0.955126]\n",
      "5770 [D loss: 0.545139, acc.: 78.12%] [G loss: 0.876756]\n",
      "5771 [D loss: 0.621812, acc.: 59.38%] [G loss: 0.862172]\n",
      "5772 [D loss: 0.702113, acc.: 59.38%] [G loss: 0.785158]\n",
      "5773 [D loss: 0.686256, acc.: 53.12%] [G loss: 0.854788]\n",
      "5774 [D loss: 0.656414, acc.: 68.75%] [G loss: 0.853853]\n",
      "5775 [D loss: 0.616640, acc.: 62.50%] [G loss: 0.943798]\n",
      "5776 [D loss: 0.718758, acc.: 56.25%] [G loss: 0.937208]\n",
      "5777 [D loss: 0.690997, acc.: 59.38%] [G loss: 0.912145]\n",
      "5778 [D loss: 0.785421, acc.: 31.25%] [G loss: 0.913020]\n",
      "5779 [D loss: 0.686796, acc.: 62.50%] [G loss: 0.911059]\n",
      "5780 [D loss: 0.712784, acc.: 53.12%] [G loss: 0.909211]\n",
      "5781 [D loss: 0.646137, acc.: 65.62%] [G loss: 0.882785]\n",
      "5782 [D loss: 0.673463, acc.: 56.25%] [G loss: 0.858339]\n",
      "5783 [D loss: 0.577505, acc.: 71.88%] [G loss: 0.845795]\n",
      "5784 [D loss: 0.612734, acc.: 71.88%] [G loss: 0.874885]\n",
      "5785 [D loss: 0.609899, acc.: 68.75%] [G loss: 0.901239]\n",
      "5786 [D loss: 0.630251, acc.: 68.75%] [G loss: 0.914964]\n",
      "5787 [D loss: 0.717416, acc.: 65.62%] [G loss: 0.886592]\n",
      "5788 [D loss: 0.615217, acc.: 65.62%] [G loss: 0.972610]\n",
      "5789 [D loss: 0.611022, acc.: 65.62%] [G loss: 1.001500]\n",
      "5790 [D loss: 0.688538, acc.: 46.88%] [G loss: 0.835604]\n",
      "5791 [D loss: 0.674741, acc.: 71.88%] [G loss: 0.911007]\n",
      "5792 [D loss: 0.593345, acc.: 59.38%] [G loss: 0.998044]\n",
      "5793 [D loss: 0.800317, acc.: 53.12%] [G loss: 0.854850]\n",
      "5794 [D loss: 0.584764, acc.: 65.62%] [G loss: 0.868968]\n",
      "5795 [D loss: 0.684731, acc.: 59.38%] [G loss: 0.822711]\n",
      "5796 [D loss: 0.742595, acc.: 43.75%] [G loss: 0.813674]\n",
      "5797 [D loss: 0.677970, acc.: 62.50%] [G loss: 0.791052]\n",
      "5798 [D loss: 0.666337, acc.: 62.50%] [G loss: 0.841067]\n",
      "5799 [D loss: 0.558138, acc.: 68.75%] [G loss: 0.997644]\n",
      "5800 [D loss: 0.686096, acc.: 53.12%] [G loss: 0.868363]\n",
      "5801 [D loss: 0.648199, acc.: 56.25%] [G loss: 0.892835]\n",
      "5802 [D loss: 0.692518, acc.: 59.38%] [G loss: 0.906104]\n",
      "5803 [D loss: 0.647693, acc.: 59.38%] [G loss: 0.843563]\n",
      "5804 [D loss: 0.668974, acc.: 65.62%] [G loss: 0.891280]\n",
      "5805 [D loss: 0.603367, acc.: 68.75%] [G loss: 0.857228]\n",
      "5806 [D loss: 0.635312, acc.: 59.38%] [G loss: 0.893943]\n",
      "5807 [D loss: 0.679827, acc.: 59.38%] [G loss: 0.959249]\n",
      "5808 [D loss: 0.597604, acc.: 71.88%] [G loss: 0.971927]\n",
      "5809 [D loss: 0.630421, acc.: 65.62%] [G loss: 0.896957]\n",
      "5810 [D loss: 0.786612, acc.: 37.50%] [G loss: 0.905671]\n",
      "5811 [D loss: 0.682810, acc.: 65.62%] [G loss: 0.933264]\n",
      "5812 [D loss: 0.689279, acc.: 56.25%] [G loss: 0.889808]\n",
      "5813 [D loss: 0.708788, acc.: 53.12%] [G loss: 0.904904]\n",
      "5814 [D loss: 0.715944, acc.: 56.25%] [G loss: 0.924424]\n",
      "5815 [D loss: 0.686737, acc.: 56.25%] [G loss: 0.844305]\n",
      "5816 [D loss: 0.622896, acc.: 78.12%] [G loss: 0.797772]\n",
      "5817 [D loss: 0.587252, acc.: 78.12%] [G loss: 0.835386]\n",
      "5818 [D loss: 0.593884, acc.: 68.75%] [G loss: 0.894576]\n",
      "5819 [D loss: 0.644507, acc.: 71.88%] [G loss: 0.922106]\n",
      "5820 [D loss: 0.635671, acc.: 71.88%] [G loss: 0.934773]\n",
      "5821 [D loss: 0.756371, acc.: 40.62%] [G loss: 0.947917]\n",
      "5822 [D loss: 0.803663, acc.: 40.62%] [G loss: 0.949921]\n",
      "5823 [D loss: 0.676077, acc.: 68.75%] [G loss: 0.906986]\n",
      "5824 [D loss: 0.729725, acc.: 43.75%] [G loss: 0.874640]\n",
      "5825 [D loss: 0.573118, acc.: 84.38%] [G loss: 0.862932]\n",
      "5826 [D loss: 0.612060, acc.: 65.62%] [G loss: 0.980265]\n",
      "5827 [D loss: 0.707055, acc.: 53.12%] [G loss: 0.934288]\n",
      "5828 [D loss: 0.696563, acc.: 56.25%] [G loss: 0.976236]\n",
      "5829 [D loss: 0.638269, acc.: 62.50%] [G loss: 0.927697]\n",
      "5830 [D loss: 0.632573, acc.: 62.50%] [G loss: 0.885615]\n",
      "5831 [D loss: 0.681679, acc.: 43.75%] [G loss: 0.924447]\n",
      "5832 [D loss: 0.661966, acc.: 62.50%] [G loss: 0.838012]\n",
      "5833 [D loss: 0.716738, acc.: 50.00%] [G loss: 0.880314]\n",
      "5834 [D loss: 0.639849, acc.: 71.88%] [G loss: 0.910853]\n",
      "5835 [D loss: 0.688164, acc.: 50.00%] [G loss: 0.892906]\n",
      "5836 [D loss: 0.677881, acc.: 50.00%] [G loss: 0.885130]\n",
      "5837 [D loss: 0.700384, acc.: 50.00%] [G loss: 0.858454]\n",
      "5838 [D loss: 0.593269, acc.: 71.88%] [G loss: 0.863878]\n",
      "5839 [D loss: 0.722680, acc.: 46.88%] [G loss: 0.957062]\n",
      "5840 [D loss: 0.699317, acc.: 40.62%] [G loss: 0.868178]\n",
      "5841 [D loss: 0.604617, acc.: 65.62%] [G loss: 0.858141]\n",
      "5842 [D loss: 0.599845, acc.: 68.75%] [G loss: 0.935384]\n",
      "5843 [D loss: 0.691868, acc.: 59.38%] [G loss: 0.925462]\n",
      "5844 [D loss: 0.728862, acc.: 50.00%] [G loss: 0.884556]\n",
      "5845 [D loss: 0.691606, acc.: 56.25%] [G loss: 0.959731]\n",
      "5846 [D loss: 0.678059, acc.: 43.75%] [G loss: 0.968981]\n",
      "5847 [D loss: 0.670994, acc.: 65.62%] [G loss: 1.000654]\n",
      "5848 [D loss: 0.685282, acc.: 56.25%] [G loss: 0.928413]\n",
      "5849 [D loss: 0.666551, acc.: 53.12%] [G loss: 0.910429]\n",
      "5850 [D loss: 0.729705, acc.: 50.00%] [G loss: 0.845513]\n",
      "5851 [D loss: 0.721126, acc.: 56.25%] [G loss: 0.929937]\n",
      "5852 [D loss: 0.644365, acc.: 62.50%] [G loss: 0.943724]\n",
      "5853 [D loss: 0.713164, acc.: 50.00%] [G loss: 0.938023]\n",
      "5854 [D loss: 0.585280, acc.: 62.50%] [G loss: 0.944755]\n",
      "5855 [D loss: 0.613770, acc.: 65.62%] [G loss: 0.905964]\n",
      "5856 [D loss: 0.744284, acc.: 43.75%] [G loss: 0.948272]\n",
      "5857 [D loss: 0.688370, acc.: 59.38%] [G loss: 0.796877]\n",
      "5858 [D loss: 0.588578, acc.: 71.88%] [G loss: 0.788593]\n",
      "5859 [D loss: 0.595694, acc.: 68.75%] [G loss: 0.834780]\n",
      "5860 [D loss: 0.671109, acc.: 62.50%] [G loss: 0.870969]\n",
      "5861 [D loss: 0.646725, acc.: 59.38%] [G loss: 0.939123]\n",
      "5862 [D loss: 0.682954, acc.: 59.38%] [G loss: 0.894277]\n",
      "5863 [D loss: 0.664626, acc.: 62.50%] [G loss: 0.882104]\n",
      "5864 [D loss: 0.585240, acc.: 65.62%] [G loss: 0.905978]\n",
      "5865 [D loss: 0.651814, acc.: 65.62%] [G loss: 0.878883]\n",
      "5866 [D loss: 0.697567, acc.: 59.38%] [G loss: 0.885650]\n",
      "5867 [D loss: 0.671281, acc.: 56.25%] [G loss: 0.941409]\n",
      "5868 [D loss: 0.607592, acc.: 71.88%] [G loss: 0.876289]\n",
      "5869 [D loss: 0.691373, acc.: 59.38%] [G loss: 0.855684]\n",
      "5870 [D loss: 0.643984, acc.: 56.25%] [G loss: 0.724726]\n",
      "5871 [D loss: 0.610909, acc.: 71.88%] [G loss: 0.865054]\n",
      "5872 [D loss: 0.686775, acc.: 53.12%] [G loss: 0.917616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5873 [D loss: 0.757285, acc.: 50.00%] [G loss: 0.914593]\n",
      "5874 [D loss: 0.621787, acc.: 75.00%] [G loss: 0.813124]\n",
      "5875 [D loss: 0.789722, acc.: 40.62%] [G loss: 0.856058]\n",
      "5876 [D loss: 0.618485, acc.: 75.00%] [G loss: 0.845196]\n",
      "5877 [D loss: 0.636473, acc.: 62.50%] [G loss: 0.846807]\n",
      "5878 [D loss: 0.650270, acc.: 68.75%] [G loss: 0.902822]\n",
      "5879 [D loss: 0.679815, acc.: 62.50%] [G loss: 0.948964]\n",
      "5880 [D loss: 0.578829, acc.: 62.50%] [G loss: 0.871487]\n",
      "5881 [D loss: 0.636330, acc.: 62.50%] [G loss: 0.993497]\n",
      "5882 [D loss: 0.767101, acc.: 53.12%] [G loss: 0.853229]\n",
      "5883 [D loss: 0.665935, acc.: 53.12%] [G loss: 0.845761]\n",
      "5884 [D loss: 0.591720, acc.: 71.88%] [G loss: 0.851274]\n",
      "5885 [D loss: 0.696722, acc.: 59.38%] [G loss: 0.835314]\n",
      "5886 [D loss: 0.615187, acc.: 62.50%] [G loss: 0.951423]\n",
      "5887 [D loss: 0.666634, acc.: 53.12%] [G loss: 0.973312]\n",
      "5888 [D loss: 0.640313, acc.: 65.62%] [G loss: 0.897356]\n",
      "5889 [D loss: 0.666001, acc.: 62.50%] [G loss: 0.839072]\n",
      "5890 [D loss: 0.711578, acc.: 56.25%] [G loss: 0.822797]\n",
      "5891 [D loss: 0.608492, acc.: 65.62%] [G loss: 0.807835]\n",
      "5892 [D loss: 0.630199, acc.: 59.38%] [G loss: 0.915047]\n",
      "5893 [D loss: 0.642404, acc.: 65.62%] [G loss: 0.868033]\n",
      "5894 [D loss: 0.565422, acc.: 75.00%] [G loss: 0.963183]\n",
      "5895 [D loss: 0.626566, acc.: 62.50%] [G loss: 0.926563]\n",
      "5896 [D loss: 0.620833, acc.: 68.75%] [G loss: 0.902160]\n",
      "5897 [D loss: 0.680194, acc.: 62.50%] [G loss: 0.917331]\n",
      "5898 [D loss: 0.541931, acc.: 78.12%] [G loss: 1.008527]\n",
      "5899 [D loss: 0.661307, acc.: 59.38%] [G loss: 1.001953]\n",
      "5900 [D loss: 0.591893, acc.: 71.88%] [G loss: 0.994165]\n",
      "5901 [D loss: 0.678068, acc.: 62.50%] [G loss: 0.892970]\n",
      "5902 [D loss: 0.604647, acc.: 68.75%] [G loss: 0.977033]\n",
      "5903 [D loss: 0.642591, acc.: 65.62%] [G loss: 0.968383]\n",
      "5904 [D loss: 0.574708, acc.: 81.25%] [G loss: 0.806044]\n",
      "5905 [D loss: 0.659631, acc.: 59.38%] [G loss: 0.835726]\n",
      "5906 [D loss: 0.646884, acc.: 62.50%] [G loss: 0.941879]\n",
      "5907 [D loss: 0.752979, acc.: 50.00%] [G loss: 0.928129]\n",
      "5908 [D loss: 0.692355, acc.: 62.50%] [G loss: 0.916976]\n",
      "5909 [D loss: 0.715058, acc.: 40.62%] [G loss: 0.886560]\n",
      "5910 [D loss: 0.695454, acc.: 56.25%] [G loss: 0.883782]\n",
      "5911 [D loss: 0.684746, acc.: 53.12%] [G loss: 0.895972]\n",
      "5912 [D loss: 0.616455, acc.: 65.62%] [G loss: 0.853949]\n",
      "5913 [D loss: 0.736213, acc.: 50.00%] [G loss: 0.936152]\n",
      "5914 [D loss: 0.588655, acc.: 78.12%] [G loss: 0.909941]\n",
      "5915 [D loss: 0.685406, acc.: 53.12%] [G loss: 0.895484]\n",
      "5916 [D loss: 0.578208, acc.: 75.00%] [G loss: 0.893577]\n",
      "5917 [D loss: 0.750638, acc.: 43.75%] [G loss: 0.927893]\n",
      "5918 [D loss: 0.640251, acc.: 68.75%] [G loss: 0.931796]\n",
      "5919 [D loss: 0.678125, acc.: 50.00%] [G loss: 0.952792]\n",
      "5920 [D loss: 0.609289, acc.: 68.75%] [G loss: 0.905458]\n",
      "5921 [D loss: 0.605018, acc.: 68.75%] [G loss: 0.919030]\n",
      "5922 [D loss: 0.687324, acc.: 53.12%] [G loss: 0.834638]\n",
      "5923 [D loss: 0.692864, acc.: 53.12%] [G loss: 0.899170]\n",
      "5924 [D loss: 0.711608, acc.: 46.88%] [G loss: 0.841043]\n",
      "5925 [D loss: 0.649144, acc.: 62.50%] [G loss: 0.869141]\n",
      "5926 [D loss: 0.581421, acc.: 65.62%] [G loss: 0.847709]\n",
      "5927 [D loss: 0.726071, acc.: 56.25%] [G loss: 0.851405]\n",
      "5928 [D loss: 0.640687, acc.: 59.38%] [G loss: 0.865625]\n",
      "5929 [D loss: 0.758221, acc.: 40.62%] [G loss: 0.832729]\n",
      "5930 [D loss: 0.684663, acc.: 53.12%] [G loss: 0.974380]\n",
      "5931 [D loss: 0.627964, acc.: 71.88%] [G loss: 0.991076]\n",
      "5932 [D loss: 0.696217, acc.: 46.88%] [G loss: 1.011595]\n",
      "5933 [D loss: 0.672944, acc.: 56.25%] [G loss: 0.889837]\n",
      "5934 [D loss: 0.596555, acc.: 71.88%] [G loss: 0.913953]\n",
      "5935 [D loss: 0.687592, acc.: 56.25%] [G loss: 0.828652]\n",
      "5936 [D loss: 0.660877, acc.: 62.50%] [G loss: 0.938848]\n",
      "5937 [D loss: 0.643071, acc.: 65.62%] [G loss: 0.961493]\n",
      "5938 [D loss: 0.645239, acc.: 62.50%] [G loss: 0.914360]\n",
      "5939 [D loss: 0.723739, acc.: 46.88%] [G loss: 0.925383]\n",
      "5940 [D loss: 0.621148, acc.: 68.75%] [G loss: 0.941620]\n",
      "5941 [D loss: 0.699389, acc.: 53.12%] [G loss: 0.944684]\n",
      "5942 [D loss: 0.686680, acc.: 59.38%] [G loss: 0.921193]\n",
      "5943 [D loss: 0.664586, acc.: 65.62%] [G loss: 0.878932]\n",
      "5944 [D loss: 0.635020, acc.: 62.50%] [G loss: 0.815090]\n",
      "5945 [D loss: 0.662937, acc.: 59.38%] [G loss: 0.837995]\n",
      "5946 [D loss: 0.665800, acc.: 65.62%] [G loss: 0.880461]\n",
      "5947 [D loss: 0.651366, acc.: 56.25%] [G loss: 0.941095]\n",
      "5948 [D loss: 0.609518, acc.: 71.88%] [G loss: 0.904486]\n",
      "5949 [D loss: 0.571866, acc.: 68.75%] [G loss: 0.940809]\n",
      "5950 [D loss: 0.587443, acc.: 75.00%] [G loss: 0.852519]\n",
      "5951 [D loss: 0.677130, acc.: 65.62%] [G loss: 0.826776]\n",
      "5952 [D loss: 0.611688, acc.: 71.88%] [G loss: 0.893962]\n",
      "5953 [D loss: 0.650389, acc.: 46.88%] [G loss: 0.896800]\n",
      "5954 [D loss: 0.725683, acc.: 62.50%] [G loss: 0.776791]\n",
      "5955 [D loss: 0.563074, acc.: 71.88%] [G loss: 0.920811]\n",
      "5956 [D loss: 0.678196, acc.: 59.38%] [G loss: 0.833293]\n",
      "5957 [D loss: 0.594613, acc.: 68.75%] [G loss: 0.869474]\n",
      "5958 [D loss: 0.657145, acc.: 56.25%] [G loss: 0.960500]\n",
      "5959 [D loss: 0.576066, acc.: 71.88%] [G loss: 0.944793]\n",
      "5960 [D loss: 0.603297, acc.: 62.50%] [G loss: 0.921265]\n",
      "5961 [D loss: 0.769549, acc.: 37.50%] [G loss: 0.952797]\n",
      "5962 [D loss: 0.644873, acc.: 71.88%] [G loss: 0.944336]\n",
      "5963 [D loss: 0.628737, acc.: 68.75%] [G loss: 0.907474]\n",
      "5964 [D loss: 0.720939, acc.: 53.12%] [G loss: 0.910103]\n",
      "5965 [D loss: 0.730665, acc.: 46.88%] [G loss: 0.816342]\n",
      "5966 [D loss: 0.655895, acc.: 50.00%] [G loss: 0.799085]\n",
      "5967 [D loss: 0.629886, acc.: 68.75%] [G loss: 0.856250]\n",
      "5968 [D loss: 0.625945, acc.: 68.75%] [G loss: 0.807161]\n",
      "5969 [D loss: 0.651669, acc.: 59.38%] [G loss: 0.863675]\n",
      "5970 [D loss: 0.625814, acc.: 62.50%] [G loss: 0.774594]\n",
      "5971 [D loss: 0.657852, acc.: 68.75%] [G loss: 0.897596]\n",
      "5972 [D loss: 0.648120, acc.: 68.75%] [G loss: 0.866123]\n",
      "5973 [D loss: 0.711487, acc.: 59.38%] [G loss: 0.806878]\n",
      "5974 [D loss: 0.618333, acc.: 65.62%] [G loss: 0.894313]\n",
      "5975 [D loss: 0.630170, acc.: 65.62%] [G loss: 0.996342]\n",
      "5976 [D loss: 0.599089, acc.: 75.00%] [G loss: 0.923817]\n",
      "5977 [D loss: 0.707154, acc.: 53.12%] [G loss: 0.924903]\n",
      "5978 [D loss: 0.548876, acc.: 78.12%] [G loss: 0.766950]\n",
      "5979 [D loss: 0.723129, acc.: 50.00%] [G loss: 0.875315]\n",
      "5980 [D loss: 0.626523, acc.: 68.75%] [G loss: 0.817946]\n",
      "5981 [D loss: 0.639314, acc.: 65.62%] [G loss: 0.847963]\n",
      "5982 [D loss: 0.684304, acc.: 46.88%] [G loss: 0.908624]\n",
      "5983 [D loss: 0.815230, acc.: 34.38%] [G loss: 0.856661]\n",
      "5984 [D loss: 0.719475, acc.: 56.25%] [G loss: 0.833454]\n",
      "5985 [D loss: 0.640764, acc.: 65.62%] [G loss: 0.850951]\n",
      "5986 [D loss: 0.591172, acc.: 65.62%] [G loss: 0.941400]\n",
      "5987 [D loss: 0.657718, acc.: 62.50%] [G loss: 0.944702]\n",
      "5988 [D loss: 0.653245, acc.: 62.50%] [G loss: 0.912396]\n",
      "5989 [D loss: 0.704571, acc.: 53.12%] [G loss: 0.937149]\n",
      "5990 [D loss: 0.672716, acc.: 62.50%] [G loss: 0.891941]\n",
      "5991 [D loss: 0.663983, acc.: 59.38%] [G loss: 0.846163]\n",
      "5992 [D loss: 0.751363, acc.: 46.88%] [G loss: 0.889862]\n",
      "5993 [D loss: 0.658114, acc.: 65.62%] [G loss: 0.872004]\n",
      "5994 [D loss: 0.650893, acc.: 53.12%] [G loss: 0.881361]\n",
      "5995 [D loss: 0.699768, acc.: 65.62%] [G loss: 0.778975]\n",
      "5996 [D loss: 0.621861, acc.: 62.50%] [G loss: 0.859110]\n",
      "5997 [D loss: 0.688817, acc.: 43.75%] [G loss: 0.967264]\n",
      "5998 [D loss: 0.586024, acc.: 78.12%] [G loss: 0.909730]\n",
      "5999 [D loss: 0.614995, acc.: 65.62%] [G loss: 0.931257]\n",
      "6000 [D loss: 0.704792, acc.: 65.62%] [G loss: 0.899035]\n",
      "6001 [D loss: 0.610654, acc.: 71.88%] [G loss: 0.889026]\n",
      "6002 [D loss: 0.666159, acc.: 62.50%] [G loss: 0.889665]\n",
      "6003 [D loss: 0.766676, acc.: 37.50%] [G loss: 0.918396]\n",
      "6004 [D loss: 0.636208, acc.: 59.38%] [G loss: 0.863097]\n",
      "6005 [D loss: 0.644674, acc.: 65.62%] [G loss: 0.865020]\n",
      "6006 [D loss: 0.676776, acc.: 59.38%] [G loss: 0.876017]\n",
      "6007 [D loss: 0.689193, acc.: 53.12%] [G loss: 0.893139]\n",
      "6008 [D loss: 0.690223, acc.: 59.38%] [G loss: 0.966577]\n",
      "6009 [D loss: 0.669425, acc.: 53.12%] [G loss: 0.911720]\n",
      "6010 [D loss: 0.559069, acc.: 71.88%] [G loss: 0.899803]\n",
      "6011 [D loss: 0.567624, acc.: 71.88%] [G loss: 0.908591]\n",
      "6012 [D loss: 0.653041, acc.: 65.62%] [G loss: 0.959621]\n",
      "6013 [D loss: 0.697596, acc.: 50.00%] [G loss: 0.832796]\n",
      "6014 [D loss: 0.639386, acc.: 59.38%] [G loss: 0.861120]\n",
      "6015 [D loss: 0.623334, acc.: 59.38%] [G loss: 0.932206]\n",
      "6016 [D loss: 0.770290, acc.: 56.25%] [G loss: 0.859696]\n",
      "6017 [D loss: 0.713836, acc.: 56.25%] [G loss: 0.962613]\n",
      "6018 [D loss: 0.654619, acc.: 59.38%] [G loss: 0.899660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6019 [D loss: 0.608691, acc.: 65.62%] [G loss: 0.902600]\n",
      "6020 [D loss: 0.711289, acc.: 59.38%] [G loss: 0.916127]\n",
      "6021 [D loss: 0.650073, acc.: 56.25%] [G loss: 0.880893]\n",
      "6022 [D loss: 0.642427, acc.: 59.38%] [G loss: 0.876206]\n",
      "6023 [D loss: 0.657193, acc.: 68.75%] [G loss: 0.841234]\n",
      "6024 [D loss: 0.608758, acc.: 65.62%] [G loss: 0.897530]\n",
      "6025 [D loss: 0.631839, acc.: 62.50%] [G loss: 0.911660]\n",
      "6026 [D loss: 0.644144, acc.: 56.25%] [G loss: 0.921077]\n",
      "6027 [D loss: 0.704758, acc.: 46.88%] [G loss: 0.932593]\n",
      "6028 [D loss: 0.716207, acc.: 53.12%] [G loss: 0.913598]\n",
      "6029 [D loss: 0.673900, acc.: 50.00%] [G loss: 0.937168]\n",
      "6030 [D loss: 0.608792, acc.: 68.75%] [G loss: 0.913738]\n",
      "6031 [D loss: 0.675980, acc.: 50.00%] [G loss: 0.848810]\n",
      "6032 [D loss: 0.680533, acc.: 62.50%] [G loss: 0.791970]\n",
      "6033 [D loss: 0.677586, acc.: 50.00%] [G loss: 0.855576]\n",
      "6034 [D loss: 0.638929, acc.: 65.62%] [G loss: 0.882465]\n",
      "6035 [D loss: 0.654600, acc.: 68.75%] [G loss: 0.881724]\n",
      "6036 [D loss: 0.695227, acc.: 53.12%] [G loss: 0.899454]\n",
      "6037 [D loss: 0.702002, acc.: 56.25%] [G loss: 0.884378]\n",
      "6038 [D loss: 0.709804, acc.: 59.38%] [G loss: 0.892129]\n",
      "6039 [D loss: 0.672880, acc.: 62.50%] [G loss: 0.832837]\n",
      "6040 [D loss: 0.743180, acc.: 50.00%] [G loss: 0.818419]\n",
      "6041 [D loss: 0.611814, acc.: 62.50%] [G loss: 0.898051]\n",
      "6042 [D loss: 0.648774, acc.: 59.38%] [G loss: 0.960345]\n",
      "6043 [D loss: 0.710597, acc.: 59.38%] [G loss: 1.015195]\n",
      "6044 [D loss: 0.726059, acc.: 46.88%] [G loss: 0.992873]\n",
      "6045 [D loss: 0.668481, acc.: 56.25%] [G loss: 0.955872]\n",
      "6046 [D loss: 0.593740, acc.: 84.38%] [G loss: 0.872727]\n",
      "6047 [D loss: 0.637872, acc.: 65.62%] [G loss: 0.836365]\n",
      "6048 [D loss: 0.682741, acc.: 62.50%] [G loss: 0.831784]\n",
      "6049 [D loss: 0.660349, acc.: 59.38%] [G loss: 0.917343]\n",
      "6050 [D loss: 0.670428, acc.: 65.62%] [G loss: 0.903926]\n",
      "6051 [D loss: 0.652279, acc.: 65.62%] [G loss: 0.885149]\n",
      "6052 [D loss: 0.716138, acc.: 53.12%] [G loss: 0.920839]\n",
      "6053 [D loss: 0.693839, acc.: 59.38%] [G loss: 0.800936]\n",
      "6054 [D loss: 0.605638, acc.: 62.50%] [G loss: 0.831212]\n",
      "6055 [D loss: 0.646290, acc.: 53.12%] [G loss: 0.892273]\n",
      "6056 [D loss: 0.633763, acc.: 65.62%] [G loss: 0.962498]\n",
      "6057 [D loss: 0.643641, acc.: 59.38%] [G loss: 0.869388]\n",
      "6058 [D loss: 0.664235, acc.: 59.38%] [G loss: 0.919900]\n",
      "6059 [D loss: 0.727681, acc.: 50.00%] [G loss: 0.922200]\n",
      "6060 [D loss: 0.735675, acc.: 53.12%] [G loss: 0.847558]\n",
      "6061 [D loss: 0.678873, acc.: 71.88%] [G loss: 1.003965]\n",
      "6062 [D loss: 0.615396, acc.: 62.50%] [G loss: 0.886295]\n",
      "6063 [D loss: 0.619558, acc.: 65.62%] [G loss: 0.977248]\n",
      "6064 [D loss: 0.725407, acc.: 50.00%] [G loss: 0.972112]\n",
      "6065 [D loss: 0.657677, acc.: 56.25%] [G loss: 0.971068]\n",
      "6066 [D loss: 0.544426, acc.: 78.12%] [G loss: 0.987566]\n",
      "6067 [D loss: 0.756130, acc.: 40.62%] [G loss: 0.939879]\n",
      "6068 [D loss: 0.620108, acc.: 71.88%] [G loss: 0.984653]\n",
      "6069 [D loss: 0.616160, acc.: 71.88%] [G loss: 0.914793]\n",
      "6070 [D loss: 0.680442, acc.: 62.50%] [G loss: 0.824297]\n",
      "6071 [D loss: 0.577030, acc.: 65.62%] [G loss: 0.830118]\n",
      "6072 [D loss: 0.650099, acc.: 68.75%] [G loss: 0.887298]\n",
      "6073 [D loss: 0.651383, acc.: 59.38%] [G loss: 0.897350]\n",
      "6074 [D loss: 0.610751, acc.: 71.88%] [G loss: 0.893088]\n",
      "6075 [D loss: 0.812335, acc.: 50.00%] [G loss: 0.944134]\n",
      "6076 [D loss: 0.649258, acc.: 65.62%] [G loss: 0.891703]\n",
      "6077 [D loss: 0.628299, acc.: 65.62%] [G loss: 0.939818]\n",
      "6078 [D loss: 0.692119, acc.: 50.00%] [G loss: 0.900536]\n",
      "6079 [D loss: 0.637217, acc.: 59.38%] [G loss: 0.881511]\n",
      "6080 [D loss: 0.705613, acc.: 56.25%] [G loss: 0.859282]\n",
      "6081 [D loss: 0.623682, acc.: 68.75%] [G loss: 0.838176]\n",
      "6082 [D loss: 0.619877, acc.: 62.50%] [G loss: 0.891663]\n",
      "6083 [D loss: 0.721879, acc.: 50.00%] [G loss: 0.871144]\n",
      "6084 [D loss: 0.659375, acc.: 65.62%] [G loss: 0.834148]\n",
      "6085 [D loss: 0.697101, acc.: 53.12%] [G loss: 0.802210]\n",
      "6086 [D loss: 0.636536, acc.: 62.50%] [G loss: 0.808196]\n",
      "6087 [D loss: 0.592224, acc.: 59.38%] [G loss: 0.962163]\n",
      "6088 [D loss: 0.595205, acc.: 65.62%] [G loss: 0.872985]\n",
      "6089 [D loss: 0.661150, acc.: 56.25%] [G loss: 0.936632]\n",
      "6090 [D loss: 0.762612, acc.: 46.88%] [G loss: 0.742434]\n",
      "6091 [D loss: 0.698366, acc.: 53.12%] [G loss: 0.800447]\n",
      "6092 [D loss: 0.669767, acc.: 56.25%] [G loss: 0.862164]\n",
      "6093 [D loss: 0.706361, acc.: 53.12%] [G loss: 0.911845]\n",
      "6094 [D loss: 0.677391, acc.: 59.38%] [G loss: 0.928026]\n",
      "6095 [D loss: 0.640490, acc.: 59.38%] [G loss: 0.928925]\n",
      "6096 [D loss: 0.640638, acc.: 59.38%] [G loss: 0.891199]\n",
      "6097 [D loss: 0.738252, acc.: 50.00%] [G loss: 0.873310]\n",
      "6098 [D loss: 0.724727, acc.: 50.00%] [G loss: 0.937512]\n",
      "6099 [D loss: 0.618660, acc.: 62.50%] [G loss: 0.865852]\n",
      "6100 [D loss: 0.592995, acc.: 68.75%] [G loss: 0.950864]\n",
      "6101 [D loss: 0.604835, acc.: 68.75%] [G loss: 0.893715]\n",
      "6102 [D loss: 0.701951, acc.: 56.25%] [G loss: 0.947045]\n",
      "6103 [D loss: 0.595858, acc.: 68.75%] [G loss: 0.971466]\n",
      "6104 [D loss: 0.672700, acc.: 56.25%] [G loss: 1.090577]\n",
      "6105 [D loss: 0.684498, acc.: 68.75%] [G loss: 0.960675]\n",
      "6106 [D loss: 0.574720, acc.: 84.38%] [G loss: 0.989314]\n",
      "6107 [D loss: 0.775377, acc.: 53.12%] [G loss: 0.922844]\n",
      "6108 [D loss: 0.748931, acc.: 56.25%] [G loss: 0.941337]\n",
      "6109 [D loss: 0.717024, acc.: 56.25%] [G loss: 0.938370]\n",
      "6110 [D loss: 0.620704, acc.: 71.88%] [G loss: 0.861840]\n",
      "6111 [D loss: 0.689111, acc.: 59.38%] [G loss: 0.844429]\n",
      "6112 [D loss: 0.678295, acc.: 59.38%] [G loss: 0.951679]\n",
      "6113 [D loss: 0.671945, acc.: 68.75%] [G loss: 0.949803]\n",
      "6114 [D loss: 0.618020, acc.: 68.75%] [G loss: 0.894386]\n",
      "6115 [D loss: 0.702411, acc.: 53.12%] [G loss: 0.888815]\n",
      "6116 [D loss: 0.715381, acc.: 56.25%] [G loss: 0.864062]\n",
      "6117 [D loss: 0.620421, acc.: 59.38%] [G loss: 0.968229]\n",
      "6118 [D loss: 0.620516, acc.: 59.38%] [G loss: 0.998098]\n",
      "6119 [D loss: 0.588222, acc.: 75.00%] [G loss: 0.871352]\n",
      "6120 [D loss: 0.649809, acc.: 59.38%] [G loss: 0.900176]\n",
      "6121 [D loss: 0.672319, acc.: 59.38%] [G loss: 0.919274]\n",
      "6122 [D loss: 0.574484, acc.: 71.88%] [G loss: 0.801064]\n",
      "6123 [D loss: 0.685594, acc.: 62.50%] [G loss: 0.835584]\n",
      "6124 [D loss: 0.656454, acc.: 59.38%] [G loss: 0.867293]\n",
      "6125 [D loss: 0.668889, acc.: 62.50%] [G loss: 0.837019]\n",
      "6126 [D loss: 0.649528, acc.: 65.62%] [G loss: 0.776587]\n",
      "6127 [D loss: 0.700938, acc.: 56.25%] [G loss: 0.854109]\n",
      "6128 [D loss: 0.585754, acc.: 71.88%] [G loss: 0.817922]\n",
      "6129 [D loss: 0.700043, acc.: 56.25%] [G loss: 0.874305]\n",
      "6130 [D loss: 0.697519, acc.: 46.88%] [G loss: 0.850938]\n",
      "6131 [D loss: 0.681260, acc.: 46.88%] [G loss: 0.936086]\n",
      "6132 [D loss: 0.608472, acc.: 75.00%] [G loss: 0.925366]\n",
      "6133 [D loss: 0.613654, acc.: 62.50%] [G loss: 0.959458]\n",
      "6134 [D loss: 0.699717, acc.: 62.50%] [G loss: 0.845854]\n",
      "6135 [D loss: 0.779524, acc.: 53.12%] [G loss: 0.839534]\n",
      "6136 [D loss: 0.550071, acc.: 75.00%] [G loss: 0.885156]\n",
      "6137 [D loss: 0.742339, acc.: 50.00%] [G loss: 0.810312]\n",
      "6138 [D loss: 0.592938, acc.: 65.62%] [G loss: 0.951087]\n",
      "6139 [D loss: 0.578476, acc.: 78.12%] [G loss: 0.906618]\n",
      "6140 [D loss: 0.716301, acc.: 59.38%] [G loss: 0.899423]\n",
      "6141 [D loss: 0.677040, acc.: 62.50%] [G loss: 0.850531]\n",
      "6142 [D loss: 0.609064, acc.: 62.50%] [G loss: 0.738909]\n",
      "6143 [D loss: 0.616116, acc.: 62.50%] [G loss: 0.874838]\n",
      "6144 [D loss: 0.665384, acc.: 62.50%] [G loss: 0.858506]\n",
      "6145 [D loss: 0.629173, acc.: 68.75%] [G loss: 0.759954]\n",
      "6146 [D loss: 0.684099, acc.: 59.38%] [G loss: 0.876858]\n",
      "6147 [D loss: 0.621514, acc.: 65.62%] [G loss: 0.851150]\n",
      "6148 [D loss: 0.641709, acc.: 62.50%] [G loss: 0.828097]\n",
      "6149 [D loss: 0.624193, acc.: 71.88%] [G loss: 0.818014]\n",
      "6150 [D loss: 0.744079, acc.: 56.25%] [G loss: 0.816002]\n",
      "6151 [D loss: 0.595560, acc.: 71.88%] [G loss: 0.890312]\n",
      "6152 [D loss: 0.578542, acc.: 71.88%] [G loss: 0.896233]\n",
      "6153 [D loss: 0.702420, acc.: 53.12%] [G loss: 0.788829]\n",
      "6154 [D loss: 0.660747, acc.: 62.50%] [G loss: 0.915630]\n",
      "6155 [D loss: 0.627582, acc.: 68.75%] [G loss: 0.852951]\n",
      "6156 [D loss: 0.618623, acc.: 71.88%] [G loss: 0.786288]\n",
      "6157 [D loss: 0.630614, acc.: 65.62%] [G loss: 0.866264]\n",
      "6158 [D loss: 0.676884, acc.: 71.88%] [G loss: 0.885671]\n",
      "6159 [D loss: 0.691140, acc.: 59.38%] [G loss: 0.897036]\n",
      "6160 [D loss: 0.651948, acc.: 62.50%] [G loss: 0.861810]\n",
      "6161 [D loss: 0.605313, acc.: 62.50%] [G loss: 0.851663]\n",
      "6162 [D loss: 0.740610, acc.: 56.25%] [G loss: 0.766260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6163 [D loss: 0.665271, acc.: 53.12%] [G loss: 0.828373]\n",
      "6164 [D loss: 0.645190, acc.: 65.62%] [G loss: 0.907114]\n",
      "6165 [D loss: 0.686822, acc.: 56.25%] [G loss: 0.740782]\n",
      "6166 [D loss: 0.664812, acc.: 62.50%] [G loss: 0.880089]\n",
      "6167 [D loss: 0.759649, acc.: 56.25%] [G loss: 0.887584]\n",
      "6168 [D loss: 0.628835, acc.: 78.12%] [G loss: 1.039523]\n",
      "6169 [D loss: 0.707220, acc.: 53.12%] [G loss: 1.010346]\n",
      "6170 [D loss: 0.732084, acc.: 56.25%] [G loss: 0.944642]\n",
      "6171 [D loss: 0.637102, acc.: 71.88%] [G loss: 0.885200]\n",
      "6172 [D loss: 0.638216, acc.: 62.50%] [G loss: 0.868099]\n",
      "6173 [D loss: 0.709293, acc.: 56.25%] [G loss: 0.889861]\n",
      "6174 [D loss: 0.691081, acc.: 68.75%] [G loss: 0.921984]\n",
      "6175 [D loss: 0.632257, acc.: 65.62%] [G loss: 0.920221]\n",
      "6176 [D loss: 0.757669, acc.: 50.00%] [G loss: 0.860279]\n",
      "6177 [D loss: 0.720346, acc.: 50.00%] [G loss: 0.885238]\n",
      "6178 [D loss: 0.687582, acc.: 50.00%] [G loss: 0.860109]\n",
      "6179 [D loss: 0.642658, acc.: 59.38%] [G loss: 0.885342]\n",
      "6180 [D loss: 0.674616, acc.: 59.38%] [G loss: 0.884974]\n",
      "6181 [D loss: 0.632763, acc.: 62.50%] [G loss: 0.872743]\n",
      "6182 [D loss: 0.699486, acc.: 43.75%] [G loss: 0.915062]\n",
      "6183 [D loss: 0.626456, acc.: 62.50%] [G loss: 0.927096]\n",
      "6184 [D loss: 0.648764, acc.: 59.38%] [G loss: 0.898773]\n",
      "6185 [D loss: 0.670433, acc.: 62.50%] [G loss: 0.874382]\n",
      "6186 [D loss: 0.662662, acc.: 65.62%] [G loss: 0.934978]\n",
      "6187 [D loss: 0.713642, acc.: 46.88%] [G loss: 0.833703]\n",
      "6188 [D loss: 0.605233, acc.: 68.75%] [G loss: 0.880117]\n",
      "6189 [D loss: 0.689555, acc.: 53.12%] [G loss: 0.858208]\n",
      "6190 [D loss: 0.668612, acc.: 56.25%] [G loss: 0.840798]\n",
      "6191 [D loss: 0.610381, acc.: 71.88%] [G loss: 0.852544]\n",
      "6192 [D loss: 0.695731, acc.: 59.38%] [G loss: 0.808074]\n",
      "6193 [D loss: 0.599780, acc.: 71.88%] [G loss: 0.886079]\n",
      "6194 [D loss: 0.637280, acc.: 65.62%] [G loss: 0.942949]\n",
      "6195 [D loss: 0.646086, acc.: 56.25%] [G loss: 0.887967]\n",
      "6196 [D loss: 0.649203, acc.: 65.62%] [G loss: 0.859391]\n",
      "6197 [D loss: 0.607386, acc.: 62.50%] [G loss: 0.845732]\n",
      "6198 [D loss: 0.663390, acc.: 62.50%] [G loss: 0.936712]\n",
      "6199 [D loss: 0.704885, acc.: 59.38%] [G loss: 0.833686]\n",
      "6200 [D loss: 0.687455, acc.: 53.12%] [G loss: 0.892390]\n",
      "6201 [D loss: 0.579058, acc.: 71.88%] [G loss: 0.905232]\n",
      "6202 [D loss: 0.652284, acc.: 59.38%] [G loss: 0.899711]\n",
      "6203 [D loss: 0.705576, acc.: 50.00%] [G loss: 0.843971]\n",
      "6204 [D loss: 0.686753, acc.: 56.25%] [G loss: 0.890144]\n",
      "6205 [D loss: 0.700077, acc.: 46.88%] [G loss: 0.836362]\n",
      "6206 [D loss: 0.700307, acc.: 46.88%] [G loss: 0.846275]\n",
      "6207 [D loss: 0.634109, acc.: 62.50%] [G loss: 0.877871]\n",
      "6208 [D loss: 0.632018, acc.: 56.25%] [G loss: 0.930870]\n",
      "6209 [D loss: 0.694505, acc.: 59.38%] [G loss: 0.876352]\n",
      "6210 [D loss: 0.661610, acc.: 65.62%] [G loss: 0.941293]\n",
      "6211 [D loss: 0.675881, acc.: 56.25%] [G loss: 0.870350]\n",
      "6212 [D loss: 0.660556, acc.: 56.25%] [G loss: 0.898580]\n",
      "6213 [D loss: 0.724440, acc.: 50.00%] [G loss: 0.903586]\n",
      "6214 [D loss: 0.703199, acc.: 53.12%] [G loss: 0.862391]\n",
      "6215 [D loss: 0.655693, acc.: 65.62%] [G loss: 0.865351]\n",
      "6216 [D loss: 0.654906, acc.: 59.38%] [G loss: 0.849991]\n",
      "6217 [D loss: 0.635186, acc.: 65.62%] [G loss: 0.792170]\n",
      "6218 [D loss: 0.610994, acc.: 71.88%] [G loss: 0.891820]\n",
      "6219 [D loss: 0.644654, acc.: 68.75%] [G loss: 0.961050]\n",
      "6220 [D loss: 0.616357, acc.: 75.00%] [G loss: 0.900694]\n",
      "6221 [D loss: 0.756314, acc.: 50.00%] [G loss: 0.859735]\n",
      "6222 [D loss: 0.634393, acc.: 68.75%] [G loss: 0.804602]\n",
      "6223 [D loss: 0.671314, acc.: 65.62%] [G loss: 0.814834]\n",
      "6224 [D loss: 0.661104, acc.: 56.25%] [G loss: 0.796416]\n",
      "6225 [D loss: 0.708312, acc.: 50.00%] [G loss: 0.890936]\n",
      "6226 [D loss: 0.714065, acc.: 59.38%] [G loss: 0.852186]\n",
      "6227 [D loss: 0.667511, acc.: 62.50%] [G loss: 0.915539]\n",
      "6228 [D loss: 0.654662, acc.: 62.50%] [G loss: 0.895138]\n",
      "6229 [D loss: 0.630799, acc.: 71.88%] [G loss: 0.888208]\n",
      "6230 [D loss: 0.541914, acc.: 78.12%] [G loss: 0.886120]\n",
      "6231 [D loss: 0.707589, acc.: 53.12%] [G loss: 0.820766]\n",
      "6232 [D loss: 0.656265, acc.: 71.88%] [G loss: 0.927935]\n",
      "6233 [D loss: 0.853009, acc.: 37.50%] [G loss: 0.861888]\n",
      "6234 [D loss: 0.597233, acc.: 78.12%] [G loss: 0.827875]\n",
      "6235 [D loss: 0.658860, acc.: 62.50%] [G loss: 0.875074]\n",
      "6236 [D loss: 0.705480, acc.: 53.12%] [G loss: 0.864763]\n",
      "6237 [D loss: 0.685815, acc.: 50.00%] [G loss: 0.882631]\n",
      "6238 [D loss: 0.646600, acc.: 68.75%] [G loss: 0.978026]\n",
      "6239 [D loss: 0.708543, acc.: 50.00%] [G loss: 0.936088]\n",
      "6240 [D loss: 0.672923, acc.: 50.00%] [G loss: 0.979070]\n",
      "6241 [D loss: 0.584389, acc.: 65.62%] [G loss: 0.948759]\n",
      "6242 [D loss: 0.655856, acc.: 56.25%] [G loss: 0.887789]\n",
      "6243 [D loss: 0.647079, acc.: 62.50%] [G loss: 0.821271]\n",
      "6244 [D loss: 0.727698, acc.: 43.75%] [G loss: 0.880818]\n",
      "6245 [D loss: 0.710002, acc.: 56.25%] [G loss: 0.776000]\n",
      "6246 [D loss: 0.675475, acc.: 53.12%] [G loss: 0.832443]\n",
      "6247 [D loss: 0.705919, acc.: 59.38%] [G loss: 0.806152]\n",
      "6248 [D loss: 0.636045, acc.: 65.62%] [G loss: 0.746652]\n",
      "6249 [D loss: 0.788637, acc.: 37.50%] [G loss: 0.769265]\n",
      "6250 [D loss: 0.582971, acc.: 71.88%] [G loss: 0.868222]\n",
      "6251 [D loss: 0.587824, acc.: 75.00%] [G loss: 0.833544]\n",
      "6252 [D loss: 0.638535, acc.: 65.62%] [G loss: 0.896980]\n",
      "6253 [D loss: 0.751921, acc.: 50.00%] [G loss: 0.849203]\n",
      "6254 [D loss: 0.624782, acc.: 68.75%] [G loss: 0.811916]\n",
      "6255 [D loss: 0.657815, acc.: 59.38%] [G loss: 0.870679]\n",
      "6256 [D loss: 0.635378, acc.: 46.88%] [G loss: 0.836734]\n",
      "6257 [D loss: 0.694902, acc.: 62.50%] [G loss: 0.835567]\n",
      "6258 [D loss: 0.617172, acc.: 75.00%] [G loss: 0.904014]\n",
      "6259 [D loss: 0.630528, acc.: 65.62%] [G loss: 0.903769]\n",
      "6260 [D loss: 0.598437, acc.: 75.00%] [G loss: 0.976913]\n",
      "6261 [D loss: 0.687817, acc.: 59.38%] [G loss: 0.911329]\n",
      "6262 [D loss: 0.675780, acc.: 62.50%] [G loss: 0.816325]\n",
      "6263 [D loss: 0.706462, acc.: 43.75%] [G loss: 0.853215]\n",
      "6264 [D loss: 0.692821, acc.: 56.25%] [G loss: 0.877938]\n",
      "6265 [D loss: 0.669538, acc.: 59.38%] [G loss: 0.854374]\n",
      "6266 [D loss: 0.616246, acc.: 65.62%] [G loss: 0.887645]\n",
      "6267 [D loss: 0.705622, acc.: 53.12%] [G loss: 0.933600]\n",
      "6268 [D loss: 0.749704, acc.: 43.75%] [G loss: 0.945115]\n",
      "6269 [D loss: 0.652345, acc.: 65.62%] [G loss: 0.900402]\n",
      "6270 [D loss: 0.763019, acc.: 50.00%] [G loss: 0.920390]\n",
      "6271 [D loss: 0.716139, acc.: 53.12%] [G loss: 0.950318]\n",
      "6272 [D loss: 0.716682, acc.: 50.00%] [G loss: 0.925006]\n",
      "6273 [D loss: 0.656264, acc.: 59.38%] [G loss: 0.958322]\n",
      "6274 [D loss: 0.624544, acc.: 68.75%] [G loss: 0.958371]\n",
      "6275 [D loss: 0.777805, acc.: 59.38%] [G loss: 0.897226]\n",
      "6276 [D loss: 0.739017, acc.: 56.25%] [G loss: 0.873702]\n",
      "6277 [D loss: 0.634489, acc.: 68.75%] [G loss: 0.851270]\n",
      "6278 [D loss: 0.618231, acc.: 65.62%] [G loss: 0.842342]\n",
      "6279 [D loss: 0.648518, acc.: 68.75%] [G loss: 0.818059]\n",
      "6280 [D loss: 0.789657, acc.: 53.12%] [G loss: 0.830518]\n",
      "6281 [D loss: 0.736920, acc.: 53.12%] [G loss: 0.778399]\n",
      "6282 [D loss: 0.672743, acc.: 50.00%] [G loss: 0.808716]\n",
      "6283 [D loss: 0.630180, acc.: 59.38%] [G loss: 0.778016]\n",
      "6284 [D loss: 0.658765, acc.: 78.12%] [G loss: 0.829574]\n",
      "6285 [D loss: 0.667652, acc.: 62.50%] [G loss: 0.788721]\n",
      "6286 [D loss: 0.632915, acc.: 65.62%] [G loss: 0.875995]\n",
      "6287 [D loss: 0.692725, acc.: 46.88%] [G loss: 0.870135]\n",
      "6288 [D loss: 0.628711, acc.: 68.75%] [G loss: 0.863351]\n",
      "6289 [D loss: 0.747032, acc.: 50.00%] [G loss: 0.800121]\n",
      "6290 [D loss: 0.746449, acc.: 62.50%] [G loss: 0.861051]\n",
      "6291 [D loss: 0.576590, acc.: 78.12%] [G loss: 0.845505]\n",
      "6292 [D loss: 0.623806, acc.: 68.75%] [G loss: 0.923407]\n",
      "6293 [D loss: 0.655705, acc.: 62.50%] [G loss: 0.896236]\n",
      "6294 [D loss: 0.619029, acc.: 62.50%] [G loss: 0.950798]\n",
      "6295 [D loss: 0.654861, acc.: 62.50%] [G loss: 0.882508]\n",
      "6296 [D loss: 0.625487, acc.: 71.88%] [G loss: 0.853308]\n",
      "6297 [D loss: 0.835114, acc.: 40.62%] [G loss: 0.816714]\n",
      "6298 [D loss: 0.637666, acc.: 71.88%] [G loss: 0.894309]\n",
      "6299 [D loss: 0.733597, acc.: 50.00%] [G loss: 0.825701]\n",
      "6300 [D loss: 0.638703, acc.: 65.62%] [G loss: 0.800839]\n",
      "6301 [D loss: 0.617953, acc.: 65.62%] [G loss: 0.911688]\n",
      "6302 [D loss: 0.549138, acc.: 81.25%] [G loss: 0.900443]\n",
      "6303 [D loss: 0.640911, acc.: 53.12%] [G loss: 0.898791]\n",
      "6304 [D loss: 0.706773, acc.: 46.88%] [G loss: 0.928417]\n",
      "6305 [D loss: 0.666099, acc.: 59.38%] [G loss: 0.971924]\n",
      "6306 [D loss: 0.704566, acc.: 56.25%] [G loss: 0.899334]\n",
      "6307 [D loss: 0.710255, acc.: 53.12%] [G loss: 0.809894]\n",
      "6308 [D loss: 0.689736, acc.: 59.38%] [G loss: 0.778862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6309 [D loss: 0.630732, acc.: 68.75%] [G loss: 0.820361]\n",
      "6310 [D loss: 0.717909, acc.: 59.38%] [G loss: 0.858056]\n",
      "6311 [D loss: 0.629360, acc.: 62.50%] [G loss: 0.869076]\n",
      "6312 [D loss: 0.722600, acc.: 40.62%] [G loss: 0.829289]\n",
      "6313 [D loss: 0.704147, acc.: 59.38%] [G loss: 0.755658]\n",
      "6314 [D loss: 0.630125, acc.: 59.38%] [G loss: 0.800806]\n",
      "6315 [D loss: 0.682820, acc.: 59.38%] [G loss: 0.873512]\n",
      "6316 [D loss: 0.612311, acc.: 75.00%] [G loss: 0.871459]\n",
      "6317 [D loss: 0.763264, acc.: 56.25%] [G loss: 0.828445]\n",
      "6318 [D loss: 0.587564, acc.: 75.00%] [G loss: 0.857834]\n",
      "6319 [D loss: 0.679402, acc.: 53.12%] [G loss: 0.929999]\n",
      "6320 [D loss: 0.660163, acc.: 62.50%] [G loss: 0.867074]\n",
      "6321 [D loss: 0.696854, acc.: 62.50%] [G loss: 0.892907]\n",
      "6322 [D loss: 0.645940, acc.: 75.00%] [G loss: 0.832226]\n",
      "6323 [D loss: 0.709117, acc.: 59.38%] [G loss: 0.868597]\n",
      "6324 [D loss: 0.659220, acc.: 56.25%] [G loss: 0.898591]\n",
      "6325 [D loss: 0.610556, acc.: 65.62%] [G loss: 0.935721]\n",
      "6326 [D loss: 0.612506, acc.: 68.75%] [G loss: 0.905959]\n",
      "6327 [D loss: 0.585871, acc.: 71.88%] [G loss: 0.871968]\n",
      "6328 [D loss: 0.618941, acc.: 75.00%] [G loss: 0.837074]\n",
      "6329 [D loss: 0.661686, acc.: 65.62%] [G loss: 0.819247]\n",
      "6330 [D loss: 0.644264, acc.: 53.12%] [G loss: 0.809508]\n",
      "6331 [D loss: 0.726322, acc.: 62.50%] [G loss: 0.897319]\n",
      "6332 [D loss: 0.704762, acc.: 56.25%] [G loss: 0.814858]\n",
      "6333 [D loss: 0.542056, acc.: 75.00%] [G loss: 0.906372]\n",
      "6334 [D loss: 0.712451, acc.: 53.12%] [G loss: 0.827017]\n",
      "6335 [D loss: 0.668973, acc.: 62.50%] [G loss: 0.922560]\n",
      "6336 [D loss: 0.700819, acc.: 59.38%] [G loss: 0.935549]\n",
      "6337 [D loss: 0.684501, acc.: 53.12%] [G loss: 0.995388]\n",
      "6338 [D loss: 0.674130, acc.: 65.62%] [G loss: 0.960661]\n",
      "6339 [D loss: 0.786398, acc.: 46.88%] [G loss: 0.795946]\n",
      "6340 [D loss: 0.753457, acc.: 53.12%] [G loss: 0.848363]\n",
      "6341 [D loss: 0.737459, acc.: 50.00%] [G loss: 0.903757]\n",
      "6342 [D loss: 0.733719, acc.: 46.88%] [G loss: 0.856791]\n",
      "6343 [D loss: 0.661814, acc.: 56.25%] [G loss: 0.886137]\n",
      "6344 [D loss: 0.732071, acc.: 56.25%] [G loss: 0.873248]\n",
      "6345 [D loss: 0.683896, acc.: 56.25%] [G loss: 0.999942]\n",
      "6346 [D loss: 0.729940, acc.: 50.00%] [G loss: 0.852576]\n",
      "6347 [D loss: 0.589370, acc.: 68.75%] [G loss: 0.992407]\n",
      "6348 [D loss: 0.695041, acc.: 53.12%] [G loss: 0.934751]\n",
      "6349 [D loss: 0.648635, acc.: 65.62%] [G loss: 0.894081]\n",
      "6350 [D loss: 0.667751, acc.: 59.38%] [G loss: 0.893727]\n",
      "6351 [D loss: 0.651014, acc.: 62.50%] [G loss: 0.820779]\n",
      "6352 [D loss: 0.562181, acc.: 78.12%] [G loss: 0.890052]\n",
      "6353 [D loss: 0.762882, acc.: 59.38%] [G loss: 0.852506]\n",
      "6354 [D loss: 0.654166, acc.: 62.50%] [G loss: 0.872297]\n",
      "6355 [D loss: 0.658454, acc.: 65.62%] [G loss: 0.884824]\n",
      "6356 [D loss: 0.559817, acc.: 78.12%] [G loss: 0.857355]\n",
      "6357 [D loss: 0.677170, acc.: 56.25%] [G loss: 0.883708]\n",
      "6358 [D loss: 0.746680, acc.: 50.00%] [G loss: 1.019623]\n",
      "6359 [D loss: 0.697435, acc.: 56.25%] [G loss: 0.957537]\n",
      "6360 [D loss: 0.671768, acc.: 65.62%] [G loss: 1.015191]\n",
      "6361 [D loss: 0.627713, acc.: 75.00%] [G loss: 0.904551]\n",
      "6362 [D loss: 0.704591, acc.: 62.50%] [G loss: 0.909251]\n",
      "6363 [D loss: 0.671326, acc.: 53.12%] [G loss: 0.904493]\n",
      "6364 [D loss: 0.656417, acc.: 62.50%] [G loss: 0.847163]\n",
      "6365 [D loss: 0.561044, acc.: 78.12%] [G loss: 0.843969]\n",
      "6366 [D loss: 0.647800, acc.: 62.50%] [G loss: 0.862429]\n",
      "6367 [D loss: 0.667187, acc.: 62.50%] [G loss: 0.951296]\n",
      "6368 [D loss: 0.666036, acc.: 53.12%] [G loss: 0.858846]\n",
      "6369 [D loss: 0.671504, acc.: 62.50%] [G loss: 0.921806]\n",
      "6370 [D loss: 0.737486, acc.: 50.00%] [G loss: 0.852263]\n",
      "6371 [D loss: 0.674234, acc.: 50.00%] [G loss: 0.808083]\n",
      "6372 [D loss: 0.715489, acc.: 62.50%] [G loss: 0.838750]\n",
      "6373 [D loss: 0.632632, acc.: 59.38%] [G loss: 0.905477]\n",
      "6374 [D loss: 0.589574, acc.: 65.62%] [G loss: 0.873145]\n",
      "6375 [D loss: 0.679989, acc.: 62.50%] [G loss: 0.962008]\n",
      "6376 [D loss: 0.587371, acc.: 65.62%] [G loss: 0.875194]\n",
      "6377 [D loss: 0.670232, acc.: 50.00%] [G loss: 0.906306]\n",
      "6378 [D loss: 0.659437, acc.: 59.38%] [G loss: 0.914209]\n",
      "6379 [D loss: 0.625361, acc.: 59.38%] [G loss: 0.913913]\n",
      "6380 [D loss: 0.633805, acc.: 65.62%] [G loss: 0.925775]\n",
      "6381 [D loss: 0.723583, acc.: 53.12%] [G loss: 0.930718]\n",
      "6382 [D loss: 0.660457, acc.: 62.50%] [G loss: 1.035850]\n",
      "6383 [D loss: 0.708540, acc.: 53.12%] [G loss: 0.872233]\n",
      "6384 [D loss: 0.652030, acc.: 62.50%] [G loss: 0.877491]\n",
      "6385 [D loss: 0.653589, acc.: 56.25%] [G loss: 0.884458]\n",
      "6386 [D loss: 0.583871, acc.: 75.00%] [G loss: 0.938277]\n",
      "6387 [D loss: 0.643775, acc.: 59.38%] [G loss: 0.828726]\n",
      "6388 [D loss: 0.627125, acc.: 65.62%] [G loss: 0.806062]\n",
      "6389 [D loss: 0.609180, acc.: 75.00%] [G loss: 0.942505]\n",
      "6390 [D loss: 0.579300, acc.: 78.12%] [G loss: 0.906560]\n",
      "6391 [D loss: 0.647207, acc.: 62.50%] [G loss: 0.894719]\n",
      "6392 [D loss: 0.657783, acc.: 62.50%] [G loss: 0.908304]\n",
      "6393 [D loss: 0.672126, acc.: 62.50%] [G loss: 0.860465]\n",
      "6394 [D loss: 0.677691, acc.: 46.88%] [G loss: 0.914167]\n",
      "6395 [D loss: 0.623719, acc.: 65.62%] [G loss: 0.979739]\n",
      "6396 [D loss: 0.737756, acc.: 46.88%] [G loss: 0.844000]\n",
      "6397 [D loss: 0.710141, acc.: 50.00%] [G loss: 0.808121]\n",
      "6398 [D loss: 0.704654, acc.: 43.75%] [G loss: 0.930654]\n",
      "6399 [D loss: 0.641354, acc.: 65.62%] [G loss: 0.864623]\n",
      "6400 [D loss: 0.635291, acc.: 62.50%] [G loss: 0.893639]\n",
      "6401 [D loss: 0.675827, acc.: 56.25%] [G loss: 0.886331]\n",
      "6402 [D loss: 0.590985, acc.: 71.88%] [G loss: 0.991258]\n",
      "6403 [D loss: 0.636866, acc.: 65.62%] [G loss: 0.919588]\n",
      "6404 [D loss: 0.632016, acc.: 65.62%] [G loss: 0.865857]\n",
      "6405 [D loss: 0.611322, acc.: 71.88%] [G loss: 0.897728]\n",
      "6406 [D loss: 0.680593, acc.: 62.50%] [G loss: 0.866939]\n",
      "6407 [D loss: 0.652011, acc.: 56.25%] [G loss: 0.935392]\n",
      "6408 [D loss: 0.610458, acc.: 71.88%] [G loss: 0.945297]\n",
      "6409 [D loss: 0.737107, acc.: 53.12%] [G loss: 0.850177]\n",
      "6410 [D loss: 0.752678, acc.: 46.88%] [G loss: 0.947421]\n",
      "6411 [D loss: 0.625522, acc.: 62.50%] [G loss: 0.914378]\n",
      "6412 [D loss: 0.713363, acc.: 53.12%] [G loss: 0.796960]\n",
      "6413 [D loss: 0.748292, acc.: 56.25%] [G loss: 0.785557]\n",
      "6414 [D loss: 0.564907, acc.: 78.12%] [G loss: 0.862770]\n",
      "6415 [D loss: 0.690430, acc.: 56.25%] [G loss: 0.813156]\n",
      "6416 [D loss: 0.722070, acc.: 46.88%] [G loss: 0.868039]\n",
      "6417 [D loss: 0.688511, acc.: 62.50%] [G loss: 0.883818]\n",
      "6418 [D loss: 0.627546, acc.: 65.62%] [G loss: 0.840679]\n",
      "6419 [D loss: 0.712426, acc.: 46.88%] [G loss: 0.763658]\n",
      "6420 [D loss: 0.683960, acc.: 53.12%] [G loss: 0.861885]\n",
      "6421 [D loss: 0.646975, acc.: 56.25%] [G loss: 0.862175]\n",
      "6422 [D loss: 0.644512, acc.: 62.50%] [G loss: 0.775856]\n",
      "6423 [D loss: 0.679235, acc.: 43.75%] [G loss: 0.812852]\n",
      "6424 [D loss: 0.648916, acc.: 62.50%] [G loss: 0.838898]\n",
      "6425 [D loss: 0.681220, acc.: 62.50%] [G loss: 0.820574]\n",
      "6426 [D loss: 0.758997, acc.: 50.00%] [G loss: 0.845666]\n",
      "6427 [D loss: 0.604701, acc.: 65.62%] [G loss: 0.796770]\n",
      "6428 [D loss: 0.642120, acc.: 65.62%] [G loss: 0.788959]\n",
      "6429 [D loss: 0.685574, acc.: 53.12%] [G loss: 0.750396]\n",
      "6430 [D loss: 0.506114, acc.: 81.25%] [G loss: 0.880869]\n",
      "6431 [D loss: 0.678199, acc.: 56.25%] [G loss: 0.951956]\n",
      "6432 [D loss: 0.655202, acc.: 59.38%] [G loss: 0.863784]\n",
      "6433 [D loss: 0.582600, acc.: 78.12%] [G loss: 0.824879]\n",
      "6434 [D loss: 0.704514, acc.: 50.00%] [G loss: 0.849897]\n",
      "6435 [D loss: 0.655293, acc.: 50.00%] [G loss: 0.798511]\n",
      "6436 [D loss: 0.598948, acc.: 62.50%] [G loss: 0.878315]\n",
      "6437 [D loss: 0.725139, acc.: 40.62%] [G loss: 0.860843]\n",
      "6438 [D loss: 0.719510, acc.: 56.25%] [G loss: 0.790260]\n",
      "6439 [D loss: 0.628322, acc.: 65.62%] [G loss: 0.854941]\n",
      "6440 [D loss: 0.737679, acc.: 43.75%] [G loss: 0.835442]\n",
      "6441 [D loss: 0.656309, acc.: 62.50%] [G loss: 0.849337]\n",
      "6442 [D loss: 0.615545, acc.: 68.75%] [G loss: 0.871060]\n",
      "6443 [D loss: 0.664705, acc.: 62.50%] [G loss: 0.838972]\n",
      "6444 [D loss: 0.697452, acc.: 56.25%] [G loss: 0.771817]\n",
      "6445 [D loss: 0.629740, acc.: 65.62%] [G loss: 0.836846]\n",
      "6446 [D loss: 0.593804, acc.: 75.00%] [G loss: 0.805741]\n",
      "6447 [D loss: 0.706996, acc.: 46.88%] [G loss: 0.887312]\n",
      "6448 [D loss: 0.682542, acc.: 65.62%] [G loss: 0.884131]\n",
      "6449 [D loss: 0.691549, acc.: 62.50%] [G loss: 0.830871]\n",
      "6450 [D loss: 0.680762, acc.: 53.12%] [G loss: 0.838858]\n",
      "6451 [D loss: 0.751437, acc.: 43.75%] [G loss: 0.906471]\n",
      "6452 [D loss: 0.711114, acc.: 50.00%] [G loss: 0.883382]\n",
      "6453 [D loss: 0.631477, acc.: 62.50%] [G loss: 0.817627]\n",
      "6454 [D loss: 0.632434, acc.: 53.12%] [G loss: 0.889331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6455 [D loss: 0.716223, acc.: 46.88%] [G loss: 0.914747]\n",
      "6456 [D loss: 0.633457, acc.: 71.88%] [G loss: 0.904431]\n",
      "6457 [D loss: 0.738705, acc.: 50.00%] [G loss: 0.930448]\n",
      "6458 [D loss: 0.739203, acc.: 43.75%] [G loss: 0.873044]\n",
      "6459 [D loss: 0.639947, acc.: 75.00%] [G loss: 0.912017]\n",
      "6460 [D loss: 0.589552, acc.: 71.88%] [G loss: 0.940897]\n",
      "6461 [D loss: 0.767756, acc.: 50.00%] [G loss: 0.967223]\n",
      "6462 [D loss: 0.671962, acc.: 53.12%] [G loss: 0.955376]\n",
      "6463 [D loss: 0.608150, acc.: 75.00%] [G loss: 0.992152]\n",
      "6464 [D loss: 0.649631, acc.: 56.25%] [G loss: 0.959650]\n",
      "6465 [D loss: 0.662032, acc.: 56.25%] [G loss: 0.906323]\n",
      "6466 [D loss: 0.659717, acc.: 68.75%] [G loss: 0.953384]\n",
      "6467 [D loss: 0.682711, acc.: 56.25%] [G loss: 0.920562]\n",
      "6468 [D loss: 0.717926, acc.: 53.12%] [G loss: 0.871713]\n",
      "6469 [D loss: 0.675765, acc.: 59.38%] [G loss: 0.816565]\n",
      "6470 [D loss: 0.647313, acc.: 56.25%] [G loss: 0.823992]\n",
      "6471 [D loss: 0.614073, acc.: 59.38%] [G loss: 0.907074]\n",
      "6472 [D loss: 0.726861, acc.: 46.88%] [G loss: 0.868783]\n",
      "6473 [D loss: 0.733629, acc.: 53.12%] [G loss: 0.859700]\n",
      "6474 [D loss: 0.617210, acc.: 68.75%] [G loss: 0.972283]\n",
      "6475 [D loss: 0.674629, acc.: 59.38%] [G loss: 0.915895]\n",
      "6476 [D loss: 0.729144, acc.: 43.75%] [G loss: 0.901519]\n",
      "6477 [D loss: 0.647947, acc.: 65.62%] [G loss: 0.943868]\n",
      "6478 [D loss: 0.663951, acc.: 59.38%] [G loss: 0.881770]\n",
      "6479 [D loss: 0.672339, acc.: 65.62%] [G loss: 0.744608]\n",
      "6480 [D loss: 0.624816, acc.: 68.75%] [G loss: 0.868236]\n",
      "6481 [D loss: 0.594690, acc.: 78.12%] [G loss: 0.853872]\n",
      "6482 [D loss: 0.707987, acc.: 59.38%] [G loss: 1.020550]\n",
      "6483 [D loss: 0.654793, acc.: 65.62%] [G loss: 0.936019]\n",
      "6484 [D loss: 0.609276, acc.: 71.88%] [G loss: 0.914400]\n",
      "6485 [D loss: 0.643229, acc.: 62.50%] [G loss: 0.917325]\n",
      "6486 [D loss: 0.702013, acc.: 56.25%] [G loss: 0.828220]\n",
      "6487 [D loss: 0.683141, acc.: 50.00%] [G loss: 0.924032]\n",
      "6488 [D loss: 0.697000, acc.: 50.00%] [G loss: 0.919675]\n",
      "6489 [D loss: 0.657874, acc.: 62.50%] [G loss: 0.946175]\n",
      "6490 [D loss: 0.581966, acc.: 68.75%] [G loss: 0.946209]\n",
      "6491 [D loss: 0.701865, acc.: 59.38%] [G loss: 0.878125]\n",
      "6492 [D loss: 0.651141, acc.: 56.25%] [G loss: 0.878229]\n",
      "6493 [D loss: 0.551171, acc.: 87.50%] [G loss: 0.878053]\n",
      "6494 [D loss: 0.684362, acc.: 62.50%] [G loss: 0.929629]\n",
      "6495 [D loss: 0.705467, acc.: 53.12%] [G loss: 0.938315]\n",
      "6496 [D loss: 0.646308, acc.: 71.88%] [G loss: 0.860910]\n",
      "6497 [D loss: 0.643439, acc.: 65.62%] [G loss: 0.832301]\n",
      "6498 [D loss: 0.612722, acc.: 65.62%] [G loss: 0.855048]\n",
      "6499 [D loss: 0.672977, acc.: 62.50%] [G loss: 0.844322]\n",
      "6500 [D loss: 0.626149, acc.: 75.00%] [G loss: 0.897087]\n",
      "6501 [D loss: 0.693830, acc.: 53.12%] [G loss: 0.863942]\n",
      "6502 [D loss: 0.710953, acc.: 50.00%] [G loss: 0.860170]\n",
      "6503 [D loss: 0.585696, acc.: 68.75%] [G loss: 0.832805]\n",
      "6504 [D loss: 0.627309, acc.: 65.62%] [G loss: 0.842818]\n",
      "6505 [D loss: 0.617781, acc.: 59.38%] [G loss: 0.885866]\n",
      "6506 [D loss: 0.605219, acc.: 68.75%] [G loss: 0.887858]\n",
      "6507 [D loss: 0.671871, acc.: 62.50%] [G loss: 0.858332]\n",
      "6508 [D loss: 0.600506, acc.: 65.62%] [G loss: 0.794521]\n",
      "6509 [D loss: 0.728895, acc.: 50.00%] [G loss: 0.830849]\n",
      "6510 [D loss: 0.681072, acc.: 43.75%] [G loss: 0.865688]\n",
      "6511 [D loss: 0.665771, acc.: 59.38%] [G loss: 0.848768]\n",
      "6512 [D loss: 0.592802, acc.: 71.88%] [G loss: 0.918388]\n",
      "6513 [D loss: 0.702722, acc.: 56.25%] [G loss: 0.849225]\n",
      "6514 [D loss: 0.631990, acc.: 65.62%] [G loss: 1.044615]\n",
      "6515 [D loss: 0.712061, acc.: 50.00%] [G loss: 0.925454]\n",
      "6516 [D loss: 0.694589, acc.: 56.25%] [G loss: 0.928682]\n",
      "6517 [D loss: 0.686613, acc.: 59.38%] [G loss: 0.810735]\n",
      "6518 [D loss: 0.596420, acc.: 68.75%] [G loss: 0.867296]\n",
      "6519 [D loss: 0.734336, acc.: 43.75%] [G loss: 0.913041]\n",
      "6520 [D loss: 0.586203, acc.: 75.00%] [G loss: 0.914022]\n",
      "6521 [D loss: 0.638404, acc.: 62.50%] [G loss: 0.931313]\n",
      "6522 [D loss: 0.599336, acc.: 68.75%] [G loss: 0.871392]\n",
      "6523 [D loss: 0.704405, acc.: 50.00%] [G loss: 0.952736]\n",
      "6524 [D loss: 0.808316, acc.: 34.38%] [G loss: 0.883670]\n",
      "6525 [D loss: 0.615559, acc.: 62.50%] [G loss: 0.852175]\n",
      "6526 [D loss: 0.660454, acc.: 59.38%] [G loss: 0.913652]\n",
      "6527 [D loss: 0.740617, acc.: 46.88%] [G loss: 0.885165]\n",
      "6528 [D loss: 0.730267, acc.: 50.00%] [G loss: 0.842388]\n",
      "6529 [D loss: 0.690340, acc.: 56.25%] [G loss: 0.924003]\n",
      "6530 [D loss: 0.684104, acc.: 56.25%] [G loss: 0.868035]\n",
      "6531 [D loss: 0.692103, acc.: 46.88%] [G loss: 0.900164]\n",
      "6532 [D loss: 0.650620, acc.: 43.75%] [G loss: 0.968278]\n",
      "6533 [D loss: 0.664496, acc.: 50.00%] [G loss: 0.890585]\n",
      "6534 [D loss: 0.712869, acc.: 46.88%] [G loss: 0.933714]\n",
      "6535 [D loss: 0.715918, acc.: 43.75%] [G loss: 0.980240]\n",
      "6536 [D loss: 0.767022, acc.: 43.75%] [G loss: 0.934562]\n",
      "6537 [D loss: 0.637673, acc.: 68.75%] [G loss: 0.906136]\n",
      "6538 [D loss: 0.712936, acc.: 46.88%] [G loss: 0.849167]\n",
      "6539 [D loss: 0.615565, acc.: 62.50%] [G loss: 0.883607]\n",
      "6540 [D loss: 0.729792, acc.: 56.25%] [G loss: 0.821894]\n",
      "6541 [D loss: 0.603621, acc.: 71.88%] [G loss: 1.049650]\n",
      "6542 [D loss: 0.645923, acc.: 75.00%] [G loss: 1.001830]\n",
      "6543 [D loss: 0.697957, acc.: 46.88%] [G loss: 0.914917]\n",
      "6544 [D loss: 0.696726, acc.: 50.00%] [G loss: 0.930577]\n",
      "6545 [D loss: 0.677475, acc.: 56.25%] [G loss: 0.893326]\n",
      "6546 [D loss: 0.650248, acc.: 71.88%] [G loss: 0.882501]\n",
      "6547 [D loss: 0.541957, acc.: 81.25%] [G loss: 0.905024]\n",
      "6548 [D loss: 0.729953, acc.: 46.88%] [G loss: 0.888306]\n",
      "6549 [D loss: 0.633871, acc.: 65.62%] [G loss: 0.869334]\n",
      "6550 [D loss: 0.699971, acc.: 46.88%] [G loss: 0.887696]\n",
      "6551 [D loss: 0.622993, acc.: 59.38%] [G loss: 0.858563]\n",
      "6552 [D loss: 0.588349, acc.: 68.75%] [G loss: 0.847730]\n",
      "6553 [D loss: 0.773831, acc.: 50.00%] [G loss: 0.871791]\n",
      "6554 [D loss: 0.597672, acc.: 65.62%] [G loss: 0.822159]\n",
      "6555 [D loss: 0.614884, acc.: 75.00%] [G loss: 0.839720]\n",
      "6556 [D loss: 0.521329, acc.: 81.25%] [G loss: 0.865114]\n",
      "6557 [D loss: 0.670761, acc.: 56.25%] [G loss: 0.895190]\n",
      "6558 [D loss: 0.645164, acc.: 65.62%] [G loss: 0.925961]\n",
      "6559 [D loss: 0.750127, acc.: 56.25%] [G loss: 0.962720]\n",
      "6560 [D loss: 0.714672, acc.: 50.00%] [G loss: 0.983212]\n",
      "6561 [D loss: 0.642066, acc.: 59.38%] [G loss: 0.954334]\n",
      "6562 [D loss: 0.791673, acc.: 46.88%] [G loss: 0.918898]\n",
      "6563 [D loss: 0.619372, acc.: 75.00%] [G loss: 0.962362]\n",
      "6564 [D loss: 0.691727, acc.: 53.12%] [G loss: 0.898994]\n",
      "6565 [D loss: 0.780530, acc.: 50.00%] [G loss: 0.890432]\n",
      "6566 [D loss: 0.641724, acc.: 65.62%] [G loss: 0.997772]\n",
      "6567 [D loss: 0.644049, acc.: 62.50%] [G loss: 0.884359]\n",
      "6568 [D loss: 0.710556, acc.: 59.38%] [G loss: 0.864522]\n",
      "6569 [D loss: 0.609035, acc.: 71.88%] [G loss: 0.853482]\n",
      "6570 [D loss: 0.575659, acc.: 78.12%] [G loss: 1.004907]\n",
      "6571 [D loss: 0.748796, acc.: 53.12%] [G loss: 0.850569]\n",
      "6572 [D loss: 0.637176, acc.: 71.88%] [G loss: 0.876962]\n",
      "6573 [D loss: 0.668597, acc.: 56.25%] [G loss: 0.916210]\n",
      "6574 [D loss: 0.584080, acc.: 75.00%] [G loss: 0.944491]\n",
      "6575 [D loss: 0.623986, acc.: 59.38%] [G loss: 0.937123]\n",
      "6576 [D loss: 0.600226, acc.: 71.88%] [G loss: 0.909001]\n",
      "6577 [D loss: 0.685003, acc.: 59.38%] [G loss: 0.923792]\n",
      "6578 [D loss: 0.676974, acc.: 65.62%] [G loss: 0.919355]\n",
      "6579 [D loss: 0.753658, acc.: 43.75%] [G loss: 0.931829]\n",
      "6580 [D loss: 0.653711, acc.: 50.00%] [G loss: 0.919331]\n",
      "6581 [D loss: 0.764963, acc.: 50.00%] [G loss: 0.923987]\n",
      "6582 [D loss: 0.673310, acc.: 59.38%] [G loss: 0.880593]\n",
      "6583 [D loss: 0.610986, acc.: 65.62%] [G loss: 0.954081]\n",
      "6584 [D loss: 0.631295, acc.: 59.38%] [G loss: 0.863207]\n",
      "6585 [D loss: 0.712605, acc.: 53.12%] [G loss: 0.893821]\n",
      "6586 [D loss: 0.698656, acc.: 56.25%] [G loss: 0.914199]\n",
      "6587 [D loss: 0.645481, acc.: 62.50%] [G loss: 0.847679]\n",
      "6588 [D loss: 0.772686, acc.: 62.50%] [G loss: 0.837037]\n",
      "6589 [D loss: 0.706972, acc.: 65.62%] [G loss: 0.868955]\n",
      "6590 [D loss: 0.727864, acc.: 43.75%] [G loss: 0.839796]\n",
      "6591 [D loss: 0.690465, acc.: 59.38%] [G loss: 0.896189]\n",
      "6592 [D loss: 0.601893, acc.: 62.50%] [G loss: 0.864046]\n",
      "6593 [D loss: 0.588007, acc.: 65.62%] [G loss: 0.829557]\n",
      "6594 [D loss: 0.598563, acc.: 59.38%] [G loss: 0.856137]\n",
      "6595 [D loss: 0.593455, acc.: 65.62%] [G loss: 0.861655]\n",
      "6596 [D loss: 0.800926, acc.: 50.00%] [G loss: 0.931012]\n",
      "6597 [D loss: 0.661103, acc.: 59.38%] [G loss: 0.924563]\n",
      "6598 [D loss: 0.648505, acc.: 68.75%] [G loss: 0.949088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599 [D loss: 0.625408, acc.: 65.62%] [G loss: 0.929345]\n",
      "6600 [D loss: 0.669635, acc.: 62.50%] [G loss: 0.958271]\n",
      "6601 [D loss: 0.676480, acc.: 56.25%] [G loss: 0.894835]\n",
      "6602 [D loss: 0.614641, acc.: 65.62%] [G loss: 0.923518]\n",
      "6603 [D loss: 0.561514, acc.: 78.12%] [G loss: 0.943594]\n",
      "6604 [D loss: 0.623200, acc.: 71.88%] [G loss: 0.860697]\n",
      "6605 [D loss: 0.773914, acc.: 56.25%] [G loss: 0.849221]\n",
      "6606 [D loss: 0.566835, acc.: 71.88%] [G loss: 0.919044]\n",
      "6607 [D loss: 0.676008, acc.: 75.00%] [G loss: 0.903519]\n",
      "6608 [D loss: 0.570109, acc.: 71.88%] [G loss: 0.982997]\n",
      "6609 [D loss: 0.665044, acc.: 56.25%] [G loss: 0.978196]\n",
      "6610 [D loss: 0.602304, acc.: 59.38%] [G loss: 0.947766]\n",
      "6611 [D loss: 0.636447, acc.: 65.62%] [G loss: 0.890238]\n",
      "6612 [D loss: 0.646606, acc.: 62.50%] [G loss: 0.770079]\n",
      "6613 [D loss: 0.697295, acc.: 59.38%] [G loss: 0.872208]\n",
      "6614 [D loss: 0.615469, acc.: 75.00%] [G loss: 0.883708]\n",
      "6615 [D loss: 0.632082, acc.: 68.75%] [G loss: 0.849336]\n",
      "6616 [D loss: 0.649729, acc.: 56.25%] [G loss: 0.928505]\n",
      "6617 [D loss: 0.685049, acc.: 56.25%] [G loss: 0.841377]\n",
      "6618 [D loss: 0.639227, acc.: 65.62%] [G loss: 0.920159]\n",
      "6619 [D loss: 0.634285, acc.: 65.62%] [G loss: 0.893526]\n",
      "6620 [D loss: 0.752575, acc.: 46.88%] [G loss: 0.879594]\n",
      "6621 [D loss: 0.683216, acc.: 65.62%] [G loss: 0.986186]\n",
      "6622 [D loss: 0.703552, acc.: 59.38%] [G loss: 0.879751]\n",
      "6623 [D loss: 0.634298, acc.: 68.75%] [G loss: 0.853659]\n",
      "6624 [D loss: 0.645612, acc.: 65.62%] [G loss: 0.892325]\n",
      "6625 [D loss: 0.690745, acc.: 59.38%] [G loss: 0.861930]\n",
      "6626 [D loss: 0.715031, acc.: 50.00%] [G loss: 0.877222]\n",
      "6627 [D loss: 0.630984, acc.: 59.38%] [G loss: 0.900154]\n",
      "6628 [D loss: 0.610252, acc.: 68.75%] [G loss: 0.969712]\n",
      "6629 [D loss: 0.619138, acc.: 65.62%] [G loss: 0.980226]\n",
      "6630 [D loss: 0.719363, acc.: 43.75%] [G loss: 0.942650]\n",
      "6631 [D loss: 0.615946, acc.: 65.62%] [G loss: 0.906985]\n",
      "6632 [D loss: 0.674184, acc.: 59.38%] [G loss: 0.911182]\n",
      "6633 [D loss: 0.525518, acc.: 78.12%] [G loss: 0.959305]\n",
      "6634 [D loss: 0.694232, acc.: 59.38%] [G loss: 0.905545]\n",
      "6635 [D loss: 0.668349, acc.: 56.25%] [G loss: 0.883785]\n",
      "6636 [D loss: 0.638685, acc.: 65.62%] [G loss: 0.931962]\n",
      "6637 [D loss: 0.652722, acc.: 71.88%] [G loss: 0.891805]\n",
      "6638 [D loss: 0.553375, acc.: 68.75%] [G loss: 0.895115]\n",
      "6639 [D loss: 0.696239, acc.: 59.38%] [G loss: 0.906476]\n",
      "6640 [D loss: 0.797456, acc.: 43.75%] [G loss: 0.892667]\n",
      "6641 [D loss: 0.645539, acc.: 62.50%] [G loss: 0.995876]\n",
      "6642 [D loss: 0.664526, acc.: 65.62%] [G loss: 0.940336]\n",
      "6643 [D loss: 0.761396, acc.: 50.00%] [G loss: 0.871210]\n",
      "6644 [D loss: 0.665816, acc.: 59.38%] [G loss: 0.850875]\n",
      "6645 [D loss: 0.672217, acc.: 53.12%] [G loss: 0.963555]\n",
      "6646 [D loss: 0.702860, acc.: 53.12%] [G loss: 0.956564]\n",
      "6647 [D loss: 0.689968, acc.: 53.12%] [G loss: 0.884729]\n",
      "6648 [D loss: 0.702325, acc.: 56.25%] [G loss: 0.938456]\n",
      "6649 [D loss: 0.657894, acc.: 59.38%] [G loss: 0.987350]\n",
      "6650 [D loss: 0.697525, acc.: 50.00%] [G loss: 0.888529]\n",
      "6651 [D loss: 0.651904, acc.: 62.50%] [G loss: 0.793167]\n",
      "6652 [D loss: 0.663692, acc.: 59.38%] [G loss: 0.745555]\n",
      "6653 [D loss: 0.645121, acc.: 62.50%] [G loss: 0.899652]\n",
      "6654 [D loss: 0.673110, acc.: 59.38%] [G loss: 0.899369]\n",
      "6655 [D loss: 0.649624, acc.: 68.75%] [G loss: 0.836698]\n",
      "6656 [D loss: 0.648865, acc.: 62.50%] [G loss: 0.897256]\n",
      "6657 [D loss: 0.727453, acc.: 53.12%] [G loss: 0.804668]\n",
      "6658 [D loss: 0.650380, acc.: 62.50%] [G loss: 0.901641]\n",
      "6659 [D loss: 0.652121, acc.: 56.25%] [G loss: 0.858168]\n",
      "6660 [D loss: 0.694028, acc.: 53.12%] [G loss: 0.830494]\n",
      "6661 [D loss: 0.709700, acc.: 53.12%] [G loss: 0.948637]\n",
      "6662 [D loss: 0.706590, acc.: 59.38%] [G loss: 0.913739]\n",
      "6663 [D loss: 0.653093, acc.: 59.38%] [G loss: 0.877473]\n",
      "6664 [D loss: 0.582020, acc.: 71.88%] [G loss: 0.986023]\n",
      "6665 [D loss: 0.635870, acc.: 53.12%] [G loss: 0.853089]\n",
      "6666 [D loss: 0.755609, acc.: 50.00%] [G loss: 0.903606]\n",
      "6667 [D loss: 0.676427, acc.: 59.38%] [G loss: 0.865198]\n",
      "6668 [D loss: 0.714641, acc.: 56.25%] [G loss: 0.845929]\n",
      "6669 [D loss: 0.737137, acc.: 53.12%] [G loss: 0.992271]\n",
      "6670 [D loss: 0.637854, acc.: 65.62%] [G loss: 0.865252]\n",
      "6671 [D loss: 0.707268, acc.: 50.00%] [G loss: 0.888907]\n",
      "6672 [D loss: 0.624842, acc.: 71.88%] [G loss: 0.908539]\n",
      "6673 [D loss: 0.651176, acc.: 62.50%] [G loss: 0.881132]\n",
      "6674 [D loss: 0.727309, acc.: 50.00%] [G loss: 0.854433]\n",
      "6675 [D loss: 0.595585, acc.: 65.62%] [G loss: 0.925121]\n",
      "6676 [D loss: 0.620378, acc.: 68.75%] [G loss: 0.905214]\n",
      "6677 [D loss: 0.717801, acc.: 50.00%] [G loss: 0.836926]\n",
      "6678 [D loss: 0.654951, acc.: 56.25%] [G loss: 0.938210]\n",
      "6679 [D loss: 0.630975, acc.: 59.38%] [G loss: 1.008521]\n",
      "6680 [D loss: 0.710298, acc.: 46.88%] [G loss: 0.898334]\n",
      "6681 [D loss: 0.651768, acc.: 62.50%] [G loss: 0.926966]\n",
      "6682 [D loss: 0.675430, acc.: 53.12%] [G loss: 0.910957]\n",
      "6683 [D loss: 0.635696, acc.: 62.50%] [G loss: 0.934171]\n",
      "6684 [D loss: 0.685214, acc.: 53.12%] [G loss: 0.925667]\n",
      "6685 [D loss: 0.696368, acc.: 56.25%] [G loss: 0.823069]\n",
      "6686 [D loss: 0.619400, acc.: 56.25%] [G loss: 0.843629]\n",
      "6687 [D loss: 0.660553, acc.: 62.50%] [G loss: 0.841620]\n",
      "6688 [D loss: 0.680631, acc.: 53.12%] [G loss: 0.870175]\n",
      "6689 [D loss: 0.607961, acc.: 59.38%] [G loss: 0.890007]\n",
      "6690 [D loss: 0.734741, acc.: 50.00%] [G loss: 0.878129]\n",
      "6691 [D loss: 0.663019, acc.: 59.38%] [G loss: 0.883110]\n",
      "6692 [D loss: 0.606517, acc.: 68.75%] [G loss: 0.893536]\n",
      "6693 [D loss: 0.785932, acc.: 34.38%] [G loss: 0.933856]\n",
      "6694 [D loss: 0.663940, acc.: 62.50%] [G loss: 0.941554]\n",
      "6695 [D loss: 0.664632, acc.: 53.12%] [G loss: 0.918425]\n",
      "6696 [D loss: 0.727610, acc.: 50.00%] [G loss: 0.900618]\n",
      "6697 [D loss: 0.717516, acc.: 53.12%] [G loss: 0.829056]\n",
      "6698 [D loss: 0.708486, acc.: 62.50%] [G loss: 0.833522]\n",
      "6699 [D loss: 0.647248, acc.: 62.50%] [G loss: 0.871778]\n",
      "6700 [D loss: 0.695843, acc.: 56.25%] [G loss: 0.906213]\n",
      "6701 [D loss: 0.647600, acc.: 56.25%] [G loss: 0.866037]\n",
      "6702 [D loss: 0.646346, acc.: 59.38%] [G loss: 0.897262]\n",
      "6703 [D loss: 0.589234, acc.: 71.88%] [G loss: 0.846132]\n",
      "6704 [D loss: 0.747206, acc.: 46.88%] [G loss: 0.883314]\n",
      "6705 [D loss: 0.698017, acc.: 50.00%] [G loss: 0.911043]\n",
      "6706 [D loss: 0.651455, acc.: 59.38%] [G loss: 0.898598]\n",
      "6707 [D loss: 0.721894, acc.: 40.62%] [G loss: 0.820888]\n",
      "6708 [D loss: 0.623508, acc.: 65.62%] [G loss: 0.838978]\n",
      "6709 [D loss: 0.732229, acc.: 50.00%] [G loss: 1.000659]\n",
      "6710 [D loss: 0.701268, acc.: 50.00%] [G loss: 0.904809]\n",
      "6711 [D loss: 0.705273, acc.: 50.00%] [G loss: 0.837037]\n",
      "6712 [D loss: 0.627010, acc.: 71.88%] [G loss: 0.831732]\n",
      "6713 [D loss: 0.625095, acc.: 65.62%] [G loss: 0.960682]\n",
      "6714 [D loss: 0.616922, acc.: 62.50%] [G loss: 0.846889]\n",
      "6715 [D loss: 0.665237, acc.: 68.75%] [G loss: 0.894492]\n",
      "6716 [D loss: 0.668978, acc.: 50.00%] [G loss: 0.826623]\n",
      "6717 [D loss: 0.682382, acc.: 53.12%] [G loss: 0.940156]\n",
      "6718 [D loss: 0.643067, acc.: 62.50%] [G loss: 0.930512]\n",
      "6719 [D loss: 0.605383, acc.: 62.50%] [G loss: 0.924750]\n",
      "6720 [D loss: 0.667408, acc.: 62.50%] [G loss: 0.939832]\n",
      "6721 [D loss: 0.704366, acc.: 50.00%] [G loss: 0.811389]\n",
      "6722 [D loss: 0.638498, acc.: 62.50%] [G loss: 0.828209]\n",
      "6723 [D loss: 0.710972, acc.: 43.75%] [G loss: 0.839952]\n",
      "6724 [D loss: 0.678943, acc.: 53.12%] [G loss: 0.823876]\n",
      "6725 [D loss: 0.614788, acc.: 68.75%] [G loss: 0.784897]\n",
      "6726 [D loss: 0.702896, acc.: 56.25%] [G loss: 0.806766]\n",
      "6727 [D loss: 0.641472, acc.: 62.50%] [G loss: 0.854941]\n",
      "6728 [D loss: 0.686911, acc.: 56.25%] [G loss: 0.822663]\n",
      "6729 [D loss: 0.630874, acc.: 53.12%] [G loss: 0.798220]\n",
      "6730 [D loss: 0.628826, acc.: 59.38%] [G loss: 0.807427]\n",
      "6731 [D loss: 0.627232, acc.: 71.88%] [G loss: 0.792230]\n",
      "6732 [D loss: 0.537590, acc.: 87.50%] [G loss: 0.816433]\n",
      "6733 [D loss: 0.688439, acc.: 59.38%] [G loss: 0.809117]\n",
      "6734 [D loss: 0.623931, acc.: 59.38%] [G loss: 0.819271]\n",
      "6735 [D loss: 0.856088, acc.: 31.25%] [G loss: 0.829266]\n",
      "6736 [D loss: 0.600030, acc.: 68.75%] [G loss: 0.861517]\n",
      "6737 [D loss: 0.584107, acc.: 75.00%] [G loss: 0.895897]\n",
      "6738 [D loss: 0.593857, acc.: 65.62%] [G loss: 0.851090]\n",
      "6739 [D loss: 0.748680, acc.: 40.62%] [G loss: 0.820803]\n",
      "6740 [D loss: 0.610501, acc.: 68.75%] [G loss: 0.863511]\n",
      "6741 [D loss: 0.662768, acc.: 56.25%] [G loss: 0.819928]\n",
      "6742 [D loss: 0.655574, acc.: 65.62%] [G loss: 0.861543]\n",
      "6743 [D loss: 0.586613, acc.: 68.75%] [G loss: 0.817484]\n",
      "6744 [D loss: 0.694379, acc.: 46.88%] [G loss: 0.801629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6745 [D loss: 0.615769, acc.: 71.88%] [G loss: 0.816476]\n",
      "6746 [D loss: 0.644081, acc.: 53.12%] [G loss: 0.817443]\n",
      "6747 [D loss: 0.713517, acc.: 65.62%] [G loss: 0.864708]\n",
      "6748 [D loss: 0.618036, acc.: 59.38%] [G loss: 0.860944]\n",
      "6749 [D loss: 0.706297, acc.: 53.12%] [G loss: 0.874194]\n",
      "6750 [D loss: 0.682858, acc.: 53.12%] [G loss: 0.799939]\n",
      "6751 [D loss: 0.536599, acc.: 81.25%] [G loss: 0.907063]\n",
      "6752 [D loss: 0.592846, acc.: 75.00%] [G loss: 0.778083]\n",
      "6753 [D loss: 0.594723, acc.: 75.00%] [G loss: 0.873950]\n",
      "6754 [D loss: 0.650273, acc.: 65.62%] [G loss: 0.852939]\n",
      "6755 [D loss: 0.650270, acc.: 46.88%] [G loss: 0.951535]\n",
      "6756 [D loss: 0.646115, acc.: 68.75%] [G loss: 0.867699]\n",
      "6757 [D loss: 0.774080, acc.: 43.75%] [G loss: 0.867074]\n",
      "6758 [D loss: 0.743486, acc.: 50.00%] [G loss: 0.903491]\n",
      "6759 [D loss: 0.700210, acc.: 65.62%] [G loss: 0.876732]\n",
      "6760 [D loss: 0.716900, acc.: 50.00%] [G loss: 0.757307]\n",
      "6761 [D loss: 0.648315, acc.: 71.88%] [G loss: 0.805480]\n",
      "6762 [D loss: 0.686734, acc.: 53.12%] [G loss: 0.843234]\n",
      "6763 [D loss: 0.648979, acc.: 53.12%] [G loss: 0.910428]\n",
      "6764 [D loss: 0.659817, acc.: 65.62%] [G loss: 0.875541]\n",
      "6765 [D loss: 0.660682, acc.: 62.50%] [G loss: 0.877882]\n",
      "6766 [D loss: 0.630598, acc.: 68.75%] [G loss: 0.935145]\n",
      "6767 [D loss: 0.643873, acc.: 71.88%] [G loss: 0.868256]\n",
      "6768 [D loss: 0.678927, acc.: 56.25%] [G loss: 0.796769]\n",
      "6769 [D loss: 0.684443, acc.: 59.38%] [G loss: 0.839334]\n",
      "6770 [D loss: 0.636698, acc.: 62.50%] [G loss: 0.910049]\n",
      "6771 [D loss: 0.691193, acc.: 59.38%] [G loss: 0.966583]\n",
      "6772 [D loss: 0.632666, acc.: 68.75%] [G loss: 0.867173]\n",
      "6773 [D loss: 0.695032, acc.: 56.25%] [G loss: 0.919923]\n",
      "6774 [D loss: 0.650333, acc.: 71.88%] [G loss: 0.974720]\n",
      "6775 [D loss: 0.678239, acc.: 50.00%] [G loss: 0.952007]\n",
      "6776 [D loss: 0.628576, acc.: 62.50%] [G loss: 0.905322]\n",
      "6777 [D loss: 0.587700, acc.: 71.88%] [G loss: 0.987969]\n",
      "6778 [D loss: 0.587072, acc.: 78.12%] [G loss: 0.922834]\n",
      "6779 [D loss: 0.682348, acc.: 68.75%] [G loss: 0.932652]\n",
      "6780 [D loss: 0.750712, acc.: 46.88%] [G loss: 0.926434]\n",
      "6781 [D loss: 0.648382, acc.: 62.50%] [G loss: 0.932133]\n",
      "6782 [D loss: 0.569243, acc.: 75.00%] [G loss: 1.007366]\n",
      "6783 [D loss: 0.768120, acc.: 50.00%] [G loss: 0.985901]\n",
      "6784 [D loss: 0.655802, acc.: 65.62%] [G loss: 0.975849]\n",
      "6785 [D loss: 0.688913, acc.: 53.12%] [G loss: 0.825678]\n",
      "6786 [D loss: 0.696530, acc.: 50.00%] [G loss: 0.872570]\n",
      "6787 [D loss: 0.684237, acc.: 53.12%] [G loss: 0.939454]\n",
      "6788 [D loss: 0.721026, acc.: 50.00%] [G loss: 0.885592]\n",
      "6789 [D loss: 0.604960, acc.: 62.50%] [G loss: 0.931675]\n",
      "6790 [D loss: 0.662869, acc.: 62.50%] [G loss: 0.926259]\n",
      "6791 [D loss: 0.667361, acc.: 56.25%] [G loss: 0.885173]\n",
      "6792 [D loss: 0.650394, acc.: 62.50%] [G loss: 0.862755]\n",
      "6793 [D loss: 0.717095, acc.: 46.88%] [G loss: 0.803488]\n",
      "6794 [D loss: 0.728156, acc.: 53.12%] [G loss: 0.879582]\n",
      "6795 [D loss: 0.661063, acc.: 53.12%] [G loss: 0.843443]\n",
      "6796 [D loss: 0.668169, acc.: 65.62%] [G loss: 0.884491]\n",
      "6797 [D loss: 0.639170, acc.: 59.38%] [G loss: 0.903954]\n",
      "6798 [D loss: 0.713879, acc.: 50.00%] [G loss: 0.925056]\n",
      "6799 [D loss: 0.702707, acc.: 53.12%] [G loss: 0.867664]\n",
      "6800 [D loss: 0.606824, acc.: 71.88%] [G loss: 0.817124]\n",
      "6801 [D loss: 0.651325, acc.: 75.00%] [G loss: 0.829730]\n",
      "6802 [D loss: 0.653485, acc.: 62.50%] [G loss: 0.843938]\n",
      "6803 [D loss: 0.706864, acc.: 50.00%] [G loss: 0.875441]\n",
      "6804 [D loss: 0.680550, acc.: 68.75%] [G loss: 0.838738]\n",
      "6805 [D loss: 0.644123, acc.: 59.38%] [G loss: 0.941605]\n",
      "6806 [D loss: 0.657672, acc.: 59.38%] [G loss: 0.898057]\n",
      "6807 [D loss: 0.653381, acc.: 59.38%] [G loss: 0.876680]\n",
      "6808 [D loss: 0.592282, acc.: 68.75%] [G loss: 0.920356]\n",
      "6809 [D loss: 0.593386, acc.: 68.75%] [G loss: 0.838943]\n",
      "6810 [D loss: 0.695391, acc.: 50.00%] [G loss: 0.851710]\n",
      "6811 [D loss: 0.713767, acc.: 59.38%] [G loss: 0.855645]\n",
      "6812 [D loss: 0.704394, acc.: 59.38%] [G loss: 0.874650]\n",
      "6813 [D loss: 0.635667, acc.: 65.62%] [G loss: 0.927476]\n",
      "6814 [D loss: 0.631199, acc.: 65.62%] [G loss: 0.946013]\n",
      "6815 [D loss: 0.707217, acc.: 56.25%] [G loss: 0.876777]\n",
      "6816 [D loss: 0.644829, acc.: 62.50%] [G loss: 0.819134]\n",
      "6817 [D loss: 0.684460, acc.: 46.88%] [G loss: 0.860222]\n",
      "6818 [D loss: 0.630185, acc.: 65.62%] [G loss: 0.906175]\n",
      "6819 [D loss: 0.696795, acc.: 50.00%] [G loss: 0.929757]\n",
      "6820 [D loss: 0.675120, acc.: 62.50%] [G loss: 0.868454]\n",
      "6821 [D loss: 0.649909, acc.: 62.50%] [G loss: 0.931291]\n",
      "6822 [D loss: 0.661811, acc.: 56.25%] [G loss: 0.901304]\n",
      "6823 [D loss: 0.642452, acc.: 62.50%] [G loss: 0.856684]\n",
      "6824 [D loss: 0.648122, acc.: 65.62%] [G loss: 0.866749]\n",
      "6825 [D loss: 0.621777, acc.: 62.50%] [G loss: 0.850260]\n",
      "6826 [D loss: 0.638183, acc.: 62.50%] [G loss: 0.874010]\n",
      "6827 [D loss: 0.670012, acc.: 53.12%] [G loss: 0.873268]\n",
      "6828 [D loss: 0.564857, acc.: 78.12%] [G loss: 0.926677]\n",
      "6829 [D loss: 0.687132, acc.: 59.38%] [G loss: 0.950735]\n",
      "6830 [D loss: 0.768217, acc.: 50.00%] [G loss: 0.915388]\n",
      "6831 [D loss: 0.653093, acc.: 62.50%] [G loss: 0.956970]\n",
      "6832 [D loss: 0.624610, acc.: 81.25%] [G loss: 0.863540]\n",
      "6833 [D loss: 0.642420, acc.: 53.12%] [G loss: 0.821995]\n",
      "6834 [D loss: 0.671297, acc.: 56.25%] [G loss: 0.946011]\n",
      "6835 [D loss: 0.598008, acc.: 75.00%] [G loss: 0.796736]\n",
      "6836 [D loss: 0.744799, acc.: 53.12%] [G loss: 0.861944]\n",
      "6837 [D loss: 0.706378, acc.: 59.38%] [G loss: 0.866886]\n",
      "6838 [D loss: 0.574703, acc.: 78.12%] [G loss: 0.939851]\n",
      "6839 [D loss: 0.630719, acc.: 59.38%] [G loss: 0.887344]\n",
      "6840 [D loss: 0.635512, acc.: 65.62%] [G loss: 0.801012]\n",
      "6841 [D loss: 0.564364, acc.: 75.00%] [G loss: 0.817051]\n",
      "6842 [D loss: 0.708455, acc.: 40.62%] [G loss: 0.804045]\n",
      "6843 [D loss: 0.659349, acc.: 65.62%] [G loss: 0.836780]\n",
      "6844 [D loss: 0.704329, acc.: 62.50%] [G loss: 0.928521]\n",
      "6845 [D loss: 0.582045, acc.: 71.88%] [G loss: 0.897156]\n",
      "6846 [D loss: 0.650890, acc.: 62.50%] [G loss: 0.870871]\n",
      "6847 [D loss: 0.693128, acc.: 50.00%] [G loss: 0.821729]\n",
      "6848 [D loss: 0.614367, acc.: 59.38%] [G loss: 0.902823]\n",
      "6849 [D loss: 0.561356, acc.: 81.25%] [G loss: 0.829026]\n",
      "6850 [D loss: 0.759056, acc.: 46.88%] [G loss: 0.924495]\n",
      "6851 [D loss: 0.685637, acc.: 56.25%] [G loss: 0.919620]\n",
      "6852 [D loss: 0.596380, acc.: 75.00%] [G loss: 0.970344]\n",
      "6853 [D loss: 0.660153, acc.: 62.50%] [G loss: 0.960557]\n",
      "6854 [D loss: 0.615729, acc.: 71.88%] [G loss: 0.911816]\n",
      "6855 [D loss: 0.598518, acc.: 68.75%] [G loss: 0.920736]\n",
      "6856 [D loss: 0.709502, acc.: 56.25%] [G loss: 0.913857]\n",
      "6857 [D loss: 0.737014, acc.: 46.88%] [G loss: 0.932358]\n",
      "6858 [D loss: 0.669841, acc.: 59.38%] [G loss: 0.969159]\n",
      "6859 [D loss: 0.627165, acc.: 65.62%] [G loss: 0.889458]\n",
      "6860 [D loss: 0.717414, acc.: 56.25%] [G loss: 0.850318]\n",
      "6861 [D loss: 0.662499, acc.: 56.25%] [G loss: 0.821208]\n",
      "6862 [D loss: 0.577624, acc.: 65.62%] [G loss: 0.860458]\n",
      "6863 [D loss: 0.634349, acc.: 65.62%] [G loss: 0.862560]\n",
      "6864 [D loss: 0.685669, acc.: 56.25%] [G loss: 0.872468]\n",
      "6865 [D loss: 0.627835, acc.: 62.50%] [G loss: 0.869915]\n",
      "6866 [D loss: 0.648947, acc.: 56.25%] [G loss: 0.898645]\n",
      "6867 [D loss: 0.624513, acc.: 65.62%] [G loss: 0.949339]\n",
      "6868 [D loss: 0.576343, acc.: 75.00%] [G loss: 0.873358]\n",
      "6869 [D loss: 0.780405, acc.: 46.88%] [G loss: 0.957367]\n",
      "6870 [D loss: 0.600444, acc.: 71.88%] [G loss: 0.882630]\n",
      "6871 [D loss: 0.769663, acc.: 46.88%] [G loss: 0.866574]\n",
      "6872 [D loss: 0.700070, acc.: 53.12%] [G loss: 0.883317]\n",
      "6873 [D loss: 0.731024, acc.: 43.75%] [G loss: 0.891512]\n",
      "6874 [D loss: 0.689452, acc.: 53.12%] [G loss: 0.805353]\n",
      "6875 [D loss: 0.658899, acc.: 62.50%] [G loss: 0.842224]\n",
      "6876 [D loss: 0.779133, acc.: 59.38%] [G loss: 0.828755]\n",
      "6877 [D loss: 0.644591, acc.: 62.50%] [G loss: 0.845252]\n",
      "6878 [D loss: 0.636837, acc.: 59.38%] [G loss: 0.922012]\n",
      "6879 [D loss: 0.574293, acc.: 65.62%] [G loss: 0.903562]\n",
      "6880 [D loss: 0.712952, acc.: 50.00%] [G loss: 0.837896]\n",
      "6881 [D loss: 0.757665, acc.: 46.88%] [G loss: 0.839303]\n",
      "6882 [D loss: 0.710473, acc.: 50.00%] [G loss: 0.927932]\n",
      "6883 [D loss: 0.669327, acc.: 56.25%] [G loss: 0.793005]\n",
      "6884 [D loss: 0.659235, acc.: 62.50%] [G loss: 0.932492]\n",
      "6885 [D loss: 0.654868, acc.: 68.75%] [G loss: 0.871902]\n",
      "6886 [D loss: 0.621553, acc.: 65.62%] [G loss: 0.854876]\n",
      "6887 [D loss: 0.697606, acc.: 53.12%] [G loss: 0.796288]\n",
      "6888 [D loss: 0.688502, acc.: 62.50%] [G loss: 0.929127]\n",
      "6889 [D loss: 0.769610, acc.: 40.62%] [G loss: 0.804095]\n",
      "6890 [D loss: 0.635502, acc.: 71.88%] [G loss: 0.787948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6891 [D loss: 0.494161, acc.: 84.38%] [G loss: 0.844207]\n",
      "6892 [D loss: 0.719211, acc.: 50.00%] [G loss: 0.865681]\n",
      "6893 [D loss: 0.595722, acc.: 75.00%] [G loss: 0.877146]\n",
      "6894 [D loss: 0.741360, acc.: 46.88%] [G loss: 0.858494]\n",
      "6895 [D loss: 0.593583, acc.: 71.88%] [G loss: 0.860294]\n",
      "6896 [D loss: 0.681674, acc.: 62.50%] [G loss: 0.866651]\n",
      "6897 [D loss: 0.725295, acc.: 53.12%] [G loss: 0.853654]\n",
      "6898 [D loss: 0.659020, acc.: 62.50%] [G loss: 0.878923]\n",
      "6899 [D loss: 0.683343, acc.: 53.12%] [G loss: 0.903575]\n",
      "6900 [D loss: 0.597604, acc.: 65.62%] [G loss: 0.860078]\n",
      "6901 [D loss: 0.744563, acc.: 56.25%] [G loss: 0.880053]\n",
      "6902 [D loss: 0.661555, acc.: 59.38%] [G loss: 0.929430]\n",
      "6903 [D loss: 0.593669, acc.: 59.38%] [G loss: 0.862258]\n",
      "6904 [D loss: 0.606065, acc.: 65.62%] [G loss: 0.883472]\n",
      "6905 [D loss: 0.757128, acc.: 43.75%] [G loss: 0.922291]\n",
      "6906 [D loss: 0.620580, acc.: 68.75%] [G loss: 0.910674]\n",
      "6907 [D loss: 0.670009, acc.: 59.38%] [G loss: 0.868757]\n",
      "6908 [D loss: 0.770776, acc.: 37.50%] [G loss: 0.893054]\n",
      "6909 [D loss: 0.787679, acc.: 40.62%] [G loss: 0.868548]\n",
      "6910 [D loss: 0.590392, acc.: 65.62%] [G loss: 0.909804]\n",
      "6911 [D loss: 0.719114, acc.: 56.25%] [G loss: 0.942711]\n",
      "6912 [D loss: 0.638396, acc.: 62.50%] [G loss: 0.902713]\n",
      "6913 [D loss: 0.641774, acc.: 59.38%] [G loss: 0.945000]\n",
      "6914 [D loss: 0.706830, acc.: 50.00%] [G loss: 0.890943]\n",
      "6915 [D loss: 0.666094, acc.: 59.38%] [G loss: 1.034732]\n",
      "6916 [D loss: 0.654126, acc.: 62.50%] [G loss: 0.889301]\n",
      "6917 [D loss: 0.654893, acc.: 56.25%] [G loss: 0.899322]\n",
      "6918 [D loss: 0.693531, acc.: 59.38%] [G loss: 0.928359]\n",
      "6919 [D loss: 0.526655, acc.: 81.25%] [G loss: 0.908852]\n",
      "6920 [D loss: 0.756130, acc.: 50.00%] [G loss: 0.974169]\n",
      "6921 [D loss: 0.667347, acc.: 53.12%] [G loss: 0.893676]\n",
      "6922 [D loss: 0.651572, acc.: 71.88%] [G loss: 0.888162]\n",
      "6923 [D loss: 0.710989, acc.: 59.38%] [G loss: 0.844727]\n",
      "6924 [D loss: 0.704301, acc.: 53.12%] [G loss: 0.900461]\n",
      "6925 [D loss: 0.636793, acc.: 59.38%] [G loss: 0.835356]\n",
      "6926 [D loss: 0.611348, acc.: 68.75%] [G loss: 0.812785]\n",
      "6927 [D loss: 0.778856, acc.: 46.88%] [G loss: 0.846882]\n",
      "6928 [D loss: 0.607801, acc.: 68.75%] [G loss: 0.883810]\n",
      "6929 [D loss: 0.707924, acc.: 53.12%] [G loss: 0.885124]\n",
      "6930 [D loss: 0.638613, acc.: 59.38%] [G loss: 0.957139]\n",
      "6931 [D loss: 0.638335, acc.: 59.38%] [G loss: 0.983401]\n",
      "6932 [D loss: 0.696731, acc.: 53.12%] [G loss: 0.851140]\n",
      "6933 [D loss: 0.637784, acc.: 65.62%] [G loss: 0.918310]\n",
      "6934 [D loss: 0.675816, acc.: 50.00%] [G loss: 0.948422]\n",
      "6935 [D loss: 0.719304, acc.: 59.38%] [G loss: 0.871334]\n",
      "6936 [D loss: 0.662259, acc.: 56.25%] [G loss: 0.875599]\n",
      "6937 [D loss: 0.577189, acc.: 65.62%] [G loss: 0.825921]\n",
      "6938 [D loss: 0.624263, acc.: 75.00%] [G loss: 0.888291]\n",
      "6939 [D loss: 0.697739, acc.: 62.50%] [G loss: 0.898041]\n",
      "6940 [D loss: 0.714941, acc.: 50.00%] [G loss: 0.856043]\n",
      "6941 [D loss: 0.664794, acc.: 62.50%] [G loss: 0.934677]\n",
      "6942 [D loss: 0.698525, acc.: 56.25%] [G loss: 0.901439]\n",
      "6943 [D loss: 0.631432, acc.: 59.38%] [G loss: 0.884362]\n",
      "6944 [D loss: 0.671917, acc.: 56.25%] [G loss: 0.782369]\n",
      "6945 [D loss: 0.617733, acc.: 68.75%] [G loss: 0.752022]\n",
      "6946 [D loss: 0.739458, acc.: 56.25%] [G loss: 0.807605]\n",
      "6947 [D loss: 0.701068, acc.: 53.12%] [G loss: 0.827209]\n",
      "6948 [D loss: 0.648724, acc.: 62.50%] [G loss: 0.903592]\n",
      "6949 [D loss: 0.671606, acc.: 59.38%] [G loss: 0.887815]\n",
      "6950 [D loss: 0.742163, acc.: 46.88%] [G loss: 0.905318]\n",
      "6951 [D loss: 0.653573, acc.: 59.38%] [G loss: 0.946799]\n",
      "6952 [D loss: 0.754931, acc.: 59.38%] [G loss: 0.897609]\n",
      "6953 [D loss: 0.724459, acc.: 56.25%] [G loss: 0.961421]\n",
      "6954 [D loss: 0.671813, acc.: 43.75%] [G loss: 0.918233]\n",
      "6955 [D loss: 0.752427, acc.: 50.00%] [G loss: 0.890951]\n",
      "6956 [D loss: 0.757568, acc.: 37.50%] [G loss: 0.891943]\n",
      "6957 [D loss: 0.652803, acc.: 68.75%] [G loss: 0.879551]\n",
      "6958 [D loss: 0.730662, acc.: 46.88%] [G loss: 0.833176]\n",
      "6959 [D loss: 0.686363, acc.: 59.38%] [G loss: 0.846105]\n",
      "6960 [D loss: 0.655794, acc.: 68.75%] [G loss: 0.790538]\n",
      "6961 [D loss: 0.625823, acc.: 62.50%] [G loss: 0.851851]\n",
      "6962 [D loss: 0.677510, acc.: 59.38%] [G loss: 0.807928]\n",
      "6963 [D loss: 0.709611, acc.: 50.00%] [G loss: 0.876331]\n",
      "6964 [D loss: 0.658394, acc.: 59.38%] [G loss: 0.903784]\n",
      "6965 [D loss: 0.575119, acc.: 59.38%] [G loss: 0.900855]\n",
      "6966 [D loss: 0.664774, acc.: 65.62%] [G loss: 0.954875]\n",
      "6967 [D loss: 0.687413, acc.: 59.38%] [G loss: 0.908285]\n",
      "6968 [D loss: 0.645729, acc.: 59.38%] [G loss: 0.856997]\n",
      "6969 [D loss: 0.659336, acc.: 59.38%] [G loss: 0.872903]\n",
      "6970 [D loss: 0.675900, acc.: 59.38%] [G loss: 0.960523]\n",
      "6971 [D loss: 0.737711, acc.: 43.75%] [G loss: 0.897840]\n",
      "6972 [D loss: 0.641003, acc.: 59.38%] [G loss: 0.974925]\n",
      "6973 [D loss: 0.625056, acc.: 71.88%] [G loss: 0.911268]\n",
      "6974 [D loss: 0.646479, acc.: 65.62%] [G loss: 0.872803]\n",
      "6975 [D loss: 0.640067, acc.: 65.62%] [G loss: 0.902104]\n",
      "6976 [D loss: 0.738186, acc.: 50.00%] [G loss: 0.942495]\n",
      "6977 [D loss: 0.713564, acc.: 46.88%] [G loss: 0.939017]\n",
      "6978 [D loss: 0.695578, acc.: 56.25%] [G loss: 0.878257]\n",
      "6979 [D loss: 0.623565, acc.: 59.38%] [G loss: 0.914477]\n",
      "6980 [D loss: 0.588475, acc.: 71.88%] [G loss: 0.907714]\n",
      "6981 [D loss: 0.689375, acc.: 59.38%] [G loss: 0.875155]\n",
      "6982 [D loss: 0.778014, acc.: 31.25%] [G loss: 0.918897]\n",
      "6983 [D loss: 0.672999, acc.: 46.88%] [G loss: 0.863244]\n",
      "6984 [D loss: 0.635382, acc.: 68.75%] [G loss: 0.865986]\n",
      "6985 [D loss: 0.747995, acc.: 50.00%] [G loss: 0.984198]\n",
      "6986 [D loss: 0.590402, acc.: 71.88%] [G loss: 0.907414]\n",
      "6987 [D loss: 0.720725, acc.: 59.38%] [G loss: 0.974100]\n",
      "6988 [D loss: 0.637436, acc.: 65.62%] [G loss: 0.881762]\n",
      "6989 [D loss: 0.746173, acc.: 46.88%] [G loss: 0.847291]\n",
      "6990 [D loss: 0.662049, acc.: 53.12%] [G loss: 0.938960]\n",
      "6991 [D loss: 0.663367, acc.: 62.50%] [G loss: 1.035155]\n",
      "6992 [D loss: 0.610376, acc.: 62.50%] [G loss: 1.040115]\n",
      "6993 [D loss: 0.647095, acc.: 59.38%] [G loss: 0.880933]\n",
      "6994 [D loss: 0.637190, acc.: 68.75%] [G loss: 0.854624]\n",
      "6995 [D loss: 0.672954, acc.: 59.38%] [G loss: 0.812131]\n",
      "6996 [D loss: 0.710445, acc.: 56.25%] [G loss: 0.774253]\n",
      "6997 [D loss: 0.603008, acc.: 75.00%] [G loss: 0.836747]\n",
      "6998 [D loss: 0.614725, acc.: 68.75%] [G loss: 0.792655]\n",
      "6999 [D loss: 0.737050, acc.: 53.12%] [G loss: 0.867825]\n",
      "7000 [D loss: 0.657627, acc.: 65.62%] [G loss: 0.817265]\n",
      "7001 [D loss: 0.701372, acc.: 53.12%] [G loss: 0.883193]\n",
      "7002 [D loss: 0.661550, acc.: 65.62%] [G loss: 0.973037]\n",
      "7003 [D loss: 0.647805, acc.: 53.12%] [G loss: 0.996141]\n",
      "7004 [D loss: 0.611870, acc.: 68.75%] [G loss: 1.041177]\n",
      "7005 [D loss: 0.783120, acc.: 40.62%] [G loss: 1.002321]\n",
      "7006 [D loss: 0.631814, acc.: 59.38%] [G loss: 0.895457]\n",
      "7007 [D loss: 0.614012, acc.: 75.00%] [G loss: 0.812501]\n",
      "7008 [D loss: 0.639487, acc.: 59.38%] [G loss: 0.958404]\n",
      "7009 [D loss: 0.625367, acc.: 59.38%] [G loss: 0.859564]\n",
      "7010 [D loss: 0.604699, acc.: 68.75%] [G loss: 0.907465]\n",
      "7011 [D loss: 0.586963, acc.: 65.62%] [G loss: 0.926381]\n",
      "7012 [D loss: 0.682703, acc.: 68.75%] [G loss: 0.878078]\n",
      "7013 [D loss: 0.657402, acc.: 53.12%] [G loss: 0.842061]\n",
      "7014 [D loss: 0.714397, acc.: 56.25%] [G loss: 0.829157]\n",
      "7015 [D loss: 0.764314, acc.: 43.75%] [G loss: 0.852365]\n",
      "7016 [D loss: 0.587889, acc.: 62.50%] [G loss: 0.849261]\n",
      "7017 [D loss: 0.618802, acc.: 71.88%] [G loss: 0.889766]\n",
      "7018 [D loss: 0.632261, acc.: 62.50%] [G loss: 0.877595]\n",
      "7019 [D loss: 0.751750, acc.: 53.12%] [G loss: 0.919216]\n",
      "7020 [D loss: 0.746743, acc.: 43.75%] [G loss: 0.861638]\n",
      "7021 [D loss: 0.714225, acc.: 62.50%] [G loss: 0.898560]\n",
      "7022 [D loss: 0.608955, acc.: 65.62%] [G loss: 0.849937]\n",
      "7023 [D loss: 0.653460, acc.: 59.38%] [G loss: 0.856364]\n",
      "7024 [D loss: 0.695978, acc.: 59.38%] [G loss: 0.888404]\n",
      "7025 [D loss: 0.667546, acc.: 50.00%] [G loss: 0.824244]\n",
      "7026 [D loss: 0.644163, acc.: 59.38%] [G loss: 0.858922]\n",
      "7027 [D loss: 0.820536, acc.: 34.38%] [G loss: 0.868295]\n",
      "7028 [D loss: 0.686660, acc.: 56.25%] [G loss: 0.985709]\n",
      "7029 [D loss: 0.664056, acc.: 59.38%] [G loss: 0.879153]\n",
      "7030 [D loss: 0.625262, acc.: 62.50%] [G loss: 0.877486]\n",
      "7031 [D loss: 0.635639, acc.: 68.75%] [G loss: 0.900657]\n",
      "7032 [D loss: 0.676646, acc.: 56.25%] [G loss: 0.944005]\n",
      "7033 [D loss: 0.711309, acc.: 53.12%] [G loss: 0.859965]\n",
      "7034 [D loss: 0.668014, acc.: 62.50%] [G loss: 0.908229]\n",
      "7035 [D loss: 0.583399, acc.: 62.50%] [G loss: 0.944676]\n",
      "7036 [D loss: 0.660327, acc.: 59.38%] [G loss: 0.853978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7037 [D loss: 0.671156, acc.: 62.50%] [G loss: 0.924917]\n",
      "7038 [D loss: 0.551483, acc.: 81.25%] [G loss: 0.836042]\n",
      "7039 [D loss: 0.724830, acc.: 50.00%] [G loss: 0.870372]\n",
      "7040 [D loss: 0.770540, acc.: 56.25%] [G loss: 0.864006]\n",
      "7041 [D loss: 0.777657, acc.: 56.25%] [G loss: 0.914962]\n",
      "7042 [D loss: 0.634946, acc.: 62.50%] [G loss: 0.893292]\n",
      "7043 [D loss: 0.651282, acc.: 59.38%] [G loss: 0.864030]\n",
      "7044 [D loss: 0.688474, acc.: 68.75%] [G loss: 0.847589]\n",
      "7045 [D loss: 0.621660, acc.: 65.62%] [G loss: 0.776394]\n",
      "7046 [D loss: 0.699869, acc.: 65.62%] [G loss: 0.799525]\n",
      "7047 [D loss: 0.572919, acc.: 75.00%] [G loss: 0.784681]\n",
      "7048 [D loss: 0.692216, acc.: 59.38%] [G loss: 0.923738]\n",
      "7049 [D loss: 0.604993, acc.: 65.62%] [G loss: 0.892780]\n",
      "7050 [D loss: 0.697878, acc.: 50.00%] [G loss: 0.833672]\n",
      "7051 [D loss: 0.667886, acc.: 56.25%] [G loss: 0.849309]\n",
      "7052 [D loss: 0.570974, acc.: 71.88%] [G loss: 0.869895]\n",
      "7053 [D loss: 0.730688, acc.: 56.25%] [G loss: 0.835213]\n",
      "7054 [D loss: 0.704795, acc.: 62.50%] [G loss: 0.890831]\n",
      "7055 [D loss: 0.547168, acc.: 84.38%] [G loss: 0.922190]\n",
      "7056 [D loss: 0.658624, acc.: 56.25%] [G loss: 0.798195]\n",
      "7057 [D loss: 0.667738, acc.: 46.88%] [G loss: 0.892932]\n",
      "7058 [D loss: 0.645371, acc.: 59.38%] [G loss: 0.971840]\n",
      "7059 [D loss: 0.672612, acc.: 62.50%] [G loss: 0.896234]\n",
      "7060 [D loss: 0.732595, acc.: 56.25%] [G loss: 0.887010]\n",
      "7061 [D loss: 0.643858, acc.: 50.00%] [G loss: 0.885675]\n",
      "7062 [D loss: 0.606977, acc.: 62.50%] [G loss: 0.869917]\n",
      "7063 [D loss: 0.658542, acc.: 68.75%] [G loss: 0.856624]\n",
      "7064 [D loss: 0.707528, acc.: 50.00%] [G loss: 0.794639]\n",
      "7065 [D loss: 0.655012, acc.: 59.38%] [G loss: 0.820908]\n",
      "7066 [D loss: 0.657788, acc.: 50.00%] [G loss: 0.879386]\n",
      "7067 [D loss: 0.634971, acc.: 68.75%] [G loss: 0.886493]\n",
      "7068 [D loss: 0.652754, acc.: 59.38%] [G loss: 0.768899]\n",
      "7069 [D loss: 0.834583, acc.: 43.75%] [G loss: 0.856785]\n",
      "7070 [D loss: 0.749484, acc.: 53.12%] [G loss: 0.763782]\n",
      "7071 [D loss: 0.654952, acc.: 62.50%] [G loss: 0.831579]\n",
      "7072 [D loss: 0.672467, acc.: 56.25%] [G loss: 0.798356]\n",
      "7073 [D loss: 0.754341, acc.: 65.62%] [G loss: 0.824459]\n",
      "7074 [D loss: 0.570081, acc.: 75.00%] [G loss: 0.819559]\n",
      "7075 [D loss: 0.683082, acc.: 71.88%] [G loss: 0.895093]\n",
      "7076 [D loss: 0.657597, acc.: 62.50%] [G loss: 0.869233]\n",
      "7077 [D loss: 0.635103, acc.: 62.50%] [G loss: 0.887242]\n",
      "7078 [D loss: 0.706855, acc.: 56.25%] [G loss: 0.822127]\n",
      "7079 [D loss: 0.673303, acc.: 59.38%] [G loss: 0.876912]\n",
      "7080 [D loss: 0.767501, acc.: 43.75%] [G loss: 0.836961]\n",
      "7081 [D loss: 0.605356, acc.: 65.62%] [G loss: 0.823563]\n",
      "7082 [D loss: 0.705052, acc.: 62.50%] [G loss: 0.798118]\n",
      "7083 [D loss: 0.703495, acc.: 46.88%] [G loss: 0.828065]\n",
      "7084 [D loss: 0.759669, acc.: 50.00%] [G loss: 0.821101]\n",
      "7085 [D loss: 0.672823, acc.: 53.12%] [G loss: 0.848326]\n",
      "7086 [D loss: 0.672927, acc.: 65.62%] [G loss: 0.929634]\n",
      "7087 [D loss: 0.655995, acc.: 62.50%] [G loss: 0.869674]\n",
      "7088 [D loss: 0.631949, acc.: 75.00%] [G loss: 0.876782]\n",
      "7089 [D loss: 0.666493, acc.: 62.50%] [G loss: 0.895001]\n",
      "7090 [D loss: 0.726297, acc.: 65.62%] [G loss: 0.886098]\n",
      "7091 [D loss: 0.737929, acc.: 56.25%] [G loss: 0.852733]\n",
      "7092 [D loss: 0.681095, acc.: 50.00%] [G loss: 0.888602]\n",
      "7093 [D loss: 0.699152, acc.: 46.88%] [G loss: 0.839935]\n",
      "7094 [D loss: 0.646709, acc.: 68.75%] [G loss: 0.827419]\n",
      "7095 [D loss: 0.673922, acc.: 59.38%] [G loss: 0.912249]\n",
      "7096 [D loss: 0.630925, acc.: 65.62%] [G loss: 0.826086]\n",
      "7097 [D loss: 0.609667, acc.: 68.75%] [G loss: 0.888059]\n",
      "7098 [D loss: 0.596761, acc.: 65.62%] [G loss: 1.005771]\n",
      "7099 [D loss: 0.688270, acc.: 46.88%] [G loss: 0.962985]\n",
      "7100 [D loss: 0.719184, acc.: 56.25%] [G loss: 0.878430]\n",
      "7101 [D loss: 0.685263, acc.: 56.25%] [G loss: 0.808403]\n",
      "7102 [D loss: 0.736592, acc.: 50.00%] [G loss: 0.796522]\n",
      "7103 [D loss: 0.642118, acc.: 59.38%] [G loss: 0.815979]\n",
      "7104 [D loss: 0.565557, acc.: 68.75%] [G loss: 0.881700]\n",
      "7105 [D loss: 0.638236, acc.: 62.50%] [G loss: 0.896834]\n",
      "7106 [D loss: 0.662480, acc.: 65.62%] [G loss: 0.861687]\n",
      "7107 [D loss: 0.724537, acc.: 50.00%] [G loss: 0.830790]\n",
      "7108 [D loss: 0.676553, acc.: 62.50%] [G loss: 0.804624]\n",
      "7109 [D loss: 0.725667, acc.: 56.25%] [G loss: 0.825137]\n",
      "7110 [D loss: 0.684838, acc.: 59.38%] [G loss: 0.917889]\n",
      "7111 [D loss: 0.742244, acc.: 50.00%] [G loss: 0.912131]\n",
      "7112 [D loss: 0.727299, acc.: 46.88%] [G loss: 0.902478]\n",
      "7113 [D loss: 0.712200, acc.: 62.50%] [G loss: 0.866659]\n",
      "7114 [D loss: 0.693393, acc.: 50.00%] [G loss: 0.826753]\n",
      "7115 [D loss: 0.606893, acc.: 65.62%] [G loss: 0.964222]\n",
      "7116 [D loss: 0.702005, acc.: 50.00%] [G loss: 0.858064]\n",
      "7117 [D loss: 0.709591, acc.: 40.62%] [G loss: 0.854413]\n",
      "7118 [D loss: 0.561078, acc.: 75.00%] [G loss: 0.950051]\n",
      "7119 [D loss: 0.763554, acc.: 43.75%] [G loss: 0.899439]\n",
      "7120 [D loss: 0.704029, acc.: 50.00%] [G loss: 0.827151]\n",
      "7121 [D loss: 0.707764, acc.: 53.12%] [G loss: 0.936658]\n",
      "7122 [D loss: 0.624678, acc.: 65.62%] [G loss: 0.926674]\n",
      "7123 [D loss: 0.600549, acc.: 62.50%] [G loss: 0.899374]\n",
      "7124 [D loss: 0.587231, acc.: 75.00%] [G loss: 0.804637]\n",
      "7125 [D loss: 0.609550, acc.: 65.62%] [G loss: 0.810151]\n",
      "7126 [D loss: 0.656952, acc.: 50.00%] [G loss: 0.819397]\n",
      "7127 [D loss: 0.728912, acc.: 50.00%] [G loss: 0.789314]\n",
      "7128 [D loss: 0.679277, acc.: 56.25%] [G loss: 0.777785]\n",
      "7129 [D loss: 0.600595, acc.: 78.12%] [G loss: 0.796194]\n",
      "7130 [D loss: 0.667729, acc.: 59.38%] [G loss: 0.846010]\n",
      "7131 [D loss: 0.739863, acc.: 53.12%] [G loss: 0.901689]\n",
      "7132 [D loss: 0.666202, acc.: 59.38%] [G loss: 0.920662]\n",
      "7133 [D loss: 0.586287, acc.: 78.12%] [G loss: 0.851303]\n",
      "7134 [D loss: 0.673408, acc.: 53.12%] [G loss: 0.889183]\n",
      "7135 [D loss: 0.687186, acc.: 65.62%] [G loss: 0.883402]\n",
      "7136 [D loss: 0.618443, acc.: 68.75%] [G loss: 0.860099]\n",
      "7137 [D loss: 0.649833, acc.: 59.38%] [G loss: 0.869358]\n",
      "7138 [D loss: 0.520891, acc.: 81.25%] [G loss: 0.882188]\n",
      "7139 [D loss: 0.738852, acc.: 43.75%] [G loss: 0.817086]\n",
      "7140 [D loss: 0.617912, acc.: 65.62%] [G loss: 0.801685]\n",
      "7141 [D loss: 0.641088, acc.: 62.50%] [G loss: 0.849360]\n",
      "7142 [D loss: 0.580217, acc.: 75.00%] [G loss: 0.885530]\n",
      "7143 [D loss: 0.680108, acc.: 53.12%] [G loss: 0.813051]\n",
      "7144 [D loss: 0.812013, acc.: 31.25%] [G loss: 0.792189]\n",
      "7145 [D loss: 0.585755, acc.: 75.00%] [G loss: 0.791289]\n",
      "7146 [D loss: 0.640719, acc.: 59.38%] [G loss: 0.935692]\n",
      "7147 [D loss: 0.726755, acc.: 46.88%] [G loss: 0.851939]\n",
      "7148 [D loss: 0.727885, acc.: 46.88%] [G loss: 0.830973]\n",
      "7149 [D loss: 0.610372, acc.: 71.88%] [G loss: 0.859192]\n",
      "7150 [D loss: 0.733429, acc.: 43.75%] [G loss: 0.879658]\n",
      "7151 [D loss: 0.657975, acc.: 62.50%] [G loss: 0.833494]\n",
      "7152 [D loss: 0.668780, acc.: 62.50%] [G loss: 0.897262]\n",
      "7153 [D loss: 0.661346, acc.: 65.62%] [G loss: 0.920758]\n",
      "7154 [D loss: 0.615487, acc.: 65.62%] [G loss: 0.938638]\n",
      "7155 [D loss: 0.691998, acc.: 50.00%] [G loss: 0.952739]\n",
      "7156 [D loss: 0.673735, acc.: 53.12%] [G loss: 0.901831]\n",
      "7157 [D loss: 0.575853, acc.: 68.75%] [G loss: 0.830693]\n",
      "7158 [D loss: 0.739439, acc.: 40.62%] [G loss: 0.864060]\n",
      "7159 [D loss: 0.708407, acc.: 46.88%] [G loss: 0.805052]\n",
      "7160 [D loss: 0.689237, acc.: 50.00%] [G loss: 0.827952]\n",
      "7161 [D loss: 0.669733, acc.: 46.88%] [G loss: 0.823957]\n",
      "7162 [D loss: 0.730571, acc.: 46.88%] [G loss: 0.883972]\n",
      "7163 [D loss: 0.693119, acc.: 62.50%] [G loss: 0.871677]\n",
      "7164 [D loss: 0.650130, acc.: 59.38%] [G loss: 0.850987]\n",
      "7165 [D loss: 0.607526, acc.: 62.50%] [G loss: 0.901590]\n",
      "7166 [D loss: 0.706578, acc.: 62.50%] [G loss: 0.941969]\n",
      "7167 [D loss: 0.744111, acc.: 43.75%] [G loss: 0.916787]\n",
      "7168 [D loss: 0.855388, acc.: 31.25%] [G loss: 0.766599]\n",
      "7169 [D loss: 0.728359, acc.: 50.00%] [G loss: 0.818331]\n",
      "7170 [D loss: 0.688855, acc.: 50.00%] [G loss: 0.902096]\n",
      "7171 [D loss: 0.693582, acc.: 62.50%] [G loss: 0.866459]\n",
      "7172 [D loss: 0.712405, acc.: 50.00%] [G loss: 0.873164]\n",
      "7173 [D loss: 0.608699, acc.: 68.75%] [G loss: 0.868303]\n",
      "7174 [D loss: 0.723992, acc.: 50.00%] [G loss: 0.803009]\n",
      "7175 [D loss: 0.679444, acc.: 59.38%] [G loss: 0.842479]\n",
      "7176 [D loss: 0.710829, acc.: 56.25%] [G loss: 0.841951]\n",
      "7177 [D loss: 0.696469, acc.: 56.25%] [G loss: 0.839482]\n",
      "7178 [D loss: 0.621341, acc.: 71.88%] [G loss: 0.956849]\n",
      "7179 [D loss: 0.675271, acc.: 71.88%] [G loss: 0.849256]\n",
      "7180 [D loss: 0.629932, acc.: 71.88%] [G loss: 0.894540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7181 [D loss: 0.652249, acc.: 71.88%] [G loss: 0.775267]\n",
      "7182 [D loss: 0.635590, acc.: 65.62%] [G loss: 0.868303]\n",
      "7183 [D loss: 0.647550, acc.: 68.75%] [G loss: 0.834022]\n",
      "7184 [D loss: 0.701418, acc.: 46.88%] [G loss: 0.811126]\n",
      "7185 [D loss: 0.672099, acc.: 62.50%] [G loss: 0.865417]\n",
      "7186 [D loss: 0.634608, acc.: 65.62%] [G loss: 0.881237]\n",
      "7187 [D loss: 0.614718, acc.: 65.62%] [G loss: 0.915273]\n",
      "7188 [D loss: 0.690688, acc.: 68.75%] [G loss: 0.930195]\n",
      "7189 [D loss: 0.642294, acc.: 56.25%] [G loss: 0.895312]\n",
      "7190 [D loss: 0.738191, acc.: 46.88%] [G loss: 0.834222]\n",
      "7191 [D loss: 0.659587, acc.: 65.62%] [G loss: 0.872637]\n",
      "7192 [D loss: 0.684084, acc.: 59.38%] [G loss: 0.971209]\n",
      "7193 [D loss: 0.712676, acc.: 50.00%] [G loss: 0.879132]\n",
      "7194 [D loss: 0.740262, acc.: 50.00%] [G loss: 0.801383]\n",
      "7195 [D loss: 0.632324, acc.: 68.75%] [G loss: 0.908178]\n",
      "7196 [D loss: 0.623411, acc.: 65.62%] [G loss: 0.847406]\n",
      "7197 [D loss: 0.659057, acc.: 62.50%] [G loss: 0.867369]\n",
      "7198 [D loss: 0.752583, acc.: 46.88%] [G loss: 0.845603]\n",
      "7199 [D loss: 0.628404, acc.: 71.88%] [G loss: 0.937703]\n",
      "7200 [D loss: 0.669589, acc.: 68.75%] [G loss: 0.989465]\n",
      "7201 [D loss: 0.724702, acc.: 56.25%] [G loss: 0.916097]\n",
      "7202 [D loss: 0.641129, acc.: 59.38%] [G loss: 0.851254]\n",
      "7203 [D loss: 0.649153, acc.: 65.62%] [G loss: 0.907097]\n",
      "7204 [D loss: 0.680159, acc.: 50.00%] [G loss: 0.938891]\n",
      "7205 [D loss: 0.637157, acc.: 62.50%] [G loss: 1.027881]\n",
      "7206 [D loss: 0.760814, acc.: 43.75%] [G loss: 0.964732]\n",
      "7207 [D loss: 0.640170, acc.: 62.50%] [G loss: 0.985167]\n",
      "7208 [D loss: 0.653402, acc.: 62.50%] [G loss: 0.852892]\n",
      "7209 [D loss: 0.690264, acc.: 46.88%] [G loss: 0.900463]\n",
      "7210 [D loss: 0.667778, acc.: 59.38%] [G loss: 0.869154]\n",
      "7211 [D loss: 0.752115, acc.: 53.12%] [G loss: 0.819356]\n",
      "7212 [D loss: 0.710550, acc.: 59.38%] [G loss: 0.832845]\n",
      "7213 [D loss: 0.683262, acc.: 50.00%] [G loss: 0.901349]\n",
      "7214 [D loss: 0.594117, acc.: 68.75%] [G loss: 0.824895]\n",
      "7215 [D loss: 0.616803, acc.: 68.75%] [G loss: 0.806326]\n",
      "7216 [D loss: 0.651506, acc.: 62.50%] [G loss: 0.766012]\n",
      "7217 [D loss: 0.702384, acc.: 59.38%] [G loss: 0.876016]\n",
      "7218 [D loss: 0.699800, acc.: 46.88%] [G loss: 0.992124]\n",
      "7219 [D loss: 0.697561, acc.: 50.00%] [G loss: 0.900986]\n",
      "7220 [D loss: 0.679715, acc.: 59.38%] [G loss: 0.888201]\n",
      "7221 [D loss: 0.764276, acc.: 46.88%] [G loss: 0.848935]\n",
      "7222 [D loss: 0.622739, acc.: 62.50%] [G loss: 0.853848]\n",
      "7223 [D loss: 0.637483, acc.: 59.38%] [G loss: 0.867899]\n",
      "7224 [D loss: 0.662007, acc.: 59.38%] [G loss: 0.796023]\n",
      "7225 [D loss: 0.724230, acc.: 46.88%] [G loss: 0.863973]\n",
      "7226 [D loss: 0.674006, acc.: 59.38%] [G loss: 0.847133]\n",
      "7227 [D loss: 0.639694, acc.: 68.75%] [G loss: 0.928282]\n",
      "7228 [D loss: 0.691846, acc.: 56.25%] [G loss: 0.907482]\n",
      "7229 [D loss: 0.664627, acc.: 56.25%] [G loss: 0.939580]\n",
      "7230 [D loss: 0.663577, acc.: 68.75%] [G loss: 0.831332]\n",
      "7231 [D loss: 0.679331, acc.: 65.62%] [G loss: 0.826311]\n",
      "7232 [D loss: 0.665980, acc.: 53.12%] [G loss: 0.874811]\n",
      "7233 [D loss: 0.720883, acc.: 50.00%] [G loss: 0.937609]\n",
      "7234 [D loss: 0.729869, acc.: 37.50%] [G loss: 1.028908]\n",
      "7235 [D loss: 0.790843, acc.: 43.75%] [G loss: 0.873545]\n",
      "7236 [D loss: 0.731514, acc.: 50.00%] [G loss: 0.855309]\n",
      "7237 [D loss: 0.681369, acc.: 50.00%] [G loss: 0.936052]\n",
      "7238 [D loss: 0.644220, acc.: 68.75%] [G loss: 0.832114]\n",
      "7239 [D loss: 0.675181, acc.: 56.25%] [G loss: 0.841441]\n",
      "7240 [D loss: 0.603811, acc.: 68.75%] [G loss: 0.823635]\n",
      "7241 [D loss: 0.717608, acc.: 62.50%] [G loss: 0.855798]\n",
      "7242 [D loss: 0.646357, acc.: 56.25%] [G loss: 0.846403]\n",
      "7243 [D loss: 0.620611, acc.: 65.62%] [G loss: 0.827899]\n",
      "7244 [D loss: 0.623987, acc.: 56.25%] [G loss: 0.823963]\n",
      "7245 [D loss: 0.581520, acc.: 75.00%] [G loss: 0.780317]\n",
      "7246 [D loss: 0.633023, acc.: 65.62%] [G loss: 0.816110]\n",
      "7247 [D loss: 0.626073, acc.: 68.75%] [G loss: 0.837835]\n",
      "7248 [D loss: 0.733933, acc.: 50.00%] [G loss: 0.829836]\n",
      "7249 [D loss: 0.707886, acc.: 50.00%] [G loss: 0.854224]\n",
      "7250 [D loss: 0.623197, acc.: 71.88%] [G loss: 0.922233]\n",
      "7251 [D loss: 0.602415, acc.: 71.88%] [G loss: 0.906685]\n",
      "7252 [D loss: 0.569356, acc.: 68.75%] [G loss: 0.913042]\n",
      "7253 [D loss: 0.642868, acc.: 62.50%] [G loss: 0.869287]\n",
      "7254 [D loss: 0.680692, acc.: 59.38%] [G loss: 0.877693]\n",
      "7255 [D loss: 0.763834, acc.: 59.38%] [G loss: 0.847025]\n",
      "7256 [D loss: 0.613616, acc.: 68.75%] [G loss: 0.806293]\n",
      "7257 [D loss: 0.664710, acc.: 62.50%] [G loss: 0.874149]\n",
      "7258 [D loss: 0.673226, acc.: 62.50%] [G loss: 0.835272]\n",
      "7259 [D loss: 0.638738, acc.: 65.62%] [G loss: 0.998703]\n",
      "7260 [D loss: 0.566038, acc.: 78.12%] [G loss: 0.920160]\n",
      "7261 [D loss: 0.673056, acc.: 68.75%] [G loss: 0.956704]\n",
      "7262 [D loss: 0.739864, acc.: 46.88%] [G loss: 0.845298]\n",
      "7263 [D loss: 0.722161, acc.: 46.88%] [G loss: 0.942371]\n",
      "7264 [D loss: 0.622661, acc.: 75.00%] [G loss: 0.956756]\n",
      "7265 [D loss: 0.752127, acc.: 46.88%] [G loss: 0.995533]\n",
      "7266 [D loss: 0.614822, acc.: 71.88%] [G loss: 1.007769]\n",
      "7267 [D loss: 0.628332, acc.: 59.38%] [G loss: 0.950721]\n",
      "7268 [D loss: 0.701809, acc.: 59.38%] [G loss: 0.910584]\n",
      "7269 [D loss: 0.647138, acc.: 62.50%] [G loss: 0.942786]\n",
      "7270 [D loss: 0.678821, acc.: 62.50%] [G loss: 0.899781]\n",
      "7271 [D loss: 0.570913, acc.: 65.62%] [G loss: 0.929468]\n",
      "7272 [D loss: 0.777237, acc.: 43.75%] [G loss: 0.932622]\n",
      "7273 [D loss: 0.689012, acc.: 62.50%] [G loss: 0.902350]\n",
      "7274 [D loss: 0.691529, acc.: 53.12%] [G loss: 0.903624]\n",
      "7275 [D loss: 0.635666, acc.: 62.50%] [G loss: 0.789079]\n",
      "7276 [D loss: 0.657199, acc.: 56.25%] [G loss: 0.891368]\n",
      "7277 [D loss: 0.730166, acc.: 53.12%] [G loss: 0.925370]\n",
      "7278 [D loss: 0.717871, acc.: 68.75%] [G loss: 0.884857]\n",
      "7279 [D loss: 0.613473, acc.: 59.38%] [G loss: 0.880732]\n",
      "7280 [D loss: 0.669277, acc.: 59.38%] [G loss: 0.973722]\n",
      "7281 [D loss: 0.582031, acc.: 65.62%] [G loss: 0.925638]\n",
      "7282 [D loss: 0.716760, acc.: 50.00%] [G loss: 0.962050]\n",
      "7283 [D loss: 0.569685, acc.: 75.00%] [G loss: 0.837146]\n",
      "7284 [D loss: 0.689539, acc.: 46.88%] [G loss: 0.849273]\n",
      "7285 [D loss: 0.764791, acc.: 43.75%] [G loss: 0.909327]\n",
      "7286 [D loss: 0.618742, acc.: 65.62%] [G loss: 0.913313]\n",
      "7287 [D loss: 0.719651, acc.: 53.12%] [G loss: 0.883302]\n",
      "7288 [D loss: 0.707815, acc.: 59.38%] [G loss: 0.935923]\n",
      "7289 [D loss: 0.675473, acc.: 50.00%] [G loss: 0.954257]\n",
      "7290 [D loss: 0.679742, acc.: 59.38%] [G loss: 0.806818]\n",
      "7291 [D loss: 0.775272, acc.: 43.75%] [G loss: 0.862191]\n",
      "7292 [D loss: 0.728970, acc.: 50.00%] [G loss: 0.905396]\n",
      "7293 [D loss: 0.678629, acc.: 50.00%] [G loss: 0.852821]\n",
      "7294 [D loss: 0.658259, acc.: 59.38%] [G loss: 0.833016]\n",
      "7295 [D loss: 0.666335, acc.: 56.25%] [G loss: 0.895205]\n",
      "7296 [D loss: 0.582105, acc.: 59.38%] [G loss: 0.847466]\n",
      "7297 [D loss: 0.765160, acc.: 43.75%] [G loss: 0.851255]\n",
      "7298 [D loss: 0.634470, acc.: 59.38%] [G loss: 0.817696]\n",
      "7299 [D loss: 0.688090, acc.: 59.38%] [G loss: 0.855261]\n",
      "7300 [D loss: 0.735080, acc.: 40.62%] [G loss: 0.830514]\n",
      "7301 [D loss: 0.678675, acc.: 59.38%] [G loss: 0.788597]\n",
      "7302 [D loss: 0.689325, acc.: 56.25%] [G loss: 0.870513]\n",
      "7303 [D loss: 0.623092, acc.: 75.00%] [G loss: 0.836639]\n",
      "7304 [D loss: 0.671286, acc.: 59.38%] [G loss: 0.854339]\n",
      "7305 [D loss: 0.647858, acc.: 59.38%] [G loss: 0.907504]\n",
      "7306 [D loss: 0.690605, acc.: 53.12%] [G loss: 0.829248]\n",
      "7307 [D loss: 0.674590, acc.: 68.75%] [G loss: 0.829347]\n",
      "7308 [D loss: 0.575851, acc.: 78.12%] [G loss: 0.835195]\n",
      "7309 [D loss: 0.757351, acc.: 50.00%] [G loss: 0.839028]\n",
      "7310 [D loss: 0.681191, acc.: 65.62%] [G loss: 0.863628]\n",
      "7311 [D loss: 0.702330, acc.: 53.12%] [G loss: 0.868279]\n",
      "7312 [D loss: 0.774034, acc.: 53.12%] [G loss: 0.874201]\n",
      "7313 [D loss: 0.762728, acc.: 40.62%] [G loss: 0.827410]\n",
      "7314 [D loss: 0.701970, acc.: 50.00%] [G loss: 0.885561]\n",
      "7315 [D loss: 0.610290, acc.: 68.75%] [G loss: 0.861202]\n",
      "7316 [D loss: 0.580336, acc.: 75.00%] [G loss: 0.889096]\n",
      "7317 [D loss: 0.705425, acc.: 59.38%] [G loss: 0.830801]\n",
      "7318 [D loss: 0.730189, acc.: 56.25%] [G loss: 0.843284]\n",
      "7319 [D loss: 0.755440, acc.: 46.88%] [G loss: 0.894929]\n",
      "7320 [D loss: 0.644657, acc.: 50.00%] [G loss: 0.864220]\n",
      "7321 [D loss: 0.581557, acc.: 81.25%] [G loss: 0.963858]\n",
      "7322 [D loss: 0.658396, acc.: 53.12%] [G loss: 0.923093]\n",
      "7323 [D loss: 0.695117, acc.: 62.50%] [G loss: 0.946564]\n",
      "7324 [D loss: 0.652263, acc.: 53.12%] [G loss: 0.886820]\n",
      "7325 [D loss: 0.579313, acc.: 75.00%] [G loss: 0.929254]\n",
      "7326 [D loss: 0.752868, acc.: 40.62%] [G loss: 0.883933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7327 [D loss: 0.669343, acc.: 53.12%] [G loss: 0.889230]\n",
      "7328 [D loss: 0.704274, acc.: 65.62%] [G loss: 0.870978]\n",
      "7329 [D loss: 0.630679, acc.: 53.12%] [G loss: 0.933619]\n",
      "7330 [D loss: 0.740907, acc.: 34.38%] [G loss: 0.871235]\n",
      "7331 [D loss: 0.634213, acc.: 62.50%] [G loss: 0.929031]\n",
      "7332 [D loss: 0.673980, acc.: 56.25%] [G loss: 0.866556]\n",
      "7333 [D loss: 0.656198, acc.: 62.50%] [G loss: 0.979204]\n",
      "7334 [D loss: 0.743344, acc.: 53.12%] [G loss: 0.841178]\n",
      "7335 [D loss: 0.717863, acc.: 53.12%] [G loss: 0.915467]\n",
      "7336 [D loss: 0.676339, acc.: 59.38%] [G loss: 0.963306]\n",
      "7337 [D loss: 0.737729, acc.: 50.00%] [G loss: 0.827429]\n",
      "7338 [D loss: 0.637972, acc.: 65.62%] [G loss: 0.831920]\n",
      "7339 [D loss: 0.641772, acc.: 62.50%] [G loss: 0.779843]\n",
      "7340 [D loss: 0.687932, acc.: 56.25%] [G loss: 0.887151]\n",
      "7341 [D loss: 0.795982, acc.: 37.50%] [G loss: 0.888798]\n",
      "7342 [D loss: 0.732655, acc.: 43.75%] [G loss: 0.957589]\n",
      "7343 [D loss: 0.633213, acc.: 56.25%] [G loss: 0.865661]\n",
      "7344 [D loss: 0.545432, acc.: 87.50%] [G loss: 0.899922]\n",
      "7345 [D loss: 0.604153, acc.: 62.50%] [G loss: 0.865428]\n",
      "7346 [D loss: 0.692000, acc.: 50.00%] [G loss: 0.884605]\n",
      "7347 [D loss: 0.688193, acc.: 56.25%] [G loss: 0.912979]\n",
      "7348 [D loss: 0.650666, acc.: 59.38%] [G loss: 0.784177]\n",
      "7349 [D loss: 0.621509, acc.: 71.88%] [G loss: 0.867608]\n",
      "7350 [D loss: 0.670482, acc.: 56.25%] [G loss: 0.908992]\n",
      "7351 [D loss: 0.785512, acc.: 37.50%] [G loss: 0.839606]\n",
      "7352 [D loss: 0.679192, acc.: 53.12%] [G loss: 0.942100]\n",
      "7353 [D loss: 0.575193, acc.: 68.75%] [G loss: 0.896735]\n",
      "7354 [D loss: 0.737982, acc.: 43.75%] [G loss: 0.888714]\n",
      "7355 [D loss: 0.627535, acc.: 71.88%] [G loss: 0.794132]\n",
      "7356 [D loss: 0.679745, acc.: 56.25%] [G loss: 0.805426]\n",
      "7357 [D loss: 0.639455, acc.: 56.25%] [G loss: 0.812734]\n",
      "7358 [D loss: 0.611928, acc.: 75.00%] [G loss: 0.801345]\n",
      "7359 [D loss: 0.631955, acc.: 62.50%] [G loss: 0.811357]\n",
      "7360 [D loss: 0.672790, acc.: 62.50%] [G loss: 0.870200]\n",
      "7361 [D loss: 0.702590, acc.: 50.00%] [G loss: 0.841071]\n",
      "7362 [D loss: 0.716205, acc.: 46.88%] [G loss: 0.848469]\n",
      "7363 [D loss: 0.675012, acc.: 46.88%] [G loss: 0.868875]\n",
      "7364 [D loss: 0.627994, acc.: 56.25%] [G loss: 0.885485]\n",
      "7365 [D loss: 0.643461, acc.: 62.50%] [G loss: 0.880112]\n",
      "7366 [D loss: 0.636764, acc.: 68.75%] [G loss: 0.887287]\n",
      "7367 [D loss: 0.739979, acc.: 43.75%] [G loss: 0.930885]\n",
      "7368 [D loss: 0.666094, acc.: 53.12%] [G loss: 0.885348]\n",
      "7369 [D loss: 0.699407, acc.: 62.50%] [G loss: 0.918599]\n",
      "7370 [D loss: 0.630801, acc.: 62.50%] [G loss: 0.924396]\n",
      "7371 [D loss: 0.724965, acc.: 56.25%] [G loss: 0.850531]\n",
      "7372 [D loss: 0.688298, acc.: 56.25%] [G loss: 0.912463]\n",
      "7373 [D loss: 0.720887, acc.: 50.00%] [G loss: 0.835439]\n",
      "7374 [D loss: 0.664200, acc.: 59.38%] [G loss: 0.881712]\n",
      "7375 [D loss: 0.577325, acc.: 75.00%] [G loss: 0.838592]\n",
      "7376 [D loss: 0.775827, acc.: 50.00%] [G loss: 0.820154]\n",
      "7377 [D loss: 0.780280, acc.: 46.88%] [G loss: 0.876104]\n",
      "7378 [D loss: 0.664882, acc.: 65.62%] [G loss: 0.861273]\n",
      "7379 [D loss: 0.680009, acc.: 62.50%] [G loss: 0.890668]\n",
      "7380 [D loss: 0.782053, acc.: 25.00%] [G loss: 0.935439]\n",
      "7381 [D loss: 0.683583, acc.: 62.50%] [G loss: 0.826197]\n",
      "7382 [D loss: 0.715647, acc.: 43.75%] [G loss: 0.844145]\n",
      "7383 [D loss: 0.625662, acc.: 65.62%] [G loss: 0.822276]\n",
      "7384 [D loss: 0.635889, acc.: 59.38%] [G loss: 0.829261]\n",
      "7385 [D loss: 0.745354, acc.: 56.25%] [G loss: 0.845297]\n",
      "7386 [D loss: 0.785878, acc.: 46.88%] [G loss: 0.838618]\n",
      "7387 [D loss: 0.642374, acc.: 56.25%] [G loss: 0.980937]\n",
      "7388 [D loss: 0.579631, acc.: 78.12%] [G loss: 0.919161]\n",
      "7389 [D loss: 0.685858, acc.: 56.25%] [G loss: 0.863545]\n",
      "7390 [D loss: 0.660540, acc.: 59.38%] [G loss: 0.833200]\n",
      "7391 [D loss: 0.623625, acc.: 65.62%] [G loss: 0.793042]\n",
      "7392 [D loss: 0.797985, acc.: 43.75%] [G loss: 0.912636]\n",
      "7393 [D loss: 0.643047, acc.: 59.38%] [G loss: 0.890669]\n",
      "7394 [D loss: 0.682501, acc.: 50.00%] [G loss: 0.929157]\n",
      "7395 [D loss: 0.657538, acc.: 53.12%] [G loss: 0.911766]\n",
      "7396 [D loss: 0.621571, acc.: 59.38%] [G loss: 1.011364]\n",
      "7397 [D loss: 0.715871, acc.: 46.88%] [G loss: 0.905537]\n",
      "7398 [D loss: 0.691586, acc.: 56.25%] [G loss: 0.888137]\n",
      "7399 [D loss: 0.685674, acc.: 65.62%] [G loss: 0.912609]\n",
      "7400 [D loss: 0.583988, acc.: 71.88%] [G loss: 0.873063]\n",
      "7401 [D loss: 0.652101, acc.: 56.25%] [G loss: 0.833456]\n",
      "7402 [D loss: 0.630854, acc.: 68.75%] [G loss: 0.891282]\n",
      "7403 [D loss: 0.622430, acc.: 59.38%] [G loss: 1.037828]\n",
      "7404 [D loss: 0.597440, acc.: 68.75%] [G loss: 0.978907]\n",
      "7405 [D loss: 0.691647, acc.: 68.75%] [G loss: 0.925043]\n",
      "7406 [D loss: 0.731387, acc.: 53.12%] [G loss: 0.894173]\n",
      "7407 [D loss: 0.686340, acc.: 46.88%] [G loss: 0.885981]\n",
      "7408 [D loss: 0.610803, acc.: 62.50%] [G loss: 0.967564]\n",
      "7409 [D loss: 0.732800, acc.: 43.75%] [G loss: 0.899984]\n",
      "7410 [D loss: 0.766585, acc.: 43.75%] [G loss: 0.835061]\n",
      "7411 [D loss: 0.690203, acc.: 59.38%] [G loss: 0.864065]\n",
      "7412 [D loss: 0.711921, acc.: 56.25%] [G loss: 0.916847]\n",
      "7413 [D loss: 0.714074, acc.: 50.00%] [G loss: 0.800064]\n",
      "7414 [D loss: 0.630603, acc.: 71.88%] [G loss: 0.858905]\n",
      "7415 [D loss: 0.634127, acc.: 53.12%] [G loss: 0.800793]\n",
      "7416 [D loss: 0.620404, acc.: 68.75%] [G loss: 0.894236]\n",
      "7417 [D loss: 0.690764, acc.: 56.25%] [G loss: 0.841876]\n",
      "7418 [D loss: 0.614214, acc.: 75.00%] [G loss: 0.927558]\n",
      "7419 [D loss: 0.684754, acc.: 59.38%] [G loss: 0.876229]\n",
      "7420 [D loss: 0.618824, acc.: 78.12%] [G loss: 0.822812]\n",
      "7421 [D loss: 0.742077, acc.: 53.12%] [G loss: 0.869198]\n",
      "7422 [D loss: 0.606000, acc.: 68.75%] [G loss: 0.882115]\n",
      "7423 [D loss: 0.694487, acc.: 53.12%] [G loss: 0.919024]\n",
      "7424 [D loss: 0.630375, acc.: 56.25%] [G loss: 0.856104]\n",
      "7425 [D loss: 0.720179, acc.: 50.00%] [G loss: 0.848139]\n",
      "7426 [D loss: 0.670060, acc.: 62.50%] [G loss: 0.875377]\n",
      "7427 [D loss: 0.641375, acc.: 59.38%] [G loss: 0.857765]\n",
      "7428 [D loss: 0.693868, acc.: 59.38%] [G loss: 0.932231]\n",
      "7429 [D loss: 0.649486, acc.: 46.88%] [G loss: 0.902773]\n",
      "7430 [D loss: 0.717214, acc.: 56.25%] [G loss: 0.860883]\n",
      "7431 [D loss: 0.646938, acc.: 62.50%] [G loss: 0.869644]\n",
      "7432 [D loss: 0.675292, acc.: 53.12%] [G loss: 0.887057]\n",
      "7433 [D loss: 0.653070, acc.: 75.00%] [G loss: 0.830268]\n",
      "7434 [D loss: 0.653829, acc.: 50.00%] [G loss: 0.799854]\n",
      "7435 [D loss: 0.715917, acc.: 46.88%] [G loss: 0.831311]\n",
      "7436 [D loss: 0.683478, acc.: 65.62%] [G loss: 0.841071]\n",
      "7437 [D loss: 0.759249, acc.: 43.75%] [G loss: 0.856026]\n",
      "7438 [D loss: 0.606365, acc.: 71.88%] [G loss: 0.865521]\n",
      "7439 [D loss: 0.732386, acc.: 50.00%] [G loss: 0.821312]\n",
      "7440 [D loss: 0.708246, acc.: 53.12%] [G loss: 0.911303]\n",
      "7441 [D loss: 0.681842, acc.: 56.25%] [G loss: 0.807007]\n",
      "7442 [D loss: 0.633488, acc.: 62.50%] [G loss: 0.878815]\n",
      "7443 [D loss: 0.710444, acc.: 56.25%] [G loss: 0.858697]\n",
      "7444 [D loss: 0.673597, acc.: 56.25%] [G loss: 0.865135]\n",
      "7445 [D loss: 0.632605, acc.: 62.50%] [G loss: 0.855473]\n",
      "7446 [D loss: 0.749486, acc.: 40.62%] [G loss: 0.803914]\n",
      "7447 [D loss: 0.630685, acc.: 65.62%] [G loss: 0.838773]\n",
      "7448 [D loss: 0.731652, acc.: 50.00%] [G loss: 0.842010]\n",
      "7449 [D loss: 0.644892, acc.: 62.50%] [G loss: 0.920121]\n",
      "7450 [D loss: 0.678826, acc.: 59.38%] [G loss: 0.879475]\n",
      "7451 [D loss: 0.744616, acc.: 37.50%] [G loss: 0.833138]\n",
      "7452 [D loss: 0.646584, acc.: 65.62%] [G loss: 0.863728]\n",
      "7453 [D loss: 0.665824, acc.: 53.12%] [G loss: 0.866340]\n",
      "7454 [D loss: 0.679579, acc.: 59.38%] [G loss: 0.853724]\n",
      "7455 [D loss: 0.686711, acc.: 50.00%] [G loss: 0.865110]\n",
      "7456 [D loss: 0.720733, acc.: 53.12%] [G loss: 0.829593]\n",
      "7457 [D loss: 0.654301, acc.: 62.50%] [G loss: 0.935112]\n",
      "7458 [D loss: 0.690543, acc.: 43.75%] [G loss: 0.952185]\n",
      "7459 [D loss: 0.679231, acc.: 56.25%] [G loss: 0.916563]\n",
      "7460 [D loss: 0.642798, acc.: 59.38%] [G loss: 0.972573]\n",
      "7461 [D loss: 0.610518, acc.: 68.75%] [G loss: 0.810474]\n",
      "7462 [D loss: 0.739437, acc.: 50.00%] [G loss: 0.898029]\n",
      "7463 [D loss: 0.611763, acc.: 68.75%] [G loss: 0.914681]\n",
      "7464 [D loss: 0.687394, acc.: 50.00%] [G loss: 0.855306]\n",
      "7465 [D loss: 0.698689, acc.: 59.38%] [G loss: 0.889091]\n",
      "7466 [D loss: 0.566838, acc.: 78.12%] [G loss: 0.848990]\n",
      "7467 [D loss: 0.716603, acc.: 50.00%] [G loss: 0.894006]\n",
      "7468 [D loss: 0.711488, acc.: 53.12%] [G loss: 0.921697]\n",
      "7469 [D loss: 0.704106, acc.: 56.25%] [G loss: 0.879400]\n",
      "7470 [D loss: 0.619920, acc.: 62.50%] [G loss: 0.905696]\n",
      "7471 [D loss: 0.600135, acc.: 78.12%] [G loss: 0.845131]\n",
      "7472 [D loss: 0.669308, acc.: 59.38%] [G loss: 0.863515]\n",
      "7473 [D loss: 0.714690, acc.: 53.12%] [G loss: 0.849526]\n",
      "7474 [D loss: 0.667581, acc.: 59.38%] [G loss: 0.838171]\n",
      "7475 [D loss: 0.593701, acc.: 71.88%] [G loss: 0.830240]\n",
      "7476 [D loss: 0.714006, acc.: 46.88%] [G loss: 0.844873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477 [D loss: 0.724560, acc.: 59.38%] [G loss: 0.791389]\n",
      "7478 [D loss: 0.687410, acc.: 59.38%] [G loss: 0.848332]\n",
      "7479 [D loss: 0.660897, acc.: 53.12%] [G loss: 0.866639]\n",
      "7480 [D loss: 0.665175, acc.: 59.38%] [G loss: 0.882872]\n",
      "7481 [D loss: 0.731309, acc.: 53.12%] [G loss: 0.943121]\n",
      "7482 [D loss: 0.701841, acc.: 50.00%] [G loss: 0.883984]\n",
      "7483 [D loss: 0.696163, acc.: 62.50%] [G loss: 0.960506]\n",
      "7484 [D loss: 0.640575, acc.: 62.50%] [G loss: 0.904329]\n",
      "7485 [D loss: 0.593445, acc.: 78.12%] [G loss: 0.876578]\n",
      "7486 [D loss: 0.670840, acc.: 65.62%] [G loss: 0.806995]\n",
      "7487 [D loss: 0.589115, acc.: 68.75%] [G loss: 0.814628]\n",
      "7488 [D loss: 0.595816, acc.: 78.12%] [G loss: 0.898461]\n",
      "7489 [D loss: 0.795504, acc.: 40.62%] [G loss: 0.835908]\n",
      "7490 [D loss: 0.734849, acc.: 65.62%] [G loss: 0.853936]\n",
      "7491 [D loss: 0.662778, acc.: 59.38%] [G loss: 0.807449]\n",
      "7492 [D loss: 0.598290, acc.: 65.62%] [G loss: 0.867902]\n",
      "7493 [D loss: 0.689438, acc.: 56.25%] [G loss: 0.870277]\n",
      "7494 [D loss: 0.689087, acc.: 56.25%] [G loss: 0.809905]\n",
      "7495 [D loss: 0.598904, acc.: 68.75%] [G loss: 0.861172]\n",
      "7496 [D loss: 0.683304, acc.: 53.12%] [G loss: 0.860012]\n",
      "7497 [D loss: 0.629306, acc.: 62.50%] [G loss: 0.922682]\n",
      "7498 [D loss: 0.757184, acc.: 46.88%] [G loss: 0.903304]\n",
      "7499 [D loss: 0.611678, acc.: 68.75%] [G loss: 0.883531]\n",
      "7500 [D loss: 0.549263, acc.: 78.12%] [G loss: 0.890294]\n",
      "7501 [D loss: 0.707809, acc.: 56.25%] [G loss: 0.849687]\n",
      "7502 [D loss: 0.600887, acc.: 65.62%] [G loss: 0.838386]\n",
      "7503 [D loss: 0.649255, acc.: 68.75%] [G loss: 0.802233]\n",
      "7504 [D loss: 0.732460, acc.: 46.88%] [G loss: 0.846743]\n",
      "7505 [D loss: 0.640952, acc.: 68.75%] [G loss: 0.778347]\n",
      "7506 [D loss: 0.654266, acc.: 59.38%] [G loss: 0.776311]\n",
      "7507 [D loss: 0.602960, acc.: 75.00%] [G loss: 0.905929]\n",
      "7508 [D loss: 0.613011, acc.: 65.62%] [G loss: 0.805070]\n",
      "7509 [D loss: 0.655752, acc.: 65.62%] [G loss: 0.878928]\n",
      "7510 [D loss: 0.724279, acc.: 65.62%] [G loss: 0.859535]\n",
      "7511 [D loss: 0.684013, acc.: 50.00%] [G loss: 0.806939]\n",
      "7512 [D loss: 0.700952, acc.: 56.25%] [G loss: 0.866731]\n",
      "7513 [D loss: 0.558455, acc.: 78.12%] [G loss: 0.793102]\n",
      "7514 [D loss: 0.693694, acc.: 46.88%] [G loss: 0.794797]\n",
      "7515 [D loss: 0.672603, acc.: 62.50%] [G loss: 0.844548]\n",
      "7516 [D loss: 0.684413, acc.: 53.12%] [G loss: 0.800556]\n",
      "7517 [D loss: 0.651418, acc.: 56.25%] [G loss: 0.853119]\n",
      "7518 [D loss: 0.700209, acc.: 53.12%] [G loss: 0.835497]\n",
      "7519 [D loss: 0.636639, acc.: 62.50%] [G loss: 0.852622]\n",
      "7520 [D loss: 0.651230, acc.: 59.38%] [G loss: 0.836942]\n",
      "7521 [D loss: 0.613249, acc.: 59.38%] [G loss: 0.807000]\n",
      "7522 [D loss: 0.602722, acc.: 68.75%] [G loss: 0.837577]\n",
      "7523 [D loss: 0.683633, acc.: 56.25%] [G loss: 0.842924]\n",
      "7524 [D loss: 0.627664, acc.: 62.50%] [G loss: 0.833999]\n",
      "7525 [D loss: 0.683575, acc.: 59.38%] [G loss: 0.814632]\n",
      "7526 [D loss: 0.734193, acc.: 37.50%] [G loss: 0.765052]\n",
      "7527 [D loss: 0.693449, acc.: 62.50%] [G loss: 0.801356]\n",
      "7528 [D loss: 0.683871, acc.: 46.88%] [G loss: 0.824605]\n",
      "7529 [D loss: 0.662049, acc.: 56.25%] [G loss: 0.866168]\n",
      "7530 [D loss: 0.633402, acc.: 65.62%] [G loss: 0.825547]\n",
      "7531 [D loss: 0.732332, acc.: 56.25%] [G loss: 0.868520]\n",
      "7532 [D loss: 0.654979, acc.: 59.38%] [G loss: 0.875185]\n",
      "7533 [D loss: 0.732179, acc.: 53.12%] [G loss: 0.818539]\n",
      "7534 [D loss: 0.627774, acc.: 65.62%] [G loss: 0.932642]\n",
      "7535 [D loss: 0.657241, acc.: 59.38%] [G loss: 0.872113]\n",
      "7536 [D loss: 0.680867, acc.: 53.12%] [G loss: 0.789043]\n",
      "7537 [D loss: 0.686935, acc.: 53.12%] [G loss: 0.830162]\n",
      "7538 [D loss: 0.727054, acc.: 46.88%] [G loss: 0.807325]\n",
      "7539 [D loss: 0.685160, acc.: 56.25%] [G loss: 0.836968]\n",
      "7540 [D loss: 0.685740, acc.: 56.25%] [G loss: 0.809300]\n",
      "7541 [D loss: 0.688759, acc.: 62.50%] [G loss: 0.817827]\n",
      "7542 [D loss: 0.694123, acc.: 53.12%] [G loss: 0.792900]\n",
      "7543 [D loss: 0.642316, acc.: 65.62%] [G loss: 0.807497]\n",
      "7544 [D loss: 0.729627, acc.: 53.12%] [G loss: 0.921365]\n",
      "7545 [D loss: 0.643310, acc.: 71.88%] [G loss: 0.898496]\n",
      "7546 [D loss: 0.689880, acc.: 59.38%] [G loss: 0.903381]\n",
      "7547 [D loss: 0.638042, acc.: 68.75%] [G loss: 0.801540]\n",
      "7548 [D loss: 0.652902, acc.: 53.12%] [G loss: 0.894081]\n",
      "7549 [D loss: 0.746499, acc.: 50.00%] [G loss: 0.871664]\n",
      "7550 [D loss: 0.744892, acc.: 46.88%] [G loss: 0.869563]\n",
      "7551 [D loss: 0.735930, acc.: 43.75%] [G loss: 0.844671]\n",
      "7552 [D loss: 0.683652, acc.: 59.38%] [G loss: 0.886144]\n",
      "7553 [D loss: 0.654997, acc.: 56.25%] [G loss: 0.841739]\n",
      "7554 [D loss: 0.648170, acc.: 62.50%] [G loss: 0.915471]\n",
      "7555 [D loss: 0.656709, acc.: 50.00%] [G loss: 0.920569]\n",
      "7556 [D loss: 0.688144, acc.: 59.38%] [G loss: 0.923437]\n",
      "7557 [D loss: 0.701799, acc.: 50.00%] [G loss: 0.844974]\n",
      "7558 [D loss: 0.691597, acc.: 53.12%] [G loss: 0.847423]\n",
      "7559 [D loss: 0.625025, acc.: 56.25%] [G loss: 0.841461]\n",
      "7560 [D loss: 0.673969, acc.: 65.62%] [G loss: 0.932276]\n",
      "7561 [D loss: 0.663849, acc.: 53.12%] [G loss: 0.819790]\n",
      "7562 [D loss: 0.740939, acc.: 53.12%] [G loss: 0.927700]\n",
      "7563 [D loss: 0.682429, acc.: 53.12%] [G loss: 0.911977]\n",
      "7564 [D loss: 0.640541, acc.: 53.12%] [G loss: 0.918256]\n",
      "7565 [D loss: 0.673343, acc.: 59.38%] [G loss: 0.927275]\n",
      "7566 [D loss: 0.682745, acc.: 56.25%] [G loss: 0.933351]\n",
      "7567 [D loss: 0.782405, acc.: 46.88%] [G loss: 0.889577]\n",
      "7568 [D loss: 0.662106, acc.: 65.62%] [G loss: 0.876241]\n",
      "7569 [D loss: 0.602945, acc.: 68.75%] [G loss: 0.853831]\n",
      "7570 [D loss: 0.691164, acc.: 53.12%] [G loss: 0.842663]\n",
      "7571 [D loss: 0.729454, acc.: 43.75%] [G loss: 0.844797]\n",
      "7572 [D loss: 0.694781, acc.: 53.12%] [G loss: 0.897349]\n",
      "7573 [D loss: 0.617925, acc.: 59.38%] [G loss: 0.958784]\n",
      "7574 [D loss: 0.698850, acc.: 46.88%] [G loss: 0.900104]\n",
      "7575 [D loss: 0.641691, acc.: 62.50%] [G loss: 0.912481]\n",
      "7576 [D loss: 0.623415, acc.: 71.88%] [G loss: 0.850462]\n",
      "7577 [D loss: 0.653602, acc.: 59.38%] [G loss: 0.897002]\n",
      "7578 [D loss: 0.724861, acc.: 46.88%] [G loss: 0.873276]\n",
      "7579 [D loss: 0.688574, acc.: 62.50%] [G loss: 0.951840]\n",
      "7580 [D loss: 0.631018, acc.: 71.88%] [G loss: 0.896946]\n",
      "7581 [D loss: 0.603419, acc.: 75.00%] [G loss: 0.807679]\n",
      "7582 [D loss: 0.747411, acc.: 59.38%] [G loss: 0.887750]\n",
      "7583 [D loss: 0.623245, acc.: 65.62%] [G loss: 0.881114]\n",
      "7584 [D loss: 0.666925, acc.: 56.25%] [G loss: 0.941330]\n",
      "7585 [D loss: 0.560692, acc.: 75.00%] [G loss: 0.910711]\n",
      "7586 [D loss: 0.655304, acc.: 46.88%] [G loss: 0.921514]\n",
      "7587 [D loss: 0.713100, acc.: 43.75%] [G loss: 0.823738]\n",
      "7588 [D loss: 0.585061, acc.: 75.00%] [G loss: 0.876274]\n",
      "7589 [D loss: 0.647872, acc.: 56.25%] [G loss: 0.884932]\n",
      "7590 [D loss: 0.725450, acc.: 56.25%] [G loss: 0.808538]\n",
      "7591 [D loss: 0.556772, acc.: 75.00%] [G loss: 0.960752]\n",
      "7592 [D loss: 0.650882, acc.: 62.50%] [G loss: 0.862797]\n",
      "7593 [D loss: 0.564968, acc.: 71.88%] [G loss: 0.959369]\n",
      "7594 [D loss: 0.691422, acc.: 53.12%] [G loss: 0.961547]\n",
      "7595 [D loss: 0.745865, acc.: 50.00%] [G loss: 1.012527]\n",
      "7596 [D loss: 0.684952, acc.: 59.38%] [G loss: 0.930106]\n",
      "7597 [D loss: 0.657530, acc.: 68.75%] [G loss: 0.962271]\n",
      "7598 [D loss: 0.703987, acc.: 40.62%] [G loss: 0.860522]\n",
      "7599 [D loss: 0.607782, acc.: 65.62%] [G loss: 0.871506]\n",
      "7600 [D loss: 0.671908, acc.: 62.50%] [G loss: 0.908526]\n",
      "7601 [D loss: 0.697714, acc.: 50.00%] [G loss: 0.818370]\n",
      "7602 [D loss: 0.660568, acc.: 56.25%] [G loss: 0.780719]\n",
      "7603 [D loss: 0.695303, acc.: 53.12%] [G loss: 0.812766]\n",
      "7604 [D loss: 0.694326, acc.: 56.25%] [G loss: 0.887720]\n",
      "7605 [D loss: 0.580995, acc.: 68.75%] [G loss: 0.812502]\n",
      "7606 [D loss: 0.589679, acc.: 78.12%] [G loss: 0.842187]\n",
      "7607 [D loss: 0.669258, acc.: 59.38%] [G loss: 0.827325]\n",
      "7608 [D loss: 0.656253, acc.: 65.62%] [G loss: 0.854695]\n",
      "7609 [D loss: 0.636780, acc.: 71.88%] [G loss: 0.927412]\n",
      "7610 [D loss: 0.671274, acc.: 59.38%] [G loss: 0.806891]\n",
      "7611 [D loss: 0.715605, acc.: 56.25%] [G loss: 0.817690]\n",
      "7612 [D loss: 0.670911, acc.: 62.50%] [G loss: 0.831025]\n",
      "7613 [D loss: 0.716777, acc.: 53.12%] [G loss: 0.892281]\n",
      "7614 [D loss: 0.722058, acc.: 40.62%] [G loss: 0.852575]\n",
      "7615 [D loss: 0.625613, acc.: 71.88%] [G loss: 0.871529]\n",
      "7616 [D loss: 0.631714, acc.: 62.50%] [G loss: 0.833868]\n",
      "7617 [D loss: 0.730807, acc.: 53.12%] [G loss: 0.867484]\n",
      "7618 [D loss: 0.605604, acc.: 65.62%] [G loss: 0.929657]\n",
      "7619 [D loss: 0.661114, acc.: 56.25%] [G loss: 0.947318]\n",
      "7620 [D loss: 0.736102, acc.: 43.75%] [G loss: 0.834325]\n",
      "7621 [D loss: 0.709466, acc.: 59.38%] [G loss: 0.828584]\n",
      "7622 [D loss: 0.645170, acc.: 56.25%] [G loss: 0.817654]\n",
      "7623 [D loss: 0.771678, acc.: 43.75%] [G loss: 0.874051]\n",
      "7624 [D loss: 0.680772, acc.: 53.12%] [G loss: 0.870168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7625 [D loss: 0.657602, acc.: 62.50%] [G loss: 0.868284]\n",
      "7626 [D loss: 0.749913, acc.: 50.00%] [G loss: 0.895753]\n",
      "7627 [D loss: 0.735723, acc.: 43.75%] [G loss: 0.900323]\n",
      "7628 [D loss: 0.680575, acc.: 59.38%] [G loss: 0.817938]\n",
      "7629 [D loss: 0.652848, acc.: 62.50%] [G loss: 0.867658]\n",
      "7630 [D loss: 0.541633, acc.: 75.00%] [G loss: 0.965110]\n",
      "7631 [D loss: 0.656475, acc.: 50.00%] [G loss: 0.890618]\n",
      "7632 [D loss: 0.652205, acc.: 59.38%] [G loss: 0.926028]\n",
      "7633 [D loss: 0.696632, acc.: 62.50%] [G loss: 0.888552]\n",
      "7634 [D loss: 0.699875, acc.: 62.50%] [G loss: 0.824429]\n",
      "7635 [D loss: 0.586935, acc.: 81.25%] [G loss: 0.782031]\n",
      "7636 [D loss: 0.720974, acc.: 56.25%] [G loss: 0.804473]\n",
      "7637 [D loss: 0.621517, acc.: 62.50%] [G loss: 0.876703]\n",
      "7638 [D loss: 0.720519, acc.: 53.12%] [G loss: 0.793719]\n",
      "7639 [D loss: 0.603991, acc.: 56.25%] [G loss: 0.820041]\n",
      "7640 [D loss: 0.598492, acc.: 68.75%] [G loss: 0.849591]\n",
      "7641 [D loss: 0.693686, acc.: 56.25%] [G loss: 0.874124]\n",
      "7642 [D loss: 0.695062, acc.: 56.25%] [G loss: 0.747274]\n",
      "7643 [D loss: 0.707191, acc.: 50.00%] [G loss: 0.817455]\n",
      "7644 [D loss: 0.661062, acc.: 68.75%] [G loss: 0.888128]\n",
      "7645 [D loss: 0.575876, acc.: 71.88%] [G loss: 0.837742]\n",
      "7646 [D loss: 0.636721, acc.: 65.62%] [G loss: 0.852839]\n",
      "7647 [D loss: 0.652386, acc.: 56.25%] [G loss: 0.853492]\n",
      "7648 [D loss: 0.625545, acc.: 62.50%] [G loss: 0.970361]\n",
      "7649 [D loss: 0.675291, acc.: 62.50%] [G loss: 0.869617]\n",
      "7650 [D loss: 0.598773, acc.: 62.50%] [G loss: 0.890223]\n",
      "7651 [D loss: 0.647602, acc.: 62.50%] [G loss: 0.842026]\n",
      "7652 [D loss: 0.602054, acc.: 68.75%] [G loss: 0.850945]\n",
      "7653 [D loss: 0.802200, acc.: 50.00%] [G loss: 0.965010]\n",
      "7654 [D loss: 0.738245, acc.: 56.25%] [G loss: 0.864599]\n",
      "7655 [D loss: 0.717805, acc.: 53.12%] [G loss: 0.861016]\n",
      "7656 [D loss: 0.703592, acc.: 59.38%] [G loss: 0.867742]\n",
      "7657 [D loss: 0.613308, acc.: 68.75%] [G loss: 0.872940]\n",
      "7658 [D loss: 0.626845, acc.: 59.38%] [G loss: 0.875054]\n",
      "7659 [D loss: 0.685596, acc.: 53.12%] [G loss: 0.888880]\n",
      "7660 [D loss: 0.624899, acc.: 62.50%] [G loss: 0.976299]\n",
      "7661 [D loss: 0.701571, acc.: 43.75%] [G loss: 0.913124]\n",
      "7662 [D loss: 0.694534, acc.: 53.12%] [G loss: 0.835315]\n",
      "7663 [D loss: 0.745149, acc.: 50.00%] [G loss: 0.872606]\n",
      "7664 [D loss: 0.661384, acc.: 59.38%] [G loss: 0.931048]\n",
      "7665 [D loss: 0.615540, acc.: 59.38%] [G loss: 0.834588]\n",
      "7666 [D loss: 0.671411, acc.: 53.12%] [G loss: 0.750018]\n",
      "7667 [D loss: 0.697340, acc.: 56.25%] [G loss: 0.802201]\n",
      "7668 [D loss: 0.655640, acc.: 53.12%] [G loss: 0.840230]\n",
      "7669 [D loss: 0.773612, acc.: 40.62%] [G loss: 0.818525]\n",
      "7670 [D loss: 0.664390, acc.: 59.38%] [G loss: 0.817765]\n",
      "7671 [D loss: 0.626206, acc.: 75.00%] [G loss: 0.814711]\n",
      "7672 [D loss: 0.643264, acc.: 65.62%] [G loss: 0.797152]\n",
      "7673 [D loss: 0.625751, acc.: 65.62%] [G loss: 0.918293]\n",
      "7674 [D loss: 0.668340, acc.: 59.38%] [G loss: 0.896650]\n",
      "7675 [D loss: 0.734857, acc.: 59.38%] [G loss: 0.853592]\n",
      "7676 [D loss: 0.777566, acc.: 34.38%] [G loss: 0.936034]\n",
      "7677 [D loss: 0.631431, acc.: 68.75%] [G loss: 0.864564]\n",
      "7678 [D loss: 0.672134, acc.: 56.25%] [G loss: 0.971920]\n",
      "7679 [D loss: 0.631175, acc.: 65.62%] [G loss: 0.919449]\n",
      "7680 [D loss: 0.699725, acc.: 53.12%] [G loss: 0.844143]\n",
      "7681 [D loss: 0.707163, acc.: 50.00%] [G loss: 0.843621]\n",
      "7682 [D loss: 0.723368, acc.: 59.38%] [G loss: 0.849824]\n",
      "7683 [D loss: 0.758230, acc.: 40.62%] [G loss: 0.815224]\n",
      "7684 [D loss: 0.616826, acc.: 65.62%] [G loss: 0.813735]\n",
      "7685 [D loss: 0.594477, acc.: 62.50%] [G loss: 0.874261]\n",
      "7686 [D loss: 0.785984, acc.: 50.00%] [G loss: 0.793507]\n",
      "7687 [D loss: 0.743278, acc.: 43.75%] [G loss: 0.841120]\n",
      "7688 [D loss: 0.668074, acc.: 59.38%] [G loss: 0.794058]\n",
      "7689 [D loss: 0.600751, acc.: 78.12%] [G loss: 0.884151]\n",
      "7690 [D loss: 0.699056, acc.: 59.38%] [G loss: 0.780458]\n",
      "7691 [D loss: 0.779722, acc.: 34.38%] [G loss: 0.843961]\n",
      "7692 [D loss: 0.760932, acc.: 46.88%] [G loss: 0.797443]\n",
      "7693 [D loss: 0.580597, acc.: 68.75%] [G loss: 0.847688]\n",
      "7694 [D loss: 0.695496, acc.: 53.12%] [G loss: 0.886246]\n",
      "7695 [D loss: 0.738838, acc.: 43.75%] [G loss: 0.876802]\n",
      "7696 [D loss: 0.623779, acc.: 62.50%] [G loss: 0.818774]\n",
      "7697 [D loss: 0.642742, acc.: 65.62%] [G loss: 0.877424]\n",
      "7698 [D loss: 0.679848, acc.: 59.38%] [G loss: 0.892922]\n",
      "7699 [D loss: 0.588789, acc.: 68.75%] [G loss: 0.870909]\n",
      "7700 [D loss: 0.719377, acc.: 53.12%] [G loss: 0.930806]\n",
      "7701 [D loss: 0.648458, acc.: 62.50%] [G loss: 0.909293]\n",
      "7702 [D loss: 0.618168, acc.: 59.38%] [G loss: 0.952132]\n",
      "7703 [D loss: 0.737841, acc.: 50.00%] [G loss: 0.937187]\n",
      "7704 [D loss: 0.683294, acc.: 56.25%] [G loss: 0.829176]\n",
      "7705 [D loss: 0.668257, acc.: 50.00%] [G loss: 0.858334]\n",
      "7706 [D loss: 0.568247, acc.: 68.75%] [G loss: 0.833063]\n",
      "7707 [D loss: 0.634468, acc.: 65.62%] [G loss: 0.894966]\n",
      "7708 [D loss: 0.742004, acc.: 46.88%] [G loss: 0.873065]\n",
      "7709 [D loss: 0.594138, acc.: 65.62%] [G loss: 0.883799]\n",
      "7710 [D loss: 0.760157, acc.: 34.38%] [G loss: 0.920912]\n",
      "7711 [D loss: 0.608336, acc.: 62.50%] [G loss: 0.890356]\n",
      "7712 [D loss: 0.649001, acc.: 68.75%] [G loss: 0.930891]\n",
      "7713 [D loss: 0.704446, acc.: 46.88%] [G loss: 0.959379]\n",
      "7714 [D loss: 0.565493, acc.: 75.00%] [G loss: 0.953922]\n",
      "7715 [D loss: 0.624680, acc.: 71.88%] [G loss: 0.958683]\n",
      "7716 [D loss: 0.681259, acc.: 62.50%] [G loss: 0.958099]\n",
      "7717 [D loss: 0.612643, acc.: 65.62%] [G loss: 0.944369]\n",
      "7718 [D loss: 0.652331, acc.: 65.62%] [G loss: 0.947923]\n",
      "7719 [D loss: 0.678706, acc.: 46.88%] [G loss: 0.909698]\n",
      "7720 [D loss: 0.669902, acc.: 71.88%] [G loss: 0.891641]\n",
      "7721 [D loss: 0.652573, acc.: 53.12%] [G loss: 0.946250]\n",
      "7722 [D loss: 0.647458, acc.: 53.12%] [G loss: 0.916983]\n",
      "7723 [D loss: 0.747418, acc.: 50.00%] [G loss: 0.895482]\n",
      "7724 [D loss: 0.645639, acc.: 68.75%] [G loss: 0.943779]\n",
      "7725 [D loss: 0.715503, acc.: 56.25%] [G loss: 0.971474]\n",
      "7726 [D loss: 0.736882, acc.: 53.12%] [G loss: 0.860037]\n",
      "7727 [D loss: 0.733770, acc.: 50.00%] [G loss: 0.974097]\n",
      "7728 [D loss: 0.706600, acc.: 59.38%] [G loss: 0.864765]\n",
      "7729 [D loss: 0.581122, acc.: 65.62%] [G loss: 0.855273]\n",
      "7730 [D loss: 0.748981, acc.: 50.00%] [G loss: 0.870715]\n",
      "7731 [D loss: 0.654941, acc.: 65.62%] [G loss: 0.802460]\n",
      "7732 [D loss: 0.722841, acc.: 59.38%] [G loss: 0.932387]\n",
      "7733 [D loss: 0.698991, acc.: 56.25%] [G loss: 0.871157]\n",
      "7734 [D loss: 0.741559, acc.: 40.62%] [G loss: 0.811404]\n",
      "7735 [D loss: 0.694368, acc.: 56.25%] [G loss: 0.809363]\n",
      "7736 [D loss: 0.694482, acc.: 62.50%] [G loss: 0.856525]\n",
      "7737 [D loss: 0.750539, acc.: 43.75%] [G loss: 0.789025]\n",
      "7738 [D loss: 0.739427, acc.: 50.00%] [G loss: 0.915432]\n",
      "7739 [D loss: 0.670806, acc.: 59.38%] [G loss: 0.850453]\n",
      "7740 [D loss: 0.682640, acc.: 59.38%] [G loss: 0.885311]\n",
      "7741 [D loss: 0.660218, acc.: 59.38%] [G loss: 0.867882]\n",
      "7742 [D loss: 0.697706, acc.: 53.12%] [G loss: 0.983946]\n",
      "7743 [D loss: 0.727576, acc.: 40.62%] [G loss: 0.880704]\n",
      "7744 [D loss: 0.657017, acc.: 68.75%] [G loss: 0.880155]\n",
      "7745 [D loss: 0.689399, acc.: 59.38%] [G loss: 0.867809]\n",
      "7746 [D loss: 0.663099, acc.: 62.50%] [G loss: 0.894076]\n",
      "7747 [D loss: 0.634244, acc.: 68.75%] [G loss: 0.964460]\n",
      "7748 [D loss: 0.668153, acc.: 62.50%] [G loss: 0.857692]\n",
      "7749 [D loss: 0.662970, acc.: 53.12%] [G loss: 0.792267]\n",
      "7750 [D loss: 0.651406, acc.: 53.12%] [G loss: 0.837091]\n",
      "7751 [D loss: 0.660586, acc.: 59.38%] [G loss: 0.894837]\n",
      "7752 [D loss: 0.620351, acc.: 68.75%] [G loss: 0.825620]\n",
      "7753 [D loss: 0.737722, acc.: 50.00%] [G loss: 0.840617]\n",
      "7754 [D loss: 0.647794, acc.: 65.62%] [G loss: 0.924375]\n",
      "7755 [D loss: 0.630527, acc.: 59.38%] [G loss: 0.959063]\n",
      "7756 [D loss: 0.714195, acc.: 62.50%] [G loss: 0.875709]\n",
      "7757 [D loss: 0.679198, acc.: 59.38%] [G loss: 0.847151]\n",
      "7758 [D loss: 0.672151, acc.: 56.25%] [G loss: 0.845386]\n",
      "7759 [D loss: 0.753274, acc.: 50.00%] [G loss: 0.761162]\n",
      "7760 [D loss: 0.662752, acc.: 71.88%] [G loss: 0.803826]\n",
      "7761 [D loss: 0.617990, acc.: 62.50%] [G loss: 0.797985]\n",
      "7762 [D loss: 0.657271, acc.: 56.25%] [G loss: 0.807788]\n",
      "7763 [D loss: 0.670651, acc.: 62.50%] [G loss: 0.768351]\n",
      "7764 [D loss: 0.639265, acc.: 71.88%] [G loss: 0.819337]\n",
      "7765 [D loss: 0.680412, acc.: 56.25%] [G loss: 0.813369]\n",
      "7766 [D loss: 0.589802, acc.: 62.50%] [G loss: 0.892326]\n",
      "7767 [D loss: 0.681682, acc.: 59.38%] [G loss: 0.873419]\n",
      "7768 [D loss: 0.601060, acc.: 68.75%] [G loss: 0.780389]\n",
      "7769 [D loss: 0.673622, acc.: 56.25%] [G loss: 0.865259]\n",
      "7770 [D loss: 0.639445, acc.: 62.50%] [G loss: 0.892010]\n",
      "7771 [D loss: 0.667989, acc.: 53.12%] [G loss: 0.825893]\n",
      "7772 [D loss: 0.727065, acc.: 53.12%] [G loss: 0.869658]\n",
      "7773 [D loss: 0.725302, acc.: 46.88%] [G loss: 0.823878]\n",
      "7774 [D loss: 0.674694, acc.: 46.88%] [G loss: 0.874613]\n",
      "7775 [D loss: 0.630263, acc.: 68.75%] [G loss: 0.850681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7776 [D loss: 0.698197, acc.: 50.00%] [G loss: 0.923752]\n",
      "7777 [D loss: 0.661050, acc.: 59.38%] [G loss: 0.826252]\n",
      "7778 [D loss: 0.743586, acc.: 46.88%] [G loss: 0.859686]\n",
      "7779 [D loss: 0.680503, acc.: 59.38%] [G loss: 0.801866]\n",
      "7780 [D loss: 0.664423, acc.: 53.12%] [G loss: 0.889229]\n",
      "7781 [D loss: 0.647099, acc.: 68.75%] [G loss: 0.887597]\n",
      "7782 [D loss: 0.620298, acc.: 68.75%] [G loss: 0.884292]\n",
      "7783 [D loss: 0.638747, acc.: 59.38%] [G loss: 0.906958]\n",
      "7784 [D loss: 0.678874, acc.: 59.38%] [G loss: 0.908423]\n",
      "7785 [D loss: 0.708045, acc.: 53.12%] [G loss: 0.777949]\n",
      "7786 [D loss: 0.633986, acc.: 56.25%] [G loss: 0.852589]\n",
      "7787 [D loss: 0.687628, acc.: 53.12%] [G loss: 0.823797]\n",
      "7788 [D loss: 0.671938, acc.: 59.38%] [G loss: 0.786780]\n",
      "7789 [D loss: 0.605739, acc.: 68.75%] [G loss: 0.831210]\n",
      "7790 [D loss: 0.679997, acc.: 62.50%] [G loss: 0.793294]\n",
      "7791 [D loss: 0.715552, acc.: 46.88%] [G loss: 0.816309]\n",
      "7792 [D loss: 0.651165, acc.: 50.00%] [G loss: 0.896147]\n",
      "7793 [D loss: 0.660143, acc.: 59.38%] [G loss: 0.839901]\n",
      "7794 [D loss: 0.646154, acc.: 56.25%] [G loss: 0.852690]\n",
      "7795 [D loss: 0.724239, acc.: 50.00%] [G loss: 0.866117]\n",
      "7796 [D loss: 0.798636, acc.: 37.50%] [G loss: 0.940417]\n",
      "7797 [D loss: 0.739830, acc.: 46.88%] [G loss: 0.933377]\n",
      "7798 [D loss: 0.612998, acc.: 71.88%] [G loss: 0.947419]\n",
      "7799 [D loss: 0.711718, acc.: 53.12%] [G loss: 0.932121]\n",
      "7800 [D loss: 0.669217, acc.: 53.12%] [G loss: 1.008498]\n",
      "7801 [D loss: 0.660739, acc.: 65.62%] [G loss: 0.903647]\n",
      "7802 [D loss: 0.644753, acc.: 65.62%] [G loss: 0.962092]\n",
      "7803 [D loss: 0.705779, acc.: 53.12%] [G loss: 0.886762]\n",
      "7804 [D loss: 0.699188, acc.: 59.38%] [G loss: 0.897232]\n",
      "7805 [D loss: 0.654328, acc.: 56.25%] [G loss: 0.818582]\n",
      "7806 [D loss: 0.703981, acc.: 59.38%] [G loss: 0.823179]\n",
      "7807 [D loss: 0.648089, acc.: 59.38%] [G loss: 0.853122]\n",
      "7808 [D loss: 0.665529, acc.: 59.38%] [G loss: 0.871455]\n",
      "7809 [D loss: 0.676652, acc.: 56.25%] [G loss: 0.872594]\n",
      "7810 [D loss: 0.566319, acc.: 78.12%] [G loss: 0.860985]\n",
      "7811 [D loss: 0.627528, acc.: 68.75%] [G loss: 0.861108]\n",
      "7812 [D loss: 0.697765, acc.: 50.00%] [G loss: 0.791423]\n",
      "7813 [D loss: 0.750840, acc.: 43.75%] [G loss: 0.924362]\n",
      "7814 [D loss: 0.703085, acc.: 50.00%] [G loss: 0.827549]\n",
      "7815 [D loss: 0.630593, acc.: 62.50%] [G loss: 0.935229]\n",
      "7816 [D loss: 0.714481, acc.: 53.12%] [G loss: 0.896180]\n",
      "7817 [D loss: 0.748985, acc.: 53.12%] [G loss: 0.835176]\n",
      "7818 [D loss: 0.666809, acc.: 56.25%] [G loss: 0.847358]\n",
      "7819 [D loss: 0.644259, acc.: 65.62%] [G loss: 0.832801]\n",
      "7820 [D loss: 0.689570, acc.: 53.12%] [G loss: 0.896004]\n",
      "7821 [D loss: 0.688119, acc.: 65.62%] [G loss: 0.876601]\n",
      "7822 [D loss: 0.581908, acc.: 65.62%] [G loss: 0.924742]\n",
      "7823 [D loss: 0.689295, acc.: 56.25%] [G loss: 0.951741]\n",
      "7824 [D loss: 0.630900, acc.: 68.75%] [G loss: 0.785604]\n",
      "7825 [D loss: 0.726715, acc.: 46.88%] [G loss: 0.899342]\n",
      "7826 [D loss: 0.638608, acc.: 56.25%] [G loss: 0.847802]\n",
      "7827 [D loss: 0.705709, acc.: 59.38%] [G loss: 0.894617]\n",
      "7828 [D loss: 0.667709, acc.: 59.38%] [G loss: 0.858421]\n",
      "7829 [D loss: 0.819832, acc.: 43.75%] [G loss: 0.798124]\n",
      "7830 [D loss: 0.651647, acc.: 62.50%] [G loss: 0.837771]\n",
      "7831 [D loss: 0.748426, acc.: 46.88%] [G loss: 0.859517]\n",
      "7832 [D loss: 0.698950, acc.: 43.75%] [G loss: 0.923209]\n",
      "7833 [D loss: 0.655227, acc.: 59.38%] [G loss: 0.937953]\n",
      "7834 [D loss: 0.580067, acc.: 84.38%] [G loss: 0.924323]\n",
      "7835 [D loss: 0.664116, acc.: 59.38%] [G loss: 0.894917]\n",
      "7836 [D loss: 0.704582, acc.: 50.00%] [G loss: 0.918459]\n",
      "7837 [D loss: 0.685906, acc.: 53.12%] [G loss: 0.923451]\n",
      "7838 [D loss: 0.601298, acc.: 71.88%] [G loss: 0.923891]\n",
      "7839 [D loss: 0.679668, acc.: 62.50%] [G loss: 0.875109]\n",
      "7840 [D loss: 0.654176, acc.: 62.50%] [G loss: 0.845293]\n",
      "7841 [D loss: 0.706214, acc.: 53.12%] [G loss: 0.921115]\n",
      "7842 [D loss: 0.503851, acc.: 90.62%] [G loss: 0.869967]\n",
      "7843 [D loss: 0.663628, acc.: 46.88%] [G loss: 0.978660]\n",
      "7844 [D loss: 0.689065, acc.: 59.38%] [G loss: 0.915953]\n",
      "7845 [D loss: 0.592286, acc.: 71.88%] [G loss: 0.905587]\n",
      "7846 [D loss: 0.722868, acc.: 53.12%] [G loss: 0.998551]\n",
      "7847 [D loss: 0.685636, acc.: 53.12%] [G loss: 0.887921]\n",
      "7848 [D loss: 0.726804, acc.: 53.12%] [G loss: 0.993273]\n",
      "7849 [D loss: 0.752844, acc.: 53.12%] [G loss: 0.910797]\n",
      "7850 [D loss: 0.656363, acc.: 62.50%] [G loss: 0.935847]\n",
      "7851 [D loss: 0.678409, acc.: 59.38%] [G loss: 0.851345]\n",
      "7852 [D loss: 0.648447, acc.: 62.50%] [G loss: 0.881593]\n",
      "7853 [D loss: 0.715169, acc.: 50.00%] [G loss: 0.903373]\n",
      "7854 [D loss: 0.670847, acc.: 46.88%] [G loss: 0.847197]\n",
      "7855 [D loss: 0.700421, acc.: 46.88%] [G loss: 0.840098]\n",
      "7856 [D loss: 0.705471, acc.: 53.12%] [G loss: 0.838174]\n",
      "7857 [D loss: 0.554629, acc.: 84.38%] [G loss: 0.904893]\n",
      "7858 [D loss: 0.664476, acc.: 53.12%] [G loss: 0.896743]\n",
      "7859 [D loss: 0.572096, acc.: 84.38%] [G loss: 0.891142]\n",
      "7860 [D loss: 0.748685, acc.: 53.12%] [G loss: 0.839002]\n",
      "7861 [D loss: 0.660915, acc.: 62.50%] [G loss: 0.885020]\n",
      "7862 [D loss: 0.726182, acc.: 53.12%] [G loss: 0.855206]\n",
      "7863 [D loss: 0.766335, acc.: 40.62%] [G loss: 0.844969]\n",
      "7864 [D loss: 0.632433, acc.: 68.75%] [G loss: 0.808403]\n",
      "7865 [D loss: 0.635700, acc.: 59.38%] [G loss: 0.890228]\n",
      "7866 [D loss: 0.727710, acc.: 46.88%] [G loss: 0.841169]\n",
      "7867 [D loss: 0.767790, acc.: 50.00%] [G loss: 0.847630]\n",
      "7868 [D loss: 0.649077, acc.: 53.12%] [G loss: 0.882204]\n",
      "7869 [D loss: 0.680319, acc.: 62.50%] [G loss: 0.842619]\n",
      "7870 [D loss: 0.650474, acc.: 59.38%] [G loss: 0.887695]\n",
      "7871 [D loss: 0.605556, acc.: 62.50%] [G loss: 0.786891]\n",
      "7872 [D loss: 0.721493, acc.: 50.00%] [G loss: 0.788671]\n",
      "7873 [D loss: 0.634912, acc.: 62.50%] [G loss: 0.863285]\n",
      "7874 [D loss: 0.603850, acc.: 65.62%] [G loss: 0.895705]\n",
      "7875 [D loss: 0.747913, acc.: 53.12%] [G loss: 0.868524]\n",
      "7876 [D loss: 0.639172, acc.: 59.38%] [G loss: 0.810255]\n",
      "7877 [D loss: 0.667081, acc.: 68.75%] [G loss: 0.888637]\n",
      "7878 [D loss: 0.820915, acc.: 37.50%] [G loss: 0.871403]\n",
      "7879 [D loss: 0.588607, acc.: 71.88%] [G loss: 0.822443]\n",
      "7880 [D loss: 0.645134, acc.: 68.75%] [G loss: 0.817366]\n",
      "7881 [D loss: 0.719327, acc.: 50.00%] [G loss: 0.909897]\n",
      "7882 [D loss: 0.680888, acc.: 56.25%] [G loss: 0.885013]\n",
      "7883 [D loss: 0.689893, acc.: 56.25%] [G loss: 0.816678]\n",
      "7884 [D loss: 0.775074, acc.: 50.00%] [G loss: 0.893241]\n",
      "7885 [D loss: 0.682199, acc.: 56.25%] [G loss: 0.913035]\n",
      "7886 [D loss: 0.705986, acc.: 53.12%] [G loss: 0.877577]\n",
      "7887 [D loss: 0.678325, acc.: 62.50%] [G loss: 0.816985]\n",
      "7888 [D loss: 0.610110, acc.: 68.75%] [G loss: 0.874326]\n",
      "7889 [D loss: 0.620915, acc.: 62.50%] [G loss: 1.012909]\n",
      "7890 [D loss: 0.695766, acc.: 65.62%] [G loss: 0.957136]\n",
      "7891 [D loss: 0.787168, acc.: 37.50%] [G loss: 0.876335]\n",
      "7892 [D loss: 0.619688, acc.: 65.62%] [G loss: 0.896909]\n",
      "7893 [D loss: 0.614106, acc.: 71.88%] [G loss: 1.003959]\n",
      "7894 [D loss: 0.702683, acc.: 56.25%] [G loss: 0.986871]\n",
      "7895 [D loss: 0.671302, acc.: 53.12%] [G loss: 0.942781]\n",
      "7896 [D loss: 0.692096, acc.: 56.25%] [G loss: 0.902602]\n",
      "7897 [D loss: 0.727022, acc.: 53.12%] [G loss: 0.833874]\n",
      "7898 [D loss: 0.669489, acc.: 65.62%] [G loss: 0.925228]\n",
      "7899 [D loss: 0.676478, acc.: 56.25%] [G loss: 0.933516]\n",
      "7900 [D loss: 0.618621, acc.: 71.88%] [G loss: 0.975909]\n",
      "7901 [D loss: 0.731666, acc.: 59.38%] [G loss: 0.946801]\n",
      "7902 [D loss: 0.768170, acc.: 53.12%] [G loss: 0.843465]\n",
      "7903 [D loss: 0.653413, acc.: 53.12%] [G loss: 0.882578]\n",
      "7904 [D loss: 0.634697, acc.: 59.38%] [G loss: 0.914218]\n",
      "7905 [D loss: 0.586197, acc.: 71.88%] [G loss: 0.925497]\n",
      "7906 [D loss: 0.646738, acc.: 62.50%] [G loss: 0.909701]\n",
      "7907 [D loss: 0.618186, acc.: 62.50%] [G loss: 0.825407]\n",
      "7908 [D loss: 0.643766, acc.: 75.00%] [G loss: 0.840711]\n",
      "7909 [D loss: 0.671175, acc.: 62.50%] [G loss: 0.825547]\n",
      "7910 [D loss: 0.696328, acc.: 50.00%] [G loss: 0.818564]\n",
      "7911 [D loss: 0.620550, acc.: 59.38%] [G loss: 0.783564]\n",
      "7912 [D loss: 0.570658, acc.: 65.62%] [G loss: 0.890848]\n",
      "7913 [D loss: 0.645710, acc.: 65.62%] [G loss: 0.925523]\n",
      "7914 [D loss: 0.748842, acc.: 46.88%] [G loss: 0.910413]\n",
      "7915 [D loss: 0.668748, acc.: 65.62%] [G loss: 0.833146]\n",
      "7916 [D loss: 0.711380, acc.: 56.25%] [G loss: 0.863930]\n",
      "7917 [D loss: 0.699841, acc.: 53.12%] [G loss: 0.823657]\n",
      "7918 [D loss: 0.615565, acc.: 68.75%] [G loss: 0.805724]\n",
      "7919 [D loss: 0.658815, acc.: 53.12%] [G loss: 0.867109]\n",
      "7920 [D loss: 0.670699, acc.: 68.75%] [G loss: 0.924926]\n",
      "7921 [D loss: 0.553679, acc.: 78.12%] [G loss: 1.048664]\n",
      "7922 [D loss: 0.600516, acc.: 71.88%] [G loss: 1.065171]\n",
      "7923 [D loss: 0.657278, acc.: 62.50%] [G loss: 0.870539]\n",
      "7924 [D loss: 0.713383, acc.: 50.00%] [G loss: 0.790337]\n",
      "7925 [D loss: 0.721376, acc.: 53.12%] [G loss: 0.791925]\n",
      "7926 [D loss: 0.686516, acc.: 46.88%] [G loss: 0.903363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7927 [D loss: 0.614699, acc.: 65.62%] [G loss: 0.944937]\n",
      "7928 [D loss: 0.609730, acc.: 68.75%] [G loss: 1.014755]\n",
      "7929 [D loss: 0.633937, acc.: 68.75%] [G loss: 0.932702]\n",
      "7930 [D loss: 0.645724, acc.: 50.00%] [G loss: 0.906538]\n",
      "7931 [D loss: 0.673835, acc.: 53.12%] [G loss: 0.988319]\n",
      "7932 [D loss: 0.648672, acc.: 65.62%] [G loss: 0.921478]\n",
      "7933 [D loss: 0.539715, acc.: 78.12%] [G loss: 0.883042]\n",
      "7934 [D loss: 0.709602, acc.: 56.25%] [G loss: 0.890223]\n",
      "7935 [D loss: 0.603824, acc.: 65.62%] [G loss: 0.856748]\n",
      "7936 [D loss: 0.624868, acc.: 62.50%] [G loss: 0.848671]\n",
      "7937 [D loss: 0.709851, acc.: 53.12%] [G loss: 0.731387]\n",
      "7938 [D loss: 0.641505, acc.: 65.62%] [G loss: 0.800043]\n",
      "7939 [D loss: 0.638315, acc.: 62.50%] [G loss: 0.836894]\n",
      "7940 [D loss: 0.681605, acc.: 59.38%] [G loss: 0.877988]\n",
      "7941 [D loss: 0.821987, acc.: 46.88%] [G loss: 0.858731]\n",
      "7942 [D loss: 0.645334, acc.: 59.38%] [G loss: 0.865979]\n",
      "7943 [D loss: 0.662124, acc.: 62.50%] [G loss: 0.822673]\n",
      "7944 [D loss: 0.654786, acc.: 59.38%] [G loss: 0.815492]\n",
      "7945 [D loss: 0.788674, acc.: 34.38%] [G loss: 0.856475]\n",
      "7946 [D loss: 0.808474, acc.: 40.62%] [G loss: 0.869043]\n",
      "7947 [D loss: 0.635313, acc.: 68.75%] [G loss: 0.840568]\n",
      "7948 [D loss: 0.592495, acc.: 68.75%] [G loss: 0.834217]\n",
      "7949 [D loss: 0.546325, acc.: 71.88%] [G loss: 0.871511]\n",
      "7950 [D loss: 0.652874, acc.: 65.62%] [G loss: 0.812098]\n",
      "7951 [D loss: 0.647043, acc.: 68.75%] [G loss: 0.891589]\n",
      "7952 [D loss: 0.639157, acc.: 59.38%] [G loss: 0.919686]\n",
      "7953 [D loss: 0.590299, acc.: 68.75%] [G loss: 0.878099]\n",
      "7954 [D loss: 0.704479, acc.: 65.62%] [G loss: 0.800827]\n",
      "7955 [D loss: 0.601194, acc.: 65.62%] [G loss: 0.874077]\n",
      "7956 [D loss: 0.703276, acc.: 56.25%] [G loss: 0.869860]\n",
      "7957 [D loss: 0.711274, acc.: 56.25%] [G loss: 0.827582]\n",
      "7958 [D loss: 0.698403, acc.: 59.38%] [G loss: 0.854407]\n",
      "7959 [D loss: 0.769580, acc.: 40.62%] [G loss: 0.841686]\n",
      "7960 [D loss: 0.615252, acc.: 59.38%] [G loss: 0.802642]\n",
      "7961 [D loss: 0.717893, acc.: 59.38%] [G loss: 0.810212]\n",
      "7962 [D loss: 0.744447, acc.: 56.25%] [G loss: 0.879595]\n",
      "7963 [D loss: 0.717194, acc.: 53.12%] [G loss: 0.843385]\n",
      "7964 [D loss: 0.714234, acc.: 56.25%] [G loss: 0.879698]\n",
      "7965 [D loss: 0.652773, acc.: 65.62%] [G loss: 0.860087]\n",
      "7966 [D loss: 0.639489, acc.: 71.88%] [G loss: 0.886246]\n",
      "7967 [D loss: 0.599965, acc.: 71.88%] [G loss: 0.902023]\n",
      "7968 [D loss: 0.709205, acc.: 46.88%] [G loss: 0.893673]\n",
      "7969 [D loss: 0.649203, acc.: 59.38%] [G loss: 0.837527]\n",
      "7970 [D loss: 0.667057, acc.: 59.38%] [G loss: 0.938580]\n",
      "7971 [D loss: 0.628024, acc.: 65.62%] [G loss: 0.810172]\n",
      "7972 [D loss: 0.618428, acc.: 71.88%] [G loss: 0.858513]\n",
      "7973 [D loss: 0.776902, acc.: 40.62%] [G loss: 0.820103]\n",
      "7974 [D loss: 0.654589, acc.: 65.62%] [G loss: 0.843801]\n",
      "7975 [D loss: 0.685147, acc.: 62.50%] [G loss: 0.792479]\n",
      "7976 [D loss: 0.698063, acc.: 56.25%] [G loss: 0.969485]\n",
      "7977 [D loss: 0.718512, acc.: 62.50%] [G loss: 0.945662]\n",
      "7978 [D loss: 0.658841, acc.: 50.00%] [G loss: 0.925033]\n",
      "7979 [D loss: 0.784188, acc.: 50.00%] [G loss: 0.951972]\n",
      "7980 [D loss: 0.648997, acc.: 65.62%] [G loss: 0.967431]\n",
      "7981 [D loss: 0.707261, acc.: 53.12%] [G loss: 0.867442]\n",
      "7982 [D loss: 0.630527, acc.: 62.50%] [G loss: 0.885474]\n",
      "7983 [D loss: 0.648809, acc.: 68.75%] [G loss: 0.825870]\n",
      "7984 [D loss: 0.656999, acc.: 62.50%] [G loss: 0.915495]\n",
      "7985 [D loss: 0.752853, acc.: 53.12%] [G loss: 0.911312]\n",
      "7986 [D loss: 0.593596, acc.: 68.75%] [G loss: 0.847381]\n",
      "7987 [D loss: 0.654568, acc.: 59.38%] [G loss: 0.807902]\n",
      "7988 [D loss: 0.723818, acc.: 50.00%] [G loss: 0.862678]\n",
      "7989 [D loss: 0.618413, acc.: 68.75%] [G loss: 0.968405]\n",
      "7990 [D loss: 0.627883, acc.: 65.62%] [G loss: 0.844169]\n",
      "7991 [D loss: 0.641005, acc.: 68.75%] [G loss: 0.864173]\n",
      "7992 [D loss: 0.718464, acc.: 56.25%] [G loss: 0.910159]\n",
      "7993 [D loss: 0.702770, acc.: 46.88%] [G loss: 0.891250]\n",
      "7994 [D loss: 0.659810, acc.: 68.75%] [G loss: 0.913418]\n",
      "7995 [D loss: 0.714355, acc.: 62.50%] [G loss: 0.836841]\n",
      "7996 [D loss: 0.705117, acc.: 62.50%] [G loss: 0.843600]\n",
      "7997 [D loss: 0.645985, acc.: 65.62%] [G loss: 0.772380]\n",
      "7998 [D loss: 0.771569, acc.: 40.62%] [G loss: 0.855959]\n",
      "7999 [D loss: 0.662049, acc.: 59.38%] [G loss: 0.843910]\n",
      "8000 [D loss: 0.723148, acc.: 53.12%] [G loss: 0.876803]\n",
      "8001 [D loss: 0.737062, acc.: 50.00%] [G loss: 0.870569]\n",
      "8002 [D loss: 0.672142, acc.: 62.50%] [G loss: 0.867902]\n",
      "8003 [D loss: 0.619922, acc.: 62.50%] [G loss: 0.886347]\n",
      "8004 [D loss: 0.650540, acc.: 68.75%] [G loss: 0.920462]\n",
      "8005 [D loss: 0.672907, acc.: 46.88%] [G loss: 0.884627]\n",
      "8006 [D loss: 0.713186, acc.: 53.12%] [G loss: 0.832728]\n",
      "8007 [D loss: 0.698538, acc.: 56.25%] [G loss: 0.780535]\n",
      "8008 [D loss: 0.618485, acc.: 71.88%] [G loss: 0.965436]\n",
      "8009 [D loss: 0.656505, acc.: 59.38%] [G loss: 0.867579]\n",
      "8010 [D loss: 0.702798, acc.: 46.88%] [G loss: 0.810133]\n",
      "8011 [D loss: 0.638545, acc.: 68.75%] [G loss: 0.875830]\n",
      "8012 [D loss: 0.663963, acc.: 75.00%] [G loss: 0.848395]\n",
      "8013 [D loss: 0.749826, acc.: 43.75%] [G loss: 0.862615]\n",
      "8014 [D loss: 0.600200, acc.: 65.62%] [G loss: 0.824381]\n",
      "8015 [D loss: 0.625917, acc.: 65.62%] [G loss: 0.833664]\n",
      "8016 [D loss: 0.670494, acc.: 56.25%] [G loss: 0.840985]\n",
      "8017 [D loss: 0.611289, acc.: 68.75%] [G loss: 0.856487]\n",
      "8018 [D loss: 0.595365, acc.: 71.88%] [G loss: 0.931618]\n",
      "8019 [D loss: 0.632166, acc.: 65.62%] [G loss: 0.885943]\n",
      "8020 [D loss: 0.640044, acc.: 62.50%] [G loss: 0.944143]\n",
      "8021 [D loss: 0.764699, acc.: 46.88%] [G loss: 0.887538]\n",
      "8022 [D loss: 0.657871, acc.: 68.75%] [G loss: 0.928388]\n",
      "8023 [D loss: 0.722250, acc.: 50.00%] [G loss: 0.811902]\n",
      "8024 [D loss: 0.641946, acc.: 59.38%] [G loss: 0.838073]\n",
      "8025 [D loss: 0.653883, acc.: 65.62%] [G loss: 0.851873]\n",
      "8026 [D loss: 0.676271, acc.: 56.25%] [G loss: 0.901360]\n",
      "8027 [D loss: 0.660018, acc.: 50.00%] [G loss: 0.942764]\n",
      "8028 [D loss: 0.664570, acc.: 56.25%] [G loss: 0.859548]\n",
      "8029 [D loss: 0.715470, acc.: 46.88%] [G loss: 0.914630]\n",
      "8030 [D loss: 0.767516, acc.: 43.75%] [G loss: 0.814398]\n",
      "8031 [D loss: 0.624864, acc.: 56.25%] [G loss: 0.864951]\n",
      "8032 [D loss: 0.634381, acc.: 62.50%] [G loss: 0.831355]\n",
      "8033 [D loss: 0.691741, acc.: 56.25%] [G loss: 0.844277]\n",
      "8034 [D loss: 0.673227, acc.: 68.75%] [G loss: 0.918833]\n",
      "8035 [D loss: 0.554535, acc.: 75.00%] [G loss: 0.930786]\n",
      "8036 [D loss: 0.711009, acc.: 56.25%] [G loss: 0.869911]\n",
      "8037 [D loss: 0.711092, acc.: 62.50%] [G loss: 0.868082]\n",
      "8038 [D loss: 0.652459, acc.: 56.25%] [G loss: 0.856375]\n",
      "8039 [D loss: 0.622556, acc.: 68.75%] [G loss: 0.880815]\n",
      "8040 [D loss: 0.710310, acc.: 43.75%] [G loss: 0.866818]\n",
      "8041 [D loss: 0.634044, acc.: 59.38%] [G loss: 0.918426]\n",
      "8042 [D loss: 0.689940, acc.: 68.75%] [G loss: 0.856924]\n",
      "8043 [D loss: 0.653273, acc.: 53.12%] [G loss: 0.957398]\n",
      "8044 [D loss: 0.714405, acc.: 53.12%] [G loss: 0.963197]\n",
      "8045 [D loss: 0.763001, acc.: 50.00%] [G loss: 0.925618]\n",
      "8046 [D loss: 0.667627, acc.: 50.00%] [G loss: 0.888763]\n",
      "8047 [D loss: 0.686966, acc.: 59.38%] [G loss: 0.907959]\n",
      "8048 [D loss: 0.709260, acc.: 50.00%] [G loss: 0.892955]\n",
      "8049 [D loss: 0.707404, acc.: 46.88%] [G loss: 0.902556]\n",
      "8050 [D loss: 0.670071, acc.: 68.75%] [G loss: 0.827572]\n",
      "8051 [D loss: 0.649075, acc.: 65.62%] [G loss: 0.865911]\n",
      "8052 [D loss: 0.627753, acc.: 62.50%] [G loss: 0.803055]\n",
      "8053 [D loss: 0.594799, acc.: 78.12%] [G loss: 0.876296]\n",
      "8054 [D loss: 0.715667, acc.: 53.12%] [G loss: 0.788020]\n",
      "8055 [D loss: 0.791327, acc.: 31.25%] [G loss: 0.826000]\n",
      "8056 [D loss: 0.587868, acc.: 71.88%] [G loss: 0.880070]\n",
      "8057 [D loss: 0.713188, acc.: 53.12%] [G loss: 0.797296]\n",
      "8058 [D loss: 0.676263, acc.: 46.88%] [G loss: 0.765864]\n",
      "8059 [D loss: 0.610804, acc.: 65.62%] [G loss: 0.746519]\n",
      "8060 [D loss: 0.710901, acc.: 50.00%] [G loss: 0.858819]\n",
      "8061 [D loss: 0.718720, acc.: 56.25%] [G loss: 0.852984]\n",
      "8062 [D loss: 0.722324, acc.: 53.12%] [G loss: 0.854088]\n",
      "8063 [D loss: 0.677361, acc.: 50.00%] [G loss: 0.827387]\n",
      "8064 [D loss: 0.661171, acc.: 62.50%] [G loss: 0.846212]\n",
      "8065 [D loss: 0.730703, acc.: 43.75%] [G loss: 0.800927]\n",
      "8066 [D loss: 0.763665, acc.: 50.00%] [G loss: 0.893606]\n",
      "8067 [D loss: 0.635939, acc.: 56.25%] [G loss: 0.832140]\n",
      "8068 [D loss: 0.748543, acc.: 46.88%] [G loss: 0.823448]\n",
      "8069 [D loss: 0.643073, acc.: 68.75%] [G loss: 0.841286]\n",
      "8070 [D loss: 0.765136, acc.: 40.62%] [G loss: 0.886410]\n",
      "8071 [D loss: 0.688585, acc.: 68.75%] [G loss: 0.907354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8072 [D loss: 0.635558, acc.: 62.50%] [G loss: 0.847829]\n",
      "8073 [D loss: 0.597380, acc.: 71.88%] [G loss: 0.857304]\n",
      "8074 [D loss: 0.714180, acc.: 56.25%] [G loss: 0.890126]\n",
      "8075 [D loss: 0.613226, acc.: 68.75%] [G loss: 0.854878]\n",
      "8076 [D loss: 0.656117, acc.: 62.50%] [G loss: 0.866882]\n",
      "8077 [D loss: 0.657474, acc.: 59.38%] [G loss: 0.910407]\n",
      "8078 [D loss: 0.617220, acc.: 65.62%] [G loss: 0.764606]\n",
      "8079 [D loss: 0.808487, acc.: 53.12%] [G loss: 0.864829]\n",
      "8080 [D loss: 0.736435, acc.: 46.88%] [G loss: 0.868326]\n",
      "8081 [D loss: 0.576876, acc.: 75.00%] [G loss: 0.896226]\n",
      "8082 [D loss: 0.737762, acc.: 40.62%] [G loss: 0.895066]\n",
      "8083 [D loss: 0.636798, acc.: 62.50%] [G loss: 0.933098]\n",
      "8084 [D loss: 0.564479, acc.: 68.75%] [G loss: 0.945597]\n",
      "8085 [D loss: 0.709907, acc.: 59.38%] [G loss: 0.911252]\n",
      "8086 [D loss: 0.690825, acc.: 62.50%] [G loss: 0.945763]\n",
      "8087 [D loss: 0.689435, acc.: 65.62%] [G loss: 0.829240]\n",
      "8088 [D loss: 0.679826, acc.: 65.62%] [G loss: 0.825186]\n",
      "8089 [D loss: 0.774046, acc.: 34.38%] [G loss: 0.842324]\n",
      "8090 [D loss: 0.613978, acc.: 68.75%] [G loss: 0.881830]\n",
      "8091 [D loss: 0.643630, acc.: 65.62%] [G loss: 0.893475]\n",
      "8092 [D loss: 0.594027, acc.: 71.88%] [G loss: 0.808486]\n",
      "8093 [D loss: 0.602761, acc.: 78.12%] [G loss: 0.855217]\n",
      "8094 [D loss: 0.694441, acc.: 53.12%] [G loss: 0.825184]\n",
      "8095 [D loss: 0.656187, acc.: 53.12%] [G loss: 0.831860]\n",
      "8096 [D loss: 0.600570, acc.: 62.50%] [G loss: 0.887312]\n",
      "8097 [D loss: 0.650240, acc.: 65.62%] [G loss: 0.844560]\n",
      "8098 [D loss: 0.684553, acc.: 53.12%] [G loss: 0.804916]\n",
      "8099 [D loss: 0.731243, acc.: 46.88%] [G loss: 0.852457]\n",
      "8100 [D loss: 0.725843, acc.: 65.62%] [G loss: 0.868600]\n",
      "8101 [D loss: 0.659744, acc.: 65.62%] [G loss: 0.863597]\n",
      "8102 [D loss: 0.633095, acc.: 65.62%] [G loss: 0.832831]\n",
      "8103 [D loss: 0.621503, acc.: 75.00%] [G loss: 0.848430]\n",
      "8104 [D loss: 0.724302, acc.: 46.88%] [G loss: 0.826588]\n",
      "8105 [D loss: 0.717318, acc.: 59.38%] [G loss: 0.904030]\n",
      "8106 [D loss: 0.672812, acc.: 56.25%] [G loss: 0.865821]\n",
      "8107 [D loss: 0.669903, acc.: 62.50%] [G loss: 0.759291]\n",
      "8108 [D loss: 0.604217, acc.: 68.75%] [G loss: 0.822052]\n",
      "8109 [D loss: 0.686566, acc.: 50.00%] [G loss: 0.878900]\n",
      "8110 [D loss: 0.656137, acc.: 59.38%] [G loss: 0.870830]\n",
      "8111 [D loss: 0.747366, acc.: 59.38%] [G loss: 0.879093]\n",
      "8112 [D loss: 0.696110, acc.: 53.12%] [G loss: 0.945633]\n",
      "8113 [D loss: 0.704884, acc.: 50.00%] [G loss: 0.907145]\n",
      "8114 [D loss: 0.672604, acc.: 53.12%] [G loss: 0.937017]\n",
      "8115 [D loss: 0.705297, acc.: 50.00%] [G loss: 0.951121]\n",
      "8116 [D loss: 0.719259, acc.: 43.75%] [G loss: 0.818790]\n",
      "8117 [D loss: 0.641342, acc.: 68.75%] [G loss: 0.889329]\n",
      "8118 [D loss: 0.651421, acc.: 59.38%] [G loss: 0.854014]\n",
      "8119 [D loss: 0.680891, acc.: 53.12%] [G loss: 0.866426]\n",
      "8120 [D loss: 0.695676, acc.: 50.00%] [G loss: 0.889042]\n",
      "8121 [D loss: 0.649753, acc.: 62.50%] [G loss: 0.871424]\n",
      "8122 [D loss: 0.615136, acc.: 71.88%] [G loss: 0.940537]\n",
      "8123 [D loss: 0.655625, acc.: 65.62%] [G loss: 0.944776]\n",
      "8124 [D loss: 0.681961, acc.: 71.88%] [G loss: 0.931983]\n",
      "8125 [D loss: 0.705430, acc.: 62.50%] [G loss: 1.040637]\n",
      "8126 [D loss: 0.696167, acc.: 56.25%] [G loss: 0.958166]\n",
      "8127 [D loss: 0.656808, acc.: 59.38%] [G loss: 0.947688]\n",
      "8128 [D loss: 0.581527, acc.: 62.50%] [G loss: 0.866011]\n",
      "8129 [D loss: 0.664069, acc.: 62.50%] [G loss: 0.755597]\n",
      "8130 [D loss: 0.624123, acc.: 62.50%] [G loss: 0.802314]\n",
      "8131 [D loss: 0.699187, acc.: 53.12%] [G loss: 0.787956]\n",
      "8132 [D loss: 0.651038, acc.: 56.25%] [G loss: 0.900291]\n",
      "8133 [D loss: 0.684602, acc.: 59.38%] [G loss: 0.861445]\n",
      "8134 [D loss: 0.632601, acc.: 65.62%] [G loss: 0.978082]\n",
      "8135 [D loss: 0.650470, acc.: 53.12%] [G loss: 0.904447]\n",
      "8136 [D loss: 0.633076, acc.: 62.50%] [G loss: 0.810487]\n",
      "8137 [D loss: 0.693282, acc.: 56.25%] [G loss: 0.797557]\n",
      "8138 [D loss: 0.592776, acc.: 62.50%] [G loss: 0.901003]\n",
      "8139 [D loss: 0.660588, acc.: 59.38%] [G loss: 0.865429]\n",
      "8140 [D loss: 0.608332, acc.: 65.62%] [G loss: 0.836665]\n",
      "8141 [D loss: 0.628852, acc.: 65.62%] [G loss: 0.815663]\n",
      "8142 [D loss: 0.671603, acc.: 62.50%] [G loss: 0.875046]\n",
      "8143 [D loss: 0.639683, acc.: 62.50%] [G loss: 0.868269]\n",
      "8144 [D loss: 0.674986, acc.: 56.25%] [G loss: 0.830501]\n",
      "8145 [D loss: 0.655150, acc.: 68.75%] [G loss: 0.921631]\n",
      "8146 [D loss: 0.771069, acc.: 40.62%] [G loss: 0.808878]\n",
      "8147 [D loss: 0.710280, acc.: 53.12%] [G loss: 0.973336]\n",
      "8148 [D loss: 0.617477, acc.: 68.75%] [G loss: 0.853496]\n",
      "8149 [D loss: 0.748246, acc.: 50.00%] [G loss: 0.881466]\n",
      "8150 [D loss: 0.683560, acc.: 65.62%] [G loss: 0.901230]\n",
      "8151 [D loss: 0.678887, acc.: 56.25%] [G loss: 0.851143]\n",
      "8152 [D loss: 0.723148, acc.: 40.62%] [G loss: 0.881622]\n",
      "8153 [D loss: 0.780013, acc.: 50.00%] [G loss: 0.829359]\n",
      "8154 [D loss: 0.696687, acc.: 56.25%] [G loss: 0.920787]\n",
      "8155 [D loss: 0.641056, acc.: 62.50%] [G loss: 0.911620]\n",
      "8156 [D loss: 0.685611, acc.: 59.38%] [G loss: 0.876248]\n",
      "8157 [D loss: 0.716259, acc.: 53.12%] [G loss: 0.847401]\n",
      "8158 [D loss: 0.741738, acc.: 46.88%] [G loss: 0.835325]\n",
      "8159 [D loss: 0.694974, acc.: 59.38%] [G loss: 0.851548]\n",
      "8160 [D loss: 0.584322, acc.: 71.88%] [G loss: 0.897135]\n",
      "8161 [D loss: 0.579750, acc.: 68.75%] [G loss: 0.892707]\n",
      "8162 [D loss: 0.603984, acc.: 68.75%] [G loss: 0.783190]\n",
      "8163 [D loss: 0.675139, acc.: 59.38%] [G loss: 0.890450]\n",
      "8164 [D loss: 0.709123, acc.: 50.00%] [G loss: 0.852362]\n",
      "8165 [D loss: 0.703922, acc.: 53.12%] [G loss: 0.898416]\n",
      "8166 [D loss: 0.674427, acc.: 53.12%] [G loss: 0.923424]\n",
      "8167 [D loss: 0.714152, acc.: 53.12%] [G loss: 0.849322]\n",
      "8168 [D loss: 0.695860, acc.: 59.38%] [G loss: 0.778769]\n",
      "8169 [D loss: 0.682538, acc.: 53.12%] [G loss: 0.872093]\n",
      "8170 [D loss: 0.643365, acc.: 71.88%] [G loss: 0.893100]\n",
      "8171 [D loss: 0.667542, acc.: 62.50%] [G loss: 0.807964]\n",
      "8172 [D loss: 0.653880, acc.: 59.38%] [G loss: 0.817334]\n",
      "8173 [D loss: 0.637170, acc.: 71.88%] [G loss: 0.854748]\n",
      "8174 [D loss: 0.653001, acc.: 59.38%] [G loss: 0.826644]\n",
      "8175 [D loss: 0.646475, acc.: 62.50%] [G loss: 0.868230]\n",
      "8176 [D loss: 0.677100, acc.: 53.12%] [G loss: 0.791655]\n",
      "8177 [D loss: 0.655236, acc.: 59.38%] [G loss: 0.808719]\n",
      "8178 [D loss: 0.792206, acc.: 34.38%] [G loss: 0.843725]\n",
      "8179 [D loss: 0.646719, acc.: 68.75%] [G loss: 0.839464]\n",
      "8180 [D loss: 0.693246, acc.: 50.00%] [G loss: 0.867192]\n",
      "8181 [D loss: 0.598607, acc.: 65.62%] [G loss: 0.857858]\n",
      "8182 [D loss: 0.609173, acc.: 62.50%] [G loss: 0.811046]\n",
      "8183 [D loss: 0.700571, acc.: 65.62%] [G loss: 0.873791]\n",
      "8184 [D loss: 0.864811, acc.: 31.25%] [G loss: 0.803852]\n",
      "8185 [D loss: 0.660930, acc.: 59.38%] [G loss: 0.929306]\n",
      "8186 [D loss: 0.691107, acc.: 56.25%] [G loss: 0.882694]\n",
      "8187 [D loss: 0.702579, acc.: 53.12%] [G loss: 0.862665]\n",
      "8188 [D loss: 0.663498, acc.: 53.12%] [G loss: 0.901537]\n",
      "8189 [D loss: 0.628860, acc.: 56.25%] [G loss: 0.887801]\n",
      "8190 [D loss: 0.637347, acc.: 71.88%] [G loss: 0.831042]\n",
      "8191 [D loss: 0.654331, acc.: 62.50%] [G loss: 0.861123]\n",
      "8192 [D loss: 0.648860, acc.: 59.38%] [G loss: 0.923589]\n",
      "8193 [D loss: 0.654115, acc.: 68.75%] [G loss: 0.922286]\n",
      "8194 [D loss: 0.672496, acc.: 59.38%] [G loss: 0.898902]\n",
      "8195 [D loss: 0.624610, acc.: 65.62%] [G loss: 0.842676]\n",
      "8196 [D loss: 0.658805, acc.: 65.62%] [G loss: 0.817719]\n",
      "8197 [D loss: 0.648809, acc.: 59.38%] [G loss: 0.804663]\n",
      "8198 [D loss: 0.718855, acc.: 43.75%] [G loss: 0.875303]\n",
      "8199 [D loss: 0.761075, acc.: 50.00%] [G loss: 0.957608]\n",
      "8200 [D loss: 0.655240, acc.: 59.38%] [G loss: 0.901484]\n",
      "8201 [D loss: 0.666996, acc.: 62.50%] [G loss: 0.928149]\n",
      "8202 [D loss: 0.605495, acc.: 71.88%] [G loss: 0.903575]\n",
      "8203 [D loss: 0.653515, acc.: 59.38%] [G loss: 0.873699]\n",
      "8204 [D loss: 0.622385, acc.: 65.62%] [G loss: 0.912116]\n",
      "8205 [D loss: 0.600270, acc.: 65.62%] [G loss: 0.879661]\n",
      "8206 [D loss: 0.770443, acc.: 59.38%] [G loss: 0.877461]\n",
      "8207 [D loss: 0.704517, acc.: 59.38%] [G loss: 0.890489]\n",
      "8208 [D loss: 0.639166, acc.: 62.50%] [G loss: 0.945294]\n",
      "8209 [D loss: 0.785367, acc.: 43.75%] [G loss: 0.930324]\n",
      "8210 [D loss: 0.691018, acc.: 53.12%] [G loss: 0.815868]\n",
      "8211 [D loss: 0.721212, acc.: 59.38%] [G loss: 0.843470]\n",
      "8212 [D loss: 0.615445, acc.: 65.62%] [G loss: 0.833331]\n",
      "8213 [D loss: 0.684727, acc.: 56.25%] [G loss: 0.806260]\n",
      "8214 [D loss: 0.587858, acc.: 71.88%] [G loss: 0.853821]\n",
      "8215 [D loss: 0.679797, acc.: 46.88%] [G loss: 0.798885]\n",
      "8216 [D loss: 0.715679, acc.: 59.38%] [G loss: 0.796820]\n",
      "8217 [D loss: 0.724007, acc.: 65.62%] [G loss: 0.911429]\n",
      "8218 [D loss: 0.693165, acc.: 59.38%] [G loss: 0.928007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8219 [D loss: 0.742295, acc.: 46.88%] [G loss: 0.915655]\n",
      "8220 [D loss: 0.599937, acc.: 71.88%] [G loss: 0.959437]\n",
      "8221 [D loss: 0.646179, acc.: 68.75%] [G loss: 0.843354]\n",
      "8222 [D loss: 0.639359, acc.: 68.75%] [G loss: 0.865722]\n",
      "8223 [D loss: 0.687919, acc.: 59.38%] [G loss: 0.927986]\n",
      "8224 [D loss: 0.682441, acc.: 56.25%] [G loss: 0.891714]\n",
      "8225 [D loss: 0.651070, acc.: 65.62%] [G loss: 0.900691]\n",
      "8226 [D loss: 0.619518, acc.: 65.62%] [G loss: 0.955507]\n",
      "8227 [D loss: 0.633264, acc.: 68.75%] [G loss: 0.903630]\n",
      "8228 [D loss: 0.680787, acc.: 56.25%] [G loss: 0.824104]\n",
      "8229 [D loss: 0.636691, acc.: 62.50%] [G loss: 0.864491]\n",
      "8230 [D loss: 0.714663, acc.: 53.12%] [G loss: 0.983392]\n",
      "8231 [D loss: 0.593216, acc.: 78.12%] [G loss: 0.896465]\n",
      "8232 [D loss: 0.648973, acc.: 59.38%] [G loss: 0.911401]\n",
      "8233 [D loss: 0.593404, acc.: 65.62%] [G loss: 0.908332]\n",
      "8234 [D loss: 0.657486, acc.: 62.50%] [G loss: 0.817966]\n",
      "8235 [D loss: 0.679627, acc.: 53.12%] [G loss: 0.852621]\n",
      "8236 [D loss: 0.682457, acc.: 53.12%] [G loss: 0.844315]\n",
      "8237 [D loss: 0.588607, acc.: 68.75%] [G loss: 0.857633]\n",
      "8238 [D loss: 0.826769, acc.: 37.50%] [G loss: 0.810392]\n",
      "8239 [D loss: 0.615596, acc.: 68.75%] [G loss: 0.769279]\n",
      "8240 [D loss: 0.740210, acc.: 50.00%] [G loss: 0.846412]\n",
      "8241 [D loss: 0.733853, acc.: 46.88%] [G loss: 0.855258]\n",
      "8242 [D loss: 0.669213, acc.: 59.38%] [G loss: 0.835723]\n",
      "8243 [D loss: 0.731232, acc.: 46.88%] [G loss: 0.837171]\n",
      "8244 [D loss: 0.623137, acc.: 71.88%] [G loss: 0.852882]\n",
      "8245 [D loss: 0.684233, acc.: 59.38%] [G loss: 0.905413]\n",
      "8246 [D loss: 0.638828, acc.: 53.12%] [G loss: 0.848085]\n",
      "8247 [D loss: 0.629893, acc.: 62.50%] [G loss: 0.931078]\n",
      "8248 [D loss: 0.727281, acc.: 53.12%] [G loss: 0.891873]\n",
      "8249 [D loss: 0.689467, acc.: 59.38%] [G loss: 0.896275]\n",
      "8250 [D loss: 0.684639, acc.: 56.25%] [G loss: 0.896379]\n",
      "8251 [D loss: 0.768589, acc.: 46.88%] [G loss: 0.819678]\n",
      "8252 [D loss: 0.675402, acc.: 65.62%] [G loss: 0.787505]\n",
      "8253 [D loss: 0.685920, acc.: 56.25%] [G loss: 0.745066]\n",
      "8254 [D loss: 0.624237, acc.: 71.88%] [G loss: 0.848240]\n",
      "8255 [D loss: 0.662176, acc.: 62.50%] [G loss: 0.846224]\n",
      "8256 [D loss: 0.667974, acc.: 59.38%] [G loss: 0.810180]\n",
      "8257 [D loss: 0.691586, acc.: 62.50%] [G loss: 0.931928]\n",
      "8258 [D loss: 0.743450, acc.: 50.00%] [G loss: 0.834731]\n",
      "8259 [D loss: 0.711555, acc.: 62.50%] [G loss: 0.886304]\n",
      "8260 [D loss: 0.755435, acc.: 50.00%] [G loss: 0.907831]\n",
      "8261 [D loss: 0.776545, acc.: 53.12%] [G loss: 0.875811]\n",
      "8262 [D loss: 0.658400, acc.: 62.50%] [G loss: 0.814885]\n",
      "8263 [D loss: 0.656446, acc.: 53.12%] [G loss: 0.891480]\n",
      "8264 [D loss: 0.651708, acc.: 62.50%] [G loss: 0.952353]\n",
      "8265 [D loss: 0.605161, acc.: 65.62%] [G loss: 0.933353]\n",
      "8266 [D loss: 0.655843, acc.: 56.25%] [G loss: 0.959809]\n",
      "8267 [D loss: 0.664921, acc.: 65.62%] [G loss: 0.914045]\n",
      "8268 [D loss: 0.675555, acc.: 65.62%] [G loss: 0.857894]\n",
      "8269 [D loss: 0.711308, acc.: 56.25%] [G loss: 0.909021]\n",
      "8270 [D loss: 0.563177, acc.: 78.12%] [G loss: 0.946270]\n",
      "8271 [D loss: 0.597504, acc.: 68.75%] [G loss: 0.915430]\n",
      "8272 [D loss: 0.709826, acc.: 50.00%] [G loss: 0.864779]\n",
      "8273 [D loss: 0.648885, acc.: 59.38%] [G loss: 0.852094]\n",
      "8274 [D loss: 0.605630, acc.: 65.62%] [G loss: 0.954194]\n",
      "8275 [D loss: 0.695831, acc.: 50.00%] [G loss: 0.936070]\n",
      "8276 [D loss: 0.655432, acc.: 62.50%] [G loss: 0.890747]\n",
      "8277 [D loss: 0.727442, acc.: 53.12%] [G loss: 0.979263]\n",
      "8278 [D loss: 0.735017, acc.: 50.00%] [G loss: 0.942202]\n",
      "8279 [D loss: 0.709376, acc.: 65.62%] [G loss: 0.952223]\n",
      "8280 [D loss: 0.724229, acc.: 56.25%] [G loss: 0.866972]\n",
      "8281 [D loss: 0.693168, acc.: 62.50%] [G loss: 0.877762]\n",
      "8282 [D loss: 0.767140, acc.: 50.00%] [G loss: 0.834987]\n",
      "8283 [D loss: 0.663201, acc.: 59.38%] [G loss: 0.788669]\n",
      "8284 [D loss: 0.703783, acc.: 59.38%] [G loss: 0.829274]\n",
      "8285 [D loss: 0.736656, acc.: 37.50%] [G loss: 0.836493]\n",
      "8286 [D loss: 0.707698, acc.: 46.88%] [G loss: 0.896622]\n",
      "8287 [D loss: 0.709597, acc.: 56.25%] [G loss: 0.865288]\n",
      "8288 [D loss: 0.675763, acc.: 59.38%] [G loss: 0.858991]\n",
      "8289 [D loss: 0.664615, acc.: 50.00%] [G loss: 0.851753]\n",
      "8290 [D loss: 0.655982, acc.: 56.25%] [G loss: 0.961406]\n",
      "8291 [D loss: 0.624638, acc.: 65.62%] [G loss: 0.946620]\n",
      "8292 [D loss: 0.750247, acc.: 46.88%] [G loss: 0.944160]\n",
      "8293 [D loss: 0.790499, acc.: 37.50%] [G loss: 0.854560]\n",
      "8294 [D loss: 0.653596, acc.: 59.38%] [G loss: 0.823310]\n",
      "8295 [D loss: 0.639600, acc.: 65.62%] [G loss: 0.858646]\n",
      "8296 [D loss: 0.612558, acc.: 68.75%] [G loss: 0.865134]\n",
      "8297 [D loss: 0.723268, acc.: 53.12%] [G loss: 0.955467]\n",
      "8298 [D loss: 0.734991, acc.: 46.88%] [G loss: 0.925291]\n",
      "8299 [D loss: 0.658731, acc.: 65.62%] [G loss: 0.844472]\n",
      "8300 [D loss: 0.669891, acc.: 75.00%] [G loss: 0.807663]\n",
      "8301 [D loss: 0.621326, acc.: 59.38%] [G loss: 0.877893]\n",
      "8302 [D loss: 0.777746, acc.: 50.00%] [G loss: 0.872231]\n",
      "8303 [D loss: 0.682977, acc.: 53.12%] [G loss: 0.845397]\n",
      "8304 [D loss: 0.651096, acc.: 62.50%] [G loss: 0.846327]\n",
      "8305 [D loss: 0.635469, acc.: 68.75%] [G loss: 0.934280]\n",
      "8306 [D loss: 0.671094, acc.: 56.25%] [G loss: 0.958601]\n",
      "8307 [D loss: 0.731328, acc.: 53.12%] [G loss: 1.009297]\n",
      "8308 [D loss: 0.711213, acc.: 53.12%] [G loss: 0.941871]\n",
      "8309 [D loss: 0.639604, acc.: 56.25%] [G loss: 0.951846]\n",
      "8310 [D loss: 0.664515, acc.: 62.50%] [G loss: 0.855582]\n",
      "8311 [D loss: 0.631841, acc.: 62.50%] [G loss: 0.870508]\n",
      "8312 [D loss: 0.591350, acc.: 71.88%] [G loss: 0.904382]\n",
      "8313 [D loss: 0.604887, acc.: 68.75%] [G loss: 0.792451]\n",
      "8314 [D loss: 0.694070, acc.: 53.12%] [G loss: 0.809787]\n",
      "8315 [D loss: 0.634531, acc.: 65.62%] [G loss: 0.827748]\n",
      "8316 [D loss: 0.578243, acc.: 75.00%] [G loss: 0.840061]\n",
      "8317 [D loss: 0.702303, acc.: 59.38%] [G loss: 0.943988]\n",
      "8318 [D loss: 0.684785, acc.: 62.50%] [G loss: 0.920297]\n",
      "8319 [D loss: 0.667534, acc.: 53.12%] [G loss: 0.876811]\n",
      "8320 [D loss: 0.608849, acc.: 68.75%] [G loss: 0.872360]\n",
      "8321 [D loss: 0.638773, acc.: 68.75%] [G loss: 0.877817]\n",
      "8322 [D loss: 0.712149, acc.: 56.25%] [G loss: 0.825833]\n",
      "8323 [D loss: 0.588567, acc.: 65.62%] [G loss: 0.903007]\n",
      "8324 [D loss: 0.647457, acc.: 65.62%] [G loss: 0.845299]\n",
      "8325 [D loss: 0.716794, acc.: 56.25%] [G loss: 0.797860]\n",
      "8326 [D loss: 0.631728, acc.: 62.50%] [G loss: 0.792972]\n",
      "8327 [D loss: 0.702638, acc.: 50.00%] [G loss: 0.922544]\n",
      "8328 [D loss: 0.587356, acc.: 78.12%] [G loss: 0.880014]\n",
      "8329 [D loss: 0.642721, acc.: 62.50%] [G loss: 0.916721]\n",
      "8330 [D loss: 0.708747, acc.: 56.25%] [G loss: 0.897353]\n",
      "8331 [D loss: 0.587649, acc.: 71.88%] [G loss: 0.875762]\n",
      "8332 [D loss: 0.734270, acc.: 56.25%] [G loss: 0.864434]\n",
      "8333 [D loss: 0.800883, acc.: 34.38%] [G loss: 0.794774]\n",
      "8334 [D loss: 0.745291, acc.: 50.00%] [G loss: 0.924445]\n",
      "8335 [D loss: 0.651243, acc.: 59.38%] [G loss: 0.883827]\n",
      "8336 [D loss: 0.729642, acc.: 50.00%] [G loss: 0.852016]\n",
      "8337 [D loss: 0.668270, acc.: 59.38%] [G loss: 0.978786]\n",
      "8338 [D loss: 0.637789, acc.: 71.88%] [G loss: 0.901325]\n",
      "8339 [D loss: 0.694978, acc.: 59.38%] [G loss: 0.931939]\n",
      "8340 [D loss: 0.679948, acc.: 56.25%] [G loss: 0.926130]\n",
      "8341 [D loss: 0.707022, acc.: 56.25%] [G loss: 0.977247]\n",
      "8342 [D loss: 0.628714, acc.: 68.75%] [G loss: 0.920583]\n",
      "8343 [D loss: 0.602766, acc.: 71.88%] [G loss: 0.799393]\n",
      "8344 [D loss: 0.665857, acc.: 50.00%] [G loss: 0.842788]\n",
      "8345 [D loss: 0.789715, acc.: 37.50%] [G loss: 0.784861]\n",
      "8346 [D loss: 0.598635, acc.: 65.62%] [G loss: 0.861944]\n",
      "8347 [D loss: 0.595765, acc.: 65.62%] [G loss: 0.883085]\n",
      "8348 [D loss: 0.661130, acc.: 65.62%] [G loss: 0.850502]\n",
      "8349 [D loss: 0.683841, acc.: 59.38%] [G loss: 0.820316]\n",
      "8350 [D loss: 0.654170, acc.: 71.88%] [G loss: 0.928136]\n",
      "8351 [D loss: 0.731695, acc.: 34.38%] [G loss: 0.914520]\n",
      "8352 [D loss: 0.653376, acc.: 56.25%] [G loss: 0.886282]\n",
      "8353 [D loss: 0.709286, acc.: 56.25%] [G loss: 0.888917]\n",
      "8354 [D loss: 0.648566, acc.: 71.88%] [G loss: 0.899420]\n",
      "8355 [D loss: 0.717207, acc.: 43.75%] [G loss: 0.887702]\n",
      "8356 [D loss: 0.682482, acc.: 56.25%] [G loss: 0.879136]\n",
      "8357 [D loss: 0.599103, acc.: 75.00%] [G loss: 0.927147]\n",
      "8358 [D loss: 0.686492, acc.: 53.12%] [G loss: 0.805304]\n",
      "8359 [D loss: 0.621333, acc.: 65.62%] [G loss: 0.813068]\n",
      "8360 [D loss: 0.749338, acc.: 56.25%] [G loss: 0.767018]\n",
      "8361 [D loss: 0.585889, acc.: 75.00%] [G loss: 0.941355]\n",
      "8362 [D loss: 0.786215, acc.: 40.62%] [G loss: 0.961827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8363 [D loss: 0.657868, acc.: 59.38%] [G loss: 1.011399]\n",
      "8364 [D loss: 0.676111, acc.: 56.25%] [G loss: 0.953019]\n",
      "8365 [D loss: 0.676569, acc.: 53.12%] [G loss: 0.899697]\n",
      "8366 [D loss: 0.675331, acc.: 56.25%] [G loss: 0.891853]\n",
      "8367 [D loss: 0.748332, acc.: 46.88%] [G loss: 0.831127]\n",
      "8368 [D loss: 0.677897, acc.: 62.50%] [G loss: 0.818478]\n",
      "8369 [D loss: 0.723597, acc.: 37.50%] [G loss: 0.678942]\n",
      "8370 [D loss: 0.752323, acc.: 46.88%] [G loss: 0.864093]\n",
      "8371 [D loss: 0.708966, acc.: 53.12%] [G loss: 0.881498]\n",
      "8372 [D loss: 0.578280, acc.: 68.75%] [G loss: 0.908113]\n",
      "8373 [D loss: 0.557575, acc.: 78.12%] [G loss: 0.869271]\n",
      "8374 [D loss: 0.710027, acc.: 53.12%] [G loss: 0.881029]\n",
      "8375 [D loss: 0.866142, acc.: 34.38%] [G loss: 0.845986]\n",
      "8376 [D loss: 0.648279, acc.: 68.75%] [G loss: 0.956117]\n",
      "8377 [D loss: 0.687281, acc.: 59.38%] [G loss: 0.877636]\n",
      "8378 [D loss: 0.641335, acc.: 62.50%] [G loss: 0.878699]\n",
      "8379 [D loss: 0.717843, acc.: 46.88%] [G loss: 0.875114]\n",
      "8380 [D loss: 0.615817, acc.: 53.12%] [G loss: 0.826135]\n",
      "8381 [D loss: 0.673367, acc.: 56.25%] [G loss: 0.848196]\n",
      "8382 [D loss: 0.664422, acc.: 59.38%] [G loss: 0.884997]\n",
      "8383 [D loss: 0.640463, acc.: 71.88%] [G loss: 0.881066]\n",
      "8384 [D loss: 0.643123, acc.: 62.50%] [G loss: 0.941229]\n",
      "8385 [D loss: 0.649403, acc.: 68.75%] [G loss: 0.935773]\n",
      "8386 [D loss: 0.714567, acc.: 56.25%] [G loss: 0.912188]\n",
      "8387 [D loss: 0.676764, acc.: 59.38%] [G loss: 0.840443]\n",
      "8388 [D loss: 0.645032, acc.: 65.62%] [G loss: 0.823152]\n",
      "8389 [D loss: 0.637517, acc.: 56.25%] [G loss: 0.835534]\n",
      "8390 [D loss: 0.723415, acc.: 59.38%] [G loss: 0.844803]\n",
      "8391 [D loss: 0.642106, acc.: 65.62%] [G loss: 0.870507]\n",
      "8392 [D loss: 0.654624, acc.: 53.12%] [G loss: 0.885023]\n",
      "8393 [D loss: 0.677342, acc.: 65.62%] [G loss: 0.799377]\n",
      "8394 [D loss: 0.600443, acc.: 75.00%] [G loss: 0.828592]\n",
      "8395 [D loss: 0.616744, acc.: 65.62%] [G loss: 0.797289]\n",
      "8396 [D loss: 0.674824, acc.: 59.38%] [G loss: 0.780357]\n",
      "8397 [D loss: 0.672601, acc.: 56.25%] [G loss: 0.841567]\n",
      "8398 [D loss: 0.649557, acc.: 62.50%] [G loss: 0.892952]\n",
      "8399 [D loss: 0.653446, acc.: 62.50%] [G loss: 0.913597]\n",
      "8400 [D loss: 0.592705, acc.: 78.12%] [G loss: 0.880311]\n",
      "8401 [D loss: 0.666093, acc.: 68.75%] [G loss: 0.866199]\n",
      "8402 [D loss: 0.682268, acc.: 65.62%] [G loss: 0.870435]\n",
      "8403 [D loss: 0.644648, acc.: 68.75%] [G loss: 0.830535]\n",
      "8404 [D loss: 0.610680, acc.: 65.62%] [G loss: 0.900084]\n",
      "8405 [D loss: 0.672348, acc.: 62.50%] [G loss: 0.910209]\n",
      "8406 [D loss: 0.653664, acc.: 59.38%] [G loss: 0.974033]\n",
      "8407 [D loss: 0.750705, acc.: 56.25%] [G loss: 0.893656]\n",
      "8408 [D loss: 0.633053, acc.: 59.38%] [G loss: 0.816931]\n",
      "8409 [D loss: 0.582096, acc.: 71.88%] [G loss: 0.911346]\n",
      "8410 [D loss: 0.681136, acc.: 59.38%] [G loss: 0.866500]\n",
      "8411 [D loss: 0.771203, acc.: 53.12%] [G loss: 0.810782]\n",
      "8412 [D loss: 0.752074, acc.: 46.88%] [G loss: 0.790272]\n",
      "8413 [D loss: 0.650766, acc.: 68.75%] [G loss: 0.786815]\n",
      "8414 [D loss: 0.678456, acc.: 68.75%] [G loss: 0.881355]\n",
      "8415 [D loss: 0.624583, acc.: 68.75%] [G loss: 0.826917]\n",
      "8416 [D loss: 0.642047, acc.: 62.50%] [G loss: 0.800841]\n",
      "8417 [D loss: 0.757611, acc.: 46.88%] [G loss: 0.883568]\n",
      "8418 [D loss: 0.712951, acc.: 56.25%] [G loss: 0.812464]\n",
      "8419 [D loss: 0.610307, acc.: 75.00%] [G loss: 0.897806]\n",
      "8420 [D loss: 0.636171, acc.: 71.88%] [G loss: 0.837082]\n",
      "8421 [D loss: 0.652220, acc.: 56.25%] [G loss: 0.910181]\n",
      "8422 [D loss: 0.750807, acc.: 43.75%] [G loss: 0.861570]\n",
      "8423 [D loss: 0.611889, acc.: 75.00%] [G loss: 0.852073]\n",
      "8424 [D loss: 0.724143, acc.: 53.12%] [G loss: 0.742098]\n",
      "8425 [D loss: 0.689529, acc.: 56.25%] [G loss: 0.797425]\n",
      "8426 [D loss: 0.670670, acc.: 59.38%] [G loss: 0.859298]\n",
      "8427 [D loss: 0.705690, acc.: 56.25%] [G loss: 0.881971]\n",
      "8428 [D loss: 0.636741, acc.: 71.88%] [G loss: 0.895637]\n",
      "8429 [D loss: 0.653196, acc.: 59.38%] [G loss: 0.876638]\n",
      "8430 [D loss: 0.598608, acc.: 65.62%] [G loss: 0.834725]\n",
      "8431 [D loss: 0.609012, acc.: 65.62%] [G loss: 0.881395]\n",
      "8432 [D loss: 0.664627, acc.: 62.50%] [G loss: 0.781685]\n",
      "8433 [D loss: 0.639118, acc.: 62.50%] [G loss: 0.898442]\n",
      "8434 [D loss: 0.703971, acc.: 53.12%] [G loss: 0.859994]\n",
      "8435 [D loss: 0.717183, acc.: 50.00%] [G loss: 0.865303]\n",
      "8436 [D loss: 0.735948, acc.: 46.88%] [G loss: 0.885129]\n",
      "8437 [D loss: 0.604474, acc.: 78.12%] [G loss: 0.864041]\n",
      "8438 [D loss: 0.692020, acc.: 53.12%] [G loss: 0.801524]\n",
      "8439 [D loss: 0.652082, acc.: 56.25%] [G loss: 0.927747]\n",
      "8440 [D loss: 0.705684, acc.: 46.88%] [G loss: 0.938172]\n",
      "8441 [D loss: 0.722709, acc.: 56.25%] [G loss: 0.964057]\n",
      "8442 [D loss: 0.636514, acc.: 50.00%] [G loss: 0.874686]\n",
      "8443 [D loss: 0.679991, acc.: 65.62%] [G loss: 0.894210]\n",
      "8444 [D loss: 0.601747, acc.: 68.75%] [G loss: 0.893983]\n",
      "8445 [D loss: 0.605424, acc.: 62.50%] [G loss: 0.801025]\n",
      "8446 [D loss: 0.787766, acc.: 46.88%] [G loss: 0.837459]\n",
      "8447 [D loss: 0.699118, acc.: 56.25%] [G loss: 0.825946]\n",
      "8448 [D loss: 0.663683, acc.: 59.38%] [G loss: 0.781022]\n",
      "8449 [D loss: 0.745533, acc.: 43.75%] [G loss: 0.838490]\n",
      "8450 [D loss: 0.640363, acc.: 62.50%] [G loss: 0.894882]\n",
      "8451 [D loss: 0.634156, acc.: 62.50%] [G loss: 0.887920]\n",
      "8452 [D loss: 0.723794, acc.: 50.00%] [G loss: 0.895481]\n",
      "8453 [D loss: 0.657934, acc.: 62.50%] [G loss: 0.935184]\n",
      "8454 [D loss: 0.669329, acc.: 53.12%] [G loss: 0.929744]\n",
      "8455 [D loss: 0.606564, acc.: 71.88%] [G loss: 1.000832]\n",
      "8456 [D loss: 0.700632, acc.: 53.12%] [G loss: 0.872806]\n",
      "8457 [D loss: 0.646243, acc.: 59.38%] [G loss: 0.893636]\n",
      "8458 [D loss: 0.733188, acc.: 50.00%] [G loss: 0.959952]\n",
      "8459 [D loss: 0.682094, acc.: 62.50%] [G loss: 0.946812]\n",
      "8460 [D loss: 0.601347, acc.: 68.75%] [G loss: 0.916422]\n",
      "8461 [D loss: 0.714060, acc.: 46.88%] [G loss: 0.981591]\n",
      "8462 [D loss: 0.661147, acc.: 65.62%] [G loss: 0.801844]\n",
      "8463 [D loss: 0.713248, acc.: 53.12%] [G loss: 0.905092]\n",
      "8464 [D loss: 0.684505, acc.: 53.12%] [G loss: 0.856288]\n",
      "8465 [D loss: 0.655299, acc.: 56.25%] [G loss: 0.852990]\n",
      "8466 [D loss: 0.697061, acc.: 43.75%] [G loss: 0.825903]\n",
      "8467 [D loss: 0.613093, acc.: 68.75%] [G loss: 0.841401]\n",
      "8468 [D loss: 0.706211, acc.: 50.00%] [G loss: 0.868706]\n",
      "8469 [D loss: 0.697943, acc.: 62.50%] [G loss: 0.873809]\n",
      "8470 [D loss: 0.679547, acc.: 68.75%] [G loss: 0.896230]\n",
      "8471 [D loss: 0.705389, acc.: 56.25%] [G loss: 0.879974]\n",
      "8472 [D loss: 0.620860, acc.: 65.62%] [G loss: 0.875211]\n",
      "8473 [D loss: 0.676355, acc.: 62.50%] [G loss: 0.746650]\n",
      "8474 [D loss: 0.728538, acc.: 50.00%] [G loss: 0.813645]\n",
      "8475 [D loss: 0.642084, acc.: 65.62%] [G loss: 0.807199]\n",
      "8476 [D loss: 0.642589, acc.: 65.62%] [G loss: 0.865108]\n",
      "8477 [D loss: 0.652343, acc.: 56.25%] [G loss: 0.855786]\n",
      "8478 [D loss: 0.701973, acc.: 56.25%] [G loss: 0.912454]\n",
      "8479 [D loss: 0.713702, acc.: 56.25%] [G loss: 0.816627]\n",
      "8480 [D loss: 0.594412, acc.: 71.88%] [G loss: 0.911829]\n",
      "8481 [D loss: 0.660226, acc.: 59.38%] [G loss: 0.872557]\n",
      "8482 [D loss: 0.626930, acc.: 65.62%] [G loss: 0.882163]\n",
      "8483 [D loss: 0.663647, acc.: 62.50%] [G loss: 0.855189]\n",
      "8484 [D loss: 0.690521, acc.: 56.25%] [G loss: 0.877790]\n",
      "8485 [D loss: 0.680209, acc.: 50.00%] [G loss: 0.885513]\n",
      "8486 [D loss: 0.668255, acc.: 56.25%] [G loss: 0.938057]\n",
      "8487 [D loss: 0.713332, acc.: 62.50%] [G loss: 0.886577]\n",
      "8488 [D loss: 0.687423, acc.: 50.00%] [G loss: 0.978638]\n",
      "8489 [D loss: 0.649771, acc.: 62.50%] [G loss: 0.930691]\n",
      "8490 [D loss: 0.631631, acc.: 62.50%] [G loss: 0.939175]\n",
      "8491 [D loss: 0.662589, acc.: 59.38%] [G loss: 0.961019]\n",
      "8492 [D loss: 0.656169, acc.: 59.38%] [G loss: 0.893726]\n",
      "8493 [D loss: 0.804836, acc.: 43.75%] [G loss: 0.887582]\n",
      "8494 [D loss: 0.658589, acc.: 62.50%] [G loss: 0.791790]\n",
      "8495 [D loss: 0.657017, acc.: 65.62%] [G loss: 0.882109]\n",
      "8496 [D loss: 0.743918, acc.: 40.62%] [G loss: 0.859280]\n",
      "8497 [D loss: 0.727593, acc.: 50.00%] [G loss: 0.894617]\n",
      "8498 [D loss: 0.651495, acc.: 56.25%] [G loss: 0.935427]\n",
      "8499 [D loss: 0.725699, acc.: 50.00%] [G loss: 0.919096]\n",
      "8500 [D loss: 0.646791, acc.: 46.88%] [G loss: 0.935622]\n",
      "8501 [D loss: 0.653809, acc.: 65.62%] [G loss: 0.862195]\n",
      "8502 [D loss: 0.754863, acc.: 56.25%] [G loss: 0.934260]\n",
      "8503 [D loss: 0.750029, acc.: 46.88%] [G loss: 0.905889]\n",
      "8504 [D loss: 0.662124, acc.: 59.38%] [G loss: 0.935108]\n",
      "8505 [D loss: 0.668224, acc.: 50.00%] [G loss: 0.931075]\n",
      "8506 [D loss: 0.813466, acc.: 31.25%] [G loss: 0.924020]\n",
      "8507 [D loss: 0.688041, acc.: 59.38%] [G loss: 0.877767]\n",
      "8508 [D loss: 0.646691, acc.: 68.75%] [G loss: 0.905537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8509 [D loss: 0.636074, acc.: 68.75%] [G loss: 0.977658]\n",
      "8510 [D loss: 0.697237, acc.: 65.62%] [G loss: 0.890163]\n",
      "8511 [D loss: 0.676213, acc.: 53.12%] [G loss: 0.956936]\n",
      "8512 [D loss: 0.615450, acc.: 68.75%] [G loss: 0.880054]\n",
      "8513 [D loss: 0.691238, acc.: 59.38%] [G loss: 0.849582]\n",
      "8514 [D loss: 0.748109, acc.: 50.00%] [G loss: 0.851790]\n",
      "8515 [D loss: 0.631601, acc.: 68.75%] [G loss: 0.903233]\n",
      "8516 [D loss: 0.652383, acc.: 56.25%] [G loss: 0.878329]\n",
      "8517 [D loss: 0.660768, acc.: 62.50%] [G loss: 0.925124]\n",
      "8518 [D loss: 0.663093, acc.: 56.25%] [G loss: 0.930716]\n",
      "8519 [D loss: 0.722564, acc.: 59.38%] [G loss: 0.845429]\n",
      "8520 [D loss: 0.689749, acc.: 59.38%] [G loss: 0.872659]\n",
      "8521 [D loss: 0.671787, acc.: 56.25%] [G loss: 0.782329]\n",
      "8522 [D loss: 0.657574, acc.: 62.50%] [G loss: 0.759253]\n",
      "8523 [D loss: 0.661337, acc.: 68.75%] [G loss: 0.844705]\n",
      "8524 [D loss: 0.675195, acc.: 59.38%] [G loss: 0.889951]\n",
      "8525 [D loss: 0.609586, acc.: 78.12%] [G loss: 0.853808]\n",
      "8526 [D loss: 0.674301, acc.: 59.38%] [G loss: 0.891650]\n",
      "8527 [D loss: 0.588295, acc.: 68.75%] [G loss: 0.860033]\n",
      "8528 [D loss: 0.588054, acc.: 65.62%] [G loss: 0.970565]\n",
      "8529 [D loss: 0.792938, acc.: 46.88%] [G loss: 0.857886]\n",
      "8530 [D loss: 0.723036, acc.: 53.12%] [G loss: 0.929580]\n",
      "8531 [D loss: 0.639355, acc.: 59.38%] [G loss: 0.941045]\n",
      "8532 [D loss: 0.705055, acc.: 65.62%] [G loss: 0.950856]\n",
      "8533 [D loss: 0.592786, acc.: 71.88%] [G loss: 0.877923]\n",
      "8534 [D loss: 0.603381, acc.: 68.75%] [G loss: 0.874251]\n",
      "8535 [D loss: 0.788571, acc.: 53.12%] [G loss: 0.877756]\n",
      "8536 [D loss: 0.722239, acc.: 53.12%] [G loss: 0.890437]\n",
      "8537 [D loss: 0.663421, acc.: 56.25%] [G loss: 0.838534]\n",
      "8538 [D loss: 0.612535, acc.: 68.75%] [G loss: 0.936480]\n",
      "8539 [D loss: 0.631966, acc.: 62.50%] [G loss: 0.860877]\n",
      "8540 [D loss: 0.755565, acc.: 40.62%] [G loss: 0.912348]\n",
      "8541 [D loss: 0.675610, acc.: 59.38%] [G loss: 0.893533]\n",
      "8542 [D loss: 0.646287, acc.: 46.88%] [G loss: 0.842717]\n",
      "8543 [D loss: 0.670250, acc.: 56.25%] [G loss: 0.920572]\n",
      "8544 [D loss: 0.626589, acc.: 68.75%] [G loss: 0.836271]\n",
      "8545 [D loss: 0.674233, acc.: 59.38%] [G loss: 0.902301]\n",
      "8546 [D loss: 0.687024, acc.: 59.38%] [G loss: 0.887444]\n",
      "8547 [D loss: 0.649131, acc.: 59.38%] [G loss: 0.939299]\n",
      "8548 [D loss: 0.633815, acc.: 65.62%] [G loss: 0.967327]\n",
      "8549 [D loss: 0.648177, acc.: 62.50%] [G loss: 0.920316]\n",
      "8550 [D loss: 0.701484, acc.: 53.12%] [G loss: 0.954820]\n",
      "8551 [D loss: 0.720837, acc.: 43.75%] [G loss: 0.990562]\n",
      "8552 [D loss: 0.622392, acc.: 71.88%] [G loss: 0.902331]\n",
      "8553 [D loss: 0.585373, acc.: 78.12%] [G loss: 0.908171]\n",
      "8554 [D loss: 0.658114, acc.: 65.62%] [G loss: 0.871556]\n",
      "8555 [D loss: 0.637730, acc.: 71.88%] [G loss: 0.894326]\n",
      "8556 [D loss: 0.662719, acc.: 65.62%] [G loss: 0.860466]\n",
      "8557 [D loss: 0.632202, acc.: 68.75%] [G loss: 0.886236]\n",
      "8558 [D loss: 0.629846, acc.: 68.75%] [G loss: 0.799123]\n",
      "8559 [D loss: 0.669872, acc.: 56.25%] [G loss: 0.845015]\n",
      "8560 [D loss: 0.693904, acc.: 65.62%] [G loss: 0.850367]\n",
      "8561 [D loss: 0.663155, acc.: 59.38%] [G loss: 0.898807]\n",
      "8562 [D loss: 0.614640, acc.: 65.62%] [G loss: 0.869745]\n",
      "8563 [D loss: 0.611931, acc.: 71.88%] [G loss: 0.868334]\n",
      "8564 [D loss: 0.626302, acc.: 68.75%] [G loss: 0.767863]\n",
      "8565 [D loss: 0.738862, acc.: 56.25%] [G loss: 0.826751]\n",
      "8566 [D loss: 0.797748, acc.: 43.75%] [G loss: 0.755021]\n",
      "8567 [D loss: 0.704334, acc.: 53.12%] [G loss: 0.817906]\n",
      "8568 [D loss: 0.749247, acc.: 46.88%] [G loss: 0.955804]\n",
      "8569 [D loss: 0.638046, acc.: 65.62%] [G loss: 0.882303]\n",
      "8570 [D loss: 0.657047, acc.: 56.25%] [G loss: 0.922441]\n",
      "8571 [D loss: 0.661304, acc.: 56.25%] [G loss: 0.884696]\n",
      "8572 [D loss: 0.779882, acc.: 56.25%] [G loss: 0.856150]\n",
      "8573 [D loss: 0.697369, acc.: 65.62%] [G loss: 0.884792]\n",
      "8574 [D loss: 0.691890, acc.: 50.00%] [G loss: 0.799599]\n",
      "8575 [D loss: 0.676866, acc.: 56.25%] [G loss: 0.945460]\n",
      "8576 [D loss: 0.680859, acc.: 62.50%] [G loss: 0.860414]\n",
      "8577 [D loss: 0.623443, acc.: 65.62%] [G loss: 0.925650]\n",
      "8578 [D loss: 0.753712, acc.: 50.00%] [G loss: 0.821787]\n",
      "8579 [D loss: 0.629712, acc.: 71.88%] [G loss: 0.840433]\n",
      "8580 [D loss: 0.672287, acc.: 62.50%] [G loss: 0.748037]\n",
      "8581 [D loss: 0.619891, acc.: 62.50%] [G loss: 0.863313]\n",
      "8582 [D loss: 0.732900, acc.: 43.75%] [G loss: 0.724260]\n",
      "8583 [D loss: 0.669313, acc.: 65.62%] [G loss: 0.788620]\n",
      "8584 [D loss: 0.587127, acc.: 65.62%] [G loss: 0.864385]\n",
      "8585 [D loss: 0.599636, acc.: 68.75%] [G loss: 0.945415]\n",
      "8586 [D loss: 0.700045, acc.: 62.50%] [G loss: 0.900261]\n",
      "8587 [D loss: 0.643209, acc.: 56.25%] [G loss: 0.822646]\n",
      "8588 [D loss: 0.784857, acc.: 43.75%] [G loss: 0.855010]\n",
      "8589 [D loss: 0.776603, acc.: 34.38%] [G loss: 0.882823]\n",
      "8590 [D loss: 0.705490, acc.: 53.12%] [G loss: 0.928075]\n",
      "8591 [D loss: 0.664808, acc.: 59.38%] [G loss: 0.899928]\n",
      "8592 [D loss: 0.652178, acc.: 68.75%] [G loss: 0.838141]\n",
      "8593 [D loss: 0.615900, acc.: 65.62%] [G loss: 0.885826]\n",
      "8594 [D loss: 0.655405, acc.: 68.75%] [G loss: 0.853047]\n",
      "8595 [D loss: 0.667473, acc.: 62.50%] [G loss: 0.825436]\n",
      "8596 [D loss: 0.653268, acc.: 68.75%] [G loss: 0.883610]\n",
      "8597 [D loss: 0.608344, acc.: 56.25%] [G loss: 0.884808]\n",
      "8598 [D loss: 0.630687, acc.: 65.62%] [G loss: 0.928844]\n",
      "8599 [D loss: 0.680541, acc.: 65.62%] [G loss: 0.966414]\n",
      "8600 [D loss: 0.605812, acc.: 65.62%] [G loss: 0.951864]\n",
      "8601 [D loss: 0.602387, acc.: 62.50%] [G loss: 0.941982]\n",
      "8602 [D loss: 0.657333, acc.: 62.50%] [G loss: 0.911466]\n",
      "8603 [D loss: 0.773821, acc.: 53.12%] [G loss: 0.897584]\n",
      "8604 [D loss: 0.605430, acc.: 75.00%] [G loss: 0.901826]\n",
      "8605 [D loss: 0.620834, acc.: 62.50%] [G loss: 0.964415]\n",
      "8606 [D loss: 0.663375, acc.: 62.50%] [G loss: 0.968015]\n",
      "8607 [D loss: 0.709033, acc.: 59.38%] [G loss: 0.885498]\n",
      "8608 [D loss: 0.694669, acc.: 53.12%] [G loss: 0.889164]\n",
      "8609 [D loss: 0.665141, acc.: 56.25%] [G loss: 0.904205]\n",
      "8610 [D loss: 0.755251, acc.: 50.00%] [G loss: 0.915721]\n",
      "8611 [D loss: 0.665695, acc.: 65.62%] [G loss: 0.930066]\n",
      "8612 [D loss: 0.775690, acc.: 40.62%] [G loss: 0.876797]\n",
      "8613 [D loss: 0.699378, acc.: 50.00%] [G loss: 0.828084]\n",
      "8614 [D loss: 0.690943, acc.: 46.88%] [G loss: 0.873554]\n",
      "8615 [D loss: 0.660776, acc.: 59.38%] [G loss: 0.866228]\n",
      "8616 [D loss: 0.778191, acc.: 53.12%] [G loss: 0.865376]\n",
      "8617 [D loss: 0.695026, acc.: 46.88%] [G loss: 0.821786]\n",
      "8618 [D loss: 0.673271, acc.: 59.38%] [G loss: 0.860777]\n",
      "8619 [D loss: 0.640711, acc.: 65.62%] [G loss: 1.017208]\n",
      "8620 [D loss: 0.711716, acc.: 53.12%] [G loss: 0.955989]\n",
      "8621 [D loss: 0.706562, acc.: 53.12%] [G loss: 0.914250]\n",
      "8622 [D loss: 0.665000, acc.: 62.50%] [G loss: 0.852803]\n",
      "8623 [D loss: 0.716360, acc.: 46.88%] [G loss: 0.812597]\n",
      "8624 [D loss: 0.771926, acc.: 56.25%] [G loss: 0.818347]\n",
      "8625 [D loss: 0.713455, acc.: 56.25%] [G loss: 0.851576]\n",
      "8626 [D loss: 0.680827, acc.: 53.12%] [G loss: 0.920203]\n",
      "8627 [D loss: 0.594137, acc.: 68.75%] [G loss: 0.962885]\n",
      "8628 [D loss: 0.709985, acc.: 53.12%] [G loss: 0.857553]\n",
      "8629 [D loss: 0.683506, acc.: 53.12%] [G loss: 0.906778]\n",
      "8630 [D loss: 0.724095, acc.: 53.12%] [G loss: 0.810648]\n",
      "8631 [D loss: 0.654555, acc.: 71.88%] [G loss: 0.808982]\n",
      "8632 [D loss: 0.619050, acc.: 62.50%] [G loss: 0.795598]\n",
      "8633 [D loss: 0.747940, acc.: 56.25%] [G loss: 0.837249]\n",
      "8634 [D loss: 0.649041, acc.: 65.62%] [G loss: 0.856347]\n",
      "8635 [D loss: 0.615824, acc.: 59.38%] [G loss: 0.818463]\n",
      "8636 [D loss: 0.610392, acc.: 71.88%] [G loss: 0.900614]\n",
      "8637 [D loss: 0.636583, acc.: 68.75%] [G loss: 0.982012]\n",
      "8638 [D loss: 0.664024, acc.: 56.25%] [G loss: 0.879578]\n",
      "8639 [D loss: 0.663768, acc.: 59.38%] [G loss: 0.805581]\n",
      "8640 [D loss: 0.681711, acc.: 62.50%] [G loss: 0.870304]\n",
      "8641 [D loss: 0.654919, acc.: 68.75%] [G loss: 0.850287]\n",
      "8642 [D loss: 0.619668, acc.: 65.62%] [G loss: 0.838625]\n",
      "8643 [D loss: 0.721405, acc.: 56.25%] [G loss: 0.867675]\n",
      "8644 [D loss: 0.598702, acc.: 75.00%] [G loss: 0.845524]\n",
      "8645 [D loss: 0.685167, acc.: 56.25%] [G loss: 0.867732]\n",
      "8646 [D loss: 0.681876, acc.: 56.25%] [G loss: 0.797249]\n",
      "8647 [D loss: 0.744587, acc.: 31.25%] [G loss: 0.786854]\n",
      "8648 [D loss: 0.750204, acc.: 46.88%] [G loss: 0.843828]\n",
      "8649 [D loss: 0.704381, acc.: 65.62%] [G loss: 0.877235]\n",
      "8650 [D loss: 0.747271, acc.: 43.75%] [G loss: 0.926709]\n",
      "8651 [D loss: 0.652059, acc.: 59.38%] [G loss: 0.838679]\n",
      "8652 [D loss: 0.665634, acc.: 59.38%] [G loss: 0.802826]\n",
      "8653 [D loss: 0.674946, acc.: 59.38%] [G loss: 0.776474]\n",
      "8654 [D loss: 0.665756, acc.: 68.75%] [G loss: 0.794116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8655 [D loss: 0.666737, acc.: 53.12%] [G loss: 0.876438]\n",
      "8656 [D loss: 0.699848, acc.: 43.75%] [G loss: 0.790774]\n",
      "8657 [D loss: 0.646743, acc.: 71.88%] [G loss: 0.862177]\n",
      "8658 [D loss: 0.726803, acc.: 46.88%] [G loss: 0.905360]\n",
      "8659 [D loss: 0.682121, acc.: 53.12%] [G loss: 0.974171]\n",
      "8660 [D loss: 0.725371, acc.: 53.12%] [G loss: 0.886913]\n",
      "8661 [D loss: 0.656366, acc.: 71.88%] [G loss: 0.852306]\n",
      "8662 [D loss: 0.612163, acc.: 68.75%] [G loss: 0.906912]\n",
      "8663 [D loss: 0.581698, acc.: 75.00%] [G loss: 0.917320]\n",
      "8664 [D loss: 0.618517, acc.: 71.88%] [G loss: 0.885574]\n",
      "8665 [D loss: 0.767284, acc.: 40.62%] [G loss: 0.879669]\n",
      "8666 [D loss: 0.709099, acc.: 56.25%] [G loss: 0.830080]\n",
      "8667 [D loss: 0.608267, acc.: 68.75%] [G loss: 0.893213]\n",
      "8668 [D loss: 0.752016, acc.: 46.88%] [G loss: 0.810754]\n",
      "8669 [D loss: 0.718352, acc.: 50.00%] [G loss: 0.825391]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cd73a3f9e29b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-cd73a3f9e29b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from https://skymind.ai/wiki/generative-adversarial-network-gan\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (100,)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"./gan/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=30000, batch_size=32, save_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pseudocodice per il paper\n",
    "\n",
    "# for i in range(batches):\n",
    "    # z = noise(100)\n",
    "    # generated_images = Generator(z)\n",
    "    # (output, teacher_activations) = teacher(generated_images)\n",
    "    ''' Teacher is the pre-trained network that outputs its activations and the result \n",
    "    or it can outputs only the result and we can get the activations with K.function '''\n",
    "    # combined.train_on_batch(generated_images, (outputs,teacher_activations)) \n",
    "    '''Combined is a network that has at the start the generator and then the freezed student.\n",
    "    The labels it gets are the results and the activations of the teacher\n",
    "    The loss is to increase the distance between the labels and the output of itself\n",
    "    therefore this network has to output its intermediate activations'''\n",
    "    \n",
    "    # for j in range(ns):\n",
    "        # student.train_on_batch(generated_images, (outputs,teacher_activations))\n",
    "        ''' Student network that outputs its results and its intermediate activations, \n",
    "        and its loss is to match the output and activations of the teacher'''\n",
    "\n",
    "    ''' if i don't find a way to output intermediate activations from a model we could always\n",
    "        get the activations with K.fuction and the data, and then input them to the loss as the label.\n",
    "        This method does not seems to increase too much the training time...\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
