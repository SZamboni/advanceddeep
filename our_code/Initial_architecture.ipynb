{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 loaded\n",
      "WARNING:tensorflow:From /home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Teacher loaded frommodel-16-2.h5\n",
      "WARNING:tensorflow:From /home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Simple student loaded\n",
      "Generator loaded\n",
      "WARNING:tensorflow:From /home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0/1000 G loss: 1.5640485 S loss: 12.02066445350647\n",
      "Student test loss: [4.892328601074219, 0.10000000149011612]\n",
      "batch 1/1000 G loss: 0.20010361 S loss: 2.76778544485569\n",
      "batch 2/1000 G loss: 0.86611784 S loss: 1.874023124575615\n",
      "batch 3/1000 G loss: 0.889666 S loss: 1.7952301353216171\n",
      "batch 4/1000 G loss: 0.67435026 S loss: 1.4522293657064438\n",
      "batch 5/1000 G loss: 0.44669244 S loss: 1.2702387273311615\n",
      "batch 6/1000 G loss: 0.30275822 S loss: 1.1187561228871346\n",
      "batch 7/1000 G loss: 0.19647768 S loss: 1.0229307860136032\n",
      "batch 8/1000 G loss: 0.1623143 S loss: 0.9493248090147972\n",
      "batch 9/1000 G loss: 0.124591246 S loss: 0.8769833222031593\n",
      "batch 10/1000 G loss: 0.110888615 S loss: 0.8893177434802055\n",
      "Student test loss: [3.8602321464538574, 0.10000000149011612]\n",
      "batch 11/1000 G loss: 0.11092573 S loss: 0.8281595781445503\n",
      "batch 12/1000 G loss: 0.09544526 S loss: 0.8888693749904633\n",
      "batch 13/1000 G loss: 0.09714234 S loss: 0.8194840103387833\n",
      "batch 14/1000 G loss: 0.08662433 S loss: 0.8232038542628288\n",
      "batch 15/1000 G loss: 0.08514996 S loss: 0.8245639726519585\n",
      "batch 16/1000 G loss: 0.08152423 S loss: 0.8503966182470322\n",
      "batch 17/1000 G loss: 0.073147 S loss: 0.7458102405071259\n",
      "batch 18/1000 G loss: 0.07444755 S loss: 0.7517779767513275\n",
      "batch 19/1000 G loss: 0.07952069 S loss: 0.7668290808796883\n",
      "batch 20/1000 G loss: 0.07860238 S loss: 0.836297407746315\n",
      "Student test loss: [3.8278308685302735, 0.10000000149011612]\n",
      "batch 21/1000 G loss: 0.069574736 S loss: 0.7286306172609329\n",
      "batch 22/1000 G loss: 0.07751795 S loss: 0.7635959982872009\n",
      "batch 23/1000 G loss: 0.07210728 S loss: 0.6875277608633041\n",
      "batch 24/1000 G loss: 0.06749368 S loss: 0.6827504560351372\n",
      "batch 25/1000 G loss: 0.08034325 S loss: 0.7155419588088989\n",
      "batch 26/1000 G loss: 0.072001815 S loss: 0.6793487928807735\n",
      "batch 27/1000 G loss: 0.078201294 S loss: 0.6985746622085571\n",
      "batch 28/1000 G loss: 0.06922549 S loss: 0.6472535207867622\n",
      "batch 29/1000 G loss: 0.06497446 S loss: 0.6444332860410213\n",
      "batch 30/1000 G loss: 0.07060196 S loss: 0.6677407026290894\n",
      "Student test loss: [3.7953633934021, 0.10000000149011612]\n",
      "batch 31/1000 G loss: 0.06413265 S loss: 0.6447113193571568\n",
      "batch 32/1000 G loss: 0.06753506 S loss: 0.634501650929451\n",
      "batch 33/1000 G loss: 0.058880907 S loss: 0.6205811649560928\n",
      "batch 34/1000 G loss: 0.061753348 S loss: 0.5933602415025234\n",
      "batch 35/1000 G loss: 0.0662092 S loss: 0.6261506229639053\n",
      "batch 36/1000 G loss: 0.06512852 S loss: 0.5708816237747669\n",
      "batch 37/1000 G loss: 0.05727358 S loss: 0.618140920996666\n",
      "batch 38/1000 G loss: 0.06298665 S loss: 0.5906281359493732\n",
      "batch 39/1000 G loss: 0.057106867 S loss: 0.6088459566235542\n",
      "batch 40/1000 G loss: 0.059299238 S loss: 0.5888006575405598\n",
      "Student test loss: [3.784262798309326, 0.10000000149011612]\n",
      "batch 41/1000 G loss: 0.05622756 S loss: 0.5938748046755791\n",
      "batch 42/1000 G loss: 0.052545384 S loss: 0.5989324674010277\n",
      "batch 43/1000 G loss: 0.061252564 S loss: 0.5959536693990231\n",
      "batch 44/1000 G loss: 0.056757033 S loss: 0.5693423636257648\n",
      "batch 45/1000 G loss: 0.06150452 S loss: 0.5707184635102749\n",
      "batch 46/1000 G loss: 0.06261634 S loss: 0.5647650361061096\n",
      "batch 47/1000 G loss: 0.055513702 S loss: 0.5585714057087898\n",
      "batch 48/1000 G loss: 0.054471202 S loss: 0.529649630188942\n",
      "batch 49/1000 G loss: 0.055726536 S loss: 0.5589838214218616\n",
      "batch 50/1000 G loss: 0.056140438 S loss: 0.5772063136100769\n",
      "Student test loss: [3.804236402130127, 0.10000000149011612]\n",
      "batch 51/1000 G loss: 0.052012898 S loss: 0.5431360937654972\n",
      "batch 52/1000 G loss: 0.054360118 S loss: 0.5574033968150616\n",
      "batch 53/1000 G loss: 0.05492217 S loss: 0.5600812546908855\n",
      "batch 54/1000 G loss: 0.05846265 S loss: 0.5399625413119793\n",
      "batch 55/1000 G loss: 0.055116404 S loss: 0.5638046525418758\n",
      "batch 56/1000 G loss: 0.05480244 S loss: 0.5346611812710762\n",
      "batch 57/1000 G loss: 0.052379925 S loss: 0.5433830991387367\n",
      "batch 58/1000 G loss: 0.058049664 S loss: 0.5407925099134445\n",
      "batch 59/1000 G loss: 0.049928628 S loss: 0.5210540108382702\n",
      "batch 60/1000 G loss: 0.055896312 S loss: 0.5205656662583351\n",
      "Student test loss: [3.76395525970459, 0.10000000149011612]\n",
      "batch 61/1000 G loss: 0.05136352 S loss: 0.532504241913557\n",
      "batch 62/1000 G loss: 0.050909854 S loss: 0.5467716455459595\n",
      "batch 63/1000 G loss: 0.05385385 S loss: 0.540554404258728\n",
      "batch 64/1000 G loss: 0.050904572 S loss: 0.5399765074253082\n",
      "batch 65/1000 G loss: 0.052856214 S loss: 0.49284347891807556\n",
      "batch 66/1000 G loss: 0.04889297 S loss: 0.5144323632121086\n",
      "batch 67/1000 G loss: 0.04672104 S loss: 0.5048624500632286\n",
      "batch 68/1000 G loss: 0.053456217 S loss: 0.5030802190303802\n",
      "batch 69/1000 G loss: 0.05107558 S loss: 0.48748401552438736\n",
      "batch 70/1000 G loss: 0.051720798 S loss: 0.5153996534645557\n",
      "Student test loss: [3.7730396125793457, 0.10000000149011612]\n",
      "batch 71/1000 G loss: 0.05505149 S loss: 0.5217515304684639\n",
      "batch 72/1000 G loss: 0.050531186 S loss: 0.5073487274348736\n",
      "batch 73/1000 G loss: 0.04579562 S loss: 0.48255501314997673\n",
      "batch 74/1000 G loss: 0.051657446 S loss: 0.49027489870786667\n",
      "batch 75/1000 G loss: 0.045696635 S loss: 0.4958365447819233\n",
      "batch 76/1000 G loss: 0.04453951 S loss: 0.48434967547655106\n",
      "batch 77/1000 G loss: 0.057222553 S loss: 0.47328828647732735\n",
      "batch 78/1000 G loss: 0.05121292 S loss: 0.47879741713404655\n",
      "batch 79/1000 G loss: 0.05044648 S loss: 0.47287552431225777\n",
      "batch 80/1000 G loss: 0.04935331 S loss: 0.48436956107616425\n",
      "Student test loss: [3.777346129608154, 0.10000000149011612]\n",
      "batch 81/1000 G loss: 0.04866059 S loss: 0.5047775320708752\n",
      "batch 82/1000 G loss: 0.047722284 S loss: 0.46869993954896927\n",
      "batch 83/1000 G loss: 0.049834706 S loss: 0.4763837158679962\n",
      "batch 84/1000 G loss: 0.046614528 S loss: 0.48285315558314323\n",
      "batch 85/1000 G loss: 0.04453349 S loss: 0.46293407306075096\n",
      "batch 86/1000 G loss: 0.047064047 S loss: 0.46370191499590874\n",
      "batch 87/1000 G loss: 0.046229314 S loss: 0.4732520245015621\n",
      "batch 88/1000 G loss: 0.04850149 S loss: 0.5033590421080589\n",
      "batch 89/1000 G loss: 0.048199065 S loss: 0.46064260229468346\n",
      "batch 90/1000 G loss: 0.042364486 S loss: 0.4511907324194908\n",
      "Student test loss: [3.7477904151916506, 0.10000000149011612]\n",
      "batch 91/1000 G loss: 0.047734 S loss: 0.4447338320314884\n",
      "batch 92/1000 G loss: 0.050943054 S loss: 0.48525766655802727\n",
      "batch 93/1000 G loss: 0.043438427 S loss: 0.45549842342734337\n",
      "batch 94/1000 G loss: 0.0469789 S loss: 0.44723135605454445\n",
      "batch 95/1000 G loss: 0.048542116 S loss: 0.43647343292832375\n",
      "batch 96/1000 G loss: 0.041140635 S loss: 0.43093879148364067\n",
      "batch 97/1000 G loss: 0.04599293 S loss: 0.4560965560376644\n",
      "batch 98/1000 G loss: 0.044510126 S loss: 0.479017972946167\n",
      "batch 99/1000 G loss: 0.04568762 S loss: 0.47538208588957787\n",
      "batch 100/1000 G loss: 0.045658283 S loss: 0.44497980549931526\n",
      "Student test loss: [3.778619728088379, 0.10000000149011612]\n",
      "batch 101/1000 G loss: 0.044757243 S loss: 0.44596046209335327\n",
      "batch 102/1000 G loss: 0.043672014 S loss: 0.45822909474372864\n",
      "batch 103/1000 G loss: 0.04401947 S loss: 0.45387551188468933\n",
      "batch 104/1000 G loss: 0.048522644 S loss: 0.4399879723787308\n",
      "batch 105/1000 G loss: 0.044700824 S loss: 0.45421868935227394\n",
      "batch 106/1000 G loss: 0.046786986 S loss: 0.4620467573404312\n",
      "batch 107/1000 G loss: 0.041260928 S loss: 0.4327118620276451\n",
      "batch 108/1000 G loss: 0.047286466 S loss: 0.4620557725429535\n",
      "batch 109/1000 G loss: 0.045592617 S loss: 0.4473592974245548\n",
      "batch 110/1000 G loss: 0.0444028 S loss: 0.44165266677737236\n",
      "Student test loss: [3.717035755157471, 0.10000000149011612]\n",
      "batch 111/1000 G loss: 0.050058592 S loss: 0.45801991969347\n",
      "batch 112/1000 G loss: 0.037999436 S loss: 0.42452898249030113\n",
      "batch 113/1000 G loss: 0.044466857 S loss: 0.4406512603163719\n",
      "batch 114/1000 G loss: 0.042569928 S loss: 0.45639530196785927\n",
      "batch 115/1000 G loss: 0.041205935 S loss: 0.4379129931330681\n",
      "batch 116/1000 G loss: 0.045799036 S loss: 0.42559826374053955\n",
      "batch 117/1000 G loss: 0.046808176 S loss: 0.42011548578739166\n",
      "batch 118/1000 G loss: 0.044312872 S loss: 0.41953369230031967\n",
      "batch 119/1000 G loss: 0.047951303 S loss: 0.4168237932026386\n",
      "batch 120/1000 G loss: 0.0484147 S loss: 0.4534634202718735\n",
      "Student test loss: [3.663894083404541, 0.10000000149011612]\n",
      "batch 121/1000 G loss: 0.04313771 S loss: 0.4128866121172905\n",
      "batch 122/1000 G loss: 0.043095164 S loss: 0.4267410449683666\n",
      "batch 123/1000 G loss: 0.04039517 S loss: 0.43712086230516434\n",
      "batch 124/1000 G loss: 0.038117357 S loss: 0.418343685567379\n",
      "batch 125/1000 G loss: 0.040254354 S loss: 0.39835672453045845\n",
      "batch 126/1000 G loss: 0.04565444 S loss: 0.42009636759757996\n",
      "batch 127/1000 G loss: 0.043962386 S loss: 0.4333178550004959\n",
      "batch 128/1000 G loss: 0.041501474 S loss: 0.4117607809603214\n",
      "batch 129/1000 G loss: 0.04254677 S loss: 0.4122399650514126\n",
      "batch 130/1000 G loss: 0.042120546 S loss: 0.4068719446659088\n",
      "Student test loss: [3.701746816253662, 0.10000000149011612]\n",
      "batch 131/1000 G loss: 0.042754453 S loss: 0.4312845654785633\n",
      "batch 132/1000 G loss: 0.040374033 S loss: 0.4328193962574005\n",
      "batch 133/1000 G loss: 0.041768253 S loss: 0.45773371681571007\n",
      "batch 134/1000 G loss: 0.03291315 S loss: 0.3856957331299782\n",
      "batch 135/1000 G loss: 0.044613346 S loss: 0.4418453574180603\n",
      "batch 136/1000 G loss: 0.040909164 S loss: 0.410722304135561\n",
      "batch 137/1000 G loss: 0.036328625 S loss: 0.3952452577650547\n",
      "batch 138/1000 G loss: 0.039826207 S loss: 0.3955916278064251\n",
      "batch 139/1000 G loss: 0.042687785 S loss: 0.4195537343621254\n",
      "batch 140/1000 G loss: 0.03709722 S loss: 0.40115882083773613\n",
      "Student test loss: [3.6675250160217283, 0.10000000149011612]\n",
      "batch 141/1000 G loss: 0.043887656 S loss: 0.4107922576367855\n",
      "batch 142/1000 G loss: 0.040801384 S loss: 0.4372410215437412\n",
      "batch 143/1000 G loss: 0.03902299 S loss: 0.40750858187675476\n",
      "batch 144/1000 G loss: 0.037224628 S loss: 0.40818794816732407\n",
      "batch 145/1000 G loss: 0.041364364 S loss: 0.400018360465765\n",
      "batch 146/1000 G loss: 0.038822763 S loss: 0.40265005081892014\n",
      "batch 147/1000 G loss: 0.04384698 S loss: 0.4144159257411957\n",
      "batch 148/1000 G loss: 0.040963765 S loss: 0.4318070709705353\n",
      "batch 149/1000 G loss: 0.03978448 S loss: 0.40755587071180344\n",
      "batch 150/1000 G loss: 0.03951685 S loss: 0.4056299030780792\n",
      "Student test loss: [3.6443240768432617, 0.10000000149011612]\n",
      "batch 151/1000 G loss: 0.039509296 S loss: 0.4150409922003746\n",
      "batch 152/1000 G loss: 0.037886497 S loss: 0.40974222868680954\n",
      "batch 153/1000 G loss: 0.040226422 S loss: 0.4037967510521412\n",
      "batch 154/1000 G loss: 0.036249727 S loss: 0.39364613220095634\n",
      "batch 155/1000 G loss: 0.040045675 S loss: 0.38823020830750465\n",
      "batch 156/1000 G loss: 0.044302773 S loss: 0.3881663829088211\n",
      "batch 157/1000 G loss: 0.038849767 S loss: 0.4068729020655155\n",
      "batch 158/1000 G loss: 0.03970564 S loss: 0.3914884105324745\n",
      "batch 159/1000 G loss: 0.040222533 S loss: 0.42354875430464745\n",
      "batch 160/1000 G loss: 0.04607952 S loss: 0.404820766299963\n",
      "Student test loss: [3.676980377197266, 0.10000000149011612]\n",
      "batch 161/1000 G loss: 0.041096207 S loss: 0.3877287171781063\n",
      "batch 162/1000 G loss: 0.037876233 S loss: 0.40263722091913223\n",
      "batch 163/1000 G loss: 0.04426793 S loss: 0.4033440612256527\n",
      "batch 164/1000 G loss: 0.041868348 S loss: 0.423617422580719\n",
      "batch 165/1000 G loss: 0.032685045 S loss: 0.37305374816060066\n",
      "batch 166/1000 G loss: 0.036313713 S loss: 0.39196065068244934\n",
      "batch 167/1000 G loss: 0.041360024 S loss: 0.3960297666490078\n",
      "batch 168/1000 G loss: 0.04166448 S loss: 0.3970884270966053\n",
      "batch 169/1000 G loss: 0.034912128 S loss: 0.3411957807838917\n",
      "batch 170/1000 G loss: 0.03871943 S loss: 0.39014488458633423\n",
      "Student test loss: [3.6454613609313964, 0.10000000149011612]\n",
      "batch 171/1000 G loss: 0.042536583 S loss: 0.37967629358172417\n",
      "batch 172/1000 G loss: 0.03498987 S loss: 0.383170947432518\n",
      "batch 173/1000 G loss: 0.040141214 S loss: 0.41233812272548676\n",
      "batch 174/1000 G loss: 0.038151346 S loss: 0.3835265226662159\n",
      "batch 175/1000 G loss: 0.03721962 S loss: 0.36035491712391376\n",
      "batch 176/1000 G loss: 0.039808504 S loss: 0.37617331370711327\n",
      "batch 177/1000 G loss: 0.03439953 S loss: 0.3640616573393345\n",
      "batch 178/1000 G loss: 0.03748048 S loss: 0.3795734569430351\n",
      "batch 179/1000 G loss: 0.039019227 S loss: 0.37669431418180466\n",
      "batch 180/1000 G loss: 0.037712906 S loss: 0.38644643500447273\n",
      "Student test loss: [3.6092477912902834, 0.10000000149011612]\n",
      "batch 181/1000 G loss: 0.034250923 S loss: 0.36295933835208416\n",
      "batch 182/1000 G loss: 0.037902366 S loss: 0.38291481509804726\n",
      "batch 183/1000 G loss: 0.041471407 S loss: 0.3849771171808243\n",
      "batch 184/1000 G loss: 0.03530503 S loss: 0.35682596638798714\n",
      "batch 185/1000 G loss: 0.037343472 S loss: 0.3606663681566715\n",
      "batch 186/1000 G loss: 0.03715399 S loss: 0.3819456361234188\n",
      "batch 187/1000 G loss: 0.037277084 S loss: 0.38086112216115\n",
      "batch 188/1000 G loss: 0.03752224 S loss: 0.3808314986526966\n",
      "batch 189/1000 G loss: 0.03537786 S loss: 0.3663349226117134\n",
      "batch 190/1000 G loss: 0.035691775 S loss: 0.3778034634888172\n",
      "Student test loss: [3.6002100547790525, 0.10000000149011612]\n",
      "batch 191/1000 G loss: 0.041201893 S loss: 0.36379972100257874\n",
      "batch 192/1000 G loss: 0.0426631 S loss: 0.38574012741446495\n",
      "batch 193/1000 G loss: 0.037306122 S loss: 0.363128662109375\n",
      "batch 194/1000 G loss: 0.0387235 S loss: 0.4058987498283386\n",
      "batch 195/1000 G loss: 0.0345333 S loss: 0.38001809269189835\n",
      "batch 196/1000 G loss: 0.034697928 S loss: 0.3790360055863857\n",
      "batch 197/1000 G loss: 0.04005701 S loss: 0.3955274038016796\n",
      "batch 198/1000 G loss: 0.04787471 S loss: 0.42823950946331024\n",
      "batch 199/1000 G loss: 0.034021363 S loss: 0.3695426881313324\n",
      "batch 200/1000 G loss: 0.035013575 S loss: 0.3612353950738907\n",
      "Student test loss: [3.6490307403564453, 0.10000000149011612]\n",
      "batch 201/1000 G loss: 0.03503935 S loss: 0.3680095374584198\n",
      "batch 202/1000 G loss: 0.037644535 S loss: 0.3646475113928318\n",
      "batch 203/1000 G loss: 0.039975673 S loss: 0.35691646486520767\n",
      "batch 204/1000 G loss: 0.03750264 S loss: 0.3624436780810356\n",
      "batch 205/1000 G loss: 0.031138586 S loss: 0.3336120266467333\n",
      "batch 206/1000 G loss: 0.036851708 S loss: 0.38578496128320694\n",
      "batch 207/1000 G loss: 0.041647725 S loss: 0.3885888271033764\n",
      "batch 208/1000 G loss: 0.037351027 S loss: 0.3622554764151573\n",
      "batch 209/1000 G loss: 0.03729881 S loss: 0.3618870861828327\n",
      "batch 210/1000 G loss: 0.036153704 S loss: 0.36242663487792015\n",
      "Student test loss: [3.6556379653930664, 0.10000000149011612]\n",
      "batch 211/1000 G loss: 0.037645657 S loss: 0.37879352644085884\n",
      "batch 212/1000 G loss: 0.038274616 S loss: 0.3631874993443489\n",
      "batch 213/1000 G loss: 0.035957333 S loss: 0.3701881170272827\n",
      "batch 214/1000 G loss: 0.041080877 S loss: 0.3717644512653351\n",
      "batch 215/1000 G loss: 0.034715094 S loss: 0.35705480352044106\n",
      "batch 216/1000 G loss: 0.03472164 S loss: 0.3552453741431236\n",
      "batch 217/1000 G loss: 0.03820456 S loss: 0.38799961283802986\n",
      "batch 218/1000 G loss: 0.04059527 S loss: 0.3612789884209633\n",
      "batch 219/1000 G loss: 0.029458985 S loss: 0.35039297118782997\n",
      "batch 220/1000 G loss: 0.036181245 S loss: 0.3762897700071335\n",
      "Student test loss: [3.612230680847168, 0.10000000149011612]\n",
      "batch 221/1000 G loss: 0.038530607 S loss: 0.37922313809394836\n",
      "batch 222/1000 G loss: 0.0387234 S loss: 0.3754285164177418\n",
      "batch 223/1000 G loss: 0.037708152 S loss: 0.34515491127967834\n",
      "batch 224/1000 G loss: 0.04270599 S loss: 0.3881019316613674\n",
      "batch 225/1000 G loss: 0.03574901 S loss: 0.3536631688475609\n",
      "batch 226/1000 G loss: 0.036536355 S loss: 0.3592388816177845\n",
      "batch 227/1000 G loss: 0.032175347 S loss: 0.35121722891926765\n",
      "batch 228/1000 G loss: 0.03282859 S loss: 0.3500031530857086\n",
      "batch 229/1000 G loss: 0.037171714 S loss: 0.36895814910531044\n",
      "batch 230/1000 G loss: 0.039692905 S loss: 0.3852001316845417\n",
      "Student test loss: [3.6603453384399414, 0.10000000149011612]\n",
      "batch 231/1000 G loss: 0.0379111 S loss: 0.3615655228495598\n",
      "batch 232/1000 G loss: 0.033832457 S loss: 0.3575473129749298\n",
      "batch 233/1000 G loss: 0.04010561 S loss: 0.3852094002068043\n",
      "batch 234/1000 G loss: 0.035126984 S loss: 0.3848031684756279\n",
      "batch 235/1000 G loss: 0.036770724 S loss: 0.35116926953196526\n",
      "batch 236/1000 G loss: 0.03201033 S loss: 0.3264007717370987\n",
      "batch 237/1000 G loss: 0.03195113 S loss: 0.35515909641981125\n",
      "batch 238/1000 G loss: 0.03724669 S loss: 0.3402073197066784\n",
      "batch 239/1000 G loss: 0.03523964 S loss: 0.340602507814765\n",
      "batch 240/1000 G loss: 0.036656734 S loss: 0.35479818657040596\n",
      "Student test loss: [3.5957446060180662, 0.10000000149011612]\n",
      "batch 241/1000 G loss: 0.037090138 S loss: 0.34713554941117764\n",
      "batch 242/1000 G loss: 0.038591497 S loss: 0.34782521426677704\n",
      "batch 243/1000 G loss: 0.03633353 S loss: 0.3736528567969799\n",
      "batch 244/1000 G loss: 0.03408654 S loss: 0.3413710966706276\n",
      "batch 245/1000 G loss: 0.032417193 S loss: 0.3480699993669987\n",
      "batch 246/1000 G loss: 0.03640631 S loss: 0.3752303272485733\n",
      "batch 247/1000 G loss: 0.03341315 S loss: 0.36262886226177216\n",
      "batch 248/1000 G loss: 0.03483117 S loss: 0.35988721437752247\n",
      "batch 249/1000 G loss: 0.036938995 S loss: 0.3620402030646801\n",
      "batch 250/1000 G loss: 0.038560014 S loss: 0.3608611598610878\n",
      "Student test loss: [3.585571731567383, 0.10000000149011612]\n",
      "batch 251/1000 G loss: 0.035315204 S loss: 0.3313289601355791\n",
      "batch 252/1000 G loss: 0.032665566 S loss: 0.3394596166908741\n",
      "batch 253/1000 G loss: 0.02957751 S loss: 0.33617724291980267\n",
      "batch 254/1000 G loss: 0.037667736 S loss: 0.37591784074902534\n",
      "batch 255/1000 G loss: 0.039180376 S loss: 0.3523785471916199\n",
      "batch 256/1000 G loss: 0.03388358 S loss: 0.33364134654402733\n",
      "batch 257/1000 G loss: 0.03492242 S loss: 0.36771854013204575\n",
      "batch 258/1000 G loss: 0.03338685 S loss: 0.33865518122911453\n",
      "batch 259/1000 G loss: 0.035987098 S loss: 0.34517640992999077\n",
      "batch 260/1000 G loss: 0.032672957 S loss: 0.342491514980793\n",
      "Student test loss: [3.5708393630981443, 0.10000000149011612]\n",
      "batch 261/1000 G loss: 0.037597694 S loss: 0.3666582927107811\n",
      "batch 262/1000 G loss: 0.030654766 S loss: 0.3532157577574253\n",
      "batch 263/1000 G loss: 0.03844364 S loss: 0.38073383644223213\n",
      "batch 264/1000 G loss: 0.03516349 S loss: 0.36210013926029205\n",
      "batch 265/1000 G loss: 0.038327888 S loss: 0.3750498332083225\n",
      "batch 266/1000 G loss: 0.03212549 S loss: 0.34384729340672493\n",
      "batch 267/1000 G loss: 0.03555666 S loss: 0.33539576828479767\n",
      "batch 268/1000 G loss: 0.03264915 S loss: 0.3404267933219671\n",
      "batch 269/1000 G loss: 0.0418412 S loss: 0.35507166385650635\n",
      "batch 270/1000 G loss: 0.03548233 S loss: 0.3689374402165413\n",
      "Student test loss: [3.585238645172119, 0.10000000149011612]\n",
      "batch 271/1000 G loss: 0.041073147 S loss: 0.36166999489068985\n",
      "batch 272/1000 G loss: 0.036685105 S loss: 0.35525577887892723\n",
      "batch 273/1000 G loss: 0.03393562 S loss: 0.34088171645998955\n",
      "batch 274/1000 G loss: 0.03687056 S loss: 0.3513610251247883\n",
      "batch 275/1000 G loss: 0.031286627 S loss: 0.3390692174434662\n",
      "batch 276/1000 G loss: 0.029731223 S loss: 0.3235652819275856\n",
      "batch 277/1000 G loss: 0.032566585 S loss: 0.3503508009016514\n",
      "batch 278/1000 G loss: 0.038698427 S loss: 0.355459026992321\n",
      "batch 279/1000 G loss: 0.03628242 S loss: 0.35116974264383316\n",
      "batch 280/1000 G loss: 0.033708367 S loss: 0.36045365035533905\n",
      "Student test loss: [3.5545952239990233, 0.10000000149011612]\n",
      "batch 281/1000 G loss: 0.036169246 S loss: 0.3334278427064419\n",
      "batch 282/1000 G loss: 0.040898323 S loss: 0.36163707077503204\n",
      "batch 283/1000 G loss: 0.033795446 S loss: 0.34856698103249073\n",
      "batch 284/1000 G loss: 0.03649596 S loss: 0.33606542088091373\n",
      "batch 285/1000 G loss: 0.031185927 S loss: 0.34415964037179947\n",
      "batch 286/1000 G loss: 0.033789977 S loss: 0.3377538323402405\n",
      "batch 287/1000 G loss: 0.032125276 S loss: 0.3410450592637062\n",
      "batch 288/1000 G loss: 0.036057655 S loss: 0.3574428893625736\n",
      "batch 289/1000 G loss: 0.038273163 S loss: 0.3583504259586334\n",
      "batch 290/1000 G loss: 0.031873453 S loss: 0.33057546243071556\n",
      "Student test loss: [3.544446881866455, 0.10000000149011612]\n",
      "batch 291/1000 G loss: 0.031618915 S loss: 0.33871646225452423\n",
      "batch 292/1000 G loss: 0.037049457 S loss: 0.3395851328969002\n",
      "batch 293/1000 G loss: 0.035407044 S loss: 0.373028714209795\n",
      "batch 294/1000 G loss: 0.03298843 S loss: 0.34514107182621956\n",
      "batch 295/1000 G loss: 0.031125367 S loss: 0.3387053459882736\n",
      "batch 296/1000 G loss: 0.032637957 S loss: 0.3487446680665016\n",
      "batch 297/1000 G loss: 0.040310107 S loss: 0.38122594729065895\n",
      "batch 298/1000 G loss: 0.037644036 S loss: 0.35895878821611404\n",
      "batch 299/1000 G loss: 0.035326444 S loss: 0.347139623016119\n",
      "batch 300/1000 G loss: 0.039952993 S loss: 0.36689241975545883\n",
      "Student test loss: [3.548880065917969, 0.10000000149011612]\n",
      "batch 301/1000 G loss: 0.03645148 S loss: 0.38600026816129684\n",
      "batch 302/1000 G loss: 0.036220808 S loss: 0.36119868978857994\n",
      "batch 303/1000 G loss: 0.036328465 S loss: 0.3274869415909052\n",
      "batch 304/1000 G loss: 0.036315016 S loss: 0.35240960866212845\n",
      "batch 305/1000 G loss: 0.03557252 S loss: 0.35756389051675797\n",
      "batch 306/1000 G loss: 0.040199842 S loss: 0.3522535152733326\n",
      "batch 307/1000 G loss: 0.038018137 S loss: 0.3560698553919792\n",
      "batch 308/1000 G loss: 0.034974217 S loss: 0.32648069597780704\n",
      "batch 309/1000 G loss: 0.030999545 S loss: 0.3435719273984432\n",
      "batch 310/1000 G loss: 0.031583853 S loss: 0.3412332385778427\n",
      "Student test loss: [3.515755903625488, 0.10000000149011612]\n",
      "batch 311/1000 G loss: 0.03863796 S loss: 0.3257267866283655\n",
      "batch 312/1000 G loss: 0.035607506 S loss: 0.34815120697021484\n",
      "batch 313/1000 G loss: 0.036609292 S loss: 0.36736344546079636\n",
      "batch 314/1000 G loss: 0.034035005 S loss: 0.3458268791437149\n",
      "batch 315/1000 G loss: 0.034591295 S loss: 0.3396666422486305\n",
      "batch 316/1000 G loss: 0.036385447 S loss: 0.3920007757842541\n",
      "batch 317/1000 G loss: 0.036462516 S loss: 0.359559278935194\n",
      "batch 318/1000 G loss: 0.036258966 S loss: 0.3838264085352421\n",
      "batch 319/1000 G loss: 0.03816466 S loss: 0.33704905584454536\n",
      "batch 320/1000 G loss: 0.032522663 S loss: 0.32639018446207047\n",
      "Student test loss: [3.5010556427001953, 0.10000000149011612]\n",
      "batch 321/1000 G loss: 0.036346562 S loss: 0.3649993911385536\n",
      "batch 322/1000 G loss: 0.032288615 S loss: 0.34541401267051697\n",
      "batch 323/1000 G loss: 0.037584417 S loss: 0.36163734272122383\n",
      "batch 324/1000 G loss: 0.03653573 S loss: 0.3555578552186489\n",
      "batch 325/1000 G loss: 0.036000334 S loss: 0.34815680235624313\n",
      "batch 326/1000 G loss: 0.031864267 S loss: 0.31825099140405655\n",
      "batch 327/1000 G loss: 0.031869333 S loss: 0.33613273687660694\n",
      "batch 328/1000 G loss: 0.036433868 S loss: 0.34196537733078003\n",
      "batch 329/1000 G loss: 0.037732497 S loss: 0.36187470331788063\n",
      "batch 330/1000 G loss: 0.036882736 S loss: 0.37854937463998795\n",
      "Student test loss: [3.519607724761963, 0.10000000149011612]\n",
      "batch 331/1000 G loss: 0.03447966 S loss: 0.3503853604197502\n",
      "batch 332/1000 G loss: 0.03607191 S loss: 0.34143882244825363\n",
      "batch 333/1000 G loss: 0.04076387 S loss: 0.36160431429743767\n",
      "batch 334/1000 G loss: 0.039347988 S loss: 0.36671143770217896\n",
      "batch 335/1000 G loss: 0.03394866 S loss: 0.32701217383146286\n",
      "batch 336/1000 G loss: 0.033347078 S loss: 0.35686778277158737\n",
      "batch 337/1000 G loss: 0.033321388 S loss: 0.35426826775074005\n",
      "batch 338/1000 G loss: 0.038603928 S loss: 0.36771873012185097\n",
      "batch 339/1000 G loss: 0.034103185 S loss: 0.3474942147731781\n",
      "batch 340/1000 G loss: 0.032104682 S loss: 0.3483373783528805\n",
      "Student test loss: [3.4521275245666505, 0.10010000318288803]\n",
      "batch 341/1000 G loss: 0.031780913 S loss: 0.343832116574049\n",
      "batch 342/1000 G loss: 0.036257338 S loss: 0.33010670728981495\n",
      "batch 343/1000 G loss: 0.030022625 S loss: 0.33295703306794167\n",
      "batch 344/1000 G loss: 0.0375184 S loss: 0.36491840332746506\n",
      "batch 345/1000 G loss: 0.031623382 S loss: 0.3249993063509464\n",
      "batch 346/1000 G loss: 0.03494968 S loss: 0.3546599969267845\n",
      "batch 347/1000 G loss: 0.036422875 S loss: 0.3460072949528694\n",
      "batch 348/1000 G loss: 0.036271647 S loss: 0.3372396416962147\n",
      "batch 349/1000 G loss: 0.034748524 S loss: 0.34523147717118263\n",
      "batch 350/1000 G loss: 0.03190894 S loss: 0.3418003413826227\n",
      "Student test loss: [3.4337746974945067, 0.10010000318288803]\n",
      "batch 351/1000 G loss: 0.034983356 S loss: 0.38466424122452736\n",
      "batch 352/1000 G loss: 0.035595268 S loss: 0.33409779891371727\n",
      "batch 353/1000 G loss: 0.031658165 S loss: 0.3337068259716034\n",
      "batch 354/1000 G loss: 0.035872474 S loss: 0.3429022245109081\n",
      "batch 355/1000 G loss: 0.040375352 S loss: 0.390034556388855\n",
      "batch 356/1000 G loss: 0.032491427 S loss: 0.33797144144773483\n",
      "batch 357/1000 G loss: 0.03097207 S loss: 0.32862164825201035\n",
      "batch 358/1000 G loss: 0.03258512 S loss: 0.3459340073168278\n",
      "batch 359/1000 G loss: 0.033381738 S loss: 0.35772860422730446\n",
      "batch 360/1000 G loss: 0.031359296 S loss: 0.3281568791717291\n",
      "Student test loss: [3.413015863800049, 0.10010000318288803]\n",
      "batch 361/1000 G loss: 0.03905534 S loss: 0.3646754249930382\n",
      "batch 362/1000 G loss: 0.034043632 S loss: 0.33354245498776436\n",
      "batch 363/1000 G loss: 0.035400122 S loss: 0.3368837870657444\n",
      "batch 364/1000 G loss: 0.03726975 S loss: 0.35032322257757187\n",
      "batch 365/1000 G loss: 0.03267917 S loss: 0.338546646758914\n",
      "batch 366/1000 G loss: 0.03301459 S loss: 0.33972588926553726\n",
      "batch 367/1000 G loss: 0.037160315 S loss: 0.36167336627840996\n",
      "batch 368/1000 G loss: 0.035456687 S loss: 0.3370420914143324\n",
      "batch 369/1000 G loss: 0.02835076 S loss: 0.3217657804489136\n",
      "batch 370/1000 G loss: 0.034670632 S loss: 0.34480491280555725\n",
      "Student test loss: [3.4098543437957765, 0.10010000318288803]\n",
      "batch 371/1000 G loss: 0.034121785 S loss: 0.32037088088691235\n",
      "batch 372/1000 G loss: 0.03211161 S loss: 0.34449268132448196\n",
      "batch 373/1000 G loss: 0.035038933 S loss: 0.33526250161230564\n",
      "batch 374/1000 G loss: 0.033885747 S loss: 0.32057065330445766\n",
      "batch 375/1000 G loss: 0.03087645 S loss: 0.32505546510219574\n",
      "batch 376/1000 G loss: 0.035067305 S loss: 0.3253077417612076\n",
      "batch 377/1000 G loss: 0.03238804 S loss: 0.33832214400172234\n",
      "batch 378/1000 G loss: 0.031905044 S loss: 0.32553140446543694\n",
      "batch 379/1000 G loss: 0.03678783 S loss: 0.329630296677351\n",
      "batch 380/1000 G loss: 0.03533435 S loss: 0.3396134786307812\n",
      "Student test loss: [3.3605199768066405, 0.10010000318288803]\n",
      "batch 381/1000 G loss: 0.034683995 S loss: 0.32826116494834423\n",
      "batch 382/1000 G loss: 0.03492175 S loss: 0.3011418431997299\n",
      "batch 383/1000 G loss: 0.036509454 S loss: 0.3369947839528322\n",
      "batch 384/1000 G loss: 0.03155582 S loss: 0.312605457380414\n",
      "batch 385/1000 G loss: 0.034474336 S loss: 0.33303056098520756\n",
      "batch 386/1000 G loss: 0.028962273 S loss: 0.2972048372030258\n",
      "batch 387/1000 G loss: 0.035387643 S loss: 0.33016716316342354\n",
      "batch 388/1000 G loss: 0.03301947 S loss: 0.32521481439471245\n",
      "batch 389/1000 G loss: 0.03496867 S loss: 0.3459865041077137\n",
      "batch 390/1000 G loss: 0.033844274 S loss: 0.31698255240917206\n",
      "Student test loss: [3.383896128845215, 0.10010000318288803]\n",
      "batch 391/1000 G loss: 0.03091484 S loss: 0.32223018631339073\n",
      "batch 392/1000 G loss: 0.03496427 S loss: 0.3367960676550865\n",
      "batch 393/1000 G loss: 0.03481122 S loss: 0.3487868160009384\n",
      "batch 394/1000 G loss: 0.029798072 S loss: 0.2989055011421442\n",
      "batch 395/1000 G loss: 0.037828993 S loss: 0.3042826186865568\n",
      "batch 396/1000 G loss: 0.0337452 S loss: 0.3367648087441921\n",
      "batch 397/1000 G loss: 0.031233963 S loss: 0.31813022308051586\n",
      "batch 398/1000 G loss: 0.029857272 S loss: 0.3014401588588953\n",
      "batch 399/1000 G loss: 0.031383563 S loss: 0.325349785387516\n",
      "batch 400/1000 G loss: 0.030801697 S loss: 0.3140881583094597\n",
      "Student test loss: [3.3420300384521484, 0.10010000318288803]\n",
      "batch 401/1000 G loss: 0.033130646 S loss: 0.3280828595161438\n",
      "batch 402/1000 G loss: 0.033457376 S loss: 0.3129341769963503\n",
      "batch 403/1000 G loss: 0.03685368 S loss: 0.3039523418992758\n",
      "batch 404/1000 G loss: 0.031093694 S loss: 0.2901016939431429\n",
      "batch 405/1000 G loss: 0.03098185 S loss: 0.30690960586071014\n",
      "batch 406/1000 G loss: 0.033932824 S loss: 0.3071069549769163\n",
      "batch 407/1000 G loss: 0.032680623 S loss: 0.32816956005990505\n",
      "batch 408/1000 G loss: 0.03096968 S loss: 0.3106955997645855\n",
      "batch 409/1000 G loss: 0.031956904 S loss: 0.2971079144626856\n",
      "batch 410/1000 G loss: 0.034528308 S loss: 0.3184965215623379\n",
      "Student test loss: [3.3265387458801268, 0.10010000318288803]\n",
      "batch 411/1000 G loss: 0.031206112 S loss: 0.29713130183517933\n",
      "batch 412/1000 G loss: 0.032519642 S loss: 0.31703756377100945\n",
      "batch 413/1000 G loss: 0.030293882 S loss: 0.2931153830140829\n",
      "batch 414/1000 G loss: 0.03132796 S loss: 0.29686474427580833\n",
      "batch 415/1000 G loss: 0.031032696 S loss: 0.3042513560503721\n",
      "batch 416/1000 G loss: 0.033153683 S loss: 0.3028471637517214\n",
      "batch 417/1000 G loss: 0.029861689 S loss: 0.31344641372561455\n",
      "batch 418/1000 G loss: 0.03155081 S loss: 0.2862079478800297\n",
      "batch 419/1000 G loss: 0.03281011 S loss: 0.31072053872048855\n",
      "batch 420/1000 G loss: 0.033504713 S loss: 0.28568768315017223\n",
      "Student test loss: [3.3371186515808104, 0.10000000149011612]\n",
      "batch 421/1000 G loss: 0.029487714 S loss: 0.2862963378429413\n",
      "batch 422/1000 G loss: 0.0344763 S loss: 0.32605403289198875\n",
      "batch 423/1000 G loss: 0.027226483 S loss: 0.26611956395208836\n",
      "batch 424/1000 G loss: 0.031955812 S loss: 0.29367814399302006\n",
      "batch 425/1000 G loss: 0.031278666 S loss: 0.29975104331970215\n",
      "batch 426/1000 G loss: 0.031025069 S loss: 0.29399389028549194\n",
      "batch 427/1000 G loss: 0.035299446 S loss: 0.2839154489338398\n",
      "batch 428/1000 G loss: 0.030746303 S loss: 0.3019533883780241\n",
      "batch 429/1000 G loss: 0.02998219 S loss: 0.28813115507364273\n",
      "batch 430/1000 G loss: 0.026867915 S loss: 0.29359931126236916\n",
      "Student test loss: [3.314517157745361, 0.10000000149011612]\n",
      "batch 431/1000 G loss: 0.025994992 S loss: 0.2858945671468973\n",
      "batch 432/1000 G loss: 0.03027903 S loss: 0.3020968995988369\n",
      "batch 433/1000 G loss: 0.031372007 S loss: 0.2959256060421467\n",
      "batch 434/1000 G loss: 0.0301065 S loss: 0.28819760121405125\n",
      "batch 435/1000 G loss: 0.028608702 S loss: 0.2748361639678478\n",
      "batch 436/1000 G loss: 0.028559133 S loss: 0.28994358517229557\n",
      "batch 437/1000 G loss: 0.028808784 S loss: 0.2945132851600647\n",
      "batch 438/1000 G loss: 0.031977005 S loss: 0.3092174995690584\n",
      "batch 439/1000 G loss: 0.03769475 S loss: 0.2854850459843874\n",
      "batch 440/1000 G loss: 0.027854394 S loss: 0.29986860789358616\n",
      "Student test loss: [3.2790705013275145, 0.10000000149011612]\n",
      "batch 441/1000 G loss: 0.028781954 S loss: 0.2608942277729511\n",
      "batch 442/1000 G loss: 0.029231748 S loss: 0.2925539016723633\n",
      "batch 443/1000 G loss: 0.03146791 S loss: 0.30247912742197514\n",
      "batch 444/1000 G loss: 0.033062562 S loss: 0.3103350717574358\n",
      "batch 445/1000 G loss: 0.028337672 S loss: 0.28237619437277317\n",
      "batch 446/1000 G loss: 0.033384494 S loss: 0.2919579800218344\n",
      "batch 447/1000 G loss: 0.02938636 S loss: 0.29323374293744564\n",
      "batch 448/1000 G loss: 0.032958094 S loss: 0.29920100793242455\n",
      "batch 449/1000 G loss: 0.0322002 S loss: 0.3069729506969452\n",
      "batch 450/1000 G loss: 0.028407209 S loss: 0.27164057083427906\n",
      "Student test loss: [3.294166166687012, 0.10010000318288803]\n",
      "batch 451/1000 G loss: 0.03274908 S loss: 0.2989205773919821\n",
      "batch 452/1000 G loss: 0.028361928 S loss: 0.2587771452963352\n",
      "batch 453/1000 G loss: 0.028699942 S loss: 0.27719644270837307\n",
      "batch 454/1000 G loss: 0.03669025 S loss: 0.3250914830714464\n",
      "batch 455/1000 G loss: 0.027759127 S loss: 0.2545351404696703\n",
      "batch 456/1000 G loss: 0.030989813 S loss: 0.27517305314540863\n",
      "batch 457/1000 G loss: 0.02846654 S loss: 0.2758689634501934\n",
      "batch 458/1000 G loss: 0.030583883 S loss: 0.278530890122056\n",
      "batch 459/1000 G loss: 0.030470183 S loss: 0.2946051899343729\n",
      "batch 460/1000 G loss: 0.026567396 S loss: 0.2772290911525488\n",
      "Student test loss: [3.29215838432312, 0.10019999742507935]\n",
      "batch 461/1000 G loss: 0.031561717 S loss: 0.3030853047966957\n",
      "batch 462/1000 G loss: 0.029702406 S loss: 0.2785146627575159\n",
      "batch 463/1000 G loss: 0.027726566 S loss: 0.2816055715084076\n",
      "batch 464/1000 G loss: 0.028667364 S loss: 0.28334196098148823\n",
      "batch 465/1000 G loss: 0.029239018 S loss: 0.2823571041226387\n",
      "batch 466/1000 G loss: 0.0268186 S loss: 0.2692766208201647\n",
      "batch 467/1000 G loss: 0.029013729 S loss: 0.26997483149170876\n",
      "batch 468/1000 G loss: 0.030397285 S loss: 0.2804655358195305\n",
      "batch 469/1000 G loss: 0.028269965 S loss: 0.2577983718365431\n",
      "batch 470/1000 G loss: 0.029897418 S loss: 0.2822054475545883\n",
      "Student test loss: [3.2699862705230713, 0.10010000318288803]\n",
      "batch 471/1000 G loss: 0.028170621 S loss: 0.2742000464349985\n",
      "batch 472/1000 G loss: 0.029995464 S loss: 0.29649121314287186\n",
      "batch 473/1000 G loss: 0.025001273 S loss: 0.2712901942431927\n",
      "batch 474/1000 G loss: 0.027937591 S loss: 0.2823541797697544\n",
      "batch 475/1000 G loss: 0.031581674 S loss: 0.3089375365525484\n",
      "batch 476/1000 G loss: 0.026587779 S loss: 0.267309483140707\n",
      "batch 477/1000 G loss: 0.027912688 S loss: 0.2804700192064047\n",
      "batch 478/1000 G loss: 0.027331065 S loss: 0.2673831321299076\n",
      "batch 479/1000 G loss: 0.029649863 S loss: 0.2683749794960022\n",
      "batch 480/1000 G loss: 0.02757151 S loss: 0.2630922868847847\n",
      "Student test loss: [3.233737623596191, 0.10000000149011612]\n",
      "batch 481/1000 G loss: 0.030274255 S loss: 0.27140119299292564\n",
      "batch 482/1000 G loss: 0.02849421 S loss: 0.2771091628819704\n",
      "batch 483/1000 G loss: 0.028618861 S loss: 0.27413323894143105\n",
      "batch 484/1000 G loss: 0.026918754 S loss: 0.2658334020525217\n",
      "batch 485/1000 G loss: 0.032814622 S loss: 0.27806307189166546\n",
      "batch 486/1000 G loss: 0.031483043 S loss: 0.273617135360837\n",
      "batch 487/1000 G loss: 0.02685948 S loss: 0.2798666376620531\n",
      "batch 488/1000 G loss: 0.02965683 S loss: 0.26004709117114544\n",
      "batch 489/1000 G loss: 0.027059376 S loss: 0.2688729502260685\n",
      "batch 490/1000 G loss: 0.025616039 S loss: 0.25864847749471664\n",
      "Student test loss: [3.244183801269531, 0.10019999742507935]\n",
      "batch 491/1000 G loss: 0.027449407 S loss: 0.2798096649348736\n",
      "batch 492/1000 G loss: 0.028472193 S loss: 0.2740602008998394\n",
      "batch 493/1000 G loss: 0.030034628 S loss: 0.2591806501150131\n",
      "batch 494/1000 G loss: 0.027750524 S loss: 0.2528288923203945\n",
      "batch 495/1000 G loss: 0.02650234 S loss: 0.2551363334059715\n",
      "batch 496/1000 G loss: 0.023553539 S loss: 0.2515238970518112\n",
      "batch 497/1000 G loss: 0.02868471 S loss: 0.27112461254000664\n",
      "batch 498/1000 G loss: 0.024215553 S loss: 0.26314976811408997\n",
      "batch 499/1000 G loss: 0.030602261 S loss: 0.27114385925233364\n",
      "batch 500/1000 G loss: 0.024680175 S loss: 0.2625709753483534\n",
      "Student test loss: [3.2554327743530274, 0.10000000149011612]\n",
      "batch 501/1000 G loss: 0.025643485 S loss: 0.2665348034352064\n",
      "batch 502/1000 G loss: 0.031397693 S loss: 0.2728678360581398\n",
      "batch 503/1000 G loss: 0.025772635 S loss: 0.24415479227900505\n",
      "batch 504/1000 G loss: 0.02879852 S loss: 0.25701941177248955\n",
      "batch 505/1000 G loss: 0.027010337 S loss: 0.262304799631238\n",
      "batch 506/1000 G loss: 0.026638363 S loss: 0.25874288007616997\n",
      "batch 507/1000 G loss: 0.028210305 S loss: 0.2645384594798088\n",
      "batch 508/1000 G loss: 0.029240256 S loss: 0.26444631442427635\n",
      "batch 509/1000 G loss: 0.026068142 S loss: 0.25030908547341824\n",
      "batch 510/1000 G loss: 0.02495975 S loss: 0.2637451495975256\n",
      "Student test loss: [3.2508702182769778, 0.10000000149011612]\n",
      "batch 511/1000 G loss: 0.026923347 S loss: 0.2517731823027134\n",
      "batch 512/1000 G loss: 0.025947265 S loss: 0.2641328498721123\n",
      "batch 513/1000 G loss: 0.02467449 S loss: 0.2578291054815054\n",
      "batch 514/1000 G loss: 0.026355898 S loss: 0.2484369557350874\n",
      "batch 515/1000 G loss: 0.027885124 S loss: 0.2564551252871752\n",
      "batch 516/1000 G loss: 0.032135714 S loss: 0.25366242229938507\n",
      "batch 517/1000 G loss: 0.028448481 S loss: 0.2856415268033743\n",
      "batch 518/1000 G loss: 0.0258391 S loss: 0.2543775625526905\n",
      "batch 519/1000 G loss: 0.02736092 S loss: 0.2635543681681156\n",
      "batch 520/1000 G loss: 0.02776005 S loss: 0.2611782792955637\n",
      "Student test loss: [3.2309511932373045, 0.10010000318288803]\n",
      "batch 521/1000 G loss: 0.028108796 S loss: 0.2507916111499071\n",
      "batch 522/1000 G loss: 0.02514863 S loss: 0.2516302280128002\n",
      "batch 523/1000 G loss: 0.027491264 S loss: 0.28648686222732067\n",
      "batch 524/1000 G loss: 0.025780799 S loss: 0.2644889336079359\n",
      "batch 525/1000 G loss: 0.02613996 S loss: 0.2504648696631193\n",
      "batch 526/1000 G loss: 0.02572313 S loss: 0.2608636077493429\n",
      "batch 527/1000 G loss: 0.026964024 S loss: 0.2292015440762043\n",
      "batch 528/1000 G loss: 0.02620229 S loss: 0.257098451256752\n",
      "batch 529/1000 G loss: 0.024637146 S loss: 0.23805665597319603\n",
      "batch 530/1000 G loss: 0.028714934 S loss: 0.2614150308072567\n",
      "Student test loss: [3.2033733043670654, 0.10000000149011612]\n",
      "batch 531/1000 G loss: 0.031583376 S loss: 0.2675032243132591\n",
      "batch 532/1000 G loss: 0.02822318 S loss: 0.24455945566296577\n",
      "batch 533/1000 G loss: 0.027555233 S loss: 0.24407105892896652\n",
      "batch 534/1000 G loss: 0.025732491 S loss: 0.24489093199372292\n",
      "batch 535/1000 G loss: 0.024726184 S loss: 0.25140537694096565\n",
      "batch 536/1000 G loss: 0.026155438 S loss: 0.25184089690446854\n",
      "batch 537/1000 G loss: 0.02679326 S loss: 0.24439500272274017\n",
      "batch 538/1000 G loss: 0.024206718 S loss: 0.23871622420847416\n",
      "batch 539/1000 G loss: 0.028056668 S loss: 0.24565844982862473\n",
      "batch 540/1000 G loss: 0.024978653 S loss: 0.2512137722223997\n",
      "Student test loss: [3.2183650508880617, 0.10010000318288803]\n",
      "batch 541/1000 G loss: 0.023551028 S loss: 0.23835538886487484\n",
      "batch 542/1000 G loss: 0.024762964 S loss: 0.23786990530788898\n",
      "batch 543/1000 G loss: 0.024348538 S loss: 0.22883512824773788\n",
      "batch 544/1000 G loss: 0.02528606 S loss: 0.2410587389022112\n",
      "batch 545/1000 G loss: 0.024688497 S loss: 0.23082087561488152\n",
      "batch 546/1000 G loss: 0.026094627 S loss: 0.23495949245989323\n",
      "batch 547/1000 G loss: 0.025318222 S loss: 0.23337624967098236\n",
      "batch 548/1000 G loss: 0.0238897 S loss: 0.24062583781778812\n",
      "batch 549/1000 G loss: 0.027656958 S loss: 0.2554536573588848\n",
      "batch 550/1000 G loss: 0.025021244 S loss: 0.2363748475909233\n",
      "Student test loss: [3.229960988998413, 0.10010000318288803]\n",
      "batch 551/1000 G loss: 0.023377076 S loss: 0.22622466087341309\n",
      "batch 552/1000 G loss: 0.029357877 S loss: 0.2372301835566759\n",
      "batch 553/1000 G loss: 0.028736731 S loss: 0.24964955262839794\n",
      "batch 554/1000 G loss: 0.022990445 S loss: 0.23383552767336369\n",
      "batch 555/1000 G loss: 0.024931546 S loss: 0.24162401258945465\n",
      "batch 556/1000 G loss: 0.026123317 S loss: 0.24307551607489586\n",
      "batch 557/1000 G loss: 0.022715582 S loss: 0.23282497748732567\n",
      "batch 558/1000 G loss: 0.027487412 S loss: 0.24918073788285255\n",
      "batch 559/1000 G loss: 0.02524925 S loss: 0.24897979386150837\n",
      "batch 560/1000 G loss: 0.026594765 S loss: 0.22898486629128456\n",
      "Student test loss: [3.2394038917541503, 0.10000000149011612]\n",
      "batch 561/1000 G loss: 0.024697386 S loss: 0.26077039912343025\n",
      "batch 562/1000 G loss: 0.02324866 S loss: 0.25948695093393326\n",
      "batch 563/1000 G loss: 0.02857435 S loss: 0.26087586022913456\n",
      "batch 564/1000 G loss: 0.02475183 S loss: 0.2343530524522066\n",
      "batch 565/1000 G loss: 0.02570535 S loss: 0.24628022871911526\n",
      "batch 566/1000 G loss: 0.02806589 S loss: 0.2517834436148405\n",
      "batch 567/1000 G loss: 0.028806217 S loss: 0.2527578007429838\n",
      "batch 568/1000 G loss: 0.025112148 S loss: 0.25519191287457943\n",
      "batch 569/1000 G loss: 0.027120803 S loss: 0.25829528272151947\n",
      "batch 570/1000 G loss: 0.028095288 S loss: 0.2566452566534281\n",
      "Student test loss: [3.2281945808410644, 0.10010000318288803]\n",
      "batch 571/1000 G loss: 0.024077889 S loss: 0.24477901682257652\n",
      "batch 572/1000 G loss: 0.023783002 S loss: 0.24599668756127357\n",
      "batch 573/1000 G loss: 0.028196767 S loss: 0.23758777230978012\n",
      "batch 574/1000 G loss: 0.02797779 S loss: 0.2439477387815714\n",
      "batch 575/1000 G loss: 0.027706305 S loss: 0.24847254902124405\n",
      "batch 576/1000 G loss: 0.026863221 S loss: 0.2478539366275072\n",
      "batch 577/1000 G loss: 0.025206018 S loss: 0.2570283096283674\n",
      "batch 578/1000 G loss: 0.026869241 S loss: 0.24840244837105274\n",
      "batch 579/1000 G loss: 0.025745198 S loss: 0.23563632555305958\n",
      "batch 580/1000 G loss: 0.026624896 S loss: 0.2325319442898035\n",
      "Student test loss: [3.2203133865356444, 0.10000000149011612]\n",
      "batch 581/1000 G loss: 0.023442773 S loss: 0.2408088855445385\n",
      "batch 582/1000 G loss: 0.023706947 S loss: 0.24570315890014172\n",
      "batch 583/1000 G loss: 0.02515115 S loss: 0.2544376887381077\n",
      "batch 584/1000 G loss: 0.029583035 S loss: 0.2687536980956793\n",
      "batch 585/1000 G loss: 0.025234453 S loss: 0.23801278695464134\n",
      "batch 586/1000 G loss: 0.02527175 S loss: 0.23957432806491852\n",
      "batch 587/1000 G loss: 0.023971444 S loss: 0.24011185951530933\n",
      "batch 588/1000 G loss: 0.02330856 S loss: 0.23439250327646732\n",
      "batch 589/1000 G loss: 0.021779498 S loss: 0.23132935725152493\n",
      "batch 590/1000 G loss: 0.025969759 S loss: 0.24682802893221378\n",
      "Student test loss: [3.200399059677124, 0.09969999641180038]\n",
      "batch 591/1000 G loss: 0.025381329 S loss: 0.23752889968454838\n",
      "batch 592/1000 G loss: 0.02379018 S loss: 0.2372597474604845\n",
      "batch 593/1000 G loss: 0.022238426 S loss: 0.2284926436841488\n",
      "batch 594/1000 G loss: 0.022519104 S loss: 0.2348383441567421\n",
      "batch 595/1000 G loss: 0.024283554 S loss: 0.24815176613628864\n",
      "batch 596/1000 G loss: 0.02088812 S loss: 0.23331387341022491\n",
      "batch 597/1000 G loss: 0.024935829 S loss: 0.22146944142878056\n",
      "batch 598/1000 G loss: 0.023330055 S loss: 0.234877098351717\n",
      "batch 599/1000 G loss: 0.022289332 S loss: 0.22543030604720116\n",
      "batch 600/1000 G loss: 0.02660722 S loss: 0.2311350367963314\n",
      "Student test loss: [3.188778172302246, 0.09960000216960907]\n",
      "batch 601/1000 G loss: 0.025180344 S loss: 0.22771307453513145\n",
      "batch 602/1000 G loss: 0.023032892 S loss: 0.21857525780797005\n",
      "batch 603/1000 G loss: 0.024776839 S loss: 0.2396581806242466\n",
      "batch 604/1000 G loss: 0.023033971 S loss: 0.2302151843905449\n",
      "batch 605/1000 G loss: 0.020453302 S loss: 0.23785613849759102\n",
      "batch 606/1000 G loss: 0.021583607 S loss: 0.22899032942950726\n",
      "batch 607/1000 G loss: 0.023401594 S loss: 0.2253474872559309\n",
      "batch 608/1000 G loss: 0.021809066 S loss: 0.22629109397530556\n",
      "batch 609/1000 G loss: 0.02350231 S loss: 0.23184570483863354\n",
      "batch 610/1000 G loss: 0.02588899 S loss: 0.24090188182890415\n",
      "Student test loss: [3.165412046813965, 0.09969999641180038]\n",
      "batch 611/1000 G loss: 0.022346396 S loss: 0.23242494091391563\n",
      "batch 612/1000 G loss: 0.026753003 S loss: 0.22682934813201427\n",
      "batch 613/1000 G loss: 0.024217442 S loss: 0.23280511423945427\n",
      "batch 614/1000 G loss: 0.025722133 S loss: 0.2198526617139578\n",
      "batch 615/1000 G loss: 0.027732538 S loss: 0.2486053705215454\n",
      "batch 616/1000 G loss: 0.024495505 S loss: 0.24292282946407795\n",
      "batch 617/1000 G loss: 0.02684279 S loss: 0.24011950939893723\n",
      "batch 618/1000 G loss: 0.022854734 S loss: 0.23317387886345387\n",
      "batch 619/1000 G loss: 0.024804492 S loss: 0.23457200825214386\n",
      "batch 620/1000 G loss: 0.02397727 S loss: 0.23347554728388786\n",
      "Student test loss: [3.1588679611206056, 0.09969999641180038]\n",
      "batch 621/1000 G loss: 0.025213901 S loss: 0.23320991732180119\n",
      "batch 622/1000 G loss: 0.02498813 S loss: 0.2435304131358862\n",
      "batch 623/1000 G loss: 0.02386738 S loss: 0.21580163948237896\n",
      "batch 624/1000 G loss: 0.021829784 S loss: 0.23896580003201962\n",
      "batch 625/1000 G loss: 0.025493205 S loss: 0.23596982844173908\n",
      "batch 626/1000 G loss: 0.019490927 S loss: 0.22789335437119007\n",
      "batch 627/1000 G loss: 0.026951142 S loss: 0.2328780647367239\n",
      "batch 628/1000 G loss: 0.021735502 S loss: 0.2361548077315092\n",
      "batch 629/1000 G loss: 0.024910185 S loss: 0.23058976791799068\n",
      "batch 630/1000 G loss: 0.026283033 S loss: 0.2512416746467352\n",
      "Student test loss: [3.155404461288452, 0.09960000216960907]\n",
      "batch 631/1000 G loss: 0.0262869 S loss: 0.23516444861888885\n",
      "batch 632/1000 G loss: 0.022050343 S loss: 0.2502879537642002\n",
      "batch 633/1000 G loss: 0.023988906 S loss: 0.253274267539382\n",
      "batch 634/1000 G loss: 0.024146399 S loss: 0.2567462585866451\n",
      "batch 635/1000 G loss: 0.024412315 S loss: 0.25421942584216595\n",
      "batch 636/1000 G loss: 0.026151994 S loss: 0.24022570624947548\n",
      "batch 637/1000 G loss: 0.024705457 S loss: 0.2383293453603983\n",
      "batch 638/1000 G loss: 0.029261878 S loss: 0.2566102296113968\n",
      "batch 639/1000 G loss: 0.025901185 S loss: 0.2452964298427105\n",
      "batch 640/1000 G loss: 0.025304405 S loss: 0.22843140177428722\n",
      "Student test loss: [3.134442366027832, 0.09960000216960907]\n",
      "batch 641/1000 G loss: 0.024266005 S loss: 0.23606543987989426\n",
      "batch 642/1000 G loss: 0.022761526 S loss: 0.22593004815280437\n",
      "batch 643/1000 G loss: 0.022786489 S loss: 0.23580141924321651\n",
      "batch 644/1000 G loss: 0.020560017 S loss: 0.24551027826964855\n",
      "batch 645/1000 G loss: 0.022569187 S loss: 0.2351867537945509\n",
      "batch 646/1000 G loss: 0.024889924 S loss: 0.2243904024362564\n",
      "batch 647/1000 G loss: 0.022422127 S loss: 0.22508578561246395\n",
      "batch 648/1000 G loss: 0.025571559 S loss: 0.2261823359876871\n",
      "batch 649/1000 G loss: 0.02322632 S loss: 0.22944211773574352\n",
      "batch 650/1000 G loss: 0.026053503 S loss: 0.2380603365600109\n",
      "Student test loss: [3.1188005290985106, 0.09969999641180038]\n",
      "batch 651/1000 G loss: 0.02347636 S loss: 0.23475078120827675\n",
      "batch 652/1000 G loss: 0.024388427 S loss: 0.23117169737815857\n",
      "batch 653/1000 G loss: 0.021858308 S loss: 0.23908467404544353\n",
      "batch 654/1000 G loss: 0.021891959 S loss: 0.23701447062194347\n",
      "batch 655/1000 G loss: 0.023507178 S loss: 0.22955377586185932\n",
      "batch 656/1000 G loss: 0.02507077 S loss: 0.23876039497554302\n",
      "batch 657/1000 G loss: 0.023302302 S loss: 0.22902044281363487\n",
      "batch 658/1000 G loss: 0.023513054 S loss: 0.23062726482748985\n",
      "batch 659/1000 G loss: 0.02440709 S loss: 0.23702874779701233\n",
      "batch 660/1000 G loss: 0.025145363 S loss: 0.2513223607093096\n",
      "Student test loss: [3.113639387512207, 0.09969999641180038]\n",
      "batch 661/1000 G loss: 0.024116784 S loss: 0.22376413084566593\n",
      "batch 662/1000 G loss: 0.025156828 S loss: 0.23190075531601906\n",
      "batch 663/1000 G loss: 0.023116762 S loss: 0.21985275112092495\n",
      "batch 664/1000 G loss: 0.024862207 S loss: 0.23370729945600033\n",
      "batch 665/1000 G loss: 0.02527297 S loss: 0.22137865237891674\n",
      "batch 666/1000 G loss: 0.021407032 S loss: 0.22267861478030682\n",
      "batch 667/1000 G loss: 0.021857385 S loss: 0.2217191644012928\n",
      "batch 668/1000 G loss: 0.024610875 S loss: 0.23189321346580982\n",
      "batch 669/1000 G loss: 0.02128154 S loss: 0.2121359370648861\n",
      "batch 670/1000 G loss: 0.022456039 S loss: 0.20308510959148407\n",
      "Student test loss: [3.0613679153442384, 0.09969999641180038]\n",
      "batch 671/1000 G loss: 0.02130757 S loss: 0.22773386910557747\n",
      "batch 672/1000 G loss: 0.022014424 S loss: 0.22162660211324692\n",
      "batch 673/1000 G loss: 0.022764964 S loss: 0.22350364737212658\n",
      "batch 674/1000 G loss: 0.021633554 S loss: 0.20686323195695877\n",
      "batch 675/1000 G loss: 0.019944573 S loss: 0.2175139058381319\n",
      "batch 676/1000 G loss: 0.022792988 S loss: 0.2077971063554287\n",
      "batch 677/1000 G loss: 0.024437886 S loss: 0.21243810281157494\n",
      "batch 678/1000 G loss: 0.020583272 S loss: 0.2035166509449482\n",
      "batch 679/1000 G loss: 0.0236883 S loss: 0.21857758797705173\n",
      "batch 680/1000 G loss: 0.021525657 S loss: 0.2120954878628254\n",
      "Student test loss: [3.0670037368774414, 0.09969999641180038]\n",
      "batch 681/1000 G loss: 0.021635424 S loss: 0.21195886097848415\n",
      "batch 682/1000 G loss: 0.022243941 S loss: 0.21872983500361443\n",
      "batch 683/1000 G loss: 0.023854593 S loss: 0.19979859702289104\n",
      "batch 684/1000 G loss: 0.02503156 S loss: 0.2172787170857191\n",
      "batch 685/1000 G loss: 0.023772214 S loss: 0.22215992398560047\n",
      "batch 686/1000 G loss: 0.024835903 S loss: 0.2354026883840561\n",
      "batch 687/1000 G loss: 0.021801434 S loss: 0.21356731839478016\n",
      "batch 688/1000 G loss: 0.023905821 S loss: 0.22435816749930382\n",
      "batch 689/1000 G loss: 0.021072133 S loss: 0.21388858929276466\n",
      "batch 690/1000 G loss: 0.021060538 S loss: 0.20810357481241226\n",
      "Student test loss: [3.0916998054504394, 0.09969999641180038]\n",
      "batch 691/1000 G loss: 0.020163467 S loss: 0.2038396205753088\n",
      "batch 692/1000 G loss: 0.022024347 S loss: 0.2161346711218357\n",
      "batch 693/1000 G loss: 0.025211759 S loss: 0.21263357810676098\n",
      "batch 694/1000 G loss: 0.021279156 S loss: 0.2190232314169407\n",
      "batch 695/1000 G loss: 0.023492642 S loss: 0.23013649694621563\n",
      "batch 696/1000 G loss: 0.019975137 S loss: 0.22281388007104397\n",
      "batch 697/1000 G loss: 0.019415565 S loss: 0.19771341793239117\n",
      "batch 698/1000 G loss: 0.022389544 S loss: 0.20464900881052017\n",
      "batch 699/1000 G loss: 0.019871384 S loss: 0.19474872015416622\n",
      "batch 700/1000 G loss: 0.023516674 S loss: 0.20588960126042366\n",
      "Student test loss: [3.0614431396484374, 0.09969999641180038]\n",
      "batch 701/1000 G loss: 0.022054505 S loss: 0.19756297953426838\n",
      "batch 702/1000 G loss: 0.022385843 S loss: 0.22378351353108883\n",
      "batch 703/1000 G loss: 0.021275107 S loss: 0.2069153469055891\n",
      "batch 704/1000 G loss: 0.018993221 S loss: 0.20900688879191875\n",
      "batch 705/1000 G loss: 0.021795586 S loss: 0.22034626454114914\n",
      "batch 706/1000 G loss: 0.02050315 S loss: 0.21687588281929493\n",
      "batch 707/1000 G loss: 0.023879174 S loss: 0.2203324157744646\n",
      "batch 708/1000 G loss: 0.024048204 S loss: 0.23642234690487385\n",
      "batch 709/1000 G loss: 0.02336203 S loss: 0.20316840894520283\n",
      "batch 710/1000 G loss: 0.02085022 S loss: 0.1979620661586523\n",
      "Student test loss: [3.069030879974365, 0.09960000216960907]\n",
      "batch 711/1000 G loss: 0.024754055 S loss: 0.2183609325438738\n",
      "batch 712/1000 G loss: 0.020290682 S loss: 0.19888282753527164\n",
      "batch 713/1000 G loss: 0.021933276 S loss: 0.2039019539952278\n",
      "batch 714/1000 G loss: 0.02346364 S loss: 0.21921079233288765\n",
      "batch 715/1000 G loss: 0.020974897 S loss: 0.21017340756952763\n",
      "batch 716/1000 G loss: 0.019173224 S loss: 0.1912049576640129\n",
      "batch 717/1000 G loss: 0.022650585 S loss: 0.21924061886966228\n",
      "batch 718/1000 G loss: 0.019886114 S loss: 0.20280338823795319\n",
      "batch 719/1000 G loss: 0.025236093 S loss: 0.22573668882250786\n",
      "batch 720/1000 G loss: 0.01942722 S loss: 0.20505299232900143\n",
      "Student test loss: [3.0580386131286623, 0.09969999641180038]\n",
      "batch 721/1000 G loss: 0.026516186 S loss: 0.21758491918444633\n",
      "batch 722/1000 G loss: 0.02351189 S loss: 0.2042768280953169\n",
      "batch 723/1000 G loss: 0.019165782 S loss: 0.20500507950782776\n",
      "batch 724/1000 G loss: 0.021039851 S loss: 0.20604109019041061\n",
      "batch 725/1000 G loss: 0.019618228 S loss: 0.20024252496659756\n",
      "batch 726/1000 G loss: 0.021151414 S loss: 0.2075772024691105\n",
      "batch 727/1000 G loss: 0.024728952 S loss: 0.22506660968065262\n",
      "batch 728/1000 G loss: 0.024777569 S loss: 0.21803398989140987\n",
      "batch 729/1000 G loss: 0.021044318 S loss: 0.21121851913630962\n",
      "batch 730/1000 G loss: 0.026786746 S loss: 0.1997954249382019\n",
      "Student test loss: [3.0543723716735838, 0.09969999641180038]\n",
      "batch 731/1000 G loss: 0.025610032 S loss: 0.2104472555220127\n",
      "batch 732/1000 G loss: 0.02436933 S loss: 0.2202534954994917\n",
      "batch 733/1000 G loss: 0.025201095 S loss: 0.22115663811564445\n",
      "batch 734/1000 G loss: 0.022525858 S loss: 0.21108016930520535\n",
      "batch 735/1000 G loss: 0.026984505 S loss: 0.23294664919376373\n",
      "batch 736/1000 G loss: 0.022908358 S loss: 0.2270973138511181\n",
      "batch 737/1000 G loss: 0.021387972 S loss: 0.21663819067180157\n",
      "batch 738/1000 G loss: 0.022186693 S loss: 0.2178275678306818\n",
      "batch 739/1000 G loss: 0.023616025 S loss: 0.19602293521165848\n",
      "batch 740/1000 G loss: 0.024116103 S loss: 0.226776210591197\n",
      "Student test loss: [3.052770195388794, 0.10000000149011612]\n",
      "batch 741/1000 G loss: 0.022731269 S loss: 0.2120973877608776\n",
      "batch 742/1000 G loss: 0.019904962 S loss: 0.21316197514533997\n",
      "batch 743/1000 G loss: 0.022085646 S loss: 0.19900951348245144\n",
      "batch 744/1000 G loss: 0.024147622 S loss: 0.2191261388361454\n",
      "batch 745/1000 G loss: 0.025832629 S loss: 0.22118159756064415\n",
      "batch 746/1000 G loss: 0.021111593 S loss: 0.2180702704936266\n",
      "batch 747/1000 G loss: 0.023933928 S loss: 0.21752271056175232\n",
      "batch 748/1000 G loss: 0.021031938 S loss: 0.20699560642242432\n",
      "batch 749/1000 G loss: 0.021767031 S loss: 0.21806082874536514\n",
      "batch 750/1000 G loss: 0.023082959 S loss: 0.2197391502559185\n",
      "Student test loss: [3.066763442230225, 0.0997999981045723]\n",
      "batch 751/1000 G loss: 0.02242522 S loss: 0.22254700027406216\n",
      "batch 752/1000 G loss: 0.022514334 S loss: 0.213006729260087\n",
      "batch 753/1000 G loss: 0.02583972 S loss: 0.20778485760092735\n",
      "batch 754/1000 G loss: 0.021935312 S loss: 0.21258008107542992\n",
      "batch 755/1000 G loss: 0.023827946 S loss: 0.21986925601959229\n",
      "batch 756/1000 G loss: 0.02245906 S loss: 0.2058964967727661\n",
      "batch 757/1000 G loss: 0.02302563 S loss: 0.1982118971645832\n",
      "batch 758/1000 G loss: 0.02235993 S loss: 0.2122630998492241\n",
      "batch 759/1000 G loss: 0.019599302 S loss: 0.19768434949219227\n",
      "batch 760/1000 G loss: 0.01988507 S loss: 0.21563689783215523\n",
      "Student test loss: [3.0795384170532225, 0.10000000149011612]\n",
      "batch 761/1000 G loss: 0.022557208 S loss: 0.2114989534020424\n",
      "batch 762/1000 G loss: 0.022178791 S loss: 0.1980126854032278\n",
      "batch 763/1000 G loss: 0.01931316 S loss: 0.1914476901292801\n",
      "batch 764/1000 G loss: 0.021689873 S loss: 0.20616088062524796\n",
      "batch 765/1000 G loss: 0.022541936 S loss: 0.1978831086307764\n",
      "batch 766/1000 G loss: 0.019261835 S loss: 0.20241644978523254\n",
      "batch 767/1000 G loss: 0.02011476 S loss: 0.20601080171763897\n",
      "batch 768/1000 G loss: 0.02151221 S loss: 0.19991463422775269\n",
      "batch 769/1000 G loss: 0.022740494 S loss: 0.21410142444074154\n",
      "batch 770/1000 G loss: 0.018718246 S loss: 0.20790001563727856\n",
      "Student test loss: [3.1061602382659914, 0.10000000149011612]\n",
      "batch 771/1000 G loss: 0.021380369 S loss: 0.198764992877841\n",
      "batch 772/1000 G loss: 0.027447626 S loss: 0.2121857088059187\n",
      "batch 773/1000 G loss: 0.019497372 S loss: 0.2149283103644848\n",
      "batch 774/1000 G loss: 0.021272138 S loss: 0.2143804281949997\n",
      "batch 775/1000 G loss: 0.021218827 S loss: 0.2159495186060667\n",
      "batch 776/1000 G loss: 0.020308567 S loss: 0.20055966451764107\n",
      "batch 777/1000 G loss: 0.024441697 S loss: 0.21053896471858025\n",
      "batch 778/1000 G loss: 0.022382211 S loss: 0.21076295711100101\n",
      "batch 779/1000 G loss: 0.020411488 S loss: 0.2078183013945818\n",
      "batch 780/1000 G loss: 0.021957994 S loss: 0.20016895793378353\n",
      "Student test loss: [3.102863321685791, 0.09989999979734421]\n",
      "batch 781/1000 G loss: 0.02279501 S loss: 0.20669919066131115\n",
      "batch 782/1000 G loss: 0.020636447 S loss: 0.20000766031444073\n",
      "batch 783/1000 G loss: 0.025015192 S loss: 0.2212857659906149\n",
      "batch 784/1000 G loss: 0.01846969 S loss: 0.20174386352300644\n",
      "batch 785/1000 G loss: 0.0207687 S loss: 0.22551541589200497\n",
      "batch 786/1000 G loss: 0.022056773 S loss: 0.21234025061130524\n",
      "batch 787/1000 G loss: 0.01965734 S loss: 0.1993345282971859\n",
      "batch 788/1000 G loss: 0.020092074 S loss: 0.21915907226502895\n",
      "batch 789/1000 G loss: 0.021800928 S loss: 0.21248259581625462\n",
      "batch 790/1000 G loss: 0.022539893 S loss: 0.20088215172290802\n",
      "Student test loss: [3.1356474380493164, 0.10000000149011612]\n",
      "batch 791/1000 G loss: 0.02066347 S loss: 0.207526545971632\n",
      "batch 792/1000 G loss: 0.021947825 S loss: 0.21855956688523293\n",
      "batch 793/1000 G loss: 0.01868077 S loss: 0.20088213123381138\n",
      "batch 794/1000 G loss: 0.02206279 S loss: 0.22048796340823174\n",
      "batch 795/1000 G loss: 0.0247409 S loss: 0.21142970956861973\n",
      "batch 796/1000 G loss: 0.021329544 S loss: 0.21042058244347572\n",
      "batch 797/1000 G loss: 0.020765293 S loss: 0.2091660313308239\n",
      "batch 798/1000 G loss: 0.02145185 S loss: 0.21657724305987358\n",
      "batch 799/1000 G loss: 0.023749247 S loss: 0.22414527833461761\n",
      "batch 800/1000 G loss: 0.021772489 S loss: 0.197912460193038\n",
      "Student test loss: [3.1570927394866946, 0.09989999979734421]\n",
      "batch 801/1000 G loss: 0.02117733 S loss: 0.20579689368605614\n",
      "batch 802/1000 G loss: 0.022399178 S loss: 0.20570141822099686\n",
      "batch 803/1000 G loss: 0.022357738 S loss: 0.2101455181837082\n",
      "batch 804/1000 G loss: 0.021182925 S loss: 0.2140574585646391\n",
      "batch 805/1000 G loss: 0.021711657 S loss: 0.21764416992664337\n",
      "batch 806/1000 G loss: 0.019204007 S loss: 0.21509366109967232\n",
      "batch 807/1000 G loss: 0.022785645 S loss: 0.21045566722750664\n",
      "batch 808/1000 G loss: 0.02431969 S loss: 0.21296267583966255\n",
      "batch 809/1000 G loss: 0.023175735 S loss: 0.22523746639490128\n",
      "batch 810/1000 G loss: 0.024696482 S loss: 0.20043090544641018\n",
      "Student test loss: [3.132515096282959, 0.09989999979734421]\n",
      "batch 811/1000 G loss: 0.02025557 S loss: 0.2060315515846014\n",
      "batch 812/1000 G loss: 0.022928895 S loss: 0.21768805384635925\n",
      "batch 813/1000 G loss: 0.021567378 S loss: 0.2104835081845522\n",
      "batch 814/1000 G loss: 0.023507908 S loss: 0.21612749248743057\n",
      "batch 815/1000 G loss: 0.02251536 S loss: 0.2184386756271124\n",
      "batch 816/1000 G loss: 0.022956638 S loss: 0.21020080335438251\n",
      "batch 817/1000 G loss: 0.022703009 S loss: 0.20464082807302475\n",
      "batch 818/1000 G loss: 0.022985429 S loss: 0.21711505763232708\n",
      "batch 819/1000 G loss: 0.020999197 S loss: 0.20722396299242973\n",
      "batch 820/1000 G loss: 0.024863984 S loss: 0.22617746330797672\n",
      "Student test loss: [3.1152726593017577, 0.10000000149011612]\n",
      "batch 821/1000 G loss: 0.020491313 S loss: 0.19959493912756443\n",
      "batch 822/1000 G loss: 0.019316958 S loss: 0.1889433804899454\n",
      "batch 823/1000 G loss: 0.021823518 S loss: 0.20583859272301197\n",
      "batch 824/1000 G loss: 0.021198865 S loss: 0.20861313119530678\n",
      "batch 825/1000 G loss: 0.019713726 S loss: 0.21016012132167816\n",
      "batch 826/1000 G loss: 0.020129085 S loss: 0.2162741106003523\n",
      "batch 827/1000 G loss: 0.02194471 S loss: 0.22077660635113716\n",
      "batch 828/1000 G loss: 0.020993564 S loss: 0.19730217941105366\n",
      "batch 829/1000 G loss: 0.022281434 S loss: 0.19808387756347656\n",
      "batch 830/1000 G loss: 0.01920498 S loss: 0.20021573826670647\n",
      "Student test loss: [3.099253268432617, 0.10000000149011612]\n",
      "batch 831/1000 G loss: 0.022852117 S loss: 0.20991316251456738\n",
      "batch 832/1000 G loss: 0.020897338 S loss: 0.1979104932397604\n",
      "batch 833/1000 G loss: 0.02183005 S loss: 0.20502676628530025\n",
      "batch 834/1000 G loss: 0.02218473 S loss: 0.2012581117451191\n",
      "batch 835/1000 G loss: 0.022553613 S loss: 0.1996273659169674\n",
      "batch 836/1000 G loss: 0.021815753 S loss: 0.21238948218524456\n",
      "batch 837/1000 G loss: 0.019710626 S loss: 0.19744983315467834\n",
      "batch 838/1000 G loss: 0.02315369 S loss: 0.229903906583786\n",
      "batch 839/1000 G loss: 0.0223547 S loss: 0.2002682276070118\n",
      "batch 840/1000 G loss: 0.02125017 S loss: 0.20382460206747055\n",
      "Student test loss: [3.0942315101623534, 0.10000000149011612]\n",
      "batch 841/1000 G loss: 0.019213311 S loss: 0.20482820458710194\n",
      "batch 842/1000 G loss: 0.021439914 S loss: 0.19768318347632885\n",
      "batch 843/1000 G loss: 0.020453785 S loss: 0.2184091303497553\n",
      "batch 844/1000 G loss: 0.020212214 S loss: 0.1915819738060236\n",
      "batch 845/1000 G loss: 0.02229574 S loss: 0.2125309705734253\n",
      "batch 846/1000 G loss: 0.019636353 S loss: 0.1875688098371029\n",
      "batch 847/1000 G loss: 0.019950284 S loss: 0.20525940880179405\n",
      "batch 848/1000 G loss: 0.021185182 S loss: 0.2014086116105318\n",
      "batch 849/1000 G loss: 0.019625947 S loss: 0.19321906566619873\n",
      "batch 850/1000 G loss: 0.02366188 S loss: 0.2142065353691578\n",
      "Student test loss: [3.0964592864990235, 0.10000000149011612]\n",
      "batch 851/1000 G loss: 0.019360056 S loss: 0.20712600275874138\n",
      "batch 852/1000 G loss: 0.018737413 S loss: 0.19851014204323292\n",
      "batch 853/1000 G loss: 0.018261606 S loss: 0.19031999446451664\n",
      "batch 854/1000 G loss: 0.022141118 S loss: 0.20900026708841324\n",
      "batch 855/1000 G loss: 0.02269221 S loss: 0.19452771171927452\n",
      "batch 856/1000 G loss: 0.019593526 S loss: 0.18791040033102036\n",
      "batch 857/1000 G loss: 0.020942401 S loss: 0.19871323555707932\n",
      "batch 858/1000 G loss: 0.020596132 S loss: 0.20542792044579983\n",
      "batch 859/1000 G loss: 0.018452276 S loss: 0.19939588569104671\n",
      "batch 860/1000 G loss: 0.022930816 S loss: 0.21034851111471653\n",
      "Student test loss: [3.067418854522705, 0.10000000149011612]\n",
      "batch 861/1000 G loss: 0.02182509 S loss: 0.21068519167602062\n",
      "batch 862/1000 G loss: 0.020896206 S loss: 0.18918219581246376\n",
      "batch 863/1000 G loss: 0.020757783 S loss: 0.19545720145106316\n",
      "batch 864/1000 G loss: 0.020110548 S loss: 0.18662642687559128\n",
      "batch 865/1000 G loss: 0.019971538 S loss: 0.1985979788005352\n",
      "batch 866/1000 G loss: 0.020389304 S loss: 0.1851210705935955\n",
      "batch 867/1000 G loss: 0.017599368 S loss: 0.19629957526922226\n",
      "batch 868/1000 G loss: 0.020963944 S loss: 0.18534784205257893\n",
      "batch 869/1000 G loss: 0.02046933 S loss: 0.19922177121043205\n",
      "batch 870/1000 G loss: 0.019160613 S loss: 0.19661538675427437\n",
      "Student test loss: [3.065335316467285, 0.10000000149011612]\n",
      "batch 871/1000 G loss: 0.021208582 S loss: 0.19638085179030895\n",
      "batch 872/1000 G loss: 0.018486971 S loss: 0.1841854825615883\n",
      "batch 873/1000 G loss: 0.018855449 S loss: 0.18759586103260517\n",
      "batch 874/1000 G loss: 0.01955624 S loss: 0.18505711667239666\n",
      "batch 875/1000 G loss: 0.022777183 S loss: 0.1990796308964491\n",
      "batch 876/1000 G loss: 0.020416144 S loss: 0.195245411247015\n",
      "batch 877/1000 G loss: 0.018532958 S loss: 0.1812668088823557\n",
      "batch 878/1000 G loss: 0.018789392 S loss: 0.1834341064095497\n",
      "batch 879/1000 G loss: 0.020804426 S loss: 0.1962390597909689\n",
      "batch 880/1000 G loss: 0.02021072 S loss: 0.19242588803172112\n",
      "Student test loss: [3.0705775913238527, 0.10000000149011612]\n",
      "batch 881/1000 G loss: 0.019418664 S loss: 0.19338213466107845\n",
      "batch 882/1000 G loss: 0.0223308 S loss: 0.18688380159437656\n",
      "batch 883/1000 G loss: 0.020681022 S loss: 0.1953003667294979\n",
      "batch 884/1000 G loss: 0.018736959 S loss: 0.1889327559620142\n",
      "batch 885/1000 G loss: 0.020104844 S loss: 0.19424021430313587\n",
      "batch 886/1000 G loss: 0.020596547 S loss: 0.20472816936671734\n",
      "batch 887/1000 G loss: 0.020452926 S loss: 0.1948059070855379\n",
      "batch 888/1000 G loss: 0.021523401 S loss: 0.19198550656437874\n",
      "batch 889/1000 G loss: 0.018037643 S loss: 0.2051995601505041\n",
      "batch 890/1000 G loss: 0.019532356 S loss: 0.19656199030578136\n",
      "Student test loss: [3.0701832153320314, 0.10000000149011612]\n",
      "batch 891/1000 G loss: 0.020164821 S loss: 0.2007379550486803\n",
      "batch 892/1000 G loss: 0.020880286 S loss: 0.1992765460163355\n",
      "batch 893/1000 G loss: 0.020133793 S loss: 0.18444269336760044\n",
      "batch 894/1000 G loss: 0.019754995 S loss: 0.18152806535363197\n",
      "batch 895/1000 G loss: 0.02311367 S loss: 0.20131279155611992\n",
      "batch 896/1000 G loss: 0.023083778 S loss: 0.20667370036244392\n",
      "batch 897/1000 G loss: 0.020452484 S loss: 0.2020927555859089\n",
      "batch 898/1000 G loss: 0.019126413 S loss: 0.18686706759035587\n",
      "batch 899/1000 G loss: 0.020272575 S loss: 0.19241275824606419\n",
      "batch 900/1000 G loss: 0.018766928 S loss: 0.19261840358376503\n",
      "Student test loss: [3.083286051940918, 0.10000000149011612]\n",
      "batch 901/1000 G loss: 0.019599712 S loss: 0.19111289083957672\n",
      "batch 902/1000 G loss: 0.021419795 S loss: 0.18869180604815483\n",
      "batch 903/1000 G loss: 0.022719279 S loss: 0.20329499430954456\n",
      "batch 904/1000 G loss: 0.018561393 S loss: 0.1859019659459591\n",
      "batch 905/1000 G loss: 0.021199344 S loss: 0.20308159664273262\n",
      "batch 906/1000 G loss: 0.019259091 S loss: 0.18561229296028614\n",
      "batch 907/1000 G loss: 0.023362996 S loss: 0.19984819553792477\n",
      "batch 908/1000 G loss: 0.01753671 S loss: 0.1874652374535799\n",
      "batch 909/1000 G loss: 0.01978833 S loss: 0.1903548538684845\n",
      "batch 910/1000 G loss: 0.020214766 S loss: 0.19324010238051414\n",
      "Student test loss: [3.045678295135498, 0.10000000149011612]\n",
      "batch 911/1000 G loss: 0.019556519 S loss: 0.18248627334833145\n",
      "batch 912/1000 G loss: 0.022953112 S loss: 0.1996461506932974\n",
      "batch 913/1000 G loss: 0.018737514 S loss: 0.1873314417898655\n",
      "batch 914/1000 G loss: 0.017482745 S loss: 0.171471381559968\n",
      "batch 915/1000 G loss: 0.020922698 S loss: 0.18716813251376152\n",
      "batch 916/1000 G loss: 0.021167511 S loss: 0.19288137927651405\n",
      "batch 917/1000 G loss: 0.017597888 S loss: 0.1913709994405508\n",
      "batch 918/1000 G loss: 0.018358048 S loss: 0.18535571545362473\n",
      "batch 919/1000 G loss: 0.02051955 S loss: 0.18637258559465408\n",
      "batch 920/1000 G loss: 0.022701552 S loss: 0.18793112970888615\n",
      "Student test loss: [3.0420127510070802, 0.10000000149011612]\n",
      "batch 921/1000 G loss: 0.020324044 S loss: 0.19784421287477016\n",
      "batch 922/1000 G loss: 0.0192511 S loss: 0.1854229662567377\n",
      "batch 923/1000 G loss: 0.019919928 S loss: 0.1855161339044571\n",
      "batch 924/1000 G loss: 0.020658856 S loss: 0.18849158100783825\n",
      "batch 925/1000 G loss: 0.018305104 S loss: 0.17898927442729473\n",
      "batch 926/1000 G loss: 0.01735561 S loss: 0.18562989309430122\n",
      "batch 927/1000 G loss: 0.0165523 S loss: 0.172505222260952\n",
      "batch 928/1000 G loss: 0.019412274 S loss: 0.18427662178874016\n",
      "batch 929/1000 G loss: 0.01952821 S loss: 0.18194235302507877\n",
      "batch 930/1000 G loss: 0.018583052 S loss: 0.18587218783795834\n",
      "Student test loss: [3.0471161010742187, 0.10000000149011612]\n",
      "batch 931/1000 G loss: 0.019062823 S loss: 0.19720764830708504\n",
      "batch 932/1000 G loss: 0.018194739 S loss: 0.1737928157672286\n",
      "batch 933/1000 G loss: 0.019439956 S loss: 0.18446618504822254\n",
      "batch 934/1000 G loss: 0.020187529 S loss: 0.17725392617285252\n",
      "batch 935/1000 G loss: 0.01886322 S loss: 0.1820182055234909\n",
      "batch 936/1000 G loss: 0.021241982 S loss: 0.18733088113367558\n",
      "batch 937/1000 G loss: 0.021805014 S loss: 0.18796373903751373\n",
      "batch 938/1000 G loss: 0.019931572 S loss: 0.1797917913645506\n",
      "batch 939/1000 G loss: 0.019394089 S loss: 0.18846902810037136\n",
      "batch 940/1000 G loss: 0.019391613 S loss: 0.18608670122921467\n",
      "Student test loss: [3.046645662689209, 0.10000000149011612]\n",
      "batch 941/1000 G loss: 0.018906947 S loss: 0.19049250520765781\n",
      "batch 942/1000 G loss: 0.021105504 S loss: 0.19300090707838535\n",
      "batch 943/1000 G loss: 0.020404337 S loss: 0.1743121212348342\n",
      "batch 944/1000 G loss: 0.017060863 S loss: 0.17759312130510807\n",
      "batch 945/1000 G loss: 0.019612309 S loss: 0.19031627848744392\n",
      "batch 946/1000 G loss: 0.01913895 S loss: 0.19052430242300034\n",
      "batch 947/1000 G loss: 0.019385222 S loss: 0.1798152681440115\n",
      "batch 948/1000 G loss: 0.019191898 S loss: 0.182002367451787\n",
      "batch 949/1000 G loss: 0.01974208 S loss: 0.195475859567523\n",
      "batch 950/1000 G loss: 0.019705111 S loss: 0.17938613146543503\n",
      "Student test loss: [3.02107993888855, 0.10000000149011612]\n",
      "batch 951/1000 G loss: 0.018917918 S loss: 0.1908687073737383\n",
      "batch 952/1000 G loss: 0.019689713 S loss: 0.17848116904497147\n",
      "batch 953/1000 G loss: 0.020185651 S loss: 0.19392341002821922\n",
      "batch 954/1000 G loss: 0.018999783 S loss: 0.18352766521275043\n",
      "batch 955/1000 G loss: 0.017363526 S loss: 0.17416750080883503\n",
      "batch 956/1000 G loss: 0.01820656 S loss: 0.17799627780914307\n",
      "batch 957/1000 G loss: 0.019145075 S loss: 0.17293984442949295\n",
      "batch 958/1000 G loss: 0.01963479 S loss: 0.1755640022456646\n",
      "batch 959/1000 G loss: 0.017977223 S loss: 0.16918558720499277\n",
      "batch 960/1000 G loss: 0.020125536 S loss: 0.1790009494870901\n",
      "Student test loss: [3.0112900047302245, 0.10000000149011612]\n",
      "batch 961/1000 G loss: 0.021382973 S loss: 0.1847003512084484\n",
      "batch 962/1000 G loss: 0.020112291 S loss: 0.18152733892202377\n",
      "batch 963/1000 G loss: 0.016781343 S loss: 0.17312246467918158\n",
      "batch 964/1000 G loss: 0.018876564 S loss: 0.17474257573485374\n",
      "batch 965/1000 G loss: 0.020558268 S loss: 0.1804105658084154\n",
      "batch 966/1000 G loss: 0.015804006 S loss: 0.15477450285106897\n",
      "batch 967/1000 G loss: 0.019983158 S loss: 0.1897181887179613\n",
      "batch 968/1000 G loss: 0.023278842 S loss: 0.19115573726594448\n",
      "batch 969/1000 G loss: 0.017770667 S loss: 0.19529413804411888\n",
      "batch 970/1000 G loss: 0.017839883 S loss: 0.19013121351599693\n",
      "Student test loss: [3.0122303657531737, 0.10000000149011612]\n",
      "batch 971/1000 G loss: 0.020609312 S loss: 0.1962560322135687\n",
      "batch 972/1000 G loss: 0.019426566 S loss: 0.181458942592144\n",
      "batch 973/1000 G loss: 0.017191418 S loss: 0.17451251670718193\n",
      "batch 974/1000 G loss: 0.018954296 S loss: 0.18287922907620668\n",
      "batch 975/1000 G loss: 0.019683095 S loss: 0.16847460716962814\n",
      "batch 976/1000 G loss: 0.018059038 S loss: 0.18511893041431904\n",
      "batch 977/1000 G loss: 0.019305672 S loss: 0.18538515828549862\n",
      "batch 978/1000 G loss: 0.018704273 S loss: 0.17763016000390053\n",
      "batch 979/1000 G loss: 0.018870333 S loss: 0.1751458626240492\n",
      "batch 980/1000 G loss: 0.020141501 S loss: 0.18636172264814377\n",
      "Student test loss: [3.0249145473480223, 0.10000000149011612]\n",
      "batch 981/1000 G loss: 0.021168113 S loss: 0.18175463564693928\n",
      "batch 982/1000 G loss: 0.021135459 S loss: 0.17864501662552357\n",
      "batch 983/1000 G loss: 0.020221133 S loss: 0.18268891423940659\n",
      "batch 984/1000 G loss: 0.02024882 S loss: 0.18619877845048904\n",
      "batch 985/1000 G loss: 0.018018082 S loss: 0.1844179332256317\n",
      "batch 986/1000 G loss: 0.021736605 S loss: 0.18022369407117367\n",
      "batch 987/1000 G loss: 0.016744088 S loss: 0.18183147720992565\n",
      "batch 988/1000 G loss: 0.018384214 S loss: 0.16734704002738\n",
      "batch 989/1000 G loss: 0.02289198 S loss: 0.1889456994831562\n",
      "batch 990/1000 G loss: 0.016156768 S loss: 0.16923654451966286\n",
      "Student test loss: [3.020272581100464, 0.10000000149011612]\n",
      "batch 991/1000 G loss: 0.02078211 S loss: 0.17055697739124298\n",
      "batch 992/1000 G loss: 0.01977951 S loss: 0.1791329998522997\n",
      "batch 993/1000 G loss: 0.01783611 S loss: 0.17170486878603697\n",
      "batch 994/1000 G loss: 0.018268982 S loss: 0.1730771977454424\n",
      "batch 995/1000 G loss: 0.018362587 S loss: 0.16499709896743298\n",
      "batch 996/1000 G loss: 0.020186793 S loss: 0.1824856400489807\n",
      "batch 997/1000 G loss: 0.018726727 S loss: 0.1849979218095541\n",
      "batch 998/1000 G loss: 0.016619913 S loss: 0.1689554564654827\n",
      "batch 999/1000 G loss: 0.02095952 S loss: 0.17486050352454185\n",
      "Student test loss: [3.0049616245269775, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import keras\n",
    "from keract import get_activations\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "'''\n",
    "Function that returns the trainand test data of the CIFAR10 already preprocessed\n",
    "'''\n",
    "def getCIFAR10():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 32, 32\n",
    "    num_classes = 10\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # format of the tensor\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "        input_shape = (3, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "    # convert in to float the images\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # new normalization with z-score\n",
    "    mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "    std = np.std(x_train,axis=(0,1,2,3))\n",
    "    x_train = (x_train-mean)/(std+1e-7)\n",
    "    x_test = (x_test-mean)/(std+1e-7)\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    print('CIFAR10 loaded')\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "'''\n",
    "Small function that returns the shape of the CIFAR10 images\n",
    "'''\n",
    "def getCIFAR10InputShape():\n",
    "    img_rows, img_cols = 32, 32\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "        \n",
    "    return input_shape\n",
    "\n",
    "'''\n",
    "Function that loads from a file the teacher\n",
    "'''\n",
    "def getTeacher(file_name):\n",
    "    # Model reconstruction from JSON file\n",
    "    with open(file_name + '.json', 'r') as f:\n",
    "        model = model_from_json(f.read())\n",
    "\n",
    "    # Load weights into the new model\n",
    "    model.load_weights(file_name + '.h5')\n",
    "    \n",
    "    print('Teacher loaded from' + file_name + '.h5')\n",
    "    return model\n",
    "    \n",
    "'''\n",
    "Function that loads from a file the teacher and test it on the CIRAF10 dataset\n",
    "'''\n",
    "def testTeacher(file_name):\n",
    "    \n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    \n",
    "    model = getTeacher(file_name)\n",
    "    \n",
    "    # define optimizer\n",
    "    opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=opt_rms,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # final evaluation on test\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Teacher test loss:', score[0])\n",
    "    print('Teacher test accuracy:', score[1])\n",
    "\n",
    "    \n",
    "'''\n",
    "Function that returns a simple student done by 2 convolutions, a maxpool and a final two fully connected layers\n",
    "'''\n",
    "def getSimpleStudent(input_shape):\n",
    "    num_classes = 10\n",
    "    #model definition\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    print('Simple student loaded')\n",
    "    return model\n",
    "    \n",
    "'''\n",
    "Function to try to train the simple sutdent in order to unerstand its capabilites\n",
    "'''\n",
    "def trainSimpleStudent(epochs):\n",
    "    \n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    \n",
    "    input_shape = getCIFAR10InputShape()\n",
    "    \n",
    "    model = getSimpleStudent(input_shape)\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    batch_size = 128\n",
    "    n_batches = math.floor( x_train.shape[0] / batch_size)\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        for i in range(0,n_batches):\n",
    "            imgs = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            loss = model.train_on_batch(imgs,labels)\n",
    "            print(\"Epoch: \" + str(e+1) + \" batch \" + str(i) + \" loss: \" + str(loss[0]) + \" acc: \" + str( 100*loss[1]))\n",
    "            \n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('After epoch ' + str(e+1) + ' test loss ' + str(score[0]) + ' test accuracy ' + str(score[1]))\n",
    "\n",
    "'''\n",
    "Function that returns a simple generator\n",
    "'''\n",
    "def getGenerator():\n",
    "\n",
    "        noise_shape = (100,)\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        img_shape = getCIFAR10InputShape()\n",
    "\n",
    "        model.add(Dense(128, input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "        model.add(Reshape(img_shape))\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "        \n",
    "        print('Generator loaded')\n",
    "        return Model(noise, img)\n",
    "\n",
    "def minus_kld():\n",
    "    \n",
    "    keras_kld = tf.keras.losses.KLDivergence() \n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        \n",
    "        original_loss = keras_kld(y_true,y_pred)\n",
    "        loss_to_return = - original_loss\n",
    "        \n",
    "        return original_loss\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "        \n",
    "def main():\n",
    "    #testTeacher('model-16-2')\n",
    "    #trainSimpleStudent(4)\n",
    "    \n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    \n",
    "    teacher = getTeacher('model-16-2')\n",
    "    teacher.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    input_shape = getCIFAR10InputShape()\n",
    "    student = getSimpleStudent(input_shape)\n",
    "    \n",
    "    student.compile(loss='kld',\n",
    "                  optimizer='sgd',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    generator = getGenerator()\n",
    "    \n",
    "    # The generator takes noise as input and generated imgs\n",
    "    z = Input(shape=(100,))\n",
    "    img = generator(z)\n",
    "\n",
    "    # For the combined model we will only train the generator\n",
    "    student.trainable = False\n",
    "\n",
    "    # The valid takes generated images as input and determines validity\n",
    "    valid = student(img)\n",
    "\n",
    "    # The combined model  (stacked generator and discriminator) takes\n",
    "    # noise as input => generates images => determines validity\n",
    "    combined = Model(z, valid)\n",
    "    combined.compile(loss=minus_kld(), optimizer='adam')\n",
    "    \n",
    "    \n",
    "    n_batches = 1000\n",
    "    batch_size = 128\n",
    "    log_freq = 10\n",
    "    ns = 10\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        \n",
    "        t_predictions = teacher.predict(gen_imgs)\n",
    "        \n",
    "        g_loss = combined.train_on_batch(noise,t_predictions)\n",
    "        \n",
    "        s_loss = 0\n",
    "        for j in range(ns):\n",
    "            s_loss += student.train_on_batch(gen_imgs,t_predictions)[0]\n",
    "        \n",
    "        print('batch ' + str(i) + '/' + str(n_batches) + ' G loss: ' + str(g_loss) + ' S loss: ' + str(s_loss))\n",
    "        \n",
    "        if (i % log_freq) == 0:\n",
    "        # final evaluation on test\n",
    "            score = student.evaluate(x_test, y_test, verbose=0)\n",
    "            print('Student test loss: '  + str(score))\n",
    "        \n",
    "        \n",
    "    score = student.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Student test loss: '  + str(score))\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    kl_div = tf.keras.losses.KLDivergence()    \n",
    "    \n",
    "    for i in range(0,n_batches):\n",
    "        print('batch ' + str(i))\n",
    "        imgs = x_train[i*batch_size:(i+1)*batch_size]\n",
    "        labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "        t_predictions = teacher.predict(imgs)\n",
    "        s_predictions = student.predict(imgs)\n",
    "        print('teacher predictions: ')\n",
    "        print(t_predictions)\n",
    "        print('student predictions: ')\n",
    "        print(s_predictions)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # to print the KL divergence\n",
    "        for j in range(batch_size):\n",
    "            loss = kl_div(t_predictions[j],s_predictions[j])\n",
    "            with tf.Session() as sess:\n",
    "                init = tf.global_variables_initializer()\n",
    "                sess.run(init)\n",
    "                print(loss.eval())\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 loaded\n",
      "WARNING:tensorflow:From /home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Simple student loaded\n",
      "WARNING:tensorflow:From /home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch: 1 batch 0 loss: 2.4208488 acc: 10.15625\n",
      "Epoch: 1 batch 1 loss: 2.8245845 acc: 12.5\n",
      "Epoch: 1 batch 2 loss: 2.6858475 acc: 14.0625\n",
      "Epoch: 1 batch 3 loss: 2.195191 acc: 13.28125\n",
      "Epoch: 1 batch 4 loss: 2.2178483 acc: 21.09375\n",
      "Epoch: 1 batch 5 loss: 2.1723037 acc: 21.875\n",
      "Epoch: 1 batch 6 loss: 2.1966252 acc: 14.84375\n",
      "Epoch: 1 batch 7 loss: 2.2557395 acc: 16.40625\n",
      "Epoch: 1 batch 8 loss: 2.1758876 acc: 14.0625\n",
      "Epoch: 1 batch 9 loss: 2.1140943 acc: 18.75\n",
      "Epoch: 1 batch 10 loss: 2.1333125 acc: 19.53125\n",
      "Epoch: 1 batch 11 loss: 2.1481454 acc: 23.4375\n",
      "Epoch: 1 batch 12 loss: 2.1112804 acc: 24.21875\n",
      "Epoch: 1 batch 13 loss: 2.1517854 acc: 25.0\n",
      "Epoch: 1 batch 14 loss: 2.2812161 acc: 19.53125\n",
      "Epoch: 1 batch 15 loss: 2.240492 acc: 14.0625\n",
      "Epoch: 1 batch 16 loss: 2.1011388 acc: 21.09375\n",
      "Epoch: 1 batch 17 loss: 2.1198192 acc: 18.75\n",
      "Epoch: 1 batch 18 loss: 2.1487713 acc: 20.3125\n",
      "Epoch: 1 batch 19 loss: 2.0973768 acc: 24.21875\n",
      "Epoch: 1 batch 20 loss: 2.087379 acc: 25.0\n",
      "Epoch: 1 batch 21 loss: 2.0609012 acc: 25.0\n",
      "Epoch: 1 batch 22 loss: 2.002991 acc: 30.46875\n",
      "Epoch: 1 batch 23 loss: 2.2162204 acc: 21.875\n",
      "Epoch: 1 batch 24 loss: 2.143318 acc: 20.3125\n",
      "Epoch: 1 batch 25 loss: 2.064088 acc: 21.875\n",
      "Epoch: 1 batch 26 loss: 2.0571194 acc: 22.65625\n",
      "Epoch: 1 batch 27 loss: 1.9883413 acc: 34.375\n",
      "Epoch: 1 batch 28 loss: 2.0342805 acc: 28.90625\n",
      "Epoch: 1 batch 29 loss: 1.9050479 acc: 32.8125\n",
      "Epoch: 1 batch 30 loss: 2.1091685 acc: 22.65625\n",
      "Epoch: 1 batch 31 loss: 2.0259738 acc: 27.34375\n",
      "Epoch: 1 batch 32 loss: 1.9593996 acc: 28.125\n",
      "Epoch: 1 batch 33 loss: 2.0351634 acc: 24.21875\n",
      "Epoch: 1 batch 34 loss: 2.00098 acc: 27.34375\n",
      "Epoch: 1 batch 35 loss: 1.9636832 acc: 28.125\n",
      "Epoch: 1 batch 36 loss: 1.889015 acc: 31.25\n",
      "Epoch: 1 batch 37 loss: 2.072921 acc: 28.125\n",
      "Epoch: 1 batch 38 loss: 2.0582104 acc: 25.0\n",
      "Epoch: 1 batch 39 loss: 2.1570926 acc: 14.0625\n",
      "Epoch: 1 batch 40 loss: 2.1612868 acc: 25.0\n",
      "Epoch: 1 batch 41 loss: 1.9433274 acc: 30.46875\n",
      "Epoch: 1 batch 42 loss: 1.9895937 acc: 24.21875\n",
      "Epoch: 1 batch 43 loss: 1.855031 acc: 29.6875\n",
      "Epoch: 1 batch 44 loss: 1.9622087 acc: 25.78125\n",
      "Epoch: 1 batch 45 loss: 1.9971815 acc: 29.6875\n",
      "Epoch: 1 batch 46 loss: 1.8753693 acc: 32.8125\n",
      "Epoch: 1 batch 47 loss: 2.0527017 acc: 19.53125\n",
      "Epoch: 1 batch 48 loss: 1.8899391 acc: 30.46875\n",
      "Epoch: 1 batch 49 loss: 1.930771 acc: 32.8125\n",
      "Epoch: 1 batch 50 loss: 1.8296291 acc: 32.03125\n",
      "Epoch: 1 batch 51 loss: 2.0090132 acc: 25.78125\n",
      "Epoch: 1 batch 52 loss: 1.8540998 acc: 32.8125\n",
      "Epoch: 1 batch 53 loss: 1.9248896 acc: 30.46875\n",
      "Epoch: 1 batch 54 loss: 1.8301655 acc: 40.625\n",
      "Epoch: 1 batch 55 loss: 1.8424869 acc: 34.375\n",
      "Epoch: 1 batch 56 loss: 1.7478619 acc: 48.4375\n",
      "Epoch: 1 batch 57 loss: 1.9737659 acc: 26.5625\n",
      "Epoch: 1 batch 58 loss: 1.915361 acc: 33.59375\n",
      "Epoch: 1 batch 59 loss: 1.9906139 acc: 29.6875\n",
      "Epoch: 1 batch 60 loss: 1.9284985 acc: 25.78125\n",
      "Epoch: 1 batch 61 loss: 1.8654629 acc: 32.8125\n",
      "Epoch: 1 batch 62 loss: 1.8437505 acc: 25.78125\n",
      "Epoch: 1 batch 63 loss: 1.8079007 acc: 35.15625\n",
      "Epoch: 1 batch 64 loss: 1.7674335 acc: 36.71875\n",
      "Epoch: 1 batch 65 loss: 1.8937057 acc: 27.34375\n",
      "Epoch: 1 batch 66 loss: 1.786907 acc: 39.84375\n",
      "Epoch: 1 batch 67 loss: 1.9884557 acc: 30.46875\n",
      "Epoch: 1 batch 68 loss: 2.09409 acc: 19.53125\n",
      "Epoch: 1 batch 69 loss: 1.8430953 acc: 36.71875\n",
      "Epoch: 1 batch 70 loss: 1.8741678 acc: 37.5\n",
      "Epoch: 1 batch 71 loss: 1.7556403 acc: 37.5\n",
      "Epoch: 1 batch 72 loss: 1.7499874 acc: 36.71875\n",
      "Epoch: 1 batch 73 loss: 1.8671049 acc: 36.71875\n",
      "Epoch: 1 batch 74 loss: 1.6686592 acc: 35.9375\n",
      "Epoch: 1 batch 75 loss: 1.8123262 acc: 32.03125\n",
      "Epoch: 1 batch 76 loss: 1.8404746 acc: 33.59375\n",
      "Epoch: 1 batch 77 loss: 1.8004549 acc: 35.15625\n",
      "Epoch: 1 batch 78 loss: 1.8592643 acc: 32.8125\n",
      "Epoch: 1 batch 79 loss: 1.9281688 acc: 33.59375\n",
      "Epoch: 1 batch 80 loss: 1.9110018 acc: 35.15625\n",
      "Epoch: 1 batch 81 loss: 2.0728078 acc: 24.21875\n",
      "Epoch: 1 batch 82 loss: 1.91567 acc: 33.59375\n",
      "Epoch: 1 batch 83 loss: 1.7165992 acc: 39.0625\n",
      "Epoch: 1 batch 84 loss: 1.6448556 acc: 37.5\n",
      "Epoch: 1 batch 85 loss: 1.6859453 acc: 39.84375\n",
      "Epoch: 1 batch 86 loss: 1.7825422 acc: 33.59375\n",
      "Epoch: 1 batch 87 loss: 1.7718923 acc: 34.375\n",
      "Epoch: 1 batch 88 loss: 1.776504 acc: 35.15625\n",
      "Epoch: 1 batch 89 loss: 1.7427907 acc: 42.1875\n",
      "Epoch: 1 batch 90 loss: 1.7768197 acc: 35.9375\n",
      "Epoch: 1 batch 91 loss: 1.6033261 acc: 37.5\n",
      "Epoch: 1 batch 92 loss: 1.772981 acc: 35.15625\n",
      "Epoch: 1 batch 93 loss: 1.9943461 acc: 35.15625\n",
      "Epoch: 1 batch 94 loss: 1.9814965 acc: 26.5625\n",
      "Epoch: 1 batch 95 loss: 1.7172384 acc: 39.0625\n",
      "Epoch: 1 batch 96 loss: 1.7936182 acc: 35.9375\n",
      "Epoch: 1 batch 97 loss: 1.8288476 acc: 37.5\n",
      "Epoch: 1 batch 98 loss: 1.7644128 acc: 35.9375\n",
      "Epoch: 1 batch 99 loss: 1.7974961 acc: 42.1875\n",
      "Epoch: 1 batch 100 loss: 1.7058718 acc: 36.71875\n",
      "Epoch: 1 batch 101 loss: 1.7804518 acc: 39.0625\n",
      "Epoch: 1 batch 102 loss: 1.7588907 acc: 35.15625\n",
      "Epoch: 1 batch 103 loss: 1.7630519 acc: 31.25\n",
      "Epoch: 1 batch 104 loss: 1.8368771 acc: 34.375\n",
      "Epoch: 1 batch 105 loss: 1.5512571 acc: 49.21875\n",
      "Epoch: 1 batch 106 loss: 1.6620548 acc: 37.5\n",
      "Epoch: 1 batch 107 loss: 1.8124005 acc: 35.15625\n",
      "Epoch: 1 batch 108 loss: 1.7672004 acc: 39.0625\n",
      "Epoch: 1 batch 109 loss: 1.7763934 acc: 35.9375\n",
      "Epoch: 1 batch 110 loss: 1.9251115 acc: 33.59375\n",
      "Epoch: 1 batch 111 loss: 1.726197 acc: 38.28125\n",
      "Epoch: 1 batch 112 loss: 1.686649 acc: 42.1875\n",
      "Epoch: 1 batch 113 loss: 1.6016338 acc: 50.0\n",
      "Epoch: 1 batch 114 loss: 1.622859 acc: 44.53125\n",
      "Epoch: 1 batch 115 loss: 1.687687 acc: 42.1875\n",
      "Epoch: 1 batch 116 loss: 1.7422541 acc: 39.0625\n",
      "Epoch: 1 batch 117 loss: 1.715299 acc: 42.1875\n",
      "Epoch: 1 batch 118 loss: 1.7800939 acc: 39.0625\n",
      "Epoch: 1 batch 119 loss: 1.589282 acc: 41.40625\n",
      "Epoch: 1 batch 120 loss: 1.6235199 acc: 43.75\n",
      "Epoch: 1 batch 121 loss: 1.8400207 acc: 37.5\n",
      "Epoch: 1 batch 122 loss: 1.7019331 acc: 40.625\n",
      "Epoch: 1 batch 123 loss: 1.7426906 acc: 39.84375\n",
      "Epoch: 1 batch 124 loss: 1.799095 acc: 32.8125\n",
      "Epoch: 1 batch 125 loss: 1.5980189 acc: 43.75\n",
      "Epoch: 1 batch 126 loss: 1.6868386 acc: 39.84375\n",
      "Epoch: 1 batch 127 loss: 1.5508051 acc: 41.40625\n",
      "Epoch: 1 batch 128 loss: 1.5546902 acc: 37.5\n",
      "Epoch: 1 batch 129 loss: 1.8200703 acc: 35.15625\n",
      "Epoch: 1 batch 130 loss: 1.6083543 acc: 39.84375\n",
      "Epoch: 1 batch 131 loss: 1.6524471 acc: 41.40625\n",
      "Epoch: 1 batch 132 loss: 1.4719341 acc: 48.4375\n",
      "Epoch: 1 batch 133 loss: 1.6881557 acc: 35.9375\n",
      "Epoch: 1 batch 134 loss: 1.6689876 acc: 45.3125\n",
      "Epoch: 1 batch 135 loss: 1.8873487 acc: 30.46875\n",
      "Epoch: 1 batch 136 loss: 1.8398975 acc: 35.9375\n",
      "Epoch: 1 batch 137 loss: 1.7469304 acc: 38.28125\n",
      "Epoch: 1 batch 138 loss: 1.9317856 acc: 34.375\n",
      "Epoch: 1 batch 139 loss: 1.6747811 acc: 39.84375\n",
      "Epoch: 1 batch 140 loss: 1.6889715 acc: 35.9375\n",
      "Epoch: 1 batch 141 loss: 1.6173799 acc: 43.75\n",
      "Epoch: 1 batch 142 loss: 1.6129957 acc: 39.84375\n",
      "Epoch: 1 batch 143 loss: 1.6321563 acc: 42.1875\n",
      "Epoch: 1 batch 144 loss: 1.7561426 acc: 37.5\n",
      "Epoch: 1 batch 145 loss: 1.5673287 acc: 42.1875\n",
      "Epoch: 1 batch 146 loss: 1.5474112 acc: 49.21875\n",
      "Epoch: 1 batch 147 loss: 1.5952811 acc: 43.75\n",
      "Epoch: 1 batch 148 loss: 1.6807156 acc: 45.3125\n",
      "Epoch: 1 batch 149 loss: 1.6319613 acc: 40.625\n",
      "Epoch: 1 batch 150 loss: 1.7649534 acc: 35.9375\n",
      "Epoch: 1 batch 151 loss: 1.6106434 acc: 40.625\n",
      "Epoch: 1 batch 152 loss: 1.5902739 acc: 41.40625\n",
      "Epoch: 1 batch 153 loss: 1.530086 acc: 43.75\n",
      "Epoch: 1 batch 154 loss: 1.4937835 acc: 46.875\n",
      "Epoch: 1 batch 155 loss: 1.6170067 acc: 41.40625\n",
      "Epoch: 1 batch 156 loss: 1.6621714 acc: 44.53125\n",
      "Epoch: 1 batch 157 loss: 1.5852267 acc: 40.625\n",
      "Epoch: 1 batch 158 loss: 1.7010227 acc: 42.1875\n",
      "Epoch: 1 batch 159 loss: 1.596351 acc: 42.1875\n",
      "Epoch: 1 batch 160 loss: 1.6589726 acc: 43.75\n",
      "Epoch: 1 batch 161 loss: 1.5926906 acc: 39.84375\n",
      "Epoch: 1 batch 162 loss: 1.6085254 acc: 47.65625\n",
      "Epoch: 1 batch 163 loss: 1.5688639 acc: 43.75\n",
      "Epoch: 1 batch 164 loss: 1.5564822 acc: 39.84375\n",
      "Epoch: 1 batch 165 loss: 1.4904158 acc: 49.21875\n",
      "Epoch: 1 batch 166 loss: 1.5558724 acc: 42.96875\n",
      "Epoch: 1 batch 167 loss: 1.7597736 acc: 35.15625\n",
      "Epoch: 1 batch 168 loss: 1.6407527 acc: 46.09375\n",
      "Epoch: 1 batch 169 loss: 1.513677 acc: 47.65625\n",
      "Epoch: 1 batch 170 loss: 1.6368617 acc: 40.625\n",
      "Epoch: 1 batch 171 loss: 1.5825684 acc: 46.875\n",
      "Epoch: 1 batch 172 loss: 1.6307501 acc: 43.75\n",
      "Epoch: 1 batch 173 loss: 1.6747348 acc: 42.1875\n",
      "Epoch: 1 batch 174 loss: 1.6150842 acc: 45.3125\n",
      "Epoch: 1 batch 175 loss: 1.5938107 acc: 42.96875\n",
      "Epoch: 1 batch 176 loss: 1.6265396 acc: 42.1875\n",
      "Epoch: 1 batch 177 loss: 1.5183549 acc: 42.1875\n",
      "Epoch: 1 batch 178 loss: 1.7978537 acc: 39.84375\n",
      "Epoch: 1 batch 179 loss: 1.7422917 acc: 34.375\n",
      "Epoch: 1 batch 180 loss: 1.4927561 acc: 43.75\n",
      "Epoch: 1 batch 181 loss: 1.6265652 acc: 39.84375\n",
      "Epoch: 1 batch 182 loss: 1.4966598 acc: 46.875\n",
      "Epoch: 1 batch 183 loss: 1.6494216 acc: 45.3125\n",
      "Epoch: 1 batch 184 loss: 1.4673481 acc: 46.09375\n",
      "Epoch: 1 batch 185 loss: 1.4240229 acc: 52.34375\n",
      "Epoch: 1 batch 186 loss: 1.4122758 acc: 50.78125\n",
      "Epoch: 1 batch 187 loss: 1.4880673 acc: 45.3125\n",
      "Epoch: 1 batch 188 loss: 1.4575934 acc: 44.53125\n",
      "Epoch: 1 batch 189 loss: 1.5178108 acc: 43.75\n",
      "Epoch: 1 batch 190 loss: 1.4517287 acc: 47.65625\n",
      "Epoch: 1 batch 191 loss: 1.455033 acc: 46.875\n",
      "Epoch: 1 batch 192 loss: 1.4201865 acc: 43.75\n",
      "Epoch: 1 batch 193 loss: 1.6642232 acc: 42.1875\n",
      "Epoch: 1 batch 194 loss: 1.4297302 acc: 50.78125\n",
      "Epoch: 1 batch 195 loss: 1.4023204 acc: 47.65625\n",
      "Epoch: 1 batch 196 loss: 1.5387201 acc: 50.0\n",
      "Epoch: 1 batch 197 loss: 1.5328867 acc: 43.75\n",
      "Epoch: 1 batch 198 loss: 1.4620007 acc: 44.53125\n",
      "Epoch: 1 batch 199 loss: 1.3946894 acc: 44.53125\n",
      "Epoch: 1 batch 200 loss: 1.5626515 acc: 43.75\n",
      "Epoch: 1 batch 201 loss: 1.6042746 acc: 39.0625\n",
      "Epoch: 1 batch 202 loss: 1.7721202 acc: 39.84375\n",
      "Epoch: 1 batch 203 loss: 1.4299791 acc: 52.34375\n",
      "Epoch: 1 batch 204 loss: 1.4588871 acc: 53.125\n",
      "Epoch: 1 batch 205 loss: 1.5268526 acc: 39.0625\n",
      "Epoch: 1 batch 206 loss: 1.4942253 acc: 44.53125\n",
      "Epoch: 1 batch 207 loss: 1.3902428 acc: 45.3125\n",
      "Epoch: 1 batch 208 loss: 1.60153 acc: 42.96875\n",
      "Epoch: 1 batch 209 loss: 1.501333 acc: 50.78125\n",
      "Epoch: 1 batch 210 loss: 1.4991231 acc: 46.09375\n",
      "Epoch: 1 batch 211 loss: 1.5925777 acc: 46.09375\n",
      "Epoch: 1 batch 212 loss: 1.5942521 acc: 39.0625\n",
      "Epoch: 1 batch 213 loss: 1.5021224 acc: 51.5625\n",
      "Epoch: 1 batch 214 loss: 1.5732193 acc: 40.625\n",
      "Epoch: 1 batch 215 loss: 1.4885032 acc: 47.65625\n",
      "Epoch: 1 batch 216 loss: 1.5831009 acc: 46.875\n",
      "Epoch: 1 batch 217 loss: 1.4676604 acc: 48.4375\n",
      "Epoch: 1 batch 218 loss: 1.5042393 acc: 45.3125\n",
      "Epoch: 1 batch 219 loss: 1.5274602 acc: 52.34375\n",
      "Epoch: 1 batch 220 loss: 1.4611921 acc: 47.65625\n",
      "Epoch: 1 batch 221 loss: 1.4254732 acc: 48.4375\n",
      "Epoch: 1 batch 222 loss: 1.5333035 acc: 49.21875\n",
      "Epoch: 1 batch 223 loss: 1.6680328 acc: 41.40625\n",
      "Epoch: 1 batch 224 loss: 1.458002 acc: 46.875\n",
      "Epoch: 1 batch 225 loss: 1.398142 acc: 52.34375\n",
      "Epoch: 1 batch 226 loss: 1.4759581 acc: 41.40625\n",
      "Epoch: 1 batch 227 loss: 1.4944711 acc: 46.875\n",
      "Epoch: 1 batch 228 loss: 1.5021476 acc: 45.3125\n",
      "Epoch: 1 batch 229 loss: 1.3921456 acc: 46.09375\n",
      "Epoch: 1 batch 230 loss: 1.3925143 acc: 50.0\n",
      "Epoch: 1 batch 231 loss: 1.4989741 acc: 43.75\n",
      "Epoch: 1 batch 232 loss: 1.4049357 acc: 48.4375\n",
      "Epoch: 1 batch 233 loss: 1.4842407 acc: 41.40625\n",
      "Epoch: 1 batch 234 loss: 1.5336921 acc: 40.625\n",
      "Epoch: 1 batch 235 loss: 1.5815986 acc: 42.1875\n",
      "Epoch: 1 batch 236 loss: 1.4527143 acc: 50.78125\n",
      "Epoch: 1 batch 237 loss: 1.5051067 acc: 44.53125\n",
      "Epoch: 1 batch 238 loss: 1.476403 acc: 43.75\n",
      "Epoch: 1 batch 239 loss: 1.5027373 acc: 53.125\n",
      "Epoch: 1 batch 240 loss: 1.5008476 acc: 50.78125\n",
      "Epoch: 1 batch 241 loss: 1.4531361 acc: 50.78125\n",
      "Epoch: 1 batch 242 loss: 1.3769779 acc: 53.125\n",
      "Epoch: 1 batch 243 loss: 1.5009363 acc: 47.65625\n",
      "Epoch: 1 batch 244 loss: 1.299228 acc: 50.0\n",
      "Epoch: 1 batch 245 loss: 1.6830163 acc: 41.40625\n",
      "Epoch: 1 batch 246 loss: 1.5039682 acc: 44.53125\n",
      "Epoch: 1 batch 247 loss: 1.5197781 acc: 46.875\n",
      "Epoch: 1 batch 248 loss: 1.5314407 acc: 46.875\n",
      "Epoch: 1 batch 249 loss: 1.5764349 acc: 44.53125\n",
      "Epoch: 1 batch 250 loss: 1.611914 acc: 46.875\n",
      "Epoch: 1 batch 251 loss: 1.517694 acc: 47.65625\n",
      "Epoch: 1 batch 252 loss: 1.4807727 acc: 45.3125\n",
      "Epoch: 1 batch 253 loss: 1.4210815 acc: 50.0\n",
      "Epoch: 1 batch 254 loss: 1.5548513 acc: 49.21875\n",
      "Epoch: 1 batch 255 loss: 1.5086696 acc: 48.4375\n",
      "Epoch: 1 batch 256 loss: 1.629892 acc: 37.5\n",
      "Epoch: 1 batch 257 loss: 1.5460954 acc: 45.3125\n",
      "Epoch: 1 batch 258 loss: 1.3906159 acc: 53.125\n",
      "Epoch: 1 batch 259 loss: 1.4432256 acc: 48.4375\n",
      "Epoch: 1 batch 260 loss: 1.5955722 acc: 37.5\n",
      "Epoch: 1 batch 261 loss: 1.6815894 acc: 41.40625\n",
      "Epoch: 1 batch 262 loss: 1.5984002 acc: 44.53125\n",
      "Epoch: 1 batch 263 loss: 1.4204047 acc: 53.125\n",
      "Epoch: 1 batch 264 loss: 1.5917798 acc: 37.5\n",
      "Epoch: 1 batch 265 loss: 1.6584022 acc: 39.84375\n",
      "Epoch: 1 batch 266 loss: 1.3579274 acc: 55.46875\n",
      "Epoch: 1 batch 267 loss: 1.327864 acc: 54.6875\n",
      "Epoch: 1 batch 268 loss: 1.5338017 acc: 46.875\n",
      "Epoch: 1 batch 269 loss: 1.4670225 acc: 47.65625\n",
      "Epoch: 1 batch 270 loss: 1.4370098 acc: 47.65625\n",
      "Epoch: 1 batch 271 loss: 1.4647874 acc: 47.65625\n",
      "Epoch: 1 batch 272 loss: 1.4563355 acc: 48.4375\n",
      "Epoch: 1 batch 273 loss: 1.5373055 acc: 45.3125\n",
      "Epoch: 1 batch 274 loss: 1.4400773 acc: 50.0\n",
      "Epoch: 1 batch 275 loss: 1.4966414 acc: 38.28125\n",
      "Epoch: 1 batch 276 loss: 1.602677 acc: 44.53125\n",
      "Epoch: 1 batch 277 loss: 1.4852774 acc: 42.96875\n",
      "Epoch: 1 batch 278 loss: 1.4409353 acc: 43.75\n",
      "Epoch: 1 batch 279 loss: 1.4616731 acc: 44.53125\n",
      "Epoch: 1 batch 280 loss: 1.4920859 acc: 49.21875\n",
      "Epoch: 1 batch 281 loss: 1.5060714 acc: 50.78125\n",
      "Epoch: 1 batch 282 loss: 1.5912287 acc: 42.96875\n",
      "Epoch: 1 batch 283 loss: 1.3601853 acc: 51.5625\n",
      "Epoch: 1 batch 284 loss: 1.3743242 acc: 50.78125\n",
      "Epoch: 1 batch 285 loss: 1.3947078 acc: 53.90625\n",
      "Epoch: 1 batch 286 loss: 1.459376 acc: 46.875\n",
      "Epoch: 1 batch 287 loss: 1.3458712 acc: 52.34375\n",
      "Epoch: 1 batch 288 loss: 1.5568141 acc: 42.96875\n",
      "Epoch: 1 batch 289 loss: 1.354283 acc: 47.65625\n",
      "Epoch: 1 batch 290 loss: 1.5483421 acc: 45.3125\n",
      "Epoch: 1 batch 291 loss: 1.490031 acc: 50.0\n",
      "Epoch: 1 batch 292 loss: 1.4095163 acc: 46.875\n",
      "Epoch: 1 batch 293 loss: 1.4360931 acc: 51.5625\n",
      "Epoch: 1 batch 294 loss: 1.3286049 acc: 46.875\n",
      "Epoch: 1 batch 295 loss: 1.2846414 acc: 50.78125\n",
      "Epoch: 1 batch 296 loss: 1.3406067 acc: 45.3125\n",
      "Epoch: 1 batch 297 loss: 1.4275992 acc: 47.65625\n",
      "Epoch: 1 batch 298 loss: 1.430243 acc: 51.5625\n",
      "Epoch: 1 batch 299 loss: 1.5871401 acc: 41.40625\n",
      "Epoch: 1 batch 300 loss: 1.3786165 acc: 48.4375\n",
      "Epoch: 1 batch 301 loss: 1.339308 acc: 50.0\n",
      "Epoch: 1 batch 302 loss: 1.465827 acc: 52.34375\n",
      "Epoch: 1 batch 303 loss: 1.2317642 acc: 58.59375\n",
      "Epoch: 1 batch 304 loss: 1.3783474 acc: 54.6875\n",
      "Epoch: 1 batch 305 loss: 1.3500468 acc: 49.21875\n",
      "Epoch: 1 batch 306 loss: 1.4543033 acc: 51.5625\n",
      "Epoch: 1 batch 307 loss: 1.4149072 acc: 52.34375\n",
      "Epoch: 1 batch 308 loss: 1.4685262 acc: 46.09375\n",
      "Epoch: 1 batch 309 loss: 1.6511881 acc: 40.625\n",
      "Epoch: 1 batch 310 loss: 1.5382357 acc: 42.96875\n",
      "Epoch: 1 batch 311 loss: 1.3939263 acc: 54.6875\n",
      "Epoch: 1 batch 312 loss: 1.1832566 acc: 60.9375\n",
      "Epoch: 1 batch 313 loss: 1.4650586 acc: 46.09375\n",
      "Epoch: 1 batch 314 loss: 1.3601846 acc: 46.875\n",
      "Epoch: 1 batch 315 loss: 1.5389558 acc: 45.3125\n",
      "Epoch: 1 batch 316 loss: 1.4271371 acc: 50.78125\n",
      "Epoch: 1 batch 317 loss: 1.2937417 acc: 56.25\n",
      "Epoch: 1 batch 318 loss: 1.341872 acc: 58.59375\n",
      "Epoch: 1 batch 319 loss: 1.5231351 acc: 50.0\n",
      "Epoch: 1 batch 320 loss: 1.5582211 acc: 42.96875\n",
      "Epoch: 1 batch 321 loss: 1.4005243 acc: 47.65625\n",
      "Epoch: 1 batch 322 loss: 1.5329676 acc: 47.65625\n",
      "Epoch: 1 batch 323 loss: 1.3649466 acc: 46.09375\n",
      "Epoch: 1 batch 324 loss: 1.540024 acc: 50.78125\n",
      "Epoch: 1 batch 325 loss: 1.3263358 acc: 51.5625\n",
      "Epoch: 1 batch 326 loss: 1.291328 acc: 52.34375\n",
      "Epoch: 1 batch 327 loss: 1.4177353 acc: 53.90625\n",
      "Epoch: 1 batch 328 loss: 1.4643662 acc: 50.0\n",
      "Epoch: 1 batch 329 loss: 1.3986545 acc: 47.65625\n",
      "Epoch: 1 batch 330 loss: 1.3063884 acc: 58.59375\n",
      "Epoch: 1 batch 331 loss: 1.2002066 acc: 52.34375\n",
      "Epoch: 1 batch 332 loss: 1.7320786 acc: 38.28125\n",
      "Epoch: 1 batch 333 loss: 1.3775611 acc: 49.21875\n",
      "Epoch: 1 batch 334 loss: 1.386838 acc: 50.78125\n",
      "Epoch: 1 batch 335 loss: 1.5269768 acc: 45.3125\n",
      "Epoch: 1 batch 336 loss: 1.3256543 acc: 57.03125\n",
      "Epoch: 1 batch 337 loss: 1.5893542 acc: 45.3125\n",
      "Epoch: 1 batch 338 loss: 1.3594036 acc: 53.125\n",
      "Epoch: 1 batch 339 loss: 1.4132452 acc: 49.21875\n",
      "Epoch: 1 batch 340 loss: 1.2321287 acc: 54.6875\n",
      "Epoch: 1 batch 341 loss: 1.4129128 acc: 45.3125\n",
      "Epoch: 1 batch 342 loss: 1.5614805 acc: 42.96875\n",
      "Epoch: 1 batch 343 loss: 1.3761055 acc: 57.8125\n",
      "Epoch: 1 batch 344 loss: 1.3856946 acc: 56.25\n",
      "Epoch: 1 batch 345 loss: 1.5112796 acc: 49.21875\n",
      "Epoch: 1 batch 346 loss: 1.5039046 acc: 47.65625\n",
      "Epoch: 1 batch 347 loss: 1.4836017 acc: 51.5625\n",
      "Epoch: 1 batch 348 loss: 1.4862956 acc: 47.65625\n",
      "Epoch: 1 batch 349 loss: 1.3733858 acc: 50.78125\n",
      "Epoch: 1 batch 350 loss: 1.3740786 acc: 50.0\n",
      "Epoch: 1 batch 351 loss: 1.1506064 acc: 64.0625\n",
      "Epoch: 1 batch 352 loss: 1.4411428 acc: 50.0\n",
      "Epoch: 1 batch 353 loss: 1.3754534 acc: 50.0\n",
      "Epoch: 1 batch 354 loss: 1.3092026 acc: 49.21875\n",
      "Epoch: 1 batch 355 loss: 1.1413441 acc: 59.375\n",
      "Epoch: 1 batch 356 loss: 1.3698745 acc: 49.21875\n",
      "Epoch: 1 batch 357 loss: 1.3106644 acc: 52.34375\n",
      "Epoch: 1 batch 358 loss: 1.3275342 acc: 50.78125\n",
      "Epoch: 1 batch 359 loss: 1.4247326 acc: 53.125\n",
      "Epoch: 1 batch 360 loss: 1.2035177 acc: 53.125\n",
      "Epoch: 1 batch 361 loss: 1.4643099 acc: 44.53125\n",
      "Epoch: 1 batch 362 loss: 1.2764045 acc: 52.34375\n",
      "Epoch: 1 batch 363 loss: 1.6041603 acc: 39.84375\n",
      "Epoch: 1 batch 364 loss: 1.39556 acc: 52.34375\n",
      "Epoch: 1 batch 365 loss: 1.269191 acc: 57.8125\n",
      "Epoch: 1 batch 366 loss: 1.2529973 acc: 53.125\n",
      "Epoch: 1 batch 367 loss: 1.4035271 acc: 47.65625\n",
      "Epoch: 1 batch 368 loss: 1.1936717 acc: 60.9375\n",
      "Epoch: 1 batch 369 loss: 1.206597 acc: 57.8125\n",
      "Epoch: 1 batch 370 loss: 1.3206015 acc: 51.5625\n",
      "Epoch: 1 batch 371 loss: 1.3839362 acc: 50.78125\n",
      "Epoch: 1 batch 372 loss: 1.3148137 acc: 50.78125\n",
      "Epoch: 1 batch 373 loss: 1.2687765 acc: 56.25\n",
      "Epoch: 1 batch 374 loss: 1.3801672 acc: 52.34375\n",
      "Epoch: 1 batch 375 loss: 1.5648177 acc: 42.1875\n",
      "Epoch: 1 batch 376 loss: 1.3927082 acc: 46.875\n",
      "Epoch: 1 batch 377 loss: 1.2994064 acc: 57.8125\n",
      "Epoch: 1 batch 378 loss: 1.2851317 acc: 54.6875\n",
      "Epoch: 1 batch 379 loss: 1.2876675 acc: 52.34375\n",
      "Epoch: 1 batch 380 loss: 1.5309174 acc: 46.875\n",
      "Epoch: 1 batch 381 loss: 1.6459864 acc: 46.875\n",
      "Epoch: 1 batch 382 loss: 1.2215184 acc: 58.59375\n",
      "Epoch: 1 batch 383 loss: 1.3288664 acc: 51.5625\n",
      "Epoch: 1 batch 384 loss: 1.2085712 acc: 58.59375\n",
      "Epoch: 1 batch 385 loss: 1.3304814 acc: 56.25\n",
      "Epoch: 1 batch 386 loss: 1.2345468 acc: 60.15625\n",
      "Epoch: 1 batch 387 loss: 1.4116087 acc: 50.0\n",
      "Epoch: 1 batch 388 loss: 1.4949373 acc: 46.875\n",
      "Epoch: 1 batch 389 loss: 1.2354374 acc: 59.375\n",
      "After epoch 1 test loss 1.1875486906051635 test accuracy 0.5841000080108643\n",
      "Epoch: 2 batch 0 loss: 1.2690867 acc: 58.36295485496521\n",
      "Epoch: 2 batch 1 loss: 1.2021163 acc: 59.375\n",
      "Epoch: 2 batch 2 loss: 1.2856014 acc: 50.0\n",
      "Epoch: 2 batch 3 loss: 1.2962353 acc: 53.90625\n",
      "Epoch: 2 batch 4 loss: 1.3394775 acc: 55.46875\n",
      "Epoch: 2 batch 5 loss: 1.2695946 acc: 53.90625\n",
      "Epoch: 2 batch 6 loss: 1.4804745 acc: 49.21875\n",
      "Epoch: 2 batch 7 loss: 1.267382 acc: 46.09375\n",
      "Epoch: 2 batch 8 loss: 1.2549195 acc: 51.5625\n",
      "Epoch: 2 batch 9 loss: 1.2941947 acc: 58.59375\n",
      "Epoch: 2 batch 10 loss: 1.4520748 acc: 48.4375\n",
      "Epoch: 2 batch 11 loss: 1.4589958 acc: 50.78125\n",
      "Epoch: 2 batch 12 loss: 1.3612119 acc: 53.90625\n",
      "Epoch: 2 batch 13 loss: 1.2715658 acc: 53.90625\n",
      "Epoch: 2 batch 14 loss: 1.1432569 acc: 62.5\n",
      "Epoch: 2 batch 15 loss: 1.5919436 acc: 44.53125\n",
      "Epoch: 2 batch 16 loss: 1.199505 acc: 53.90625\n",
      "Epoch: 2 batch 17 loss: 1.3764479 acc: 46.875\n",
      "Epoch: 2 batch 18 loss: 1.2257669 acc: 55.46875\n",
      "Epoch: 2 batch 19 loss: 1.2621826 acc: 53.90625\n",
      "Epoch: 2 batch 20 loss: 1.1780717 acc: 55.46875\n",
      "Epoch: 2 batch 21 loss: 1.4062575 acc: 51.5625\n",
      "Epoch: 2 batch 22 loss: 1.2171161 acc: 55.46875\n",
      "Epoch: 2 batch 23 loss: 1.4057636 acc: 50.78125\n",
      "Epoch: 2 batch 24 loss: 1.2511898 acc: 54.6875\n",
      "Epoch: 2 batch 25 loss: 1.1581188 acc: 60.9375\n",
      "Epoch: 2 batch 26 loss: 1.457424 acc: 57.8125\n",
      "Epoch: 2 batch 27 loss: 1.1310796 acc: 63.28125\n",
      "Epoch: 2 batch 28 loss: 1.1499125 acc: 59.375\n",
      "Epoch: 2 batch 29 loss: 1.1944861 acc: 52.34375\n",
      "Epoch: 2 batch 30 loss: 1.4430351 acc: 53.125\n",
      "Epoch: 2 batch 31 loss: 1.5492687 acc: 49.21875\n",
      "Epoch: 2 batch 32 loss: 1.3574607 acc: 57.03125\n",
      "Epoch: 2 batch 33 loss: 1.2819041 acc: 57.8125\n",
      "Epoch: 2 batch 34 loss: 1.2775896 acc: 54.6875\n",
      "Epoch: 2 batch 35 loss: 1.3213307 acc: 53.125\n",
      "Epoch: 2 batch 36 loss: 1.3671768 acc: 50.78125\n",
      "Epoch: 2 batch 37 loss: 1.4646443 acc: 53.90625\n",
      "Epoch: 2 batch 38 loss: 1.2904006 acc: 60.9375\n",
      "Epoch: 2 batch 39 loss: 1.3315766 acc: 53.125\n",
      "Epoch: 2 batch 40 loss: 1.3543353 acc: 52.34375\n",
      "Epoch: 2 batch 41 loss: 1.4438905 acc: 53.125\n",
      "Epoch: 2 batch 42 loss: 1.4657385 acc: 42.1875\n",
      "Epoch: 2 batch 43 loss: 1.2510641 acc: 62.5\n",
      "Epoch: 2 batch 44 loss: 1.2318715 acc: 58.59375\n",
      "Epoch: 2 batch 45 loss: 1.1901443 acc: 56.25\n",
      "Epoch: 2 batch 46 loss: 1.1339451 acc: 58.59375\n",
      "Epoch: 2 batch 47 loss: 1.2347804 acc: 57.03125\n",
      "Epoch: 2 batch 48 loss: 1.1546624 acc: 53.90625\n",
      "Epoch: 2 batch 49 loss: 1.1147269 acc: 62.5\n",
      "Epoch: 2 batch 50 loss: 1.2948303 acc: 51.5625\n",
      "Epoch: 2 batch 51 loss: 1.3001779 acc: 56.25\n",
      "Epoch: 2 batch 52 loss: 1.2296795 acc: 57.8125\n",
      "Epoch: 2 batch 53 loss: 1.3126163 acc: 57.8125\n",
      "Epoch: 2 batch 54 loss: 1.21239 acc: 53.90625\n",
      "Epoch: 2 batch 55 loss: 1.2504709 acc: 56.25\n",
      "Epoch: 2 batch 56 loss: 1.2803545 acc: 49.21875\n",
      "Epoch: 2 batch 57 loss: 1.2623732 acc: 50.0\n",
      "Epoch: 2 batch 58 loss: 1.3606964 acc: 49.21875\n",
      "Epoch: 2 batch 59 loss: 1.4705079 acc: 49.21875\n",
      "Epoch: 2 batch 60 loss: 1.3488576 acc: 49.21875\n",
      "Epoch: 2 batch 61 loss: 1.2355559 acc: 54.6875\n",
      "Epoch: 2 batch 62 loss: 1.2299058 acc: 57.03125\n",
      "Epoch: 2 batch 63 loss: 1.172227 acc: 57.8125\n",
      "Epoch: 2 batch 64 loss: 1.1817973 acc: 51.5625\n",
      "Epoch: 2 batch 65 loss: 1.3006666 acc: 52.34375\n",
      "Epoch: 2 batch 66 loss: 1.3365393 acc: 52.34375\n",
      "Epoch: 2 batch 67 loss: 1.2495277 acc: 56.25\n",
      "Epoch: 2 batch 68 loss: 1.3252336 acc: 53.90625\n",
      "Epoch: 2 batch 69 loss: 1.3372605 acc: 52.34375\n",
      "Epoch: 2 batch 70 loss: 1.4500906 acc: 53.125\n",
      "Epoch: 2 batch 71 loss: 1.2482297 acc: 53.90625\n",
      "Epoch: 2 batch 72 loss: 1.2273638 acc: 56.25\n",
      "Epoch: 2 batch 73 loss: 1.2487079 acc: 55.46875\n",
      "Epoch: 2 batch 74 loss: 1.0885515 acc: 60.9375\n",
      "Epoch: 2 batch 75 loss: 1.3743434 acc: 53.125\n",
      "Epoch: 2 batch 76 loss: 1.2813265 acc: 51.5625\n",
      "Epoch: 2 batch 77 loss: 1.1496466 acc: 54.6875\n",
      "Epoch: 2 batch 78 loss: 1.1810621 acc: 61.71875\n",
      "Epoch: 2 batch 79 loss: 1.2348067 acc: 62.5\n",
      "Epoch: 2 batch 80 loss: 1.0818707 acc: 67.1875\n",
      "Epoch: 2 batch 81 loss: 1.5661182 acc: 43.75\n",
      "Epoch: 2 batch 82 loss: 1.332272 acc: 55.46875\n",
      "Epoch: 2 batch 83 loss: 1.1989274 acc: 64.0625\n",
      "Epoch: 2 batch 84 loss: 1.0602323 acc: 67.96875\n",
      "Epoch: 2 batch 85 loss: 1.2126327 acc: 55.46875\n",
      "Epoch: 2 batch 86 loss: 1.3629193 acc: 52.34375\n",
      "Epoch: 2 batch 87 loss: 1.2172744 acc: 56.25\n",
      "Epoch: 2 batch 88 loss: 1.0750847 acc: 58.59375\n",
      "Epoch: 2 batch 89 loss: 1.1805118 acc: 62.5\n",
      "Epoch: 2 batch 90 loss: 1.234189 acc: 57.03125\n",
      "Epoch: 2 batch 91 loss: 1.1186678 acc: 58.59375\n",
      "Epoch: 2 batch 92 loss: 1.1504614 acc: 61.71875\n",
      "Epoch: 2 batch 93 loss: 1.2873156 acc: 59.375\n",
      "Epoch: 2 batch 94 loss: 1.290083 acc: 45.3125\n",
      "Epoch: 2 batch 95 loss: 1.232961 acc: 58.59375\n",
      "Epoch: 2 batch 96 loss: 1.2571754 acc: 59.375\n",
      "Epoch: 2 batch 97 loss: 1.2677178 acc: 56.25\n",
      "Epoch: 2 batch 98 loss: 1.3096657 acc: 54.6875\n",
      "Epoch: 2 batch 99 loss: 1.2981896 acc: 56.25\n",
      "Epoch: 2 batch 100 loss: 1.0705062 acc: 65.625\n",
      "Epoch: 2 batch 101 loss: 1.2447472 acc: 60.9375\n",
      "Epoch: 2 batch 102 loss: 1.2792544 acc: 52.34375\n",
      "Epoch: 2 batch 103 loss: 1.2165276 acc: 56.25\n",
      "Epoch: 2 batch 104 loss: 1.3269984 acc: 52.34375\n",
      "Epoch: 2 batch 105 loss: 1.0630683 acc: 64.0625\n",
      "Epoch: 2 batch 106 loss: 1.2120377 acc: 57.8125\n",
      "Epoch: 2 batch 107 loss: 1.4340643 acc: 50.78125\n",
      "Epoch: 2 batch 108 loss: 1.3465025 acc: 57.03125\n",
      "Epoch: 2 batch 109 loss: 1.2782283 acc: 54.6875\n",
      "Epoch: 2 batch 110 loss: 1.3021436 acc: 51.5625\n",
      "Epoch: 2 batch 111 loss: 1.2375216 acc: 55.46875\n",
      "Epoch: 2 batch 112 loss: 1.1885557 acc: 58.59375\n",
      "Epoch: 2 batch 113 loss: 1.1939893 acc: 56.25\n",
      "Epoch: 2 batch 114 loss: 1.287524 acc: 53.125\n",
      "Epoch: 2 batch 115 loss: 1.176413 acc: 50.0\n",
      "Epoch: 2 batch 116 loss: 1.3006601 acc: 60.15625\n",
      "Epoch: 2 batch 117 loss: 1.2737751 acc: 57.03125\n",
      "Epoch: 2 batch 118 loss: 1.3140802 acc: 55.46875\n",
      "Epoch: 2 batch 119 loss: 1.1609466 acc: 58.59375\n",
      "Epoch: 2 batch 120 loss: 1.2108107 acc: 60.15625\n",
      "Epoch: 2 batch 121 loss: 1.396956 acc: 53.90625\n",
      "Epoch: 2 batch 122 loss: 1.2021939 acc: 57.03125\n",
      "Epoch: 2 batch 123 loss: 1.3415458 acc: 52.34375\n",
      "Epoch: 2 batch 124 loss: 1.3618376 acc: 55.46875\n",
      "Epoch: 2 batch 125 loss: 1.1411974 acc: 62.5\n",
      "Epoch: 2 batch 126 loss: 1.2383739 acc: 52.34375\n",
      "Epoch: 2 batch 127 loss: 1.1335843 acc: 61.71875\n",
      "Epoch: 2 batch 128 loss: 1.2147832 acc: 55.46875\n",
      "Epoch: 2 batch 129 loss: 1.3135493 acc: 54.6875\n",
      "Epoch: 2 batch 130 loss: 1.1438179 acc: 59.375\n",
      "Epoch: 2 batch 131 loss: 1.3493564 acc: 52.34375\n",
      "Epoch: 2 batch 132 loss: 1.1482596 acc: 53.125\n",
      "Epoch: 2 batch 133 loss: 1.218076 acc: 56.25\n",
      "Epoch: 2 batch 134 loss: 1.142483 acc: 61.71875\n",
      "Epoch: 2 batch 135 loss: 1.323705 acc: 50.0\n",
      "Epoch: 2 batch 136 loss: 1.271908 acc: 57.8125\n",
      "Epoch: 2 batch 137 loss: 1.1171169 acc: 60.9375\n",
      "Epoch: 2 batch 138 loss: 1.4057248 acc: 51.5625\n",
      "Epoch: 2 batch 139 loss: 1.1299863 acc: 53.125\n",
      "Epoch: 2 batch 140 loss: 1.3390821 acc: 59.375\n",
      "Epoch: 2 batch 141 loss: 1.3467927 acc: 51.5625\n",
      "Epoch: 2 batch 142 loss: 1.3588223 acc: 56.25\n",
      "Epoch: 2 batch 143 loss: 1.2229505 acc: 60.15625\n",
      "Epoch: 2 batch 144 loss: 1.345829 acc: 53.125\n",
      "Epoch: 2 batch 145 loss: 1.2206829 acc: 57.03125\n",
      "Epoch: 2 batch 146 loss: 1.2208686 acc: 54.6875\n",
      "Epoch: 2 batch 147 loss: 1.2032034 acc: 60.9375\n",
      "Epoch: 2 batch 148 loss: 1.1438727 acc: 58.59375\n",
      "Epoch: 2 batch 149 loss: 1.2300851 acc: 57.03125\n",
      "Epoch: 2 batch 150 loss: 1.071529 acc: 63.28125\n",
      "Epoch: 2 batch 151 loss: 1.1488895 acc: 57.8125\n",
      "Epoch: 2 batch 152 loss: 1.252168 acc: 55.46875\n",
      "Epoch: 2 batch 153 loss: 1.2862018 acc: 54.6875\n",
      "Epoch: 2 batch 154 loss: 1.233189 acc: 58.59375\n",
      "Epoch: 2 batch 155 loss: 1.1721509 acc: 57.8125\n",
      "Epoch: 2 batch 156 loss: 1.3255849 acc: 53.125\n",
      "Epoch: 2 batch 157 loss: 1.1865649 acc: 56.25\n",
      "Epoch: 2 batch 158 loss: 1.2384324 acc: 56.25\n",
      "Epoch: 2 batch 159 loss: 1.2322788 acc: 53.90625\n",
      "Epoch: 2 batch 160 loss: 1.2383436 acc: 57.8125\n",
      "Epoch: 2 batch 161 loss: 1.2134466 acc: 56.25\n",
      "Epoch: 2 batch 162 loss: 1.3577975 acc: 54.6875\n",
      "Epoch: 2 batch 163 loss: 1.1528778 acc: 60.9375\n",
      "Epoch: 2 batch 164 loss: 1.1267409 acc: 57.03125\n",
      "Epoch: 2 batch 165 loss: 1.1799049 acc: 62.5\n",
      "Epoch: 2 batch 166 loss: 1.1264648 acc: 67.1875\n",
      "Epoch: 2 batch 167 loss: 1.2835267 acc: 54.6875\n",
      "Epoch: 2 batch 168 loss: 1.1118271 acc: 61.71875\n",
      "Epoch: 2 batch 169 loss: 1.1361238 acc: 59.375\n",
      "Epoch: 2 batch 170 loss: 1.289016 acc: 56.25\n",
      "Epoch: 2 batch 171 loss: 1.2023863 acc: 59.375\n",
      "Epoch: 2 batch 172 loss: 1.1055061 acc: 61.71875\n",
      "Epoch: 2 batch 173 loss: 1.131684 acc: 55.46875\n",
      "Epoch: 2 batch 174 loss: 1.1083034 acc: 58.59375\n",
      "Epoch: 2 batch 175 loss: 1.3204201 acc: 53.90625\n",
      "Epoch: 2 batch 176 loss: 1.1758549 acc: 59.375\n",
      "Epoch: 2 batch 177 loss: 1.1951425 acc: 63.28125\n",
      "Epoch: 2 batch 178 loss: 1.2399573 acc: 58.59375\n",
      "Epoch: 2 batch 179 loss: 1.2050519 acc: 56.25\n",
      "Epoch: 2 batch 180 loss: 1.2090867 acc: 60.15625\n",
      "Epoch: 2 batch 181 loss: 1.2533572 acc: 58.59375\n",
      "Epoch: 2 batch 182 loss: 1.0645736 acc: 60.9375\n",
      "Epoch: 2 batch 183 loss: 1.3229952 acc: 56.25\n",
      "Epoch: 2 batch 184 loss: 1.1596031 acc: 57.03125\n",
      "Epoch: 2 batch 185 loss: 1.0992405 acc: 57.8125\n",
      "Epoch: 2 batch 186 loss: 1.0021526 acc: 62.5\n",
      "Epoch: 2 batch 187 loss: 1.0413182 acc: 61.71875\n",
      "Epoch: 2 batch 188 loss: 1.1158624 acc: 62.5\n",
      "Epoch: 2 batch 189 loss: 1.2123233 acc: 57.03125\n",
      "Epoch: 2 batch 190 loss: 1.2594321 acc: 58.59375\n",
      "Epoch: 2 batch 191 loss: 1.0784749 acc: 63.28125\n",
      "Epoch: 2 batch 192 loss: 0.98698556 acc: 67.96875\n",
      "Epoch: 2 batch 193 loss: 1.2742563 acc: 52.34375\n",
      "Epoch: 2 batch 194 loss: 1.2160316 acc: 53.90625\n",
      "Epoch: 2 batch 195 loss: 1.0779346 acc: 60.15625\n",
      "Epoch: 2 batch 196 loss: 1.2389596 acc: 52.34375\n",
      "Epoch: 2 batch 197 loss: 1.2880151 acc: 62.5\n",
      "Epoch: 2 batch 198 loss: 1.1909691 acc: 57.03125\n",
      "Epoch: 2 batch 199 loss: 1.037885 acc: 67.96875\n",
      "Epoch: 2 batch 200 loss: 1.1867821 acc: 57.03125\n",
      "Epoch: 2 batch 201 loss: 1.0286491 acc: 60.9375\n",
      "Epoch: 2 batch 202 loss: 1.0532624 acc: 66.40625\n",
      "Epoch: 2 batch 203 loss: 0.9328109 acc: 66.40625\n",
      "Epoch: 2 batch 204 loss: 1.0041142 acc: 66.40625\n",
      "Epoch: 2 batch 205 loss: 1.1544068 acc: 59.375\n",
      "Epoch: 2 batch 206 loss: 1.1027708 acc: 57.8125\n",
      "Epoch: 2 batch 207 loss: 1.0366976 acc: 60.9375\n",
      "Epoch: 2 batch 208 loss: 1.0994844 acc: 65.625\n",
      "Epoch: 2 batch 209 loss: 1.1957775 acc: 58.59375\n",
      "Epoch: 2 batch 210 loss: 1.2172601 acc: 60.9375\n",
      "Epoch: 2 batch 211 loss: 1.328229 acc: 53.90625\n",
      "Epoch: 2 batch 212 loss: 1.1730088 acc: 53.90625\n",
      "Epoch: 2 batch 213 loss: 1.1189438 acc: 64.0625\n",
      "Epoch: 2 batch 214 loss: 1.2315409 acc: 61.71875\n",
      "Epoch: 2 batch 215 loss: 1.0785983 acc: 63.28125\n",
      "Epoch: 2 batch 216 loss: 1.0934135 acc: 59.375\n",
      "Epoch: 2 batch 217 loss: 1.1980939 acc: 58.59375\n",
      "Epoch: 2 batch 218 loss: 1.1008458 acc: 64.84375\n",
      "Epoch: 2 batch 219 loss: 1.1244031 acc: 57.8125\n",
      "Epoch: 2 batch 220 loss: 0.9886703 acc: 64.84375\n",
      "Epoch: 2 batch 221 loss: 1.2016985 acc: 57.03125\n",
      "Epoch: 2 batch 222 loss: 1.0407134 acc: 64.0625\n",
      "Epoch: 2 batch 223 loss: 1.24806 acc: 53.125\n",
      "Epoch: 2 batch 224 loss: 1.1400241 acc: 56.25\n",
      "Epoch: 2 batch 225 loss: 0.9744487 acc: 67.96875\n",
      "Epoch: 2 batch 226 loss: 1.012694 acc: 63.28125\n",
      "Epoch: 2 batch 227 loss: 1.37866 acc: 54.6875\n",
      "Epoch: 2 batch 228 loss: 1.1180732 acc: 58.59375\n",
      "Epoch: 2 batch 229 loss: 0.97236747 acc: 67.1875\n",
      "Epoch: 2 batch 230 loss: 1.0675676 acc: 60.15625\n",
      "Epoch: 2 batch 231 loss: 1.1942618 acc: 57.03125\n",
      "Epoch: 2 batch 232 loss: 1.0907177 acc: 63.28125\n",
      "Epoch: 2 batch 233 loss: 1.0819699 acc: 60.9375\n",
      "Epoch: 2 batch 234 loss: 1.1146412 acc: 62.5\n",
      "Epoch: 2 batch 235 loss: 1.1883564 acc: 61.71875\n",
      "Epoch: 2 batch 236 loss: 1.0592353 acc: 65.625\n",
      "Epoch: 2 batch 237 loss: 1.0383556 acc: 62.5\n",
      "Epoch: 2 batch 238 loss: 1.0959135 acc: 64.0625\n",
      "Epoch: 2 batch 239 loss: 1.3030363 acc: 53.125\n",
      "Epoch: 2 batch 240 loss: 1.3059049 acc: 55.46875\n",
      "Epoch: 2 batch 241 loss: 1.1046051 acc: 65.625\n",
      "Epoch: 2 batch 242 loss: 1.0274577 acc: 64.0625\n",
      "Epoch: 2 batch 243 loss: 1.2186007 acc: 53.90625\n",
      "Epoch: 2 batch 244 loss: 1.0795062 acc: 62.5\n",
      "Epoch: 2 batch 245 loss: 1.2035286 acc: 60.15625\n",
      "Epoch: 2 batch 246 loss: 1.1074846 acc: 64.84375\n",
      "Epoch: 2 batch 247 loss: 1.2270701 acc: 57.8125\n",
      "Epoch: 2 batch 248 loss: 1.1425985 acc: 63.28125\n",
      "Epoch: 2 batch 249 loss: 1.1498407 acc: 63.28125\n",
      "Epoch: 2 batch 250 loss: 1.1615123 acc: 60.15625\n",
      "Epoch: 2 batch 251 loss: 1.260591 acc: 53.90625\n",
      "Epoch: 2 batch 252 loss: 1.1450502 acc: 62.5\n",
      "Epoch: 2 batch 253 loss: 1.0643215 acc: 64.0625\n",
      "Epoch: 2 batch 254 loss: 1.1982353 acc: 57.03125\n",
      "Epoch: 2 batch 255 loss: 1.215758 acc: 57.8125\n",
      "Epoch: 2 batch 256 loss: 1.2667524 acc: 60.9375\n",
      "Epoch: 2 batch 257 loss: 1.1222674 acc: 55.46875\n",
      "Epoch: 2 batch 258 loss: 1.2207526 acc: 62.5\n",
      "Epoch: 2 batch 259 loss: 1.1296285 acc: 60.9375\n",
      "Epoch: 2 batch 260 loss: 1.1412286 acc: 55.46875\n",
      "Epoch: 2 batch 261 loss: 1.3773547 acc: 53.90625\n",
      "Epoch: 2 batch 262 loss: 1.3143446 acc: 53.125\n",
      "Epoch: 2 batch 263 loss: 1.0158674 acc: 65.625\n",
      "Epoch: 2 batch 264 loss: 1.1899533 acc: 59.375\n",
      "Epoch: 2 batch 265 loss: 1.2834971 acc: 56.25\n",
      "Epoch: 2 batch 266 loss: 0.9898344 acc: 65.625\n",
      "Epoch: 2 batch 267 loss: 1.0255307 acc: 62.5\n",
      "Epoch: 2 batch 268 loss: 1.2715182 acc: 54.6875\n",
      "Epoch: 2 batch 269 loss: 1.1619194 acc: 57.8125\n",
      "Epoch: 2 batch 270 loss: 1.1105624 acc: 61.71875\n",
      "Epoch: 2 batch 271 loss: 1.1725292 acc: 59.375\n",
      "Epoch: 2 batch 272 loss: 0.99790967 acc: 62.5\n",
      "Epoch: 2 batch 273 loss: 1.1921601 acc: 60.15625\n",
      "Epoch: 2 batch 274 loss: 1.2227626 acc: 59.375\n",
      "Epoch: 2 batch 275 loss: 1.2776692 acc: 57.03125\n",
      "Epoch: 2 batch 276 loss: 1.1376497 acc: 63.28125\n",
      "Epoch: 2 batch 277 loss: 1.1172019 acc: 54.6875\n",
      "Epoch: 2 batch 278 loss: 1.257841 acc: 60.15625\n",
      "Epoch: 2 batch 279 loss: 1.2337592 acc: 60.9375\n",
      "Epoch: 2 batch 280 loss: 1.1080273 acc: 57.8125\n",
      "Epoch: 2 batch 281 loss: 1.135551 acc: 57.8125\n",
      "Epoch: 2 batch 282 loss: 1.0961853 acc: 55.46875\n",
      "Epoch: 2 batch 283 loss: 1.0999858 acc: 60.9375\n",
      "Epoch: 2 batch 284 loss: 1.1075315 acc: 58.59375\n",
      "Epoch: 2 batch 285 loss: 0.9932894 acc: 62.5\n",
      "Epoch: 2 batch 286 loss: 1.1493733 acc: 56.25\n",
      "Epoch: 2 batch 287 loss: 0.9981059 acc: 62.5\n",
      "Epoch: 2 batch 288 loss: 1.2726218 acc: 53.90625\n",
      "Epoch: 2 batch 289 loss: 1.0746481 acc: 63.28125\n",
      "Epoch: 2 batch 290 loss: 1.2398052 acc: 62.5\n",
      "Epoch: 2 batch 291 loss: 1.1773434 acc: 55.46875\n",
      "Epoch: 2 batch 292 loss: 1.2033107 acc: 60.9375\n",
      "Epoch: 2 batch 293 loss: 1.0515432 acc: 64.0625\n",
      "Epoch: 2 batch 294 loss: 1.1580298 acc: 59.375\n",
      "Epoch: 2 batch 295 loss: 1.0758748 acc: 60.15625\n",
      "Epoch: 2 batch 296 loss: 1.2221603 acc: 57.03125\n",
      "Epoch: 2 batch 297 loss: 1.074435 acc: 60.9375\n",
      "Epoch: 2 batch 298 loss: 1.1120884 acc: 61.71875\n",
      "Epoch: 2 batch 299 loss: 1.3251768 acc: 52.34375\n",
      "Epoch: 2 batch 300 loss: 1.0915481 acc: 61.71875\n",
      "Epoch: 2 batch 301 loss: 1.0969764 acc: 60.15625\n",
      "Epoch: 2 batch 302 loss: 1.0041343 acc: 64.0625\n",
      "Epoch: 2 batch 303 loss: 0.93451905 acc: 66.40625\n",
      "Epoch: 2 batch 304 loss: 1.169898 acc: 60.9375\n",
      "Epoch: 2 batch 305 loss: 1.0971024 acc: 62.5\n",
      "Epoch: 2 batch 306 loss: 1.221992 acc: 57.8125\n",
      "Epoch: 2 batch 307 loss: 1.2423873 acc: 60.15625\n",
      "Epoch: 2 batch 308 loss: 1.0703535 acc: 60.15625\n",
      "Epoch: 2 batch 309 loss: 1.3988371 acc: 50.78125\n",
      "Epoch: 2 batch 310 loss: 1.2546927 acc: 51.5625\n",
      "Epoch: 2 batch 311 loss: 1.0558848 acc: 65.625\n",
      "Epoch: 2 batch 312 loss: 0.9848673 acc: 69.53125\n",
      "Epoch: 2 batch 313 loss: 1.1037502 acc: 59.375\n",
      "Epoch: 2 batch 314 loss: 1.06629 acc: 57.03125\n",
      "Epoch: 2 batch 315 loss: 1.383493 acc: 51.5625\n",
      "Epoch: 2 batch 316 loss: 1.111064 acc: 61.71875\n",
      "Epoch: 2 batch 317 loss: 1.034202 acc: 66.40625\n",
      "Epoch: 2 batch 318 loss: 1.0829531 acc: 61.71875\n",
      "Epoch: 2 batch 319 loss: 1.2141227 acc: 56.25\n",
      "Epoch: 2 batch 320 loss: 1.1784729 acc: 58.59375\n",
      "Epoch: 2 batch 321 loss: 1.1074578 acc: 57.8125\n",
      "Epoch: 2 batch 322 loss: 1.1588084 acc: 63.28125\n",
      "Epoch: 2 batch 323 loss: 1.031411 acc: 63.28125\n",
      "Epoch: 2 batch 324 loss: 1.0942866 acc: 62.5\n",
      "Epoch: 2 batch 325 loss: 1.1063516 acc: 62.5\n",
      "Epoch: 2 batch 326 loss: 1.02382 acc: 61.71875\n",
      "Epoch: 2 batch 327 loss: 1.1760463 acc: 64.0625\n",
      "Epoch: 2 batch 328 loss: 1.1062613 acc: 64.84375\n",
      "Epoch: 2 batch 329 loss: 1.1336513 acc: 59.375\n",
      "Epoch: 2 batch 330 loss: 0.9688901 acc: 72.65625\n",
      "Epoch: 2 batch 331 loss: 1.053817 acc: 59.375\n",
      "Epoch: 2 batch 332 loss: 1.2720419 acc: 49.21875\n",
      "Epoch: 2 batch 333 loss: 1.0216725 acc: 61.71875\n",
      "Epoch: 2 batch 334 loss: 1.0692658 acc: 64.84375\n",
      "Epoch: 2 batch 335 loss: 1.2672344 acc: 53.125\n",
      "Epoch: 2 batch 336 loss: 0.9583355 acc: 69.53125\n",
      "Epoch: 2 batch 337 loss: 1.1939095 acc: 55.46875\n",
      "Epoch: 2 batch 338 loss: 1.1560017 acc: 58.59375\n",
      "Epoch: 2 batch 339 loss: 1.1919222 acc: 57.03125\n",
      "Epoch: 2 batch 340 loss: 1.0473322 acc: 64.0625\n",
      "Epoch: 2 batch 341 loss: 1.0760818 acc: 62.5\n",
      "Epoch: 2 batch 342 loss: 1.323623 acc: 51.5625\n",
      "Epoch: 2 batch 343 loss: 1.1209021 acc: 60.15625\n",
      "Epoch: 2 batch 344 loss: 1.0365961 acc: 63.28125\n",
      "Epoch: 2 batch 345 loss: 1.216156 acc: 64.0625\n",
      "Epoch: 2 batch 346 loss: 1.2099984 acc: 59.375\n",
      "Epoch: 2 batch 347 loss: 1.1720076 acc: 57.8125\n",
      "Epoch: 2 batch 348 loss: 1.0436468 acc: 59.375\n",
      "Epoch: 2 batch 349 loss: 1.0293424 acc: 60.9375\n",
      "Epoch: 2 batch 350 loss: 1.192275 acc: 59.375\n",
      "Epoch: 2 batch 351 loss: 0.93498933 acc: 69.53125\n",
      "Epoch: 2 batch 352 loss: 1.1175141 acc: 59.375\n",
      "Epoch: 2 batch 353 loss: 1.2396687 acc: 56.25\n",
      "Epoch: 2 batch 354 loss: 0.9767794 acc: 64.0625\n",
      "Epoch: 2 batch 355 loss: 0.9160373 acc: 63.28125\n",
      "Epoch: 2 batch 356 loss: 1.1265993 acc: 64.84375\n",
      "Epoch: 2 batch 357 loss: 0.9680903 acc: 62.5\n",
      "Epoch: 2 batch 358 loss: 1.1331697 acc: 62.5\n",
      "Epoch: 2 batch 359 loss: 1.1480519 acc: 60.9375\n",
      "Epoch: 2 batch 360 loss: 0.9422382 acc: 67.96875\n",
      "Epoch: 2 batch 361 loss: 1.1923552 acc: 53.125\n",
      "Epoch: 2 batch 362 loss: 1.0959368 acc: 60.15625\n",
      "Epoch: 2 batch 363 loss: 1.2447186 acc: 57.8125\n",
      "Epoch: 2 batch 364 loss: 1.1454818 acc: 60.9375\n",
      "Epoch: 2 batch 365 loss: 0.94343746 acc: 67.1875\n",
      "Epoch: 2 batch 366 loss: 1.0674794 acc: 65.625\n",
      "Epoch: 2 batch 367 loss: 1.131852 acc: 57.03125\n",
      "Epoch: 2 batch 368 loss: 1.0691619 acc: 65.625\n",
      "Epoch: 2 batch 369 loss: 0.94877 acc: 66.40625\n",
      "Epoch: 2 batch 370 loss: 1.016636 acc: 64.84375\n",
      "Epoch: 2 batch 371 loss: 0.95023334 acc: 64.84375\n",
      "Epoch: 2 batch 372 loss: 1.0725721 acc: 60.9375\n",
      "Epoch: 2 batch 373 loss: 1.1192698 acc: 59.375\n",
      "Epoch: 2 batch 374 loss: 1.1369095 acc: 61.71875\n",
      "Epoch: 2 batch 375 loss: 1.311064 acc: 53.90625\n",
      "Epoch: 2 batch 376 loss: 1.1550725 acc: 61.71875\n",
      "Epoch: 2 batch 377 loss: 0.93628854 acc: 67.1875\n",
      "Epoch: 2 batch 378 loss: 0.9947924 acc: 60.15625\n",
      "Epoch: 2 batch 379 loss: 1.0306787 acc: 67.96875\n",
      "Epoch: 2 batch 380 loss: 1.0796285 acc: 61.71875\n",
      "Epoch: 2 batch 381 loss: 1.0760756 acc: 57.8125\n",
      "Epoch: 2 batch 382 loss: 1.017207 acc: 61.71875\n",
      "Epoch: 2 batch 383 loss: 1.0558285 acc: 64.0625\n",
      "Epoch: 2 batch 384 loss: 1.009477 acc: 67.1875\n",
      "Epoch: 2 batch 385 loss: 1.1385663 acc: 62.5\n",
      "Epoch: 2 batch 386 loss: 0.9741891 acc: 64.0625\n",
      "Epoch: 2 batch 387 loss: 1.1264682 acc: 57.8125\n",
      "Epoch: 2 batch 388 loss: 1.1325054 acc: 58.59375\n",
      "Epoch: 2 batch 389 loss: 1.0419823 acc: 64.0625\n",
      "After epoch 2 test loss 0.983936419582367 test accuracy 0.654699981212616\n",
      "Epoch: 3 batch 0 loss: 0.90915895 acc: 65.51145315170288\n",
      "Epoch: 3 batch 1 loss: 0.92922616 acc: 69.53125\n",
      "Epoch: 3 batch 2 loss: 1.0787952 acc: 64.0625\n",
      "Epoch: 3 batch 3 loss: 1.0658541 acc: 60.15625\n",
      "Epoch: 3 batch 4 loss: 1.1681032 acc: 60.15625\n",
      "Epoch: 3 batch 5 loss: 1.0569495 acc: 63.28125\n",
      "Epoch: 3 batch 6 loss: 1.0836141 acc: 62.5\n",
      "Epoch: 3 batch 7 loss: 0.926655 acc: 62.5\n",
      "Epoch: 3 batch 8 loss: 1.0735099 acc: 60.9375\n",
      "Epoch: 3 batch 9 loss: 1.0506382 acc: 67.96875\n",
      "Epoch: 3 batch 10 loss: 1.0748563 acc: 60.9375\n",
      "Epoch: 3 batch 11 loss: 1.1313423 acc: 57.03125\n",
      "Epoch: 3 batch 12 loss: 1.1947992 acc: 63.28125\n",
      "Epoch: 3 batch 13 loss: 1.0935867 acc: 61.71875\n",
      "Epoch: 3 batch 14 loss: 1.0281554 acc: 70.3125\n",
      "Epoch: 3 batch 15 loss: 1.3349609 acc: 55.46875\n",
      "Epoch: 3 batch 16 loss: 1.1076978 acc: 60.9375\n",
      "Epoch: 3 batch 17 loss: 1.0659508 acc: 59.375\n",
      "Epoch: 3 batch 18 loss: 0.9778766 acc: 70.3125\n",
      "Epoch: 3 batch 19 loss: 1.0359797 acc: 67.1875\n",
      "Epoch: 3 batch 20 loss: 0.8750329 acc: 66.40625\n",
      "Epoch: 3 batch 21 loss: 1.0861745 acc: 57.03125\n",
      "Epoch: 3 batch 22 loss: 0.92373645 acc: 64.84375\n",
      "Epoch: 3 batch 23 loss: 1.2630439 acc: 57.03125\n",
      "Epoch: 3 batch 24 loss: 1.1475916 acc: 61.71875\n",
      "Epoch: 3 batch 25 loss: 1.0573839 acc: 62.5\n",
      "Epoch: 3 batch 26 loss: 1.3093302 acc: 59.375\n",
      "Epoch: 3 batch 27 loss: 0.98255223 acc: 64.84375\n",
      "Epoch: 3 batch 28 loss: 0.9340644 acc: 68.75\n",
      "Epoch: 3 batch 29 loss: 0.9205792 acc: 66.40625\n",
      "Epoch: 3 batch 30 loss: 1.04857 acc: 60.9375\n",
      "Epoch: 3 batch 31 loss: 1.0484487 acc: 67.1875\n",
      "Epoch: 3 batch 32 loss: 1.1150717 acc: 62.5\n",
      "Epoch: 3 batch 33 loss: 0.99736845 acc: 61.71875\n",
      "Epoch: 3 batch 34 loss: 1.0333111 acc: 67.1875\n",
      "Epoch: 3 batch 35 loss: 1.1605711 acc: 62.5\n",
      "Epoch: 3 batch 36 loss: 1.1364859 acc: 60.15625\n",
      "Epoch: 3 batch 37 loss: 1.1295797 acc: 60.9375\n",
      "Epoch: 3 batch 38 loss: 1.1269302 acc: 61.71875\n",
      "Epoch: 3 batch 39 loss: 1.2021636 acc: 57.03125\n",
      "Epoch: 3 batch 40 loss: 1.1111953 acc: 64.0625\n",
      "Epoch: 3 batch 41 loss: 1.0935568 acc: 64.84375\n",
      "Epoch: 3 batch 42 loss: 1.1783309 acc: 52.34375\n",
      "Epoch: 3 batch 43 loss: 1.1672392 acc: 63.28125\n",
      "Epoch: 3 batch 44 loss: 1.0014737 acc: 64.0625\n",
      "Epoch: 3 batch 45 loss: 1.0417535 acc: 60.15625\n",
      "Epoch: 3 batch 46 loss: 0.98664665 acc: 66.40625\n",
      "Epoch: 3 batch 47 loss: 1.0930462 acc: 58.59375\n",
      "Epoch: 3 batch 48 loss: 1.0132242 acc: 63.28125\n",
      "Epoch: 3 batch 49 loss: 0.9471556 acc: 61.71875\n",
      "Epoch: 3 batch 50 loss: 1.0006489 acc: 60.9375\n",
      "Epoch: 3 batch 51 loss: 1.0857791 acc: 60.15625\n",
      "Epoch: 3 batch 52 loss: 0.9944489 acc: 67.1875\n",
      "Epoch: 3 batch 53 loss: 1.2559836 acc: 59.375\n",
      "Epoch: 3 batch 54 loss: 0.9968728 acc: 67.1875\n",
      "Epoch: 3 batch 55 loss: 1.0461124 acc: 67.1875\n",
      "Epoch: 3 batch 56 loss: 1.114742 acc: 60.15625\n",
      "Epoch: 3 batch 57 loss: 1.0472115 acc: 60.9375\n",
      "Epoch: 3 batch 58 loss: 1.1978402 acc: 57.03125\n",
      "Epoch: 3 batch 59 loss: 1.2235689 acc: 55.46875\n",
      "Epoch: 3 batch 60 loss: 1.0775268 acc: 63.28125\n",
      "Epoch: 3 batch 61 loss: 0.9133565 acc: 69.53125\n",
      "Epoch: 3 batch 62 loss: 1.0528111 acc: 64.0625\n",
      "Epoch: 3 batch 63 loss: 0.9589186 acc: 70.3125\n",
      "Epoch: 3 batch 64 loss: 0.9811357 acc: 65.625\n",
      "Epoch: 3 batch 65 loss: 0.97248113 acc: 57.8125\n",
      "Epoch: 3 batch 66 loss: 1.0552441 acc: 67.1875\n",
      "Epoch: 3 batch 67 loss: 0.99442923 acc: 64.84375\n",
      "Epoch: 3 batch 68 loss: 1.0633192 acc: 59.375\n",
      "Epoch: 3 batch 69 loss: 1.0545588 acc: 63.28125\n",
      "Epoch: 3 batch 70 loss: 1.2232512 acc: 62.5\n",
      "Epoch: 3 batch 71 loss: 1.0465791 acc: 60.15625\n",
      "Epoch: 3 batch 72 loss: 1.0189598 acc: 60.9375\n",
      "Epoch: 3 batch 73 loss: 0.9833325 acc: 66.40625\n",
      "Epoch: 3 batch 74 loss: 0.85921943 acc: 67.96875\n",
      "Epoch: 3 batch 75 loss: 1.1499822 acc: 58.59375\n",
      "Epoch: 3 batch 76 loss: 1.1273652 acc: 59.375\n",
      "Epoch: 3 batch 77 loss: 1.0601599 acc: 57.8125\n",
      "Epoch: 3 batch 78 loss: 0.99210167 acc: 63.28125\n",
      "Epoch: 3 batch 79 loss: 1.0660105 acc: 67.1875\n",
      "Epoch: 3 batch 80 loss: 0.9302973 acc: 73.4375\n",
      "Epoch: 3 batch 81 loss: 1.4069766 acc: 50.78125\n",
      "Epoch: 3 batch 82 loss: 1.0573612 acc: 64.84375\n",
      "Epoch: 3 batch 83 loss: 1.0431849 acc: 64.0625\n",
      "Epoch: 3 batch 84 loss: 1.0635555 acc: 64.0625\n",
      "Epoch: 3 batch 85 loss: 1.1281853 acc: 59.375\n",
      "Epoch: 3 batch 86 loss: 1.1493523 acc: 59.375\n",
      "Epoch: 3 batch 87 loss: 0.96840477 acc: 65.625\n",
      "Epoch: 3 batch 88 loss: 0.94804704 acc: 67.96875\n",
      "Epoch: 3 batch 89 loss: 0.94059175 acc: 64.84375\n",
      "Epoch: 3 batch 90 loss: 0.9179932 acc: 64.0625\n",
      "Epoch: 3 batch 91 loss: 0.90478235 acc: 67.96875\n",
      "Epoch: 3 batch 92 loss: 1.0593174 acc: 64.0625\n",
      "Epoch: 3 batch 93 loss: 0.989113 acc: 64.0625\n",
      "Epoch: 3 batch 94 loss: 1.1280729 acc: 57.03125\n",
      "Epoch: 3 batch 95 loss: 0.9950742 acc: 61.71875\n",
      "Epoch: 3 batch 96 loss: 1.0092152 acc: 63.28125\n",
      "Epoch: 3 batch 97 loss: 1.1261477 acc: 60.15625\n",
      "Epoch: 3 batch 98 loss: 1.0370749 acc: 63.28125\n",
      "Epoch: 3 batch 99 loss: 1.1359144 acc: 62.5\n",
      "Epoch: 3 batch 100 loss: 0.95741326 acc: 65.625\n",
      "Epoch: 3 batch 101 loss: 1.0759734 acc: 60.9375\n",
      "Epoch: 3 batch 102 loss: 1.081305 acc: 66.40625\n",
      "Epoch: 3 batch 103 loss: 1.1231141 acc: 62.5\n",
      "Epoch: 3 batch 104 loss: 1.1119103 acc: 57.8125\n",
      "Epoch: 3 batch 105 loss: 0.94386244 acc: 64.84375\n",
      "Epoch: 3 batch 106 loss: 1.0612649 acc: 60.15625\n",
      "Epoch: 3 batch 107 loss: 1.0892664 acc: 66.40625\n",
      "Epoch: 3 batch 108 loss: 1.0292479 acc: 64.84375\n",
      "Epoch: 3 batch 109 loss: 1.0993155 acc: 63.28125\n",
      "Epoch: 3 batch 110 loss: 1.0762547 acc: 56.25\n",
      "Epoch: 3 batch 111 loss: 1.022946 acc: 61.71875\n",
      "Epoch: 3 batch 112 loss: 0.99792135 acc: 71.875\n",
      "Epoch: 3 batch 113 loss: 0.95678926 acc: 61.71875\n",
      "Epoch: 3 batch 114 loss: 1.1106436 acc: 62.5\n",
      "Epoch: 3 batch 115 loss: 0.93760663 acc: 63.28125\n",
      "Epoch: 3 batch 116 loss: 1.245503 acc: 61.71875\n",
      "Epoch: 3 batch 117 loss: 0.97563875 acc: 66.40625\n",
      "Epoch: 3 batch 118 loss: 1.001745 acc: 65.625\n",
      "Epoch: 3 batch 119 loss: 1.0131116 acc: 61.71875\n",
      "Epoch: 3 batch 120 loss: 1.0427539 acc: 66.40625\n",
      "Epoch: 3 batch 121 loss: 1.2002007 acc: 57.03125\n",
      "Epoch: 3 batch 122 loss: 1.0516989 acc: 66.40625\n",
      "Epoch: 3 batch 123 loss: 1.1056683 acc: 64.0625\n",
      "Epoch: 3 batch 124 loss: 1.1490909 acc: 61.71875\n",
      "Epoch: 3 batch 125 loss: 0.87700945 acc: 66.40625\n",
      "Epoch: 3 batch 126 loss: 0.9753133 acc: 64.0625\n",
      "Epoch: 3 batch 127 loss: 0.9361732 acc: 62.5\n",
      "Epoch: 3 batch 128 loss: 1.0377023 acc: 66.40625\n",
      "Epoch: 3 batch 129 loss: 1.1852456 acc: 57.03125\n",
      "Epoch: 3 batch 130 loss: 0.9946142 acc: 63.28125\n",
      "Epoch: 3 batch 131 loss: 1.0949545 acc: 67.1875\n",
      "Epoch: 3 batch 132 loss: 0.95701396 acc: 66.40625\n",
      "Epoch: 3 batch 133 loss: 1.0189643 acc: 67.96875\n",
      "Epoch: 3 batch 134 loss: 0.77329993 acc: 75.0\n",
      "Epoch: 3 batch 135 loss: 1.0542586 acc: 66.40625\n",
      "Epoch: 3 batch 136 loss: 1.018107 acc: 64.84375\n",
      "Epoch: 3 batch 137 loss: 0.949055 acc: 64.84375\n",
      "Epoch: 3 batch 138 loss: 1.3823358 acc: 53.90625\n",
      "Epoch: 3 batch 139 loss: 1.1694574 acc: 57.03125\n",
      "Epoch: 3 batch 140 loss: 1.1079967 acc: 59.375\n",
      "Epoch: 3 batch 141 loss: 1.0115222 acc: 62.5\n",
      "Epoch: 3 batch 142 loss: 1.1592487 acc: 60.15625\n",
      "Epoch: 3 batch 143 loss: 1.0776095 acc: 67.96875\n",
      "Epoch: 3 batch 144 loss: 1.1217282 acc: 63.28125\n",
      "Epoch: 3 batch 145 loss: 1.098788 acc: 59.375\n",
      "Epoch: 3 batch 146 loss: 0.9963079 acc: 64.0625\n",
      "Epoch: 3 batch 147 loss: 0.9038594 acc: 65.625\n",
      "Epoch: 3 batch 148 loss: 0.9520936 acc: 64.0625\n",
      "Epoch: 3 batch 149 loss: 1.2093647 acc: 60.9375\n",
      "Epoch: 3 batch 150 loss: 1.0305126 acc: 65.625\n",
      "Epoch: 3 batch 151 loss: 0.9084702 acc: 65.625\n",
      "Epoch: 3 batch 152 loss: 0.97919023 acc: 69.53125\n",
      "Epoch: 3 batch 153 loss: 1.1608691 acc: 57.8125\n",
      "Epoch: 3 batch 154 loss: 1.0623224 acc: 63.28125\n",
      "Epoch: 3 batch 155 loss: 0.98396415 acc: 67.96875\n",
      "Epoch: 3 batch 156 loss: 1.0723155 acc: 60.9375\n",
      "Epoch: 3 batch 157 loss: 1.1029456 acc: 57.03125\n",
      "Epoch: 3 batch 158 loss: 0.9653193 acc: 64.84375\n",
      "Epoch: 3 batch 159 loss: 1.0189637 acc: 62.5\n",
      "Epoch: 3 batch 160 loss: 0.9673555 acc: 67.96875\n",
      "Epoch: 3 batch 161 loss: 0.95066994 acc: 66.40625\n",
      "Epoch: 3 batch 162 loss: 1.0082372 acc: 64.0625\n",
      "Epoch: 3 batch 163 loss: 0.85437155 acc: 66.40625\n",
      "Epoch: 3 batch 164 loss: 1.0264273 acc: 66.40625\n",
      "Epoch: 3 batch 165 loss: 1.0532134 acc: 64.84375\n",
      "Epoch: 3 batch 166 loss: 1.00892 acc: 69.53125\n",
      "Epoch: 3 batch 167 loss: 1.1189843 acc: 57.8125\n",
      "Epoch: 3 batch 168 loss: 0.9819018 acc: 70.3125\n",
      "Epoch: 3 batch 169 loss: 0.97358876 acc: 64.0625\n",
      "Epoch: 3 batch 170 loss: 1.0695138 acc: 62.5\n",
      "Epoch: 3 batch 171 loss: 1.0811765 acc: 61.71875\n",
      "Epoch: 3 batch 172 loss: 0.92060137 acc: 66.40625\n",
      "Epoch: 3 batch 173 loss: 0.98582846 acc: 68.75\n",
      "Epoch: 3 batch 174 loss: 0.99669707 acc: 64.0625\n",
      "Epoch: 3 batch 175 loss: 1.1032028 acc: 57.8125\n",
      "Epoch: 3 batch 176 loss: 1.0093827 acc: 64.84375\n",
      "Epoch: 3 batch 177 loss: 0.941615 acc: 68.75\n",
      "Epoch: 3 batch 178 loss: 1.0886674 acc: 64.84375\n",
      "Epoch: 3 batch 179 loss: 1.0281918 acc: 64.84375\n",
      "Epoch: 3 batch 180 loss: 0.9326117 acc: 68.75\n",
      "Epoch: 3 batch 181 loss: 0.9467615 acc: 67.96875\n",
      "Epoch: 3 batch 182 loss: 0.78128576 acc: 69.53125\n",
      "Epoch: 3 batch 183 loss: 1.2443076 acc: 55.46875\n",
      "Epoch: 3 batch 184 loss: 1.0117929 acc: 63.28125\n",
      "Epoch: 3 batch 185 loss: 0.8142934 acc: 71.09375\n",
      "Epoch: 3 batch 186 loss: 0.7964963 acc: 75.78125\n",
      "Epoch: 3 batch 187 loss: 0.88154006 acc: 65.625\n",
      "Epoch: 3 batch 188 loss: 0.9912549 acc: 64.0625\n",
      "Epoch: 3 batch 189 loss: 0.99572974 acc: 67.96875\n",
      "Epoch: 3 batch 190 loss: 0.98499537 acc: 63.28125\n",
      "Epoch: 3 batch 191 loss: 0.9752041 acc: 66.40625\n",
      "Epoch: 3 batch 192 loss: 0.80353844 acc: 71.09375\n",
      "Epoch: 3 batch 193 loss: 1.2225577 acc: 52.34375\n",
      "Epoch: 3 batch 194 loss: 0.94028115 acc: 68.75\n",
      "Epoch: 3 batch 195 loss: 0.88599277 acc: 66.40625\n",
      "Epoch: 3 batch 196 loss: 1.0065461 acc: 65.625\n",
      "Epoch: 3 batch 197 loss: 1.1285543 acc: 60.15625\n",
      "Epoch: 3 batch 198 loss: 1.0374898 acc: 67.96875\n",
      "Epoch: 3 batch 199 loss: 0.8761562 acc: 70.3125\n",
      "Epoch: 3 batch 200 loss: 1.017285 acc: 66.40625\n",
      "Epoch: 3 batch 201 loss: 0.78389704 acc: 75.0\n",
      "Epoch: 3 batch 202 loss: 0.8901416 acc: 72.65625\n",
      "Epoch: 3 batch 203 loss: 0.88439816 acc: 66.40625\n",
      "Epoch: 3 batch 204 loss: 0.9631648 acc: 62.5\n",
      "Epoch: 3 batch 205 loss: 0.96729624 acc: 63.28125\n",
      "Epoch: 3 batch 206 loss: 1.0405462 acc: 64.84375\n",
      "Epoch: 3 batch 207 loss: 0.9430511 acc: 64.0625\n",
      "Epoch: 3 batch 208 loss: 1.0443932 acc: 65.625\n",
      "Epoch: 3 batch 209 loss: 1.1085538 acc: 61.71875\n",
      "Epoch: 3 batch 210 loss: 1.0451207 acc: 67.1875\n",
      "Epoch: 3 batch 211 loss: 1.0430335 acc: 63.28125\n",
      "Epoch: 3 batch 212 loss: 1.0599738 acc: 59.375\n",
      "Epoch: 3 batch 213 loss: 0.95997715 acc: 66.40625\n",
      "Epoch: 3 batch 214 loss: 0.96828604 acc: 65.625\n",
      "Epoch: 3 batch 215 loss: 0.86377203 acc: 70.3125\n",
      "Epoch: 3 batch 216 loss: 0.94097286 acc: 66.40625\n",
      "Epoch: 3 batch 217 loss: 1.0277983 acc: 68.75\n",
      "Epoch: 3 batch 218 loss: 0.9759669 acc: 65.625\n",
      "Epoch: 3 batch 219 loss: 0.9927996 acc: 62.5\n",
      "Epoch: 3 batch 220 loss: 0.98551655 acc: 68.75\n",
      "Epoch: 3 batch 221 loss: 1.0559108 acc: 66.40625\n",
      "Epoch: 3 batch 222 loss: 0.9911375 acc: 64.0625\n",
      "Epoch: 3 batch 223 loss: 1.0242016 acc: 63.28125\n",
      "Epoch: 3 batch 224 loss: 1.0313649 acc: 63.28125\n",
      "Epoch: 3 batch 225 loss: 0.8362973 acc: 73.4375\n",
      "Epoch: 3 batch 226 loss: 0.8913485 acc: 63.28125\n",
      "Epoch: 3 batch 227 loss: 1.1979034 acc: 62.5\n",
      "Epoch: 3 batch 228 loss: 1.0310719 acc: 63.28125\n",
      "Epoch: 3 batch 229 loss: 0.77733076 acc: 75.78125\n",
      "Epoch: 3 batch 230 loss: 0.89127576 acc: 68.75\n",
      "Epoch: 3 batch 231 loss: 1.0499685 acc: 64.0625\n",
      "Epoch: 3 batch 232 loss: 0.97482526 acc: 67.1875\n",
      "Epoch: 3 batch 233 loss: 0.980985 acc: 67.96875\n",
      "Epoch: 3 batch 234 loss: 0.9399346 acc: 68.75\n",
      "Epoch: 3 batch 235 loss: 1.0381947 acc: 60.9375\n",
      "Epoch: 3 batch 236 loss: 0.88357985 acc: 71.09375\n",
      "Epoch: 3 batch 237 loss: 1.0214037 acc: 60.9375\n",
      "Epoch: 3 batch 238 loss: 1.0671769 acc: 63.28125\n",
      "Epoch: 3 batch 239 loss: 1.0899243 acc: 65.625\n",
      "Epoch: 3 batch 240 loss: 1.0679508 acc: 62.5\n",
      "Epoch: 3 batch 241 loss: 0.9685293 acc: 71.875\n",
      "Epoch: 3 batch 242 loss: 0.86487114 acc: 70.3125\n",
      "Epoch: 3 batch 243 loss: 1.0744226 acc: 57.8125\n",
      "Epoch: 3 batch 244 loss: 0.912313 acc: 64.84375\n",
      "Epoch: 3 batch 245 loss: 1.04895 acc: 60.15625\n",
      "Epoch: 3 batch 246 loss: 1.0817045 acc: 63.28125\n",
      "Epoch: 3 batch 247 loss: 1.0583929 acc: 60.15625\n",
      "Epoch: 3 batch 248 loss: 0.8669369 acc: 68.75\n",
      "Epoch: 3 batch 249 loss: 1.0820682 acc: 64.0625\n",
      "Epoch: 3 batch 250 loss: 0.9803635 acc: 69.53125\n",
      "Epoch: 3 batch 251 loss: 1.0306664 acc: 60.9375\n",
      "Epoch: 3 batch 252 loss: 0.92338943 acc: 70.3125\n",
      "Epoch: 3 batch 253 loss: 0.9806576 acc: 64.0625\n",
      "Epoch: 3 batch 254 loss: 1.0759377 acc: 59.375\n",
      "Epoch: 3 batch 255 loss: 1.0476431 acc: 63.28125\n",
      "Epoch: 3 batch 256 loss: 1.0978413 acc: 57.8125\n",
      "Epoch: 3 batch 257 loss: 1.1505523 acc: 60.9375\n",
      "Epoch: 3 batch 258 loss: 0.9558519 acc: 65.625\n",
      "Epoch: 3 batch 259 loss: 1.0163866 acc: 68.75\n",
      "Epoch: 3 batch 260 loss: 1.0957024 acc: 57.8125\n",
      "Epoch: 3 batch 261 loss: 1.1148279 acc: 57.8125\n",
      "Epoch: 3 batch 262 loss: 1.1678376 acc: 56.25\n",
      "Epoch: 3 batch 263 loss: 0.8556447 acc: 71.09375\n",
      "Epoch: 3 batch 264 loss: 1.0465806 acc: 60.15625\n",
      "Epoch: 3 batch 265 loss: 1.1203146 acc: 63.28125\n",
      "Epoch: 3 batch 266 loss: 1.0413846 acc: 66.40625\n",
      "Epoch: 3 batch 267 loss: 0.88139707 acc: 67.1875\n",
      "Epoch: 3 batch 268 loss: 1.0797958 acc: 65.625\n",
      "Epoch: 3 batch 269 loss: 1.0662975 acc: 65.625\n",
      "Epoch: 3 batch 270 loss: 0.9335434 acc: 67.96875\n",
      "Epoch: 3 batch 271 loss: 0.9401794 acc: 65.625\n",
      "Epoch: 3 batch 272 loss: 0.90223557 acc: 67.96875\n",
      "Epoch: 3 batch 273 loss: 1.0614778 acc: 57.8125\n",
      "Epoch: 3 batch 274 loss: 0.99556655 acc: 62.5\n",
      "Epoch: 3 batch 275 loss: 1.1132269 acc: 57.8125\n",
      "Epoch: 3 batch 276 loss: 0.9682862 acc: 72.65625\n",
      "Epoch: 3 batch 277 loss: 0.8983907 acc: 62.5\n",
      "Epoch: 3 batch 278 loss: 1.0582845 acc: 68.75\n",
      "Epoch: 3 batch 279 loss: 1.074487 acc: 67.96875\n",
      "Epoch: 3 batch 280 loss: 0.9947666 acc: 64.84375\n",
      "Epoch: 3 batch 281 loss: 0.8556212 acc: 67.96875\n",
      "Epoch: 3 batch 282 loss: 1.0552248 acc: 63.28125\n",
      "Epoch: 3 batch 283 loss: 0.93001723 acc: 73.4375\n",
      "Epoch: 3 batch 284 loss: 0.9202167 acc: 62.5\n",
      "Epoch: 3 batch 285 loss: 0.8987825 acc: 64.84375\n",
      "Epoch: 3 batch 286 loss: 0.94913185 acc: 65.625\n",
      "Epoch: 3 batch 287 loss: 0.8616648 acc: 69.53125\n",
      "Epoch: 3 batch 288 loss: 1.1879528 acc: 54.6875\n",
      "Epoch: 3 batch 289 loss: 0.9104448 acc: 65.625\n",
      "Epoch: 3 batch 290 loss: 1.0084025 acc: 67.96875\n",
      "Epoch: 3 batch 291 loss: 1.0700973 acc: 61.71875\n",
      "Epoch: 3 batch 292 loss: 1.2004188 acc: 57.8125\n",
      "Epoch: 3 batch 293 loss: 0.9229361 acc: 67.1875\n",
      "Epoch: 3 batch 294 loss: 0.9842707 acc: 63.28125\n",
      "Epoch: 3 batch 295 loss: 0.9343693 acc: 66.40625\n",
      "Epoch: 3 batch 296 loss: 1.0699394 acc: 61.71875\n",
      "Epoch: 3 batch 297 loss: 1.0149044 acc: 60.9375\n",
      "Epoch: 3 batch 298 loss: 1.0064712 acc: 61.71875\n",
      "Epoch: 3 batch 299 loss: 1.0567507 acc: 64.0625\n",
      "Epoch: 3 batch 300 loss: 0.9818648 acc: 65.625\n",
      "Epoch: 3 batch 301 loss: 0.9116539 acc: 67.1875\n",
      "Epoch: 3 batch 302 loss: 0.8984793 acc: 71.09375\n",
      "Epoch: 3 batch 303 loss: 0.83401525 acc: 72.65625\n",
      "Epoch: 3 batch 304 loss: 1.1475424 acc: 62.5\n",
      "Epoch: 3 batch 305 loss: 0.89244413 acc: 67.96875\n",
      "Epoch: 3 batch 306 loss: 1.0778117 acc: 64.84375\n",
      "Epoch: 3 batch 307 loss: 1.0063257 acc: 66.40625\n",
      "Epoch: 3 batch 308 loss: 1.0422224 acc: 66.40625\n",
      "Epoch: 3 batch 309 loss: 1.1230884 acc: 54.6875\n",
      "Epoch: 3 batch 310 loss: 1.1554135 acc: 60.15625\n",
      "Epoch: 3 batch 311 loss: 0.99035215 acc: 66.40625\n",
      "Epoch: 3 batch 312 loss: 0.8970524 acc: 72.65625\n",
      "Epoch: 3 batch 313 loss: 0.9331344 acc: 66.40625\n",
      "Epoch: 3 batch 314 loss: 0.93088615 acc: 64.84375\n",
      "Epoch: 3 batch 315 loss: 1.1111889 acc: 62.5\n",
      "Epoch: 3 batch 316 loss: 1.0429156 acc: 63.28125\n",
      "Epoch: 3 batch 317 loss: 1.0034314 acc: 66.40625\n",
      "Epoch: 3 batch 318 loss: 1.0027915 acc: 64.84375\n",
      "Epoch: 3 batch 319 loss: 1.0704577 acc: 58.59375\n",
      "Epoch: 3 batch 320 loss: 1.1001943 acc: 58.59375\n",
      "Epoch: 3 batch 321 loss: 0.8397347 acc: 71.875\n",
      "Epoch: 3 batch 322 loss: 1.0030972 acc: 66.40625\n",
      "Epoch: 3 batch 323 loss: 0.91576254 acc: 64.84375\n",
      "Epoch: 3 batch 324 loss: 1.0240027 acc: 63.28125\n",
      "Epoch: 3 batch 325 loss: 0.9346039 acc: 68.75\n",
      "Epoch: 3 batch 326 loss: 0.83434045 acc: 71.09375\n",
      "Epoch: 3 batch 327 loss: 1.0892975 acc: 63.28125\n",
      "Epoch: 3 batch 328 loss: 1.0933656 acc: 61.71875\n",
      "Epoch: 3 batch 329 loss: 1.0394776 acc: 65.625\n",
      "Epoch: 3 batch 330 loss: 0.73852915 acc: 75.0\n",
      "Epoch: 3 batch 331 loss: 0.9174916 acc: 67.96875\n",
      "Epoch: 3 batch 332 loss: 1.2222784 acc: 59.375\n",
      "Epoch: 3 batch 333 loss: 0.9652008 acc: 63.28125\n",
      "Epoch: 3 batch 334 loss: 0.8838688 acc: 64.0625\n",
      "Epoch: 3 batch 335 loss: 1.0730125 acc: 63.28125\n",
      "Epoch: 3 batch 336 loss: 0.8025912 acc: 70.3125\n",
      "Epoch: 3 batch 337 loss: 0.92205864 acc: 69.53125\n",
      "Epoch: 3 batch 338 loss: 1.1188148 acc: 58.59375\n",
      "Epoch: 3 batch 339 loss: 0.9514867 acc: 67.96875\n",
      "Epoch: 3 batch 340 loss: 0.8990209 acc: 64.84375\n",
      "Epoch: 3 batch 341 loss: 0.962835 acc: 65.625\n",
      "Epoch: 3 batch 342 loss: 1.116905 acc: 60.9375\n",
      "Epoch: 3 batch 343 loss: 1.0183656 acc: 61.71875\n",
      "Epoch: 3 batch 344 loss: 1.0229874 acc: 64.84375\n",
      "Epoch: 3 batch 345 loss: 1.1323643 acc: 60.9375\n",
      "Epoch: 3 batch 346 loss: 0.9891933 acc: 67.96875\n",
      "Epoch: 3 batch 347 loss: 1.0200487 acc: 67.1875\n",
      "Epoch: 3 batch 348 loss: 0.9574054 acc: 64.84375\n",
      "Epoch: 3 batch 349 loss: 0.9274776 acc: 66.40625\n",
      "Epoch: 3 batch 350 loss: 0.9948522 acc: 64.0625\n",
      "Epoch: 3 batch 351 loss: 0.85621464 acc: 69.53125\n",
      "Epoch: 3 batch 352 loss: 0.957651 acc: 64.0625\n",
      "Epoch: 3 batch 353 loss: 1.0619142 acc: 65.625\n",
      "Epoch: 3 batch 354 loss: 0.87700325 acc: 67.96875\n",
      "Epoch: 3 batch 355 loss: 0.9313656 acc: 63.28125\n",
      "Epoch: 3 batch 356 loss: 1.0412002 acc: 65.625\n",
      "Epoch: 3 batch 357 loss: 0.88159513 acc: 67.96875\n",
      "Epoch: 3 batch 358 loss: 0.9408383 acc: 64.84375\n",
      "Epoch: 3 batch 359 loss: 0.9449998 acc: 66.40625\n",
      "Epoch: 3 batch 360 loss: 0.75404894 acc: 78.90625\n",
      "Epoch: 3 batch 361 loss: 1.1120226 acc: 58.59375\n",
      "Epoch: 3 batch 362 loss: 0.8471348 acc: 71.09375\n",
      "Epoch: 3 batch 363 loss: 1.1196215 acc: 53.90625\n",
      "Epoch: 3 batch 364 loss: 1.00576 acc: 65.625\n",
      "Epoch: 3 batch 365 loss: 0.9788004 acc: 67.96875\n",
      "Epoch: 3 batch 366 loss: 0.7865624 acc: 73.4375\n",
      "Epoch: 3 batch 367 loss: 1.1215097 acc: 64.0625\n",
      "Epoch: 3 batch 368 loss: 0.91755056 acc: 71.875\n",
      "Epoch: 3 batch 369 loss: 0.8388197 acc: 67.96875\n",
      "Epoch: 3 batch 370 loss: 0.94513565 acc: 67.1875\n",
      "Epoch: 3 batch 371 loss: 0.91448265 acc: 67.1875\n",
      "Epoch: 3 batch 372 loss: 0.9379206 acc: 65.625\n",
      "Epoch: 3 batch 373 loss: 0.9071628 acc: 66.40625\n",
      "Epoch: 3 batch 374 loss: 0.91997516 acc: 68.75\n",
      "Epoch: 3 batch 375 loss: 1.0614802 acc: 67.1875\n",
      "Epoch: 3 batch 376 loss: 1.0188023 acc: 67.96875\n",
      "Epoch: 3 batch 377 loss: 0.86429393 acc: 73.4375\n",
      "Epoch: 3 batch 378 loss: 0.87145346 acc: 66.40625\n",
      "Epoch: 3 batch 379 loss: 1.0286114 acc: 68.75\n",
      "Epoch: 3 batch 380 loss: 0.9150479 acc: 70.3125\n",
      "Epoch: 3 batch 381 loss: 0.89605784 acc: 67.1875\n",
      "Epoch: 3 batch 382 loss: 0.9737413 acc: 72.65625\n",
      "Epoch: 3 batch 383 loss: 0.906758 acc: 67.1875\n",
      "Epoch: 3 batch 384 loss: 0.82910776 acc: 72.65625\n",
      "Epoch: 3 batch 385 loss: 1.0302472 acc: 64.0625\n",
      "Epoch: 3 batch 386 loss: 0.9719225 acc: 71.875\n",
      "Epoch: 3 batch 387 loss: 0.8894654 acc: 66.40625\n",
      "Epoch: 3 batch 388 loss: 0.965683 acc: 66.40625\n",
      "Epoch: 3 batch 389 loss: 0.8944565 acc: 70.3125\n",
      "After epoch 3 test loss 0.906733468914032 test accuracy 0.6791999936103821\n",
      "Epoch: 4 batch 0 loss: 0.92463225 acc: 67.9206132888794\n",
      "Epoch: 4 batch 1 loss: 0.8360467 acc: 70.3125\n",
      "Epoch: 4 batch 2 loss: 0.9717711 acc: 66.40625\n",
      "Epoch: 4 batch 3 loss: 0.9509673 acc: 66.40625\n",
      "Epoch: 4 batch 4 loss: 0.9305917 acc: 67.1875\n",
      "Epoch: 4 batch 5 loss: 0.94820786 acc: 67.96875\n",
      "Epoch: 4 batch 6 loss: 1.1714935 acc: 54.6875\n",
      "Epoch: 4 batch 7 loss: 0.8766802 acc: 67.1875\n",
      "Epoch: 4 batch 8 loss: 0.98319435 acc: 64.84375\n",
      "Epoch: 4 batch 9 loss: 0.807093 acc: 74.21875\n",
      "Epoch: 4 batch 10 loss: 0.9096333 acc: 65.625\n",
      "Epoch: 4 batch 11 loss: 0.8466016 acc: 66.40625\n",
      "Epoch: 4 batch 12 loss: 1.0186348 acc: 64.0625\n",
      "Epoch: 4 batch 13 loss: 1.0297865 acc: 65.625\n",
      "Epoch: 4 batch 14 loss: 0.7934929 acc: 72.65625\n",
      "Epoch: 4 batch 15 loss: 1.1092794 acc: 61.71875\n",
      "Epoch: 4 batch 16 loss: 0.9245155 acc: 64.84375\n",
      "Epoch: 4 batch 17 loss: 0.93321013 acc: 68.75\n",
      "Epoch: 4 batch 18 loss: 0.89686763 acc: 67.1875\n",
      "Epoch: 4 batch 19 loss: 0.8336494 acc: 69.53125\n",
      "Epoch: 4 batch 20 loss: 0.73555773 acc: 71.875\n",
      "Epoch: 4 batch 21 loss: 0.93866396 acc: 67.1875\n",
      "Epoch: 4 batch 22 loss: 0.86303395 acc: 73.4375\n",
      "Epoch: 4 batch 23 loss: 1.1625346 acc: 60.15625\n",
      "Epoch: 4 batch 24 loss: 1.0040462 acc: 63.28125\n",
      "Epoch: 4 batch 25 loss: 0.9706428 acc: 65.625\n",
      "Epoch: 4 batch 26 loss: 1.1461706 acc: 64.0625\n",
      "Epoch: 4 batch 27 loss: 0.8592553 acc: 67.96875\n",
      "Epoch: 4 batch 28 loss: 0.8204678 acc: 73.4375\n",
      "Epoch: 4 batch 29 loss: 0.8906598 acc: 67.1875\n",
      "Epoch: 4 batch 30 loss: 1.0916009 acc: 64.0625\n",
      "Epoch: 4 batch 31 loss: 0.95438766 acc: 67.1875\n",
      "Epoch: 4 batch 32 loss: 1.0513508 acc: 61.71875\n",
      "Epoch: 4 batch 33 loss: 0.8980979 acc: 67.96875\n",
      "Epoch: 4 batch 34 loss: 0.82128465 acc: 72.65625\n",
      "Epoch: 4 batch 35 loss: 0.91629624 acc: 65.625\n",
      "Epoch: 4 batch 36 loss: 0.8494339 acc: 67.1875\n",
      "Epoch: 4 batch 37 loss: 1.082834 acc: 64.0625\n",
      "Epoch: 4 batch 38 loss: 1.0610027 acc: 64.0625\n",
      "Epoch: 4 batch 39 loss: 1.036065 acc: 60.9375\n",
      "Epoch: 4 batch 40 loss: 1.0909765 acc: 64.0625\n",
      "Epoch: 4 batch 41 loss: 1.079923 acc: 67.1875\n",
      "Epoch: 4 batch 42 loss: 1.0134331 acc: 60.15625\n",
      "Epoch: 4 batch 43 loss: 0.9747529 acc: 64.84375\n",
      "Epoch: 4 batch 44 loss: 0.98519295 acc: 69.53125\n",
      "Epoch: 4 batch 45 loss: 0.8846632 acc: 69.53125\n",
      "Epoch: 4 batch 46 loss: 0.86553526 acc: 68.75\n",
      "Epoch: 4 batch 47 loss: 0.955843 acc: 68.75\n",
      "Epoch: 4 batch 48 loss: 0.95970666 acc: 60.9375\n",
      "Epoch: 4 batch 49 loss: 0.7126508 acc: 73.4375\n",
      "Epoch: 4 batch 50 loss: 0.8359026 acc: 67.1875\n",
      "Epoch: 4 batch 51 loss: 0.92376703 acc: 67.1875\n",
      "Epoch: 4 batch 52 loss: 1.0206132 acc: 64.84375\n",
      "Epoch: 4 batch 53 loss: 1.0940113 acc: 62.5\n",
      "Epoch: 4 batch 54 loss: 0.7863632 acc: 71.875\n",
      "Epoch: 4 batch 55 loss: 0.90568054 acc: 69.53125\n",
      "Epoch: 4 batch 56 loss: 0.8551645 acc: 71.875\n",
      "Epoch: 4 batch 57 loss: 1.0225928 acc: 63.28125\n",
      "Epoch: 4 batch 58 loss: 0.8236385 acc: 73.4375\n",
      "Epoch: 4 batch 59 loss: 1.0525182 acc: 57.8125\n",
      "Epoch: 4 batch 60 loss: 1.0851496 acc: 62.5\n",
      "Epoch: 4 batch 61 loss: 0.89942443 acc: 64.84375\n",
      "Epoch: 4 batch 62 loss: 0.97106254 acc: 64.0625\n",
      "Epoch: 4 batch 63 loss: 0.8983041 acc: 67.96875\n",
      "Epoch: 4 batch 64 loss: 0.9456762 acc: 66.40625\n",
      "Epoch: 4 batch 65 loss: 0.96689665 acc: 64.0625\n",
      "Epoch: 4 batch 66 loss: 1.0328085 acc: 64.0625\n",
      "Epoch: 4 batch 67 loss: 0.89738 acc: 66.40625\n",
      "Epoch: 4 batch 68 loss: 0.9861417 acc: 65.625\n",
      "Epoch: 4 batch 69 loss: 0.92434275 acc: 74.21875\n",
      "Epoch: 4 batch 70 loss: 0.95954645 acc: 61.71875\n",
      "Epoch: 4 batch 71 loss: 0.9204371 acc: 67.96875\n",
      "Epoch: 4 batch 72 loss: 0.9163072 acc: 66.40625\n",
      "Epoch: 4 batch 73 loss: 0.88449323 acc: 73.4375\n",
      "Epoch: 4 batch 74 loss: 0.6995172 acc: 73.4375\n",
      "Epoch: 4 batch 75 loss: 0.91927683 acc: 66.40625\n",
      "Epoch: 4 batch 76 loss: 1.0717614 acc: 60.15625\n",
      "Epoch: 4 batch 77 loss: 0.89575464 acc: 67.1875\n",
      "Epoch: 4 batch 78 loss: 0.87856424 acc: 75.0\n",
      "Epoch: 4 batch 79 loss: 0.9292634 acc: 67.96875\n",
      "Epoch: 4 batch 80 loss: 0.986203 acc: 72.65625\n",
      "Epoch: 4 batch 81 loss: 1.293155 acc: 50.78125\n",
      "Epoch: 4 batch 82 loss: 0.89780647 acc: 75.78125\n",
      "Epoch: 4 batch 83 loss: 0.95705605 acc: 65.625\n",
      "Epoch: 4 batch 84 loss: 0.917868 acc: 71.875\n",
      "Epoch: 4 batch 85 loss: 1.105355 acc: 64.84375\n",
      "Epoch: 4 batch 86 loss: 1.0897777 acc: 58.59375\n",
      "Epoch: 4 batch 87 loss: 0.80358994 acc: 71.09375\n",
      "Epoch: 4 batch 88 loss: 0.88775957 acc: 67.96875\n",
      "Epoch: 4 batch 89 loss: 0.8884217 acc: 67.1875\n",
      "Epoch: 4 batch 90 loss: 0.8723568 acc: 71.09375\n",
      "Epoch: 4 batch 91 loss: 0.8047925 acc: 72.65625\n",
      "Epoch: 4 batch 92 loss: 0.8831994 acc: 74.21875\n",
      "Epoch: 4 batch 93 loss: 0.8276705 acc: 71.09375\n",
      "Epoch: 4 batch 94 loss: 1.0491154 acc: 63.28125\n",
      "Epoch: 4 batch 95 loss: 0.8902226 acc: 71.875\n",
      "Epoch: 4 batch 96 loss: 0.9310515 acc: 69.53125\n",
      "Epoch: 4 batch 97 loss: 0.8223115 acc: 71.875\n",
      "Epoch: 4 batch 98 loss: 0.896546 acc: 69.53125\n",
      "Epoch: 4 batch 99 loss: 0.9318627 acc: 67.96875\n",
      "Epoch: 4 batch 100 loss: 0.9096307 acc: 67.96875\n",
      "Epoch: 4 batch 101 loss: 1.0113635 acc: 60.9375\n",
      "Epoch: 4 batch 102 loss: 0.9128977 acc: 66.40625\n",
      "Epoch: 4 batch 103 loss: 1.1049167 acc: 66.40625\n",
      "Epoch: 4 batch 104 loss: 0.9597744 acc: 63.28125\n",
      "Epoch: 4 batch 105 loss: 0.8391144 acc: 75.0\n",
      "Epoch: 4 batch 106 loss: 0.86681044 acc: 71.875\n",
      "Epoch: 4 batch 107 loss: 1.0251882 acc: 63.28125\n",
      "Epoch: 4 batch 108 loss: 0.961084 acc: 69.53125\n",
      "Epoch: 4 batch 109 loss: 1.0056679 acc: 70.3125\n",
      "Epoch: 4 batch 110 loss: 0.89589924 acc: 63.28125\n",
      "Epoch: 4 batch 111 loss: 0.948001 acc: 63.28125\n",
      "Epoch: 4 batch 112 loss: 0.9181272 acc: 66.40625\n",
      "Epoch: 4 batch 113 loss: 0.77900183 acc: 75.78125\n",
      "Epoch: 4 batch 114 loss: 1.0201563 acc: 65.625\n",
      "Epoch: 4 batch 115 loss: 0.8400203 acc: 72.65625\n",
      "Epoch: 4 batch 116 loss: 0.9265492 acc: 64.84375\n",
      "Epoch: 4 batch 117 loss: 0.9514339 acc: 66.40625\n",
      "Epoch: 4 batch 118 loss: 0.87828195 acc: 66.40625\n",
      "Epoch: 4 batch 119 loss: 0.9049435 acc: 64.0625\n",
      "Epoch: 4 batch 120 loss: 0.9422232 acc: 68.75\n",
      "Epoch: 4 batch 121 loss: 0.97671676 acc: 65.625\n",
      "Epoch: 4 batch 122 loss: 0.9653357 acc: 69.53125\n",
      "Epoch: 4 batch 123 loss: 0.9159982 acc: 71.09375\n",
      "Epoch: 4 batch 124 loss: 1.0552346 acc: 66.40625\n",
      "Epoch: 4 batch 125 loss: 0.8575773 acc: 74.21875\n",
      "Epoch: 4 batch 126 loss: 0.98141617 acc: 62.5\n",
      "Epoch: 4 batch 127 loss: 0.83427036 acc: 67.96875\n",
      "Epoch: 4 batch 128 loss: 0.9010662 acc: 64.84375\n",
      "Epoch: 4 batch 129 loss: 1.0885892 acc: 62.5\n",
      "Epoch: 4 batch 130 loss: 0.8617748 acc: 66.40625\n",
      "Epoch: 4 batch 131 loss: 0.88431764 acc: 71.09375\n",
      "Epoch: 4 batch 132 loss: 0.77342004 acc: 71.875\n",
      "Epoch: 4 batch 133 loss: 0.92069817 acc: 64.0625\n",
      "Epoch: 4 batch 134 loss: 0.81170005 acc: 71.875\n",
      "Epoch: 4 batch 135 loss: 1.1151042 acc: 56.25\n",
      "Epoch: 4 batch 136 loss: 1.0079154 acc: 71.875\n",
      "Epoch: 4 batch 137 loss: 0.87822545 acc: 68.75\n",
      "Epoch: 4 batch 138 loss: 1.1187766 acc: 60.9375\n",
      "Epoch: 4 batch 139 loss: 0.74393713 acc: 73.4375\n",
      "Epoch: 4 batch 140 loss: 0.9365201 acc: 68.75\n",
      "Epoch: 4 batch 141 loss: 0.84084886 acc: 73.4375\n",
      "Epoch: 4 batch 142 loss: 0.98000216 acc: 64.0625\n",
      "Epoch: 4 batch 143 loss: 0.9806174 acc: 71.09375\n",
      "Epoch: 4 batch 144 loss: 1.0388409 acc: 63.28125\n",
      "Epoch: 4 batch 145 loss: 1.0024278 acc: 70.3125\n",
      "Epoch: 4 batch 146 loss: 0.98425025 acc: 63.28125\n",
      "Epoch: 4 batch 147 loss: 0.8244668 acc: 72.65625\n",
      "Epoch: 4 batch 148 loss: 0.84942997 acc: 68.75\n",
      "Epoch: 4 batch 149 loss: 0.9338008 acc: 71.875\n",
      "Epoch: 4 batch 150 loss: 0.93850785 acc: 66.40625\n",
      "Epoch: 4 batch 151 loss: 0.81039876 acc: 74.21875\n",
      "Epoch: 4 batch 152 loss: 0.971499 acc: 71.09375\n",
      "Epoch: 4 batch 153 loss: 0.96611905 acc: 67.1875\n",
      "Epoch: 4 batch 154 loss: 0.9494361 acc: 68.75\n",
      "Epoch: 4 batch 155 loss: 0.81564534 acc: 72.65625\n",
      "Epoch: 4 batch 156 loss: 0.9584156 acc: 67.96875\n",
      "Epoch: 4 batch 157 loss: 0.8926054 acc: 68.75\n",
      "Epoch: 4 batch 158 loss: 0.9275509 acc: 69.53125\n",
      "Epoch: 4 batch 159 loss: 0.8451338 acc: 71.875\n",
      "Epoch: 4 batch 160 loss: 0.9544043 acc: 66.40625\n",
      "Epoch: 4 batch 161 loss: 0.890951 acc: 69.53125\n",
      "Epoch: 4 batch 162 loss: 0.83521396 acc: 67.1875\n",
      "Epoch: 4 batch 163 loss: 0.8178773 acc: 69.53125\n",
      "Epoch: 4 batch 164 loss: 0.85402864 acc: 71.875\n",
      "Epoch: 4 batch 165 loss: 0.96686494 acc: 71.09375\n",
      "Epoch: 4 batch 166 loss: 0.8437388 acc: 71.09375\n",
      "Epoch: 4 batch 167 loss: 0.97176474 acc: 65.625\n",
      "Epoch: 4 batch 168 loss: 0.9205251 acc: 64.0625\n",
      "Epoch: 4 batch 169 loss: 0.974125 acc: 63.28125\n",
      "Epoch: 4 batch 170 loss: 0.9423548 acc: 67.1875\n",
      "Epoch: 4 batch 171 loss: 0.96382934 acc: 67.96875\n",
      "Epoch: 4 batch 172 loss: 0.9558035 acc: 66.40625\n",
      "Epoch: 4 batch 173 loss: 0.9194831 acc: 70.3125\n",
      "Epoch: 4 batch 174 loss: 1.0107703 acc: 67.1875\n",
      "Epoch: 4 batch 175 loss: 0.9847259 acc: 63.28125\n",
      "Epoch: 4 batch 176 loss: 0.7929625 acc: 74.21875\n",
      "Epoch: 4 batch 177 loss: 0.9889525 acc: 67.96875\n",
      "Epoch: 4 batch 178 loss: 0.9604192 acc: 65.625\n",
      "Epoch: 4 batch 179 loss: 0.988743 acc: 64.0625\n",
      "Epoch: 4 batch 180 loss: 0.84178686 acc: 69.53125\n",
      "Epoch: 4 batch 181 loss: 0.908048 acc: 69.53125\n",
      "Epoch: 4 batch 182 loss: 0.625756 acc: 79.6875\n",
      "Epoch: 4 batch 183 loss: 1.1514345 acc: 60.9375\n",
      "Epoch: 4 batch 184 loss: 0.87141126 acc: 67.1875\n",
      "Epoch: 4 batch 185 loss: 0.776456 acc: 75.78125\n",
      "Epoch: 4 batch 186 loss: 0.79500145 acc: 73.4375\n",
      "Epoch: 4 batch 187 loss: 0.80664593 acc: 68.75\n",
      "Epoch: 4 batch 188 loss: 0.81631875 acc: 72.65625\n",
      "Epoch: 4 batch 189 loss: 0.82744086 acc: 67.96875\n",
      "Epoch: 4 batch 190 loss: 0.9784316 acc: 66.40625\n",
      "Epoch: 4 batch 191 loss: 0.9182621 acc: 70.3125\n",
      "Epoch: 4 batch 192 loss: 0.81873417 acc: 71.09375\n",
      "Epoch: 4 batch 193 loss: 1.0719752 acc: 63.28125\n",
      "Epoch: 4 batch 194 loss: 0.88808954 acc: 69.53125\n",
      "Epoch: 4 batch 195 loss: 0.72430474 acc: 75.0\n",
      "Epoch: 4 batch 196 loss: 0.78333193 acc: 71.09375\n",
      "Epoch: 4 batch 197 loss: 0.8444488 acc: 67.1875\n",
      "Epoch: 4 batch 198 loss: 0.88409746 acc: 65.625\n",
      "Epoch: 4 batch 199 loss: 0.9047597 acc: 71.09375\n",
      "Epoch: 4 batch 200 loss: 0.9367791 acc: 67.96875\n",
      "Epoch: 4 batch 201 loss: 0.6883793 acc: 78.125\n",
      "Epoch: 4 batch 202 loss: 0.8760621 acc: 71.09375\n",
      "Epoch: 4 batch 203 loss: 0.68654823 acc: 73.4375\n",
      "Epoch: 4 batch 204 loss: 0.73554426 acc: 75.0\n",
      "Epoch: 4 batch 205 loss: 0.758219 acc: 77.34375\n",
      "Epoch: 4 batch 206 loss: 0.89432544 acc: 67.96875\n",
      "Epoch: 4 batch 207 loss: 0.9047147 acc: 64.84375\n",
      "Epoch: 4 batch 208 loss: 1.0123744 acc: 66.40625\n",
      "Epoch: 4 batch 209 loss: 0.88579714 acc: 70.3125\n",
      "Epoch: 4 batch 210 loss: 0.8617696 acc: 65.625\n",
      "Epoch: 4 batch 211 loss: 0.9782801 acc: 67.1875\n",
      "Epoch: 4 batch 212 loss: 0.9715638 acc: 65.625\n",
      "Epoch: 4 batch 213 loss: 0.803958 acc: 71.875\n",
      "Epoch: 4 batch 214 loss: 0.9660103 acc: 67.96875\n",
      "Epoch: 4 batch 215 loss: 0.71590054 acc: 77.34375\n",
      "Epoch: 4 batch 216 loss: 0.79596317 acc: 74.21875\n",
      "Epoch: 4 batch 217 loss: 1.034854 acc: 64.0625\n",
      "Epoch: 4 batch 218 loss: 0.91693825 acc: 68.75\n",
      "Epoch: 4 batch 219 loss: 0.8868654 acc: 71.09375\n",
      "Epoch: 4 batch 220 loss: 0.8686735 acc: 69.53125\n",
      "Epoch: 4 batch 221 loss: 0.9521346 acc: 68.75\n",
      "Epoch: 4 batch 222 loss: 0.73830485 acc: 72.65625\n",
      "Epoch: 4 batch 223 loss: 0.9762442 acc: 62.5\n",
      "Epoch: 4 batch 224 loss: 1.0090736 acc: 62.5\n",
      "Epoch: 4 batch 225 loss: 0.7965937 acc: 75.0\n",
      "Epoch: 4 batch 226 loss: 0.6895838 acc: 76.5625\n",
      "Epoch: 4 batch 227 loss: 1.0791051 acc: 70.3125\n",
      "Epoch: 4 batch 228 loss: 0.9005897 acc: 67.96875\n",
      "Epoch: 4 batch 229 loss: 0.6635554 acc: 72.65625\n",
      "Epoch: 4 batch 230 loss: 0.8867474 acc: 69.53125\n",
      "Epoch: 4 batch 231 loss: 0.87438095 acc: 67.96875\n",
      "Epoch: 4 batch 232 loss: 0.77494323 acc: 73.4375\n",
      "Epoch: 4 batch 233 loss: 0.8909173 acc: 71.875\n",
      "Epoch: 4 batch 234 loss: 0.8232082 acc: 71.875\n",
      "Epoch: 4 batch 235 loss: 1.018218 acc: 61.71875\n",
      "Epoch: 4 batch 236 loss: 0.75259143 acc: 76.5625\n",
      "Epoch: 4 batch 237 loss: 0.7110193 acc: 75.78125\n",
      "Epoch: 4 batch 238 loss: 0.88085103 acc: 73.4375\n",
      "Epoch: 4 batch 239 loss: 0.97209 acc: 69.53125\n",
      "Epoch: 4 batch 240 loss: 0.98586726 acc: 67.96875\n",
      "Epoch: 4 batch 241 loss: 0.89200026 acc: 74.21875\n",
      "Epoch: 4 batch 242 loss: 0.75412226 acc: 74.21875\n",
      "Epoch: 4 batch 243 loss: 0.8762748 acc: 71.09375\n",
      "Epoch: 4 batch 244 loss: 0.7431879 acc: 75.78125\n",
      "Epoch: 4 batch 245 loss: 0.8518994 acc: 72.65625\n",
      "Epoch: 4 batch 246 loss: 0.9660927 acc: 67.1875\n",
      "Epoch: 4 batch 247 loss: 0.8449795 acc: 70.3125\n",
      "Epoch: 4 batch 248 loss: 0.86675155 acc: 68.75\n",
      "Epoch: 4 batch 249 loss: 0.90749115 acc: 69.53125\n",
      "Epoch: 4 batch 250 loss: 0.86443514 acc: 70.3125\n",
      "Epoch: 4 batch 251 loss: 0.9714228 acc: 65.625\n",
      "Epoch: 4 batch 252 loss: 0.89648426 acc: 67.96875\n",
      "Epoch: 4 batch 253 loss: 0.9031932 acc: 64.84375\n",
      "Epoch: 4 batch 254 loss: 0.9498849 acc: 69.53125\n",
      "Epoch: 4 batch 255 loss: 0.9122057 acc: 64.84375\n",
      "Epoch: 4 batch 256 loss: 1.0345998 acc: 65.625\n",
      "Epoch: 4 batch 257 loss: 0.9434087 acc: 67.96875\n",
      "Epoch: 4 batch 258 loss: 0.88264096 acc: 71.09375\n",
      "Epoch: 4 batch 259 loss: 0.94080037 acc: 71.875\n",
      "Epoch: 4 batch 260 loss: 1.0218201 acc: 65.625\n",
      "Epoch: 4 batch 261 loss: 0.96440905 acc: 65.625\n",
      "Epoch: 4 batch 262 loss: 1.0924747 acc: 64.0625\n",
      "Epoch: 4 batch 263 loss: 0.7072437 acc: 77.34375\n",
      "Epoch: 4 batch 264 loss: 0.93236005 acc: 65.625\n",
      "Epoch: 4 batch 265 loss: 0.9751333 acc: 67.1875\n",
      "Epoch: 4 batch 266 loss: 0.88340485 acc: 70.3125\n",
      "Epoch: 4 batch 267 loss: 0.90214235 acc: 67.96875\n",
      "Epoch: 4 batch 268 loss: 1.0971571 acc: 59.375\n",
      "Epoch: 4 batch 269 loss: 0.84895635 acc: 69.53125\n",
      "Epoch: 4 batch 270 loss: 0.9177899 acc: 62.5\n",
      "Epoch: 4 batch 271 loss: 0.90072256 acc: 71.875\n",
      "Epoch: 4 batch 272 loss: 0.87286586 acc: 70.3125\n",
      "Epoch: 4 batch 273 loss: 0.9235363 acc: 66.40625\n",
      "Epoch: 4 batch 274 loss: 0.83400023 acc: 71.875\n",
      "Epoch: 4 batch 275 loss: 0.91585004 acc: 67.1875\n",
      "Epoch: 4 batch 276 loss: 0.7842742 acc: 73.4375\n",
      "Epoch: 4 batch 277 loss: 0.80950737 acc: 71.09375\n",
      "Epoch: 4 batch 278 loss: 1.1050961 acc: 63.28125\n",
      "Epoch: 4 batch 279 loss: 0.9931546 acc: 66.40625\n",
      "Epoch: 4 batch 280 loss: 0.8380304 acc: 71.875\n",
      "Epoch: 4 batch 281 loss: 0.84718907 acc: 67.96875\n",
      "Epoch: 4 batch 282 loss: 0.97889006 acc: 63.28125\n",
      "Epoch: 4 batch 283 loss: 0.84633505 acc: 74.21875\n",
      "Epoch: 4 batch 284 loss: 0.8226961 acc: 71.09375\n",
      "Epoch: 4 batch 285 loss: 0.7842017 acc: 73.4375\n",
      "Epoch: 4 batch 286 loss: 0.94534266 acc: 64.0625\n",
      "Epoch: 4 batch 287 loss: 0.79532695 acc: 69.53125\n",
      "Epoch: 4 batch 288 loss: 1.034996 acc: 64.0625\n",
      "Epoch: 4 batch 289 loss: 0.71462554 acc: 75.0\n",
      "Epoch: 4 batch 290 loss: 0.96601 acc: 66.40625\n",
      "Epoch: 4 batch 291 loss: 1.0575935 acc: 64.84375\n",
      "Epoch: 4 batch 292 loss: 0.93803173 acc: 67.1875\n",
      "Epoch: 4 batch 293 loss: 0.8015015 acc: 75.78125\n",
      "Epoch: 4 batch 294 loss: 0.88286763 acc: 70.3125\n",
      "Epoch: 4 batch 295 loss: 0.82046616 acc: 71.09375\n",
      "Epoch: 4 batch 296 loss: 0.97780347 acc: 60.15625\n",
      "Epoch: 4 batch 297 loss: 0.8744358 acc: 70.3125\n",
      "Epoch: 4 batch 298 loss: 0.9463925 acc: 68.75\n",
      "Epoch: 4 batch 299 loss: 1.0120035 acc: 62.5\n",
      "Epoch: 4 batch 300 loss: 0.80044115 acc: 73.4375\n",
      "Epoch: 4 batch 301 loss: 0.8312284 acc: 72.65625\n",
      "Epoch: 4 batch 302 loss: 0.8331224 acc: 71.09375\n",
      "Epoch: 4 batch 303 loss: 0.75746334 acc: 75.0\n",
      "Epoch: 4 batch 304 loss: 0.9691459 acc: 64.0625\n",
      "Epoch: 4 batch 305 loss: 0.7898674 acc: 70.3125\n",
      "Epoch: 4 batch 306 loss: 0.8873527 acc: 65.625\n",
      "Epoch: 4 batch 307 loss: 0.8517946 acc: 67.96875\n",
      "Epoch: 4 batch 308 loss: 0.8899184 acc: 70.3125\n",
      "Epoch: 4 batch 309 loss: 1.0276517 acc: 62.5\n",
      "Epoch: 4 batch 310 loss: 1.089124 acc: 63.28125\n",
      "Epoch: 4 batch 311 loss: 0.8994025 acc: 70.3125\n",
      "Epoch: 4 batch 312 loss: 0.79617244 acc: 74.21875\n",
      "Epoch: 4 batch 313 loss: 0.8933522 acc: 70.3125\n",
      "Epoch: 4 batch 314 loss: 0.8386895 acc: 67.1875\n",
      "Epoch: 4 batch 315 loss: 0.9700372 acc: 65.625\n",
      "Epoch: 4 batch 316 loss: 0.8438196 acc: 71.875\n",
      "Epoch: 4 batch 317 loss: 0.8223733 acc: 71.875\n",
      "Epoch: 4 batch 318 loss: 0.8549209 acc: 71.875\n",
      "Epoch: 4 batch 319 loss: 0.91089994 acc: 69.53125\n",
      "Epoch: 4 batch 320 loss: 0.8841876 acc: 64.0625\n",
      "Epoch: 4 batch 321 loss: 0.8060062 acc: 70.3125\n",
      "Epoch: 4 batch 322 loss: 0.9659333 acc: 67.1875\n",
      "Epoch: 4 batch 323 loss: 0.82024705 acc: 71.875\n",
      "Epoch: 4 batch 324 loss: 0.9881565 acc: 69.53125\n",
      "Epoch: 4 batch 325 loss: 0.86516047 acc: 72.65625\n",
      "Epoch: 4 batch 326 loss: 0.8099379 acc: 69.53125\n",
      "Epoch: 4 batch 327 loss: 0.90928227 acc: 67.1875\n",
      "Epoch: 4 batch 328 loss: 0.9205571 acc: 73.4375\n",
      "Epoch: 4 batch 329 loss: 0.84586024 acc: 74.21875\n",
      "Epoch: 4 batch 330 loss: 0.7615335 acc: 71.875\n",
      "Epoch: 4 batch 331 loss: 0.826014 acc: 72.65625\n",
      "Epoch: 4 batch 332 loss: 1.0662203 acc: 64.0625\n",
      "Epoch: 4 batch 333 loss: 0.7924981 acc: 69.53125\n",
      "Epoch: 4 batch 334 loss: 0.7766748 acc: 74.21875\n",
      "Epoch: 4 batch 335 loss: 0.99804866 acc: 65.625\n",
      "Epoch: 4 batch 336 loss: 0.7832578 acc: 71.875\n",
      "Epoch: 4 batch 337 loss: 0.93822885 acc: 64.84375\n",
      "Epoch: 4 batch 338 loss: 1.0196811 acc: 62.5\n",
      "Epoch: 4 batch 339 loss: 0.81804013 acc: 71.875\n",
      "Epoch: 4 batch 340 loss: 0.79729676 acc: 77.34375\n",
      "Epoch: 4 batch 341 loss: 0.81577003 acc: 67.96875\n",
      "Epoch: 4 batch 342 loss: 0.9960687 acc: 65.625\n",
      "Epoch: 4 batch 343 loss: 0.7962403 acc: 71.875\n",
      "Epoch: 4 batch 344 loss: 0.8502691 acc: 67.96875\n",
      "Epoch: 4 batch 345 loss: 0.875249 acc: 71.875\n",
      "Epoch: 4 batch 346 loss: 0.9225688 acc: 67.96875\n",
      "Epoch: 4 batch 347 loss: 0.8415792 acc: 69.53125\n",
      "Epoch: 4 batch 348 loss: 0.8231697 acc: 74.21875\n",
      "Epoch: 4 batch 349 loss: 0.7982018 acc: 67.96875\n",
      "Epoch: 4 batch 350 loss: 0.98310196 acc: 65.625\n",
      "Epoch: 4 batch 351 loss: 0.7878479 acc: 71.875\n",
      "Epoch: 4 batch 352 loss: 0.76154387 acc: 71.875\n",
      "Epoch: 4 batch 353 loss: 0.82702327 acc: 71.875\n",
      "Epoch: 4 batch 354 loss: 0.78533816 acc: 74.21875\n",
      "Epoch: 4 batch 355 loss: 0.81766903 acc: 68.75\n",
      "Epoch: 4 batch 356 loss: 0.9660737 acc: 67.96875\n",
      "Epoch: 4 batch 357 loss: 0.86955476 acc: 71.09375\n",
      "Epoch: 4 batch 358 loss: 0.95067734 acc: 72.65625\n",
      "Epoch: 4 batch 359 loss: 0.76606876 acc: 71.875\n",
      "Epoch: 4 batch 360 loss: 0.76033974 acc: 75.78125\n",
      "Epoch: 4 batch 361 loss: 1.0067015 acc: 64.84375\n",
      "Epoch: 4 batch 362 loss: 0.8693767 acc: 67.96875\n",
      "Epoch: 4 batch 363 loss: 1.0790915 acc: 60.15625\n",
      "Epoch: 4 batch 364 loss: 0.83138645 acc: 72.65625\n",
      "Epoch: 4 batch 365 loss: 0.7539221 acc: 78.125\n",
      "Epoch: 4 batch 366 loss: 0.8236278 acc: 74.21875\n",
      "Epoch: 4 batch 367 loss: 1.0433481 acc: 63.28125\n",
      "Epoch: 4 batch 368 loss: 0.8433921 acc: 71.09375\n",
      "Epoch: 4 batch 369 loss: 0.8013343 acc: 70.3125\n",
      "Epoch: 4 batch 370 loss: 0.8241639 acc: 73.4375\n",
      "Epoch: 4 batch 371 loss: 0.769578 acc: 71.875\n",
      "Epoch: 4 batch 372 loss: 0.87760854 acc: 69.53125\n",
      "Epoch: 4 batch 373 loss: 0.8217719 acc: 71.09375\n",
      "Epoch: 4 batch 374 loss: 0.8728046 acc: 70.3125\n",
      "Epoch: 4 batch 375 loss: 0.9609761 acc: 72.65625\n",
      "Epoch: 4 batch 376 loss: 0.90210116 acc: 69.53125\n",
      "Epoch: 4 batch 377 loss: 0.73918676 acc: 75.78125\n",
      "Epoch: 4 batch 378 loss: 0.71692294 acc: 74.21875\n",
      "Epoch: 4 batch 379 loss: 0.8869052 acc: 71.875\n",
      "Epoch: 4 batch 380 loss: 0.82047224 acc: 69.53125\n",
      "Epoch: 4 batch 381 loss: 0.8870137 acc: 67.96875\n",
      "Epoch: 4 batch 382 loss: 0.8533846 acc: 71.09375\n",
      "Epoch: 4 batch 383 loss: 0.8499967 acc: 67.96875\n",
      "Epoch: 4 batch 384 loss: 0.7530019 acc: 72.65625\n",
      "Epoch: 4 batch 385 loss: 0.9911178 acc: 64.0625\n",
      "Epoch: 4 batch 386 loss: 0.8924161 acc: 74.21875\n",
      "Epoch: 4 batch 387 loss: 0.8811184 acc: 72.65625\n",
      "Epoch: 4 batch 388 loss: 0.9447315 acc: 66.40625\n",
      "Epoch: 4 batch 389 loss: 0.75058305 acc: 72.65625\n",
      "After epoch 4 test loss 0.8579553608894348 test accuracy 0.697700023651123\n",
      "Epoch: 5 batch 0 loss: 0.74258184 acc: 69.80647444725037\n",
      "Epoch: 5 batch 1 loss: 0.7584402 acc: 73.4375\n",
      "Epoch: 5 batch 2 loss: 0.82823807 acc: 72.65625\n",
      "Epoch: 5 batch 3 loss: 0.8088218 acc: 77.34375\n",
      "Epoch: 5 batch 4 loss: 0.77585995 acc: 75.0\n",
      "Epoch: 5 batch 5 loss: 0.911226 acc: 72.65625\n",
      "Epoch: 5 batch 6 loss: 0.941119 acc: 66.40625\n",
      "Epoch: 5 batch 7 loss: 0.74177897 acc: 75.0\n",
      "Epoch: 5 batch 8 loss: 0.8196775 acc: 71.875\n",
      "Epoch: 5 batch 9 loss: 0.6507179 acc: 77.34375\n",
      "Epoch: 5 batch 10 loss: 0.8897162 acc: 73.4375\n",
      "Epoch: 5 batch 11 loss: 0.84855616 acc: 67.1875\n",
      "Epoch: 5 batch 12 loss: 0.92510253 acc: 71.875\n",
      "Epoch: 5 batch 13 loss: 0.76564753 acc: 73.4375\n",
      "Epoch: 5 batch 14 loss: 0.61197305 acc: 78.90625\n",
      "Epoch: 5 batch 15 loss: 0.9884969 acc: 63.28125\n",
      "Epoch: 5 batch 16 loss: 0.7999637 acc: 73.4375\n",
      "Epoch: 5 batch 17 loss: 0.8167888 acc: 68.75\n",
      "Epoch: 5 batch 18 loss: 0.70591223 acc: 68.75\n",
      "Epoch: 5 batch 19 loss: 0.8972326 acc: 64.0625\n",
      "Epoch: 5 batch 20 loss: 0.6419008 acc: 71.875\n",
      "Epoch: 5 batch 21 loss: 0.90051186 acc: 67.96875\n",
      "Epoch: 5 batch 22 loss: 0.80350345 acc: 70.3125\n",
      "Epoch: 5 batch 23 loss: 1.0942944 acc: 60.9375\n",
      "Epoch: 5 batch 24 loss: 0.9149486 acc: 69.53125\n",
      "Epoch: 5 batch 25 loss: 0.929569 acc: 67.1875\n",
      "Epoch: 5 batch 26 loss: 0.9547013 acc: 71.875\n",
      "Epoch: 5 batch 27 loss: 0.78953326 acc: 71.09375\n",
      "Epoch: 5 batch 28 loss: 0.6773003 acc: 75.0\n",
      "Epoch: 5 batch 29 loss: 0.7485274 acc: 75.0\n",
      "Epoch: 5 batch 30 loss: 0.8300717 acc: 74.21875\n",
      "Epoch: 5 batch 31 loss: 0.8408322 acc: 73.4375\n",
      "Epoch: 5 batch 32 loss: 0.92643374 acc: 64.84375\n",
      "Epoch: 5 batch 33 loss: 0.9382286 acc: 67.96875\n",
      "Epoch: 5 batch 34 loss: 0.95267814 acc: 67.1875\n",
      "Epoch: 5 batch 35 loss: 0.8736448 acc: 70.3125\n",
      "Epoch: 5 batch 36 loss: 0.7347693 acc: 75.0\n",
      "Epoch: 5 batch 37 loss: 0.8686328 acc: 70.3125\n",
      "Epoch: 5 batch 38 loss: 0.95580417 acc: 65.625\n",
      "Epoch: 5 batch 39 loss: 0.9529325 acc: 61.71875\n",
      "Epoch: 5 batch 40 loss: 0.9843496 acc: 66.40625\n",
      "Epoch: 5 batch 41 loss: 0.8589213 acc: 75.0\n",
      "Epoch: 5 batch 42 loss: 1.0027416 acc: 66.40625\n",
      "Epoch: 5 batch 43 loss: 0.88595307 acc: 71.875\n",
      "Epoch: 5 batch 44 loss: 0.9046779 acc: 69.53125\n",
      "Epoch: 5 batch 45 loss: 0.8039216 acc: 69.53125\n",
      "Epoch: 5 batch 46 loss: 0.73375607 acc: 72.65625\n",
      "Epoch: 5 batch 47 loss: 0.9633024 acc: 66.40625\n",
      "Epoch: 5 batch 48 loss: 0.6878724 acc: 74.21875\n",
      "Epoch: 5 batch 49 loss: 0.71956855 acc: 76.5625\n",
      "Epoch: 5 batch 50 loss: 0.73197496 acc: 75.0\n",
      "Epoch: 5 batch 51 loss: 0.83813864 acc: 71.875\n",
      "Epoch: 5 batch 52 loss: 0.83509195 acc: 73.4375\n",
      "Epoch: 5 batch 53 loss: 0.939209 acc: 68.75\n",
      "Epoch: 5 batch 54 loss: 0.80348563 acc: 70.3125\n",
      "Epoch: 5 batch 55 loss: 0.8812187 acc: 64.0625\n",
      "Epoch: 5 batch 56 loss: 0.8329462 acc: 71.09375\n",
      "Epoch: 5 batch 57 loss: 0.8879426 acc: 68.75\n",
      "Epoch: 5 batch 58 loss: 0.7163874 acc: 78.125\n",
      "Epoch: 5 batch 59 loss: 0.9223242 acc: 67.1875\n",
      "Epoch: 5 batch 60 loss: 0.7643782 acc: 68.75\n",
      "Epoch: 5 batch 61 loss: 0.8200518 acc: 74.21875\n",
      "Epoch: 5 batch 62 loss: 0.8817045 acc: 67.1875\n",
      "Epoch: 5 batch 63 loss: 0.94972706 acc: 66.40625\n",
      "Epoch: 5 batch 64 loss: 0.81048244 acc: 66.40625\n",
      "Epoch: 5 batch 65 loss: 0.8792025 acc: 74.21875\n",
      "Epoch: 5 batch 66 loss: 1.0480672 acc: 64.0625\n",
      "Epoch: 5 batch 67 loss: 0.7401229 acc: 74.21875\n",
      "Epoch: 5 batch 68 loss: 0.94855225 acc: 64.84375\n",
      "Epoch: 5 batch 69 loss: 0.86900896 acc: 75.0\n",
      "Epoch: 5 batch 70 loss: 0.92066467 acc: 67.96875\n",
      "Epoch: 5 batch 71 loss: 0.77881676 acc: 73.4375\n",
      "Epoch: 5 batch 72 loss: 0.78411835 acc: 67.1875\n",
      "Epoch: 5 batch 73 loss: 0.8593128 acc: 71.09375\n",
      "Epoch: 5 batch 74 loss: 0.7166877 acc: 81.25\n",
      "Epoch: 5 batch 75 loss: 0.79901934 acc: 75.0\n",
      "Epoch: 5 batch 76 loss: 0.8032478 acc: 67.1875\n",
      "Epoch: 5 batch 77 loss: 0.7648243 acc: 68.75\n",
      "Epoch: 5 batch 78 loss: 0.7427739 acc: 73.4375\n",
      "Epoch: 5 batch 79 loss: 0.8603735 acc: 72.65625\n",
      "Epoch: 5 batch 80 loss: 0.84990084 acc: 71.875\n",
      "Epoch: 5 batch 81 loss: 0.9563781 acc: 65.625\n",
      "Epoch: 5 batch 82 loss: 0.73670745 acc: 72.65625\n",
      "Epoch: 5 batch 83 loss: 0.9098422 acc: 69.53125\n",
      "Epoch: 5 batch 84 loss: 0.834028 acc: 69.53125\n",
      "Epoch: 5 batch 85 loss: 0.99064964 acc: 62.5\n",
      "Epoch: 5 batch 86 loss: 0.91876507 acc: 65.625\n",
      "Epoch: 5 batch 87 loss: 0.81020737 acc: 69.53125\n",
      "Epoch: 5 batch 88 loss: 0.76632726 acc: 71.875\n",
      "Epoch: 5 batch 89 loss: 0.71488005 acc: 73.4375\n",
      "Epoch: 5 batch 90 loss: 0.76580316 acc: 68.75\n",
      "Epoch: 5 batch 91 loss: 0.7214085 acc: 73.4375\n",
      "Epoch: 5 batch 92 loss: 0.90186584 acc: 67.1875\n",
      "Epoch: 5 batch 93 loss: 0.76940536 acc: 73.4375\n",
      "Epoch: 5 batch 94 loss: 0.90860796 acc: 68.75\n",
      "Epoch: 5 batch 95 loss: 0.8588309 acc: 74.21875\n",
      "Epoch: 5 batch 96 loss: 0.80040765 acc: 70.3125\n",
      "Epoch: 5 batch 97 loss: 0.9310168 acc: 68.75\n",
      "Epoch: 5 batch 98 loss: 0.842454 acc: 72.65625\n",
      "Epoch: 5 batch 99 loss: 0.9367659 acc: 73.4375\n",
      "Epoch: 5 batch 100 loss: 0.8370344 acc: 69.53125\n",
      "Epoch: 5 batch 101 loss: 0.9023173 acc: 64.0625\n",
      "Epoch: 5 batch 102 loss: 0.8769168 acc: 69.53125\n",
      "Epoch: 5 batch 103 loss: 1.0015606 acc: 67.96875\n",
      "Epoch: 5 batch 104 loss: 0.9023925 acc: 68.75\n",
      "Epoch: 5 batch 105 loss: 0.76360255 acc: 78.125\n",
      "Epoch: 5 batch 106 loss: 0.8747654 acc: 71.875\n",
      "Epoch: 5 batch 107 loss: 0.8968012 acc: 70.3125\n",
      "Epoch: 5 batch 108 loss: 0.86823446 acc: 72.65625\n",
      "Epoch: 5 batch 109 loss: 0.7124163 acc: 75.78125\n",
      "Epoch: 5 batch 110 loss: 0.88289624 acc: 71.09375\n",
      "Epoch: 5 batch 111 loss: 0.84948283 acc: 75.0\n",
      "Epoch: 5 batch 112 loss: 0.9031297 acc: 66.40625\n",
      "Epoch: 5 batch 113 loss: 0.7993616 acc: 68.75\n",
      "Epoch: 5 batch 114 loss: 0.96345925 acc: 67.1875\n",
      "Epoch: 5 batch 115 loss: 0.7422423 acc: 68.75\n",
      "Epoch: 5 batch 116 loss: 0.85182875 acc: 74.21875\n",
      "Epoch: 5 batch 117 loss: 0.70888335 acc: 76.5625\n",
      "Epoch: 5 batch 118 loss: 0.86242074 acc: 66.40625\n",
      "Epoch: 5 batch 119 loss: 0.70858717 acc: 72.65625\n",
      "Epoch: 5 batch 120 loss: 0.70717853 acc: 76.5625\n",
      "Epoch: 5 batch 121 loss: 0.8872897 acc: 69.53125\n",
      "Epoch: 5 batch 122 loss: 0.72543967 acc: 71.09375\n",
      "Epoch: 5 batch 123 loss: 0.7506002 acc: 75.0\n",
      "Epoch: 5 batch 124 loss: 0.9285818 acc: 69.53125\n",
      "Epoch: 5 batch 125 loss: 0.7218156 acc: 77.34375\n",
      "Epoch: 5 batch 126 loss: 0.91090345 acc: 72.65625\n",
      "Epoch: 5 batch 127 loss: 0.7615522 acc: 74.21875\n",
      "Epoch: 5 batch 128 loss: 0.8028955 acc: 74.21875\n",
      "Epoch: 5 batch 129 loss: 0.8928977 acc: 68.75\n",
      "Epoch: 5 batch 130 loss: 0.83559453 acc: 73.4375\n",
      "Epoch: 5 batch 131 loss: 0.9504429 acc: 66.40625\n",
      "Epoch: 5 batch 132 loss: 0.6937173 acc: 76.5625\n",
      "Epoch: 5 batch 133 loss: 0.7553248 acc: 75.78125\n",
      "Epoch: 5 batch 134 loss: 0.5797106 acc: 80.46875\n",
      "Epoch: 5 batch 135 loss: 0.837454 acc: 67.96875\n",
      "Epoch: 5 batch 136 loss: 0.72975993 acc: 73.4375\n",
      "Epoch: 5 batch 137 loss: 0.77045417 acc: 75.0\n",
      "Epoch: 5 batch 138 loss: 1.1738366 acc: 61.71875\n",
      "Epoch: 5 batch 139 loss: 0.7883296 acc: 72.65625\n",
      "Epoch: 5 batch 140 loss: 0.8037095 acc: 77.34375\n",
      "Epoch: 5 batch 141 loss: 0.860215 acc: 73.4375\n",
      "Epoch: 5 batch 142 loss: 0.9375774 acc: 67.96875\n",
      "Epoch: 5 batch 143 loss: 0.7548294 acc: 76.5625\n",
      "Epoch: 5 batch 144 loss: 0.97770876 acc: 68.75\n",
      "Epoch: 5 batch 145 loss: 0.8474964 acc: 70.3125\n",
      "Epoch: 5 batch 146 loss: 0.8828091 acc: 68.75\n",
      "Epoch: 5 batch 147 loss: 0.7120803 acc: 75.78125\n",
      "Epoch: 5 batch 148 loss: 0.86791396 acc: 69.53125\n",
      "Epoch: 5 batch 149 loss: 0.8224938 acc: 68.75\n",
      "Epoch: 5 batch 150 loss: 0.85629165 acc: 74.21875\n",
      "Epoch: 5 batch 151 loss: 0.71852183 acc: 76.5625\n",
      "Epoch: 5 batch 152 loss: 0.7601926 acc: 75.78125\n",
      "Epoch: 5 batch 153 loss: 0.88782877 acc: 67.96875\n",
      "Epoch: 5 batch 154 loss: 0.9281547 acc: 67.1875\n",
      "Epoch: 5 batch 155 loss: 0.77722293 acc: 76.5625\n",
      "Epoch: 5 batch 156 loss: 0.787903 acc: 69.53125\n",
      "Epoch: 5 batch 157 loss: 0.8906851 acc: 72.65625\n",
      "Epoch: 5 batch 158 loss: 0.8564562 acc: 71.09375\n",
      "Epoch: 5 batch 159 loss: 0.7394315 acc: 74.21875\n",
      "Epoch: 5 batch 160 loss: 0.8527132 acc: 69.53125\n",
      "Epoch: 5 batch 161 loss: 0.8038559 acc: 70.3125\n",
      "Epoch: 5 batch 162 loss: 0.8368059 acc: 71.09375\n",
      "Epoch: 5 batch 163 loss: 0.73765606 acc: 75.0\n",
      "Epoch: 5 batch 164 loss: 0.81467366 acc: 71.875\n",
      "Epoch: 5 batch 165 loss: 0.90733206 acc: 64.0625\n",
      "Epoch: 5 batch 166 loss: 0.795028 acc: 67.1875\n",
      "Epoch: 5 batch 167 loss: 0.8432572 acc: 70.3125\n",
      "Epoch: 5 batch 168 loss: 0.8119811 acc: 72.65625\n",
      "Epoch: 5 batch 169 loss: 0.8556869 acc: 71.875\n",
      "Epoch: 5 batch 170 loss: 0.7796412 acc: 75.0\n",
      "Epoch: 5 batch 171 loss: 0.92913353 acc: 67.96875\n",
      "Epoch: 5 batch 172 loss: 0.8948118 acc: 68.75\n",
      "Epoch: 5 batch 173 loss: 0.7960577 acc: 71.09375\n",
      "Epoch: 5 batch 174 loss: 1.0014989 acc: 66.40625\n",
      "Epoch: 5 batch 175 loss: 0.9566679 acc: 67.1875\n",
      "Epoch: 5 batch 176 loss: 0.7816603 acc: 73.4375\n",
      "Epoch: 5 batch 177 loss: 0.9561393 acc: 67.96875\n",
      "Epoch: 5 batch 178 loss: 0.8861972 acc: 68.75\n",
      "Epoch: 5 batch 179 loss: 0.9252206 acc: 67.1875\n",
      "Epoch: 5 batch 180 loss: 0.69532776 acc: 75.78125\n",
      "Epoch: 5 batch 181 loss: 0.7961479 acc: 68.75\n",
      "Epoch: 5 batch 182 loss: 0.6084668 acc: 77.34375\n",
      "Epoch: 5 batch 183 loss: 0.92186105 acc: 67.1875\n",
      "Epoch: 5 batch 184 loss: 0.7206389 acc: 74.21875\n",
      "Epoch: 5 batch 185 loss: 0.71340805 acc: 78.90625\n",
      "Epoch: 5 batch 186 loss: 0.75104475 acc: 74.21875\n",
      "Epoch: 5 batch 187 loss: 0.73264074 acc: 74.21875\n",
      "Epoch: 5 batch 188 loss: 0.8009172 acc: 75.0\n",
      "Epoch: 5 batch 189 loss: 0.7584926 acc: 75.0\n",
      "Epoch: 5 batch 190 loss: 0.89644617 acc: 71.09375\n",
      "Epoch: 5 batch 191 loss: 0.7436179 acc: 74.21875\n",
      "Epoch: 5 batch 192 loss: 0.752614 acc: 75.0\n",
      "Epoch: 5 batch 193 loss: 1.009585 acc: 64.84375\n",
      "Epoch: 5 batch 194 loss: 0.82381827 acc: 71.09375\n",
      "Epoch: 5 batch 195 loss: 0.73701084 acc: 75.78125\n",
      "Epoch: 5 batch 196 loss: 0.7764592 acc: 67.1875\n",
      "Epoch: 5 batch 197 loss: 0.89876497 acc: 66.40625\n",
      "Epoch: 5 batch 198 loss: 0.75985706 acc: 74.21875\n",
      "Epoch: 5 batch 199 loss: 0.89240086 acc: 75.0\n",
      "Epoch: 5 batch 200 loss: 0.92651737 acc: 67.1875\n",
      "Epoch: 5 batch 201 loss: 0.6979538 acc: 73.4375\n",
      "Epoch: 5 batch 202 loss: 0.7218895 acc: 78.125\n",
      "Epoch: 5 batch 203 loss: 0.6502992 acc: 75.0\n",
      "Epoch: 5 batch 204 loss: 0.82364863 acc: 70.3125\n",
      "Epoch: 5 batch 205 loss: 0.69780105 acc: 75.78125\n",
      "Epoch: 5 batch 206 loss: 0.71041435 acc: 74.21875\n",
      "Epoch: 5 batch 207 loss: 0.7150504 acc: 68.75\n",
      "Epoch: 5 batch 208 loss: 0.8810872 acc: 71.875\n",
      "Epoch: 5 batch 209 loss: 0.84232336 acc: 72.65625\n",
      "Epoch: 5 batch 210 loss: 0.79565954 acc: 72.65625\n",
      "Epoch: 5 batch 211 loss: 0.959145 acc: 64.0625\n",
      "Epoch: 5 batch 212 loss: 0.85233426 acc: 67.96875\n",
      "Epoch: 5 batch 213 loss: 0.78598845 acc: 70.3125\n",
      "Epoch: 5 batch 214 loss: 0.78706235 acc: 71.09375\n",
      "Epoch: 5 batch 215 loss: 0.7530812 acc: 75.0\n",
      "Epoch: 5 batch 216 loss: 0.7485267 acc: 71.09375\n",
      "Epoch: 5 batch 217 loss: 0.7765521 acc: 71.09375\n",
      "Epoch: 5 batch 218 loss: 0.6945813 acc: 76.5625\n",
      "Epoch: 5 batch 219 loss: 0.7794528 acc: 74.21875\n",
      "Epoch: 5 batch 220 loss: 0.7487713 acc: 71.875\n",
      "Epoch: 5 batch 221 loss: 0.7669005 acc: 74.21875\n",
      "Epoch: 5 batch 222 loss: 0.86084193 acc: 72.65625\n",
      "Epoch: 5 batch 223 loss: 0.91862404 acc: 67.1875\n",
      "Epoch: 5 batch 224 loss: 0.77645624 acc: 67.96875\n",
      "Epoch: 5 batch 225 loss: 0.70887566 acc: 72.65625\n",
      "Epoch: 5 batch 226 loss: 0.68255496 acc: 76.5625\n",
      "Epoch: 5 batch 227 loss: 1.1459746 acc: 66.40625\n",
      "Epoch: 5 batch 228 loss: 0.81982666 acc: 73.4375\n",
      "Epoch: 5 batch 229 loss: 0.6835535 acc: 71.875\n",
      "Epoch: 5 batch 230 loss: 0.77412415 acc: 72.65625\n",
      "Epoch: 5 batch 231 loss: 0.736948 acc: 71.875\n",
      "Epoch: 5 batch 232 loss: 0.7864707 acc: 77.34375\n",
      "Epoch: 5 batch 233 loss: 0.6777047 acc: 73.4375\n",
      "Epoch: 5 batch 234 loss: 0.75081116 acc: 73.4375\n",
      "Epoch: 5 batch 235 loss: 0.8092003 acc: 70.3125\n",
      "Epoch: 5 batch 236 loss: 0.715279 acc: 70.3125\n",
      "Epoch: 5 batch 237 loss: 0.7278478 acc: 73.4375\n",
      "Epoch: 5 batch 238 loss: 0.9191234 acc: 73.4375\n",
      "Epoch: 5 batch 239 loss: 0.80004287 acc: 75.0\n",
      "Epoch: 5 batch 240 loss: 0.9877337 acc: 65.625\n",
      "Epoch: 5 batch 241 loss: 0.8620688 acc: 75.0\n",
      "Epoch: 5 batch 242 loss: 0.7107073 acc: 75.0\n",
      "Epoch: 5 batch 243 loss: 0.8065568 acc: 71.875\n",
      "Epoch: 5 batch 244 loss: 0.68171924 acc: 75.0\n",
      "Epoch: 5 batch 245 loss: 0.8927677 acc: 69.53125\n",
      "Epoch: 5 batch 246 loss: 0.8551915 acc: 73.4375\n",
      "Epoch: 5 batch 247 loss: 0.8172697 acc: 73.4375\n",
      "Epoch: 5 batch 248 loss: 0.6454384 acc: 76.5625\n",
      "Epoch: 5 batch 249 loss: 0.8256399 acc: 76.5625\n",
      "Epoch: 5 batch 250 loss: 0.94996727 acc: 67.1875\n",
      "Epoch: 5 batch 251 loss: 0.90547943 acc: 68.75\n",
      "Epoch: 5 batch 252 loss: 0.7237704 acc: 78.125\n",
      "Epoch: 5 batch 253 loss: 0.8210074 acc: 74.21875\n",
      "Epoch: 5 batch 254 loss: 0.91964906 acc: 71.09375\n",
      "Epoch: 5 batch 255 loss: 0.8901567 acc: 71.09375\n",
      "Epoch: 5 batch 256 loss: 0.9264087 acc: 66.40625\n",
      "Epoch: 5 batch 257 loss: 0.86965847 acc: 67.96875\n",
      "Epoch: 5 batch 258 loss: 0.8824892 acc: 68.75\n",
      "Epoch: 5 batch 259 loss: 0.7194777 acc: 71.09375\n",
      "Epoch: 5 batch 260 loss: 0.94010466 acc: 69.53125\n",
      "Epoch: 5 batch 261 loss: 0.90688664 acc: 71.09375\n",
      "Epoch: 5 batch 262 loss: 1.0904934 acc: 57.03125\n",
      "Epoch: 5 batch 263 loss: 0.7495172 acc: 74.21875\n",
      "Epoch: 5 batch 264 loss: 0.775798 acc: 72.65625\n",
      "Epoch: 5 batch 265 loss: 0.92854637 acc: 67.1875\n",
      "Epoch: 5 batch 266 loss: 0.79892004 acc: 74.21875\n",
      "Epoch: 5 batch 267 loss: 0.69089997 acc: 78.125\n",
      "Epoch: 5 batch 268 loss: 0.83270127 acc: 67.1875\n",
      "Epoch: 5 batch 269 loss: 0.8031976 acc: 74.21875\n",
      "Epoch: 5 batch 270 loss: 0.83864176 acc: 67.1875\n",
      "Epoch: 5 batch 271 loss: 0.8079581 acc: 71.875\n",
      "Epoch: 5 batch 272 loss: 0.7540006 acc: 75.0\n",
      "Epoch: 5 batch 273 loss: 0.93353856 acc: 66.40625\n",
      "Epoch: 5 batch 274 loss: 0.8323133 acc: 75.78125\n",
      "Epoch: 5 batch 275 loss: 0.836298 acc: 69.53125\n",
      "Epoch: 5 batch 276 loss: 0.7116504 acc: 75.0\n",
      "Epoch: 5 batch 277 loss: 0.7950427 acc: 70.3125\n",
      "Epoch: 5 batch 278 loss: 0.8679494 acc: 71.09375\n",
      "Epoch: 5 batch 279 loss: 0.8512305 acc: 71.875\n",
      "Epoch: 5 batch 280 loss: 0.69756734 acc: 76.5625\n",
      "Epoch: 5 batch 281 loss: 0.8406712 acc: 69.53125\n",
      "Epoch: 5 batch 282 loss: 0.83922416 acc: 72.65625\n",
      "Epoch: 5 batch 283 loss: 0.8259183 acc: 71.09375\n",
      "Epoch: 5 batch 284 loss: 0.6999219 acc: 75.0\n",
      "Epoch: 5 batch 285 loss: 0.65982264 acc: 76.5625\n",
      "Epoch: 5 batch 286 loss: 0.86045015 acc: 74.21875\n",
      "Epoch: 5 batch 287 loss: 0.6766429 acc: 75.0\n",
      "Epoch: 5 batch 288 loss: 0.929129 acc: 65.625\n",
      "Epoch: 5 batch 289 loss: 0.731727 acc: 77.34375\n",
      "Epoch: 5 batch 290 loss: 0.8845397 acc: 67.96875\n",
      "Epoch: 5 batch 291 loss: 0.9931718 acc: 65.625\n",
      "Epoch: 5 batch 292 loss: 0.81385577 acc: 75.0\n",
      "Epoch: 5 batch 293 loss: 0.7757555 acc: 74.21875\n",
      "Epoch: 5 batch 294 loss: 0.77337956 acc: 73.4375\n",
      "Epoch: 5 batch 295 loss: 0.76039267 acc: 67.1875\n",
      "Epoch: 5 batch 296 loss: 0.76111376 acc: 75.0\n",
      "Epoch: 5 batch 297 loss: 0.8613608 acc: 68.75\n",
      "Epoch: 5 batch 298 loss: 0.92469347 acc: 72.65625\n",
      "Epoch: 5 batch 299 loss: 0.7628627 acc: 70.3125\n",
      "Epoch: 5 batch 300 loss: 0.82811177 acc: 75.78125\n",
      "Epoch: 5 batch 301 loss: 0.72662216 acc: 74.21875\n",
      "Epoch: 5 batch 302 loss: 0.6394377 acc: 80.46875\n",
      "Epoch: 5 batch 303 loss: 0.6402184 acc: 78.125\n",
      "Epoch: 5 batch 304 loss: 0.8444141 acc: 64.0625\n",
      "Epoch: 5 batch 305 loss: 0.8297038 acc: 67.96875\n",
      "Epoch: 5 batch 306 loss: 0.8140055 acc: 69.53125\n",
      "Epoch: 5 batch 307 loss: 0.799808 acc: 74.21875\n",
      "Epoch: 5 batch 308 loss: 0.793203 acc: 74.21875\n",
      "Epoch: 5 batch 309 loss: 1.0147033 acc: 67.1875\n",
      "Epoch: 5 batch 310 loss: 0.9685412 acc: 61.71875\n",
      "Epoch: 5 batch 311 loss: 0.8978155 acc: 70.3125\n",
      "Epoch: 5 batch 312 loss: 0.71195614 acc: 75.78125\n",
      "Epoch: 5 batch 313 loss: 0.7902459 acc: 71.09375\n",
      "Epoch: 5 batch 314 loss: 0.8086761 acc: 71.09375\n",
      "Epoch: 5 batch 315 loss: 0.8440398 acc: 71.875\n",
      "Epoch: 5 batch 316 loss: 0.8010751 acc: 73.4375\n",
      "Epoch: 5 batch 317 loss: 0.6888211 acc: 74.21875\n",
      "Epoch: 5 batch 318 loss: 0.69441056 acc: 74.21875\n",
      "Epoch: 5 batch 319 loss: 0.84911734 acc: 65.625\n",
      "Epoch: 5 batch 320 loss: 0.8535313 acc: 73.4375\n",
      "Epoch: 5 batch 321 loss: 0.69389474 acc: 74.21875\n",
      "Epoch: 5 batch 322 loss: 0.8200147 acc: 69.53125\n",
      "Epoch: 5 batch 323 loss: 0.7477908 acc: 71.09375\n",
      "Epoch: 5 batch 324 loss: 0.77788 acc: 75.0\n",
      "Epoch: 5 batch 325 loss: 0.80047566 acc: 71.875\n",
      "Epoch: 5 batch 326 loss: 0.77202 acc: 70.3125\n",
      "Epoch: 5 batch 327 loss: 0.8586717 acc: 71.875\n",
      "Epoch: 5 batch 328 loss: 0.7947174 acc: 68.75\n",
      "Epoch: 5 batch 329 loss: 0.8514956 acc: 69.53125\n",
      "Epoch: 5 batch 330 loss: 0.6471706 acc: 78.90625\n",
      "Epoch: 5 batch 331 loss: 0.8050102 acc: 76.5625\n",
      "Epoch: 5 batch 332 loss: 1.0067202 acc: 61.71875\n",
      "Epoch: 5 batch 333 loss: 0.749185 acc: 70.3125\n",
      "Epoch: 5 batch 334 loss: 0.700232 acc: 73.4375\n",
      "Epoch: 5 batch 335 loss: 0.9239974 acc: 64.84375\n",
      "Epoch: 5 batch 336 loss: 0.72447973 acc: 73.4375\n",
      "Epoch: 5 batch 337 loss: 0.8567012 acc: 67.1875\n",
      "Epoch: 5 batch 338 loss: 0.92597437 acc: 64.0625\n",
      "Epoch: 5 batch 339 loss: 0.7672896 acc: 70.3125\n",
      "Epoch: 5 batch 340 loss: 0.74013716 acc: 70.3125\n",
      "Epoch: 5 batch 341 loss: 0.751501 acc: 73.4375\n",
      "Epoch: 5 batch 342 loss: 0.8770225 acc: 68.75\n",
      "Epoch: 5 batch 343 loss: 0.7010844 acc: 76.5625\n",
      "Epoch: 5 batch 344 loss: 0.6924278 acc: 75.78125\n",
      "Epoch: 5 batch 345 loss: 0.9225211 acc: 68.75\n",
      "Epoch: 5 batch 346 loss: 0.8035862 acc: 71.875\n",
      "Epoch: 5 batch 347 loss: 0.8712839 acc: 74.21875\n",
      "Epoch: 5 batch 348 loss: 0.79266334 acc: 78.125\n",
      "Epoch: 5 batch 349 loss: 0.7429228 acc: 72.65625\n",
      "Epoch: 5 batch 350 loss: 0.8646766 acc: 71.09375\n",
      "Epoch: 5 batch 351 loss: 0.71873075 acc: 72.65625\n",
      "Epoch: 5 batch 352 loss: 0.60821974 acc: 82.03125\n",
      "Epoch: 5 batch 353 loss: 0.8084589 acc: 71.875\n",
      "Epoch: 5 batch 354 loss: 0.65693456 acc: 77.34375\n",
      "Epoch: 5 batch 355 loss: 0.7368094 acc: 69.53125\n",
      "Epoch: 5 batch 356 loss: 0.91010267 acc: 67.96875\n",
      "Epoch: 5 batch 357 loss: 0.67147565 acc: 78.125\n",
      "Epoch: 5 batch 358 loss: 0.78817904 acc: 72.65625\n",
      "Epoch: 5 batch 359 loss: 0.6785861 acc: 75.78125\n",
      "Epoch: 5 batch 360 loss: 0.66632485 acc: 75.0\n",
      "Epoch: 5 batch 361 loss: 0.83342284 acc: 70.3125\n",
      "Epoch: 5 batch 362 loss: 0.75763875 acc: 71.09375\n",
      "Epoch: 5 batch 363 loss: 0.9457045 acc: 70.3125\n",
      "Epoch: 5 batch 364 loss: 0.7672008 acc: 72.65625\n",
      "Epoch: 5 batch 365 loss: 0.6921399 acc: 75.78125\n",
      "Epoch: 5 batch 366 loss: 0.75661373 acc: 74.21875\n",
      "Epoch: 5 batch 367 loss: 1.0154543 acc: 64.84375\n",
      "Epoch: 5 batch 368 loss: 0.7569596 acc: 79.6875\n",
      "Epoch: 5 batch 369 loss: 0.71453273 acc: 74.21875\n",
      "Epoch: 5 batch 370 loss: 0.8075279 acc: 71.09375\n",
      "Epoch: 5 batch 371 loss: 0.67137027 acc: 73.4375\n",
      "Epoch: 5 batch 372 loss: 0.76435244 acc: 70.3125\n",
      "Epoch: 5 batch 373 loss: 0.7357352 acc: 71.875\n",
      "Epoch: 5 batch 374 loss: 0.7959851 acc: 68.75\n",
      "Epoch: 5 batch 375 loss: 0.9844103 acc: 66.40625\n",
      "Epoch: 5 batch 376 loss: 0.8300959 acc: 78.90625\n",
      "Epoch: 5 batch 377 loss: 0.78946745 acc: 73.4375\n",
      "Epoch: 5 batch 378 loss: 0.60110426 acc: 78.90625\n",
      "Epoch: 5 batch 379 loss: 0.7201694 acc: 72.65625\n",
      "Epoch: 5 batch 380 loss: 0.79881966 acc: 74.21875\n",
      "Epoch: 5 batch 381 loss: 0.73941326 acc: 77.34375\n",
      "Epoch: 5 batch 382 loss: 0.82800466 acc: 73.4375\n",
      "Epoch: 5 batch 383 loss: 0.80360204 acc: 75.78125\n",
      "Epoch: 5 batch 384 loss: 0.73291963 acc: 76.5625\n",
      "Epoch: 5 batch 385 loss: 0.8132484 acc: 71.875\n",
      "Epoch: 5 batch 386 loss: 0.73375654 acc: 78.90625\n",
      "Epoch: 5 batch 387 loss: 0.7522216 acc: 73.4375\n",
      "Epoch: 5 batch 388 loss: 0.7855954 acc: 70.3125\n",
      "Epoch: 5 batch 389 loss: 0.64346784 acc: 77.34375\n",
      "After epoch 5 test loss 0.854047390460968 test accuracy 0.7045000195503235\n",
      "Epoch: 6 batch 0 loss: 0.6740547 acc: 70.51737904548645\n",
      "Epoch: 6 batch 1 loss: 0.70360214 acc: 78.90625\n",
      "Epoch: 6 batch 2 loss: 0.80539274 acc: 76.5625\n",
      "Epoch: 6 batch 3 loss: 0.75874054 acc: 71.875\n",
      "Epoch: 6 batch 4 loss: 0.69823396 acc: 77.34375\n",
      "Epoch: 6 batch 5 loss: 0.8992459 acc: 73.4375\n",
      "Epoch: 6 batch 6 loss: 0.8891454 acc: 68.75\n",
      "Epoch: 6 batch 7 loss: 0.6327409 acc: 75.0\n",
      "Epoch: 6 batch 8 loss: 0.8279257 acc: 67.1875\n",
      "Epoch: 6 batch 9 loss: 0.63772815 acc: 78.125\n",
      "Epoch: 6 batch 10 loss: 0.8067361 acc: 70.3125\n",
      "Epoch: 6 batch 11 loss: 0.7513058 acc: 75.0\n",
      "Epoch: 6 batch 12 loss: 0.9132354 acc: 69.53125\n",
      "Epoch: 6 batch 13 loss: 0.71878755 acc: 71.875\n",
      "Epoch: 6 batch 14 loss: 0.63455653 acc: 80.46875\n",
      "Epoch: 6 batch 15 loss: 0.9729171 acc: 65.625\n",
      "Epoch: 6 batch 16 loss: 0.6777298 acc: 77.34375\n",
      "Epoch: 6 batch 17 loss: 0.6867157 acc: 73.4375\n",
      "Epoch: 6 batch 18 loss: 0.69850475 acc: 75.78125\n",
      "Epoch: 6 batch 19 loss: 0.71515405 acc: 76.5625\n",
      "Epoch: 6 batch 20 loss: 0.52213734 acc: 82.03125\n",
      "Epoch: 6 batch 21 loss: 0.72658587 acc: 74.21875\n",
      "Epoch: 6 batch 22 loss: 0.7163346 acc: 75.78125\n",
      "Epoch: 6 batch 23 loss: 0.8392825 acc: 69.53125\n",
      "Epoch: 6 batch 24 loss: 0.77866995 acc: 67.96875\n",
      "Epoch: 6 batch 25 loss: 0.7392677 acc: 73.4375\n",
      "Epoch: 6 batch 26 loss: 0.8523526 acc: 65.625\n",
      "Epoch: 6 batch 27 loss: 0.7293948 acc: 72.65625\n",
      "Epoch: 6 batch 28 loss: 0.65778154 acc: 81.25\n",
      "Epoch: 6 batch 29 loss: 0.83663964 acc: 69.53125\n",
      "Epoch: 6 batch 30 loss: 0.7382282 acc: 77.34375\n",
      "Epoch: 6 batch 31 loss: 0.82397157 acc: 74.21875\n",
      "Epoch: 6 batch 32 loss: 0.8370231 acc: 72.65625\n",
      "Epoch: 6 batch 33 loss: 0.72033894 acc: 71.09375\n",
      "Epoch: 6 batch 34 loss: 0.7624151 acc: 71.875\n",
      "Epoch: 6 batch 35 loss: 0.87459 acc: 71.09375\n",
      "Epoch: 6 batch 36 loss: 0.6466313 acc: 75.78125\n",
      "Epoch: 6 batch 37 loss: 0.8759111 acc: 71.09375\n",
      "Epoch: 6 batch 38 loss: 0.8545791 acc: 73.4375\n",
      "Epoch: 6 batch 39 loss: 0.7866454 acc: 68.75\n",
      "Epoch: 6 batch 40 loss: 0.8813245 acc: 67.1875\n",
      "Epoch: 6 batch 41 loss: 0.7262646 acc: 75.0\n",
      "Epoch: 6 batch 42 loss: 0.82166934 acc: 67.96875\n",
      "Epoch: 6 batch 43 loss: 0.8743996 acc: 68.75\n",
      "Epoch: 6 batch 44 loss: 0.8817382 acc: 69.53125\n",
      "Epoch: 6 batch 45 loss: 0.7227591 acc: 72.65625\n",
      "Epoch: 6 batch 46 loss: 0.6237893 acc: 77.34375\n",
      "Epoch: 6 batch 47 loss: 0.7625425 acc: 70.3125\n",
      "Epoch: 6 batch 48 loss: 0.6467843 acc: 77.34375\n",
      "Epoch: 6 batch 49 loss: 0.5880705 acc: 82.8125\n",
      "Epoch: 6 batch 50 loss: 0.75160056 acc: 70.3125\n",
      "Epoch: 6 batch 51 loss: 0.65051866 acc: 76.5625\n",
      "Epoch: 6 batch 52 loss: 0.7537132 acc: 73.4375\n",
      "Epoch: 6 batch 53 loss: 0.89538217 acc: 71.875\n",
      "Epoch: 6 batch 54 loss: 0.5951204 acc: 77.34375\n",
      "Epoch: 6 batch 55 loss: 0.76723254 acc: 73.4375\n",
      "Epoch: 6 batch 56 loss: 0.6921578 acc: 73.4375\n",
      "Epoch: 6 batch 57 loss: 0.6608547 acc: 81.25\n",
      "Epoch: 6 batch 58 loss: 0.7423573 acc: 72.65625\n",
      "Epoch: 6 batch 59 loss: 0.7906463 acc: 71.875\n",
      "Epoch: 6 batch 60 loss: 0.75618815 acc: 71.875\n",
      "Epoch: 6 batch 61 loss: 0.744971 acc: 75.0\n",
      "Epoch: 6 batch 62 loss: 0.7434864 acc: 74.21875\n",
      "Epoch: 6 batch 63 loss: 0.6666429 acc: 74.21875\n",
      "Epoch: 6 batch 64 loss: 0.71128136 acc: 76.5625\n",
      "Epoch: 6 batch 65 loss: 0.7900202 acc: 71.875\n",
      "Epoch: 6 batch 66 loss: 0.79767406 acc: 74.21875\n",
      "Epoch: 6 batch 67 loss: 0.65819883 acc: 78.90625\n",
      "Epoch: 6 batch 68 loss: 0.8411827 acc: 69.53125\n",
      "Epoch: 6 batch 69 loss: 0.7864505 acc: 78.125\n",
      "Epoch: 6 batch 70 loss: 0.86615604 acc: 68.75\n",
      "Epoch: 6 batch 71 loss: 0.69114184 acc: 75.0\n",
      "Epoch: 6 batch 72 loss: 0.7179446 acc: 71.875\n",
      "Epoch: 6 batch 73 loss: 0.6694568 acc: 81.25\n",
      "Epoch: 6 batch 74 loss: 0.71160376 acc: 78.125\n",
      "Epoch: 6 batch 75 loss: 0.64510316 acc: 76.5625\n",
      "Epoch: 6 batch 76 loss: 0.827389 acc: 72.65625\n",
      "Epoch: 6 batch 77 loss: 0.657132 acc: 78.125\n",
      "Epoch: 6 batch 78 loss: 0.6592311 acc: 74.21875\n",
      "Epoch: 6 batch 79 loss: 0.8500252 acc: 71.09375\n",
      "Epoch: 6 batch 80 loss: 0.7892394 acc: 77.34375\n",
      "Epoch: 6 batch 81 loss: 0.9580065 acc: 67.96875\n",
      "Epoch: 6 batch 82 loss: 0.7119586 acc: 78.125\n",
      "Epoch: 6 batch 83 loss: 0.6685773 acc: 76.5625\n",
      "Epoch: 6 batch 84 loss: 0.74503887 acc: 75.0\n",
      "Epoch: 6 batch 85 loss: 0.88954383 acc: 68.75\n",
      "Epoch: 6 batch 86 loss: 0.9514235 acc: 67.96875\n",
      "Epoch: 6 batch 87 loss: 0.6921774 acc: 77.34375\n",
      "Epoch: 6 batch 88 loss: 0.7197088 acc: 74.21875\n",
      "Epoch: 6 batch 89 loss: 0.6855016 acc: 78.125\n",
      "Epoch: 6 batch 90 loss: 0.6876155 acc: 75.78125\n",
      "Epoch: 6 batch 91 loss: 0.771929 acc: 80.46875\n",
      "Epoch: 6 batch 92 loss: 0.91447616 acc: 69.53125\n",
      "Epoch: 6 batch 93 loss: 0.7096277 acc: 74.21875\n",
      "Epoch: 6 batch 94 loss: 0.87858 acc: 67.96875\n",
      "Epoch: 6 batch 95 loss: 0.70455074 acc: 76.5625\n",
      "Epoch: 6 batch 96 loss: 0.7815528 acc: 71.875\n",
      "Epoch: 6 batch 97 loss: 0.7871228 acc: 71.875\n",
      "Epoch: 6 batch 98 loss: 0.7962893 acc: 70.3125\n",
      "Epoch: 6 batch 99 loss: 0.8892745 acc: 72.65625\n",
      "Epoch: 6 batch 100 loss: 0.7453078 acc: 74.21875\n",
      "Epoch: 6 batch 101 loss: 0.9247196 acc: 72.65625\n",
      "Epoch: 6 batch 102 loss: 0.7423741 acc: 75.78125\n",
      "Epoch: 6 batch 103 loss: 0.9163856 acc: 68.75\n",
      "Epoch: 6 batch 104 loss: 0.8028922 acc: 71.875\n",
      "Epoch: 6 batch 105 loss: 0.6820446 acc: 76.5625\n",
      "Epoch: 6 batch 106 loss: 0.623542 acc: 77.34375\n",
      "Epoch: 6 batch 107 loss: 0.92624307 acc: 70.3125\n",
      "Epoch: 6 batch 108 loss: 0.7319509 acc: 75.78125\n",
      "Epoch: 6 batch 109 loss: 0.7110162 acc: 73.4375\n",
      "Epoch: 6 batch 110 loss: 0.79506654 acc: 70.3125\n",
      "Epoch: 6 batch 111 loss: 0.78516275 acc: 73.4375\n",
      "Epoch: 6 batch 112 loss: 0.8482044 acc: 67.1875\n",
      "Epoch: 6 batch 113 loss: 0.65350103 acc: 77.34375\n",
      "Epoch: 6 batch 114 loss: 0.9144774 acc: 68.75\n",
      "Epoch: 6 batch 115 loss: 0.7234562 acc: 72.65625\n",
      "Epoch: 6 batch 116 loss: 0.8444854 acc: 70.3125\n",
      "Epoch: 6 batch 117 loss: 0.6819795 acc: 79.6875\n",
      "Epoch: 6 batch 118 loss: 0.7572231 acc: 76.5625\n",
      "Epoch: 6 batch 119 loss: 0.6416817 acc: 74.21875\n",
      "Epoch: 6 batch 120 loss: 0.70363295 acc: 75.78125\n",
      "Epoch: 6 batch 121 loss: 0.81601536 acc: 69.53125\n",
      "Epoch: 6 batch 122 loss: 0.7919984 acc: 75.0\n",
      "Epoch: 6 batch 123 loss: 0.86724347 acc: 67.1875\n",
      "Epoch: 6 batch 124 loss: 0.8220736 acc: 69.53125\n",
      "Epoch: 6 batch 125 loss: 0.6733745 acc: 76.5625\n",
      "Epoch: 6 batch 126 loss: 0.6421624 acc: 76.5625\n",
      "Epoch: 6 batch 127 loss: 0.7117572 acc: 77.34375\n",
      "Epoch: 6 batch 128 loss: 0.8405703 acc: 73.4375\n",
      "Epoch: 6 batch 129 loss: 0.7553804 acc: 70.3125\n",
      "Epoch: 6 batch 130 loss: 0.65555364 acc: 77.34375\n",
      "Epoch: 6 batch 131 loss: 0.7840432 acc: 74.21875\n",
      "Epoch: 6 batch 132 loss: 0.72161657 acc: 78.125\n",
      "Epoch: 6 batch 133 loss: 0.82918847 acc: 71.09375\n",
      "Epoch: 6 batch 134 loss: 0.5991253 acc: 79.6875\n",
      "Epoch: 6 batch 135 loss: 0.7908312 acc: 73.4375\n",
      "Epoch: 6 batch 136 loss: 0.72302926 acc: 74.21875\n",
      "Epoch: 6 batch 137 loss: 0.6634536 acc: 78.125\n",
      "Epoch: 6 batch 138 loss: 1.1200038 acc: 62.5\n",
      "Epoch: 6 batch 139 loss: 0.7205514 acc: 74.21875\n",
      "Epoch: 6 batch 140 loss: 0.74736774 acc: 72.65625\n",
      "Epoch: 6 batch 141 loss: 0.76804733 acc: 72.65625\n",
      "Epoch: 6 batch 142 loss: 0.70924485 acc: 75.0\n",
      "Epoch: 6 batch 143 loss: 0.71210766 acc: 73.4375\n",
      "Epoch: 6 batch 144 loss: 0.82083666 acc: 69.53125\n",
      "Epoch: 6 batch 145 loss: 0.86428076 acc: 73.4375\n",
      "Epoch: 6 batch 146 loss: 0.7906623 acc: 71.09375\n",
      "Epoch: 6 batch 147 loss: 0.66700566 acc: 77.34375\n",
      "Epoch: 6 batch 148 loss: 0.7244952 acc: 75.0\n",
      "Epoch: 6 batch 149 loss: 0.7514887 acc: 72.65625\n",
      "Epoch: 6 batch 150 loss: 0.6785286 acc: 77.34375\n",
      "Epoch: 6 batch 151 loss: 0.6923926 acc: 77.34375\n",
      "Epoch: 6 batch 152 loss: 0.7657242 acc: 75.78125\n",
      "Epoch: 6 batch 153 loss: 0.7754549 acc: 74.21875\n",
      "Epoch: 6 batch 154 loss: 0.77033156 acc: 75.78125\n",
      "Epoch: 6 batch 155 loss: 0.8381639 acc: 71.875\n",
      "Epoch: 6 batch 156 loss: 0.77298635 acc: 74.21875\n",
      "Epoch: 6 batch 157 loss: 0.7818512 acc: 71.875\n",
      "Epoch: 6 batch 158 loss: 0.7600107 acc: 75.0\n",
      "Epoch: 6 batch 159 loss: 0.70664203 acc: 75.0\n",
      "Epoch: 6 batch 160 loss: 0.7837796 acc: 69.53125\n",
      "Epoch: 6 batch 161 loss: 0.6864649 acc: 79.6875\n",
      "Epoch: 6 batch 162 loss: 0.7873277 acc: 78.125\n",
      "Epoch: 6 batch 163 loss: 0.7355531 acc: 75.78125\n",
      "Epoch: 6 batch 164 loss: 0.6113125 acc: 78.125\n",
      "Epoch: 6 batch 165 loss: 0.6385572 acc: 78.125\n",
      "Epoch: 6 batch 166 loss: 0.6998475 acc: 75.78125\n",
      "Epoch: 6 batch 167 loss: 0.7270719 acc: 76.5625\n",
      "Epoch: 6 batch 168 loss: 0.7439332 acc: 74.21875\n",
      "Epoch: 6 batch 169 loss: 0.7846967 acc: 67.1875\n",
      "Epoch: 6 batch 170 loss: 0.7303443 acc: 75.0\n",
      "Epoch: 6 batch 171 loss: 0.8256749 acc: 71.09375\n",
      "Epoch: 6 batch 172 loss: 0.7683678 acc: 75.0\n",
      "Epoch: 6 batch 173 loss: 0.69447505 acc: 74.21875\n",
      "Epoch: 6 batch 174 loss: 0.7469491 acc: 78.90625\n",
      "Epoch: 6 batch 175 loss: 0.9067971 acc: 62.5\n",
      "Epoch: 6 batch 176 loss: 0.7698594 acc: 74.21875\n",
      "Epoch: 6 batch 177 loss: 0.6981995 acc: 81.25\n",
      "Epoch: 6 batch 178 loss: 0.8749193 acc: 67.1875\n",
      "Epoch: 6 batch 179 loss: 0.7950553 acc: 72.65625\n",
      "Epoch: 6 batch 180 loss: 0.66567945 acc: 75.78125\n",
      "Epoch: 6 batch 181 loss: 0.74650764 acc: 74.21875\n",
      "Epoch: 6 batch 182 loss: 0.5142211 acc: 81.25\n",
      "Epoch: 6 batch 183 loss: 0.80515265 acc: 76.5625\n",
      "Epoch: 6 batch 184 loss: 0.6778677 acc: 73.4375\n",
      "Epoch: 6 batch 185 loss: 0.67967653 acc: 74.21875\n",
      "Epoch: 6 batch 186 loss: 0.66043174 acc: 83.59375\n",
      "Epoch: 6 batch 187 loss: 0.6600894 acc: 77.34375\n",
      "Epoch: 6 batch 188 loss: 0.63440835 acc: 76.5625\n",
      "Epoch: 6 batch 189 loss: 0.8294895 acc: 78.125\n",
      "Epoch: 6 batch 190 loss: 0.7272848 acc: 76.5625\n",
      "Epoch: 6 batch 191 loss: 0.70715106 acc: 77.34375\n",
      "Epoch: 6 batch 192 loss: 0.66471916 acc: 76.5625\n",
      "Epoch: 6 batch 193 loss: 0.977465 acc: 69.53125\n",
      "Epoch: 6 batch 194 loss: 0.72405773 acc: 73.4375\n",
      "Epoch: 6 batch 195 loss: 0.7197263 acc: 74.21875\n",
      "Epoch: 6 batch 196 loss: 0.6303569 acc: 78.125\n",
      "Epoch: 6 batch 197 loss: 0.74601775 acc: 76.5625\n",
      "Epoch: 6 batch 198 loss: 0.765455 acc: 74.21875\n",
      "Epoch: 6 batch 199 loss: 0.7863172 acc: 74.21875\n",
      "Epoch: 6 batch 200 loss: 0.81256485 acc: 72.65625\n",
      "Epoch: 6 batch 201 loss: 0.63321584 acc: 74.21875\n",
      "Epoch: 6 batch 202 loss: 0.60185134 acc: 81.25\n",
      "Epoch: 6 batch 203 loss: 0.5520966 acc: 79.6875\n",
      "Epoch: 6 batch 204 loss: 0.67029274 acc: 75.78125\n",
      "Epoch: 6 batch 205 loss: 0.5880555 acc: 77.34375\n",
      "Epoch: 6 batch 206 loss: 0.86548495 acc: 71.09375\n",
      "Epoch: 6 batch 207 loss: 0.72310513 acc: 72.65625\n",
      "Epoch: 6 batch 208 loss: 0.7506423 acc: 76.5625\n",
      "Epoch: 6 batch 209 loss: 0.837982 acc: 74.21875\n",
      "Epoch: 6 batch 210 loss: 0.64626175 acc: 76.5625\n",
      "Epoch: 6 batch 211 loss: 0.8295784 acc: 68.75\n",
      "Epoch: 6 batch 212 loss: 0.82106656 acc: 71.875\n",
      "Epoch: 6 batch 213 loss: 0.66997313 acc: 78.125\n",
      "Epoch: 6 batch 214 loss: 0.7257965 acc: 74.21875\n",
      "Epoch: 6 batch 215 loss: 0.6873115 acc: 76.5625\n",
      "Epoch: 6 batch 216 loss: 0.6640339 acc: 75.78125\n",
      "Epoch: 6 batch 217 loss: 0.78992856 acc: 75.78125\n",
      "Epoch: 6 batch 218 loss: 0.6734421 acc: 74.21875\n",
      "Epoch: 6 batch 219 loss: 0.74031556 acc: 72.65625\n",
      "Epoch: 6 batch 220 loss: 0.6392853 acc: 76.5625\n",
      "Epoch: 6 batch 221 loss: 0.7018213 acc: 72.65625\n",
      "Epoch: 6 batch 222 loss: 0.7962192 acc: 71.875\n",
      "Epoch: 6 batch 223 loss: 0.74103385 acc: 74.21875\n",
      "Epoch: 6 batch 224 loss: 0.7572113 acc: 73.4375\n",
      "Epoch: 6 batch 225 loss: 0.51303405 acc: 83.59375\n",
      "Epoch: 6 batch 226 loss: 0.60963035 acc: 78.90625\n",
      "Epoch: 6 batch 227 loss: 0.8174164 acc: 75.0\n",
      "Epoch: 6 batch 228 loss: 0.7203469 acc: 69.53125\n",
      "Epoch: 6 batch 229 loss: 0.5376249 acc: 83.59375\n",
      "Epoch: 6 batch 230 loss: 0.7081928 acc: 75.78125\n",
      "Epoch: 6 batch 231 loss: 0.79772186 acc: 75.78125\n",
      "Epoch: 6 batch 232 loss: 0.74713635 acc: 73.4375\n",
      "Epoch: 6 batch 233 loss: 0.7330871 acc: 71.875\n",
      "Epoch: 6 batch 234 loss: 0.565199 acc: 78.90625\n",
      "Epoch: 6 batch 235 loss: 0.72553265 acc: 73.4375\n",
      "Epoch: 6 batch 236 loss: 0.57536185 acc: 82.8125\n",
      "Epoch: 6 batch 237 loss: 0.6519873 acc: 75.78125\n",
      "Epoch: 6 batch 238 loss: 0.7453174 acc: 75.78125\n",
      "Epoch: 6 batch 239 loss: 0.76149976 acc: 71.09375\n",
      "Epoch: 6 batch 240 loss: 0.8698475 acc: 71.09375\n",
      "Epoch: 6 batch 241 loss: 0.71335787 acc: 80.46875\n",
      "Epoch: 6 batch 242 loss: 0.63296604 acc: 82.03125\n",
      "Epoch: 6 batch 243 loss: 0.7073612 acc: 72.65625\n",
      "Epoch: 6 batch 244 loss: 0.59516096 acc: 80.46875\n",
      "Epoch: 6 batch 245 loss: 0.72356313 acc: 75.0\n",
      "Epoch: 6 batch 246 loss: 0.69448423 acc: 76.5625\n",
      "Epoch: 6 batch 247 loss: 0.81001717 acc: 75.78125\n",
      "Epoch: 6 batch 248 loss: 0.68726885 acc: 75.0\n",
      "Epoch: 6 batch 249 loss: 0.95221865 acc: 69.53125\n",
      "Epoch: 6 batch 250 loss: 0.845139 acc: 67.1875\n",
      "Epoch: 6 batch 251 loss: 0.7626346 acc: 71.09375\n",
      "Epoch: 6 batch 252 loss: 0.7691461 acc: 76.5625\n",
      "Epoch: 6 batch 253 loss: 0.80998987 acc: 70.3125\n",
      "Epoch: 6 batch 254 loss: 1.0165087 acc: 64.84375\n",
      "Epoch: 6 batch 255 loss: 0.76565737 acc: 75.0\n",
      "Epoch: 6 batch 256 loss: 0.8416153 acc: 71.09375\n",
      "Epoch: 6 batch 257 loss: 0.7226227 acc: 78.125\n",
      "Epoch: 6 batch 258 loss: 0.7007058 acc: 71.09375\n",
      "Epoch: 6 batch 259 loss: 0.8260385 acc: 68.75\n",
      "Epoch: 6 batch 260 loss: 0.88588566 acc: 68.75\n",
      "Epoch: 6 batch 261 loss: 0.8128643 acc: 71.09375\n",
      "Epoch: 6 batch 262 loss: 0.9023714 acc: 64.84375\n",
      "Epoch: 6 batch 263 loss: 0.63567626 acc: 77.34375\n",
      "Epoch: 6 batch 264 loss: 0.7633493 acc: 72.65625\n",
      "Epoch: 6 batch 265 loss: 0.8586424 acc: 71.09375\n",
      "Epoch: 6 batch 266 loss: 0.7291093 acc: 75.0\n",
      "Epoch: 6 batch 267 loss: 0.6223278 acc: 76.5625\n",
      "Epoch: 6 batch 268 loss: 0.88071406 acc: 71.09375\n",
      "Epoch: 6 batch 269 loss: 0.6823178 acc: 76.5625\n",
      "Epoch: 6 batch 270 loss: 0.7242138 acc: 73.4375\n",
      "Epoch: 6 batch 271 loss: 0.7453681 acc: 77.34375\n",
      "Epoch: 6 batch 272 loss: 0.6349326 acc: 78.90625\n",
      "Epoch: 6 batch 273 loss: 0.92552316 acc: 66.40625\n",
      "Epoch: 6 batch 274 loss: 0.635059 acc: 75.0\n",
      "Epoch: 6 batch 275 loss: 0.6776607 acc: 76.5625\n",
      "Epoch: 6 batch 276 loss: 0.7679442 acc: 76.5625\n",
      "Epoch: 6 batch 277 loss: 0.6780041 acc: 73.4375\n",
      "Epoch: 6 batch 278 loss: 0.7542806 acc: 71.875\n",
      "Epoch: 6 batch 279 loss: 0.76468325 acc: 75.0\n",
      "Epoch: 6 batch 280 loss: 0.5274644 acc: 81.25\n",
      "Epoch: 6 batch 281 loss: 0.71995366 acc: 73.4375\n",
      "Epoch: 6 batch 282 loss: 0.6799839 acc: 80.46875\n",
      "Epoch: 6 batch 283 loss: 0.63404727 acc: 79.6875\n",
      "Epoch: 6 batch 284 loss: 0.6839004 acc: 78.90625\n",
      "Epoch: 6 batch 285 loss: 0.55570227 acc: 78.90625\n",
      "Epoch: 6 batch 286 loss: 0.8043609 acc: 73.4375\n",
      "Epoch: 6 batch 287 loss: 0.6449409 acc: 75.78125\n",
      "Epoch: 6 batch 288 loss: 0.7905127 acc: 73.4375\n",
      "Epoch: 6 batch 289 loss: 0.81565905 acc: 71.875\n",
      "Epoch: 6 batch 290 loss: 0.8877517 acc: 71.875\n",
      "Epoch: 6 batch 291 loss: 0.9846811 acc: 66.40625\n",
      "Epoch: 6 batch 292 loss: 0.7382077 acc: 75.78125\n",
      "Epoch: 6 batch 293 loss: 0.67100894 acc: 77.34375\n",
      "Epoch: 6 batch 294 loss: 0.73945343 acc: 73.4375\n",
      "Epoch: 6 batch 295 loss: 0.63604945 acc: 78.125\n",
      "Epoch: 6 batch 296 loss: 0.79597497 acc: 70.3125\n",
      "Epoch: 6 batch 297 loss: 0.72148067 acc: 74.21875\n",
      "Epoch: 6 batch 298 loss: 0.7287313 acc: 73.4375\n",
      "Epoch: 6 batch 299 loss: 0.7602156 acc: 74.21875\n",
      "Epoch: 6 batch 300 loss: 0.7685281 acc: 71.875\n",
      "Epoch: 6 batch 301 loss: 0.744797 acc: 73.4375\n",
      "Epoch: 6 batch 302 loss: 0.66529894 acc: 77.34375\n",
      "Epoch: 6 batch 303 loss: 0.6274256 acc: 81.25\n",
      "Epoch: 6 batch 304 loss: 0.6493963 acc: 75.78125\n",
      "Epoch: 6 batch 305 loss: 0.66458684 acc: 76.5625\n",
      "Epoch: 6 batch 306 loss: 0.7171829 acc: 74.21875\n",
      "Epoch: 6 batch 307 loss: 0.7687857 acc: 75.78125\n",
      "Epoch: 6 batch 308 loss: 0.6108991 acc: 78.90625\n",
      "Epoch: 6 batch 309 loss: 0.86792445 acc: 66.40625\n",
      "Epoch: 6 batch 310 loss: 0.8383552 acc: 73.4375\n",
      "Epoch: 6 batch 311 loss: 0.8196398 acc: 75.78125\n",
      "Epoch: 6 batch 312 loss: 0.74895847 acc: 75.78125\n",
      "Epoch: 6 batch 313 loss: 0.72222567 acc: 75.78125\n",
      "Epoch: 6 batch 314 loss: 0.6682703 acc: 75.78125\n",
      "Epoch: 6 batch 315 loss: 0.7774498 acc: 71.875\n",
      "Epoch: 6 batch 316 loss: 0.77039117 acc: 75.78125\n",
      "Epoch: 6 batch 317 loss: 0.72375256 acc: 74.21875\n",
      "Epoch: 6 batch 318 loss: 0.7577033 acc: 71.875\n",
      "Epoch: 6 batch 319 loss: 0.84935564 acc: 70.3125\n",
      "Epoch: 6 batch 320 loss: 0.69727564 acc: 76.5625\n",
      "Epoch: 6 batch 321 loss: 0.64715064 acc: 78.125\n",
      "Epoch: 6 batch 322 loss: 0.6320731 acc: 75.0\n",
      "Epoch: 6 batch 323 loss: 0.6993284 acc: 71.875\n",
      "Epoch: 6 batch 324 loss: 0.7853072 acc: 68.75\n",
      "Epoch: 6 batch 325 loss: 0.67032886 acc: 73.4375\n",
      "Epoch: 6 batch 326 loss: 0.67302555 acc: 75.0\n",
      "Epoch: 6 batch 327 loss: 0.8332724 acc: 77.34375\n",
      "Epoch: 6 batch 328 loss: 0.69901884 acc: 78.125\n",
      "Epoch: 6 batch 329 loss: 0.5904373 acc: 78.90625\n",
      "Epoch: 6 batch 330 loss: 0.63802004 acc: 75.0\n",
      "Epoch: 6 batch 331 loss: 0.63498783 acc: 79.6875\n",
      "Epoch: 6 batch 332 loss: 0.9094738 acc: 64.0625\n",
      "Epoch: 6 batch 333 loss: 0.6116053 acc: 75.0\n",
      "Epoch: 6 batch 334 loss: 0.6006414 acc: 81.25\n",
      "Epoch: 6 batch 335 loss: 0.90129143 acc: 70.3125\n",
      "Epoch: 6 batch 336 loss: 0.6795659 acc: 75.0\n",
      "Epoch: 6 batch 337 loss: 0.7168544 acc: 76.5625\n",
      "Epoch: 6 batch 338 loss: 0.8459399 acc: 67.1875\n",
      "Epoch: 6 batch 339 loss: 0.8120507 acc: 69.53125\n",
      "Epoch: 6 batch 340 loss: 0.686347 acc: 73.4375\n",
      "Epoch: 6 batch 341 loss: 0.7981434 acc: 72.65625\n",
      "Epoch: 6 batch 342 loss: 0.81563884 acc: 67.96875\n",
      "Epoch: 6 batch 343 loss: 0.7072053 acc: 75.78125\n",
      "Epoch: 6 batch 344 loss: 0.76054394 acc: 70.3125\n",
      "Epoch: 6 batch 345 loss: 0.78317547 acc: 76.5625\n",
      "Epoch: 6 batch 346 loss: 0.71932626 acc: 72.65625\n",
      "Epoch: 6 batch 347 loss: 0.73774254 acc: 72.65625\n",
      "Epoch: 6 batch 348 loss: 0.6473137 acc: 76.5625\n",
      "Epoch: 6 batch 349 loss: 0.8048317 acc: 74.21875\n",
      "Epoch: 6 batch 350 loss: 0.7554767 acc: 71.09375\n",
      "Epoch: 6 batch 351 loss: 0.74770105 acc: 72.65625\n",
      "Epoch: 6 batch 352 loss: 0.7089233 acc: 76.5625\n",
      "Epoch: 6 batch 353 loss: 0.62994313 acc: 77.34375\n",
      "Epoch: 6 batch 354 loss: 0.6417608 acc: 77.34375\n",
      "Epoch: 6 batch 355 loss: 0.6992998 acc: 74.21875\n",
      "Epoch: 6 batch 356 loss: 0.74265116 acc: 69.53125\n",
      "Epoch: 6 batch 357 loss: 0.63026655 acc: 78.125\n",
      "Epoch: 6 batch 358 loss: 0.7186031 acc: 76.5625\n",
      "Epoch: 6 batch 359 loss: 0.5614065 acc: 79.6875\n",
      "Epoch: 6 batch 360 loss: 0.5654483 acc: 79.6875\n",
      "Epoch: 6 batch 361 loss: 0.88015676 acc: 72.65625\n",
      "Epoch: 6 batch 362 loss: 0.7214481 acc: 75.0\n",
      "Epoch: 6 batch 363 loss: 0.94820184 acc: 67.1875\n",
      "Epoch: 6 batch 364 loss: 0.71457183 acc: 78.90625\n",
      "Epoch: 6 batch 365 loss: 0.5918653 acc: 78.125\n",
      "Epoch: 6 batch 366 loss: 0.551864 acc: 84.375\n",
      "Epoch: 6 batch 367 loss: 0.8110256 acc: 71.09375\n",
      "Epoch: 6 batch 368 loss: 0.64578784 acc: 76.5625\n",
      "Epoch: 6 batch 369 loss: 0.63234645 acc: 75.0\n",
      "Epoch: 6 batch 370 loss: 0.63565224 acc: 77.34375\n",
      "Epoch: 6 batch 371 loss: 0.5786785 acc: 78.125\n",
      "Epoch: 6 batch 372 loss: 0.66734284 acc: 77.34375\n",
      "Epoch: 6 batch 373 loss: 0.7583354 acc: 72.65625\n",
      "Epoch: 6 batch 374 loss: 0.72476506 acc: 75.0\n",
      "Epoch: 6 batch 375 loss: 0.80335975 acc: 71.875\n",
      "Epoch: 6 batch 376 loss: 0.78840375 acc: 71.875\n",
      "Epoch: 6 batch 377 loss: 0.57016754 acc: 79.6875\n",
      "Epoch: 6 batch 378 loss: 0.55095077 acc: 77.34375\n",
      "Epoch: 6 batch 379 loss: 0.8029815 acc: 72.65625\n",
      "Epoch: 6 batch 380 loss: 0.63320816 acc: 85.15625\n",
      "Epoch: 6 batch 381 loss: 0.72197807 acc: 75.0\n",
      "Epoch: 6 batch 382 loss: 0.6948906 acc: 75.0\n",
      "Epoch: 6 batch 383 loss: 0.7100569 acc: 72.65625\n",
      "Epoch: 6 batch 384 loss: 0.6457108 acc: 77.34375\n",
      "Epoch: 6 batch 385 loss: 0.7423296 acc: 74.21875\n",
      "Epoch: 6 batch 386 loss: 0.71417737 acc: 73.4375\n",
      "Epoch: 6 batch 387 loss: 0.7090436 acc: 71.09375\n",
      "Epoch: 6 batch 388 loss: 0.714388 acc: 71.09375\n",
      "Epoch: 6 batch 389 loss: 0.72558093 acc: 74.21875\n",
      "After epoch 6 test loss 0.8219730955123902 test accuracy 0.7146999835968018\n",
      "Epoch: 7 batch 0 loss: 0.6560908 acc: 71.54423594474792\n",
      "Epoch: 7 batch 1 loss: 0.6951411 acc: 75.0\n",
      "Epoch: 7 batch 2 loss: 0.68446237 acc: 72.65625\n",
      "Epoch: 7 batch 3 loss: 0.64732075 acc: 80.46875\n",
      "Epoch: 7 batch 4 loss: 0.70890385 acc: 71.09375\n",
      "Epoch: 7 batch 5 loss: 0.7216198 acc: 72.65625\n",
      "Epoch: 7 batch 6 loss: 0.82780296 acc: 70.3125\n",
      "Epoch: 7 batch 7 loss: 0.6875057 acc: 78.125\n",
      "Epoch: 7 batch 8 loss: 0.7266653 acc: 74.21875\n",
      "Epoch: 7 batch 9 loss: 0.5974641 acc: 80.46875\n",
      "Epoch: 7 batch 10 loss: 0.74589854 acc: 76.5625\n",
      "Epoch: 7 batch 11 loss: 0.75419915 acc: 72.65625\n",
      "Epoch: 7 batch 12 loss: 0.79010755 acc: 70.3125\n",
      "Epoch: 7 batch 13 loss: 0.6662482 acc: 82.03125\n",
      "Epoch: 7 batch 14 loss: 0.5975722 acc: 79.6875\n",
      "Epoch: 7 batch 15 loss: 0.8094933 acc: 72.65625\n",
      "Epoch: 7 batch 16 loss: 0.59738165 acc: 80.46875\n",
      "Epoch: 7 batch 17 loss: 0.5949921 acc: 78.90625\n",
      "Epoch: 7 batch 18 loss: 0.5978189 acc: 76.5625\n",
      "Epoch: 7 batch 19 loss: 0.6034366 acc: 77.34375\n",
      "Epoch: 7 batch 20 loss: 0.4787656 acc: 82.03125\n",
      "Epoch: 7 batch 21 loss: 0.6745452 acc: 75.78125\n",
      "Epoch: 7 batch 22 loss: 0.63602775 acc: 78.125\n",
      "Epoch: 7 batch 23 loss: 0.78036827 acc: 72.65625\n",
      "Epoch: 7 batch 24 loss: 0.7850233 acc: 75.78125\n",
      "Epoch: 7 batch 25 loss: 0.81974816 acc: 69.53125\n",
      "Epoch: 7 batch 26 loss: 0.818431 acc: 74.21875\n",
      "Epoch: 7 batch 27 loss: 0.6312709 acc: 78.125\n",
      "Epoch: 7 batch 28 loss: 0.64666533 acc: 78.90625\n",
      "Epoch: 7 batch 29 loss: 0.69146574 acc: 77.34375\n",
      "Epoch: 7 batch 30 loss: 0.64427096 acc: 75.78125\n",
      "Epoch: 7 batch 31 loss: 0.68187386 acc: 77.34375\n",
      "Epoch: 7 batch 32 loss: 0.7321799 acc: 78.125\n",
      "Epoch: 7 batch 33 loss: 0.6606182 acc: 78.90625\n",
      "Epoch: 7 batch 34 loss: 0.6768319 acc: 74.21875\n",
      "Epoch: 7 batch 35 loss: 0.7016398 acc: 71.875\n",
      "Epoch: 7 batch 36 loss: 0.6012345 acc: 75.78125\n",
      "Epoch: 7 batch 37 loss: 0.7064891 acc: 75.78125\n",
      "Epoch: 7 batch 38 loss: 0.7207233 acc: 76.5625\n",
      "Epoch: 7 batch 39 loss: 0.78498185 acc: 71.09375\n",
      "Epoch: 7 batch 40 loss: 0.8141571 acc: 73.4375\n",
      "Epoch: 7 batch 41 loss: 0.81389016 acc: 73.4375\n",
      "Epoch: 7 batch 42 loss: 0.7740787 acc: 69.53125\n",
      "Epoch: 7 batch 43 loss: 0.75280404 acc: 74.21875\n",
      "Epoch: 7 batch 44 loss: 0.7760544 acc: 75.0\n",
      "Epoch: 7 batch 45 loss: 0.72733605 acc: 75.0\n",
      "Epoch: 7 batch 46 loss: 0.6381941 acc: 80.46875\n",
      "Epoch: 7 batch 47 loss: 0.80754006 acc: 69.53125\n",
      "Epoch: 7 batch 48 loss: 0.7008444 acc: 74.21875\n",
      "Epoch: 7 batch 49 loss: 0.5233468 acc: 80.46875\n",
      "Epoch: 7 batch 50 loss: 0.72233236 acc: 69.53125\n",
      "Epoch: 7 batch 51 loss: 0.64366114 acc: 78.90625\n",
      "Epoch: 7 batch 52 loss: 0.70403314 acc: 78.125\n",
      "Epoch: 7 batch 53 loss: 0.8671236 acc: 75.0\n",
      "Epoch: 7 batch 54 loss: 0.6889826 acc: 71.09375\n",
      "Epoch: 7 batch 55 loss: 0.85200506 acc: 70.3125\n",
      "Epoch: 7 batch 56 loss: 0.6960937 acc: 78.90625\n",
      "Epoch: 7 batch 57 loss: 0.7213897 acc: 75.0\n",
      "Epoch: 7 batch 58 loss: 0.62987447 acc: 80.46875\n",
      "Epoch: 7 batch 59 loss: 0.6123533 acc: 78.90625\n",
      "Epoch: 7 batch 60 loss: 0.6918633 acc: 75.78125\n",
      "Epoch: 7 batch 61 loss: 0.7062132 acc: 75.78125\n",
      "Epoch: 7 batch 62 loss: 0.6927657 acc: 75.0\n",
      "Epoch: 7 batch 63 loss: 0.51726615 acc: 79.6875\n",
      "Epoch: 7 batch 64 loss: 0.66134423 acc: 73.4375\n",
      "Epoch: 7 batch 65 loss: 0.62237537 acc: 75.78125\n",
      "Epoch: 7 batch 66 loss: 0.79438764 acc: 74.21875\n",
      "Epoch: 7 batch 67 loss: 0.7415662 acc: 75.78125\n",
      "Epoch: 7 batch 68 loss: 0.8172914 acc: 70.3125\n",
      "Epoch: 7 batch 69 loss: 0.77275914 acc: 75.0\n",
      "Epoch: 7 batch 70 loss: 0.74285 acc: 73.4375\n",
      "Epoch: 7 batch 71 loss: 0.62099683 acc: 77.34375\n",
      "Epoch: 7 batch 72 loss: 0.70436853 acc: 77.34375\n",
      "Epoch: 7 batch 73 loss: 0.6990742 acc: 74.21875\n",
      "Epoch: 7 batch 74 loss: 0.5664497 acc: 79.6875\n",
      "Epoch: 7 batch 75 loss: 0.6711664 acc: 75.0\n",
      "Epoch: 7 batch 76 loss: 0.6832174 acc: 76.5625\n",
      "Epoch: 7 batch 77 loss: 0.7579937 acc: 67.96875\n",
      "Epoch: 7 batch 78 loss: 0.58472455 acc: 85.15625\n",
      "Epoch: 7 batch 79 loss: 0.72846353 acc: 72.65625\n",
      "Epoch: 7 batch 80 loss: 0.62390435 acc: 82.03125\n",
      "Epoch: 7 batch 81 loss: 0.9879886 acc: 69.53125\n",
      "Epoch: 7 batch 82 loss: 0.5948725 acc: 78.125\n",
      "Epoch: 7 batch 83 loss: 0.7075129 acc: 76.5625\n",
      "Epoch: 7 batch 84 loss: 0.6397133 acc: 80.46875\n",
      "Epoch: 7 batch 85 loss: 0.6720692 acc: 75.78125\n",
      "Epoch: 7 batch 86 loss: 0.862447 acc: 66.40625\n",
      "Epoch: 7 batch 87 loss: 0.555283 acc: 78.125\n",
      "Epoch: 7 batch 88 loss: 0.74183905 acc: 75.0\n",
      "Epoch: 7 batch 89 loss: 0.59879 acc: 77.34375\n",
      "Epoch: 7 batch 90 loss: 0.56205153 acc: 79.6875\n",
      "Epoch: 7 batch 91 loss: 0.80652165 acc: 70.3125\n",
      "Epoch: 7 batch 92 loss: 0.7280046 acc: 78.125\n",
      "Epoch: 7 batch 93 loss: 0.61097693 acc: 78.125\n",
      "Epoch: 7 batch 94 loss: 0.7251158 acc: 73.4375\n",
      "Epoch: 7 batch 95 loss: 0.6602203 acc: 74.21875\n",
      "Epoch: 7 batch 96 loss: 0.7740826 acc: 70.3125\n",
      "Epoch: 7 batch 97 loss: 0.7217487 acc: 75.0\n",
      "Epoch: 7 batch 98 loss: 0.654032 acc: 78.90625\n",
      "Epoch: 7 batch 99 loss: 0.6271441 acc: 78.125\n",
      "Epoch: 7 batch 100 loss: 0.6670886 acc: 77.34375\n",
      "Epoch: 7 batch 101 loss: 0.82395566 acc: 70.3125\n",
      "Epoch: 7 batch 102 loss: 0.725931 acc: 76.5625\n",
      "Epoch: 7 batch 103 loss: 0.8139708 acc: 73.4375\n",
      "Epoch: 7 batch 104 loss: 0.78197104 acc: 75.78125\n",
      "Epoch: 7 batch 105 loss: 0.6885326 acc: 77.34375\n",
      "Epoch: 7 batch 106 loss: 0.8129413 acc: 71.09375\n",
      "Epoch: 7 batch 107 loss: 0.7293192 acc: 72.65625\n",
      "Epoch: 7 batch 108 loss: 0.71211785 acc: 78.125\n",
      "Epoch: 7 batch 109 loss: 0.690239 acc: 75.78125\n",
      "Epoch: 7 batch 110 loss: 0.7223722 acc: 79.6875\n",
      "Epoch: 7 batch 111 loss: 0.7508176 acc: 69.53125\n",
      "Epoch: 7 batch 112 loss: 0.6897042 acc: 72.65625\n",
      "Epoch: 7 batch 113 loss: 0.5099201 acc: 81.25\n",
      "Epoch: 7 batch 114 loss: 0.7327165 acc: 78.125\n",
      "Epoch: 7 batch 115 loss: 0.6534921 acc: 70.3125\n",
      "Epoch: 7 batch 116 loss: 0.7339404 acc: 76.5625\n",
      "Epoch: 7 batch 117 loss: 0.68699425 acc: 80.46875\n",
      "Epoch: 7 batch 118 loss: 0.8216295 acc: 71.875\n",
      "Epoch: 7 batch 119 loss: 0.6289179 acc: 74.21875\n",
      "Epoch: 7 batch 120 loss: 0.7414873 acc: 75.0\n",
      "Epoch: 7 batch 121 loss: 0.69016176 acc: 72.65625\n",
      "Epoch: 7 batch 122 loss: 0.6880235 acc: 75.78125\n",
      "Epoch: 7 batch 123 loss: 0.71565056 acc: 74.21875\n",
      "Epoch: 7 batch 124 loss: 0.67278284 acc: 81.25\n",
      "Epoch: 7 batch 125 loss: 0.6129168 acc: 81.25\n",
      "Epoch: 7 batch 126 loss: 0.63596344 acc: 78.125\n",
      "Epoch: 7 batch 127 loss: 0.6536588 acc: 78.90625\n",
      "Epoch: 7 batch 128 loss: 0.6675143 acc: 78.90625\n",
      "Epoch: 7 batch 129 loss: 0.6936041 acc: 74.21875\n",
      "Epoch: 7 batch 130 loss: 0.6361201 acc: 78.125\n",
      "Epoch: 7 batch 131 loss: 0.8411906 acc: 75.78125\n",
      "Epoch: 7 batch 132 loss: 0.56651074 acc: 83.59375\n",
      "Epoch: 7 batch 133 loss: 0.5860832 acc: 80.46875\n",
      "Epoch: 7 batch 134 loss: 0.57611156 acc: 82.8125\n",
      "Epoch: 7 batch 135 loss: 0.76260823 acc: 75.78125\n",
      "Epoch: 7 batch 136 loss: 0.7702434 acc: 73.4375\n",
      "Epoch: 7 batch 137 loss: 0.59215146 acc: 79.6875\n",
      "Epoch: 7 batch 138 loss: 0.8172956 acc: 73.4375\n",
      "Epoch: 7 batch 139 loss: 0.5738116 acc: 77.34375\n",
      "Epoch: 7 batch 140 loss: 0.70446694 acc: 73.4375\n",
      "Epoch: 7 batch 141 loss: 0.6476472 acc: 78.90625\n",
      "Epoch: 7 batch 142 loss: 0.8185896 acc: 72.65625\n",
      "Epoch: 7 batch 143 loss: 0.53174853 acc: 84.375\n",
      "Epoch: 7 batch 144 loss: 0.8758217 acc: 69.53125\n",
      "Epoch: 7 batch 145 loss: 0.75272375 acc: 73.4375\n",
      "Epoch: 7 batch 146 loss: 0.7054089 acc: 72.65625\n",
      "Epoch: 7 batch 147 loss: 0.5584454 acc: 82.8125\n",
      "Epoch: 7 batch 148 loss: 0.6499097 acc: 78.90625\n",
      "Epoch: 7 batch 149 loss: 0.75988793 acc: 73.4375\n",
      "Epoch: 7 batch 150 loss: 0.828138 acc: 71.09375\n",
      "Epoch: 7 batch 151 loss: 0.6195004 acc: 77.34375\n",
      "Epoch: 7 batch 152 loss: 0.654366 acc: 76.5625\n",
      "Epoch: 7 batch 153 loss: 0.6738064 acc: 77.34375\n",
      "Epoch: 7 batch 154 loss: 0.68322176 acc: 79.6875\n",
      "Epoch: 7 batch 155 loss: 0.7099328 acc: 72.65625\n",
      "Epoch: 7 batch 156 loss: 0.62287915 acc: 80.46875\n",
      "Epoch: 7 batch 157 loss: 0.71201795 acc: 75.78125\n",
      "Epoch: 7 batch 158 loss: 0.71378255 acc: 75.0\n",
      "Epoch: 7 batch 159 loss: 0.5960016 acc: 79.6875\n",
      "Epoch: 7 batch 160 loss: 0.70409256 acc: 76.5625\n",
      "Epoch: 7 batch 161 loss: 0.6123048 acc: 76.5625\n",
      "Epoch: 7 batch 162 loss: 0.64185536 acc: 78.125\n",
      "Epoch: 7 batch 163 loss: 0.6412935 acc: 78.90625\n",
      "Epoch: 7 batch 164 loss: 0.68528074 acc: 75.78125\n",
      "Epoch: 7 batch 165 loss: 0.6067937 acc: 77.34375\n",
      "Epoch: 7 batch 166 loss: 0.6733741 acc: 72.65625\n",
      "Epoch: 7 batch 167 loss: 0.75876844 acc: 75.0\n",
      "Epoch: 7 batch 168 loss: 0.64321953 acc: 77.34375\n",
      "Epoch: 7 batch 169 loss: 0.67592424 acc: 79.6875\n",
      "Epoch: 7 batch 170 loss: 0.61410606 acc: 80.46875\n",
      "Epoch: 7 batch 171 loss: 0.74614716 acc: 76.5625\n",
      "Epoch: 7 batch 172 loss: 0.6808032 acc: 75.78125\n",
      "Epoch: 7 batch 173 loss: 0.6136477 acc: 74.21875\n",
      "Epoch: 7 batch 174 loss: 0.7182193 acc: 73.4375\n",
      "Epoch: 7 batch 175 loss: 0.742694 acc: 68.75\n",
      "Epoch: 7 batch 176 loss: 0.5566488 acc: 82.8125\n",
      "Epoch: 7 batch 177 loss: 0.7376952 acc: 78.125\n",
      "Epoch: 7 batch 178 loss: 0.7172486 acc: 74.21875\n",
      "Epoch: 7 batch 179 loss: 0.9992921 acc: 67.96875\n",
      "Epoch: 7 batch 180 loss: 0.713052 acc: 76.5625\n",
      "Epoch: 7 batch 181 loss: 0.7126442 acc: 75.78125\n",
      "Epoch: 7 batch 182 loss: 0.6034728 acc: 80.46875\n",
      "Epoch: 7 batch 183 loss: 0.78741074 acc: 72.65625\n",
      "Epoch: 7 batch 184 loss: 0.64514977 acc: 71.875\n",
      "Epoch: 7 batch 185 loss: 0.50813955 acc: 82.8125\n",
      "Epoch: 7 batch 186 loss: 0.63080466 acc: 81.25\n",
      "Epoch: 7 batch 187 loss: 0.6572904 acc: 77.34375\n",
      "Epoch: 7 batch 188 loss: 0.6295595 acc: 78.90625\n",
      "Epoch: 7 batch 189 loss: 0.76884514 acc: 74.21875\n",
      "Epoch: 7 batch 190 loss: 0.75945616 acc: 75.78125\n",
      "Epoch: 7 batch 191 loss: 0.6956273 acc: 77.34375\n",
      "Epoch: 7 batch 192 loss: 0.56416595 acc: 78.90625\n",
      "Epoch: 7 batch 193 loss: 0.69873506 acc: 70.3125\n",
      "Epoch: 7 batch 194 loss: 0.695544 acc: 71.875\n",
      "Epoch: 7 batch 195 loss: 0.5832661 acc: 75.0\n",
      "Epoch: 7 batch 196 loss: 0.57118475 acc: 78.90625\n",
      "Epoch: 7 batch 197 loss: 0.5980138 acc: 78.90625\n",
      "Epoch: 7 batch 198 loss: 0.5827736 acc: 79.6875\n",
      "Epoch: 7 batch 199 loss: 0.7486041 acc: 79.6875\n",
      "Epoch: 7 batch 200 loss: 0.79972094 acc: 68.75\n",
      "Epoch: 7 batch 201 loss: 0.5666333 acc: 78.125\n",
      "Epoch: 7 batch 202 loss: 0.60266924 acc: 78.125\n",
      "Epoch: 7 batch 203 loss: 0.6049119 acc: 78.90625\n",
      "Epoch: 7 batch 204 loss: 0.69615406 acc: 75.78125\n",
      "Epoch: 7 batch 205 loss: 0.4933169 acc: 82.03125\n",
      "Epoch: 7 batch 206 loss: 0.6864923 acc: 74.21875\n",
      "Epoch: 7 batch 207 loss: 0.55797064 acc: 81.25\n",
      "Epoch: 7 batch 208 loss: 0.75899076 acc: 74.21875\n",
      "Epoch: 7 batch 209 loss: 0.67679745 acc: 72.65625\n",
      "Epoch: 7 batch 210 loss: 0.7093176 acc: 77.34375\n",
      "Epoch: 7 batch 211 loss: 0.79676163 acc: 69.53125\n",
      "Epoch: 7 batch 212 loss: 0.7207864 acc: 76.5625\n",
      "Epoch: 7 batch 213 loss: 0.5472961 acc: 80.46875\n",
      "Epoch: 7 batch 214 loss: 0.64073443 acc: 80.46875\n",
      "Epoch: 7 batch 215 loss: 0.6081597 acc: 78.125\n",
      "Epoch: 7 batch 216 loss: 0.48493415 acc: 85.9375\n",
      "Epoch: 7 batch 217 loss: 0.6632974 acc: 76.5625\n",
      "Epoch: 7 batch 218 loss: 0.56678224 acc: 81.25\n",
      "Epoch: 7 batch 219 loss: 0.60279787 acc: 77.34375\n",
      "Epoch: 7 batch 220 loss: 0.59190696 acc: 78.125\n",
      "Epoch: 7 batch 221 loss: 0.6619467 acc: 75.0\n",
      "Epoch: 7 batch 222 loss: 0.6815337 acc: 78.125\n",
      "Epoch: 7 batch 223 loss: 0.7136545 acc: 72.65625\n",
      "Epoch: 7 batch 224 loss: 0.61136913 acc: 74.21875\n",
      "Epoch: 7 batch 225 loss: 0.54695106 acc: 84.375\n",
      "Epoch: 7 batch 226 loss: 0.5462658 acc: 78.90625\n",
      "Epoch: 7 batch 227 loss: 0.7723682 acc: 75.0\n",
      "Epoch: 7 batch 228 loss: 0.6585223 acc: 78.90625\n",
      "Epoch: 7 batch 229 loss: 0.5526595 acc: 77.34375\n",
      "Epoch: 7 batch 230 loss: 0.585039 acc: 76.5625\n",
      "Epoch: 7 batch 231 loss: 0.5792136 acc: 80.46875\n",
      "Epoch: 7 batch 232 loss: 0.53104687 acc: 82.03125\n",
      "Epoch: 7 batch 233 loss: 0.5038227 acc: 83.59375\n",
      "Epoch: 7 batch 234 loss: 0.5031352 acc: 82.8125\n",
      "Epoch: 7 batch 235 loss: 0.6680956 acc: 74.21875\n",
      "Epoch: 7 batch 236 loss: 0.6187385 acc: 77.34375\n",
      "Epoch: 7 batch 237 loss: 0.5147715 acc: 80.46875\n",
      "Epoch: 7 batch 238 loss: 0.6589215 acc: 81.25\n",
      "Epoch: 7 batch 239 loss: 0.7151592 acc: 75.78125\n",
      "Epoch: 7 batch 240 loss: 0.8389134 acc: 71.09375\n",
      "Epoch: 7 batch 241 loss: 0.6589912 acc: 81.25\n",
      "Epoch: 7 batch 242 loss: 0.6350692 acc: 74.21875\n",
      "Epoch: 7 batch 243 loss: 0.6424316 acc: 78.125\n",
      "Epoch: 7 batch 244 loss: 0.6615116 acc: 81.25\n",
      "Epoch: 7 batch 245 loss: 0.8202349 acc: 70.3125\n",
      "Epoch: 7 batch 246 loss: 0.7982495 acc: 71.875\n",
      "Epoch: 7 batch 247 loss: 0.65878433 acc: 76.5625\n",
      "Epoch: 7 batch 248 loss: 0.5584798 acc: 82.8125\n",
      "Epoch: 7 batch 249 loss: 0.7551582 acc: 78.90625\n",
      "Epoch: 7 batch 250 loss: 0.6287331 acc: 74.21875\n",
      "Epoch: 7 batch 251 loss: 0.7485851 acc: 78.90625\n",
      "Epoch: 7 batch 252 loss: 0.6801637 acc: 78.90625\n",
      "Epoch: 7 batch 253 loss: 0.6956426 acc: 75.78125\n",
      "Epoch: 7 batch 254 loss: 0.68844277 acc: 75.78125\n",
      "Epoch: 7 batch 255 loss: 0.67484677 acc: 73.4375\n",
      "Epoch: 7 batch 256 loss: 0.7843821 acc: 77.34375\n",
      "Epoch: 7 batch 257 loss: 0.7476435 acc: 71.875\n",
      "Epoch: 7 batch 258 loss: 0.7276622 acc: 77.34375\n",
      "Epoch: 7 batch 259 loss: 0.6668683 acc: 78.125\n",
      "Epoch: 7 batch 260 loss: 0.7271795 acc: 75.78125\n",
      "Epoch: 7 batch 261 loss: 0.80196 acc: 71.875\n",
      "Epoch: 7 batch 262 loss: 0.81629425 acc: 65.625\n",
      "Epoch: 7 batch 263 loss: 0.6221944 acc: 77.34375\n",
      "Epoch: 7 batch 264 loss: 0.70088404 acc: 75.0\n",
      "Epoch: 7 batch 265 loss: 0.6612051 acc: 80.46875\n",
      "Epoch: 7 batch 266 loss: 0.5998888 acc: 75.78125\n",
      "Epoch: 7 batch 267 loss: 0.6811903 acc: 73.4375\n",
      "Epoch: 7 batch 268 loss: 0.7337444 acc: 75.78125\n",
      "Epoch: 7 batch 269 loss: 0.6343037 acc: 76.5625\n",
      "Epoch: 7 batch 270 loss: 0.70248413 acc: 74.21875\n",
      "Epoch: 7 batch 271 loss: 0.709794 acc: 78.125\n",
      "Epoch: 7 batch 272 loss: 0.45860735 acc: 85.9375\n",
      "Epoch: 7 batch 273 loss: 0.7575116 acc: 72.65625\n",
      "Epoch: 7 batch 274 loss: 0.829649 acc: 73.4375\n",
      "Epoch: 7 batch 275 loss: 0.703098 acc: 75.78125\n",
      "Epoch: 7 batch 276 loss: 0.7105519 acc: 75.0\n",
      "Epoch: 7 batch 277 loss: 0.64829504 acc: 76.5625\n",
      "Epoch: 7 batch 278 loss: 0.7555648 acc: 76.5625\n",
      "Epoch: 7 batch 279 loss: 0.6246504 acc: 78.125\n",
      "Epoch: 7 batch 280 loss: 0.56756115 acc: 78.90625\n",
      "Epoch: 7 batch 281 loss: 0.72347736 acc: 72.65625\n",
      "Epoch: 7 batch 282 loss: 0.7202352 acc: 74.21875\n",
      "Epoch: 7 batch 283 loss: 0.59836996 acc: 84.375\n",
      "Epoch: 7 batch 284 loss: 0.5984178 acc: 78.125\n",
      "Epoch: 7 batch 285 loss: 0.55392087 acc: 82.8125\n",
      "Epoch: 7 batch 286 loss: 0.69990134 acc: 72.65625\n",
      "Epoch: 7 batch 287 loss: 0.63013357 acc: 78.90625\n",
      "Epoch: 7 batch 288 loss: 0.7209505 acc: 71.09375\n",
      "Epoch: 7 batch 289 loss: 0.5573958 acc: 84.375\n",
      "Epoch: 7 batch 290 loss: 0.69876504 acc: 78.90625\n",
      "Epoch: 7 batch 291 loss: 0.8717956 acc: 70.3125\n",
      "Epoch: 7 batch 292 loss: 0.6948309 acc: 80.46875\n",
      "Epoch: 7 batch 293 loss: 0.63076454 acc: 78.90625\n",
      "Epoch: 7 batch 294 loss: 0.7151385 acc: 76.5625\n",
      "Epoch: 7 batch 295 loss: 0.61947453 acc: 77.34375\n",
      "Epoch: 7 batch 296 loss: 0.76048017 acc: 70.3125\n",
      "Epoch: 7 batch 297 loss: 0.75686646 acc: 70.3125\n",
      "Epoch: 7 batch 298 loss: 0.6603875 acc: 80.46875\n",
      "Epoch: 7 batch 299 loss: 0.72634524 acc: 71.875\n",
      "Epoch: 7 batch 300 loss: 0.7144896 acc: 75.0\n",
      "Epoch: 7 batch 301 loss: 0.6737294 acc: 78.125\n",
      "Epoch: 7 batch 302 loss: 0.5414094 acc: 77.34375\n",
      "Epoch: 7 batch 303 loss: 0.57801545 acc: 80.46875\n",
      "Epoch: 7 batch 304 loss: 0.6217242 acc: 74.21875\n",
      "Epoch: 7 batch 305 loss: 0.60692453 acc: 75.0\n",
      "Epoch: 7 batch 306 loss: 0.6933675 acc: 76.5625\n",
      "Epoch: 7 batch 307 loss: 0.6515927 acc: 76.5625\n",
      "Epoch: 7 batch 308 loss: 0.65328246 acc: 79.6875\n",
      "Epoch: 7 batch 309 loss: 0.7117729 acc: 74.21875\n",
      "Epoch: 7 batch 310 loss: 0.82074183 acc: 74.21875\n",
      "Epoch: 7 batch 311 loss: 0.67455244 acc: 74.21875\n",
      "Epoch: 7 batch 312 loss: 0.66224635 acc: 75.78125\n",
      "Epoch: 7 batch 313 loss: 0.65000325 acc: 76.5625\n",
      "Epoch: 7 batch 314 loss: 0.70320916 acc: 76.5625\n",
      "Epoch: 7 batch 315 loss: 0.779052 acc: 75.0\n",
      "Epoch: 7 batch 316 loss: 0.6442641 acc: 78.90625\n",
      "Epoch: 7 batch 317 loss: 0.734754 acc: 75.0\n",
      "Epoch: 7 batch 318 loss: 0.5648644 acc: 82.03125\n",
      "Epoch: 7 batch 319 loss: 0.85979885 acc: 71.09375\n",
      "Epoch: 7 batch 320 loss: 0.78158474 acc: 75.0\n",
      "Epoch: 7 batch 321 loss: 0.57476425 acc: 82.03125\n",
      "Epoch: 7 batch 322 loss: 0.629842 acc: 78.125\n",
      "Epoch: 7 batch 323 loss: 0.62902236 acc: 78.90625\n",
      "Epoch: 7 batch 324 loss: 0.6742611 acc: 68.75\n",
      "Epoch: 7 batch 325 loss: 0.6653117 acc: 76.5625\n",
      "Epoch: 7 batch 326 loss: 0.70529974 acc: 75.78125\n",
      "Epoch: 7 batch 327 loss: 0.81491977 acc: 73.4375\n",
      "Epoch: 7 batch 328 loss: 0.6233012 acc: 83.59375\n",
      "Epoch: 7 batch 329 loss: 0.65299445 acc: 77.34375\n",
      "Epoch: 7 batch 330 loss: 0.4761014 acc: 82.03125\n",
      "Epoch: 7 batch 331 loss: 0.5284829 acc: 82.8125\n",
      "Epoch: 7 batch 332 loss: 0.83085203 acc: 71.875\n",
      "Epoch: 7 batch 333 loss: 0.6429384 acc: 72.65625\n",
      "Epoch: 7 batch 334 loss: 0.5637591 acc: 78.90625\n",
      "Epoch: 7 batch 335 loss: 0.7868763 acc: 70.3125\n",
      "Epoch: 7 batch 336 loss: 0.599554 acc: 75.78125\n",
      "Epoch: 7 batch 337 loss: 0.689518 acc: 73.4375\n",
      "Epoch: 7 batch 338 loss: 0.7853864 acc: 71.09375\n",
      "Epoch: 7 batch 339 loss: 0.6073785 acc: 81.25\n",
      "Epoch: 7 batch 340 loss: 0.5932819 acc: 78.90625\n",
      "Epoch: 7 batch 341 loss: 0.6124712 acc: 76.5625\n",
      "Epoch: 7 batch 342 loss: 0.8301822 acc: 69.53125\n",
      "Epoch: 7 batch 343 loss: 0.5527128 acc: 80.46875\n",
      "Epoch: 7 batch 344 loss: 0.5799992 acc: 79.6875\n",
      "Epoch: 7 batch 345 loss: 0.66341734 acc: 75.78125\n",
      "Epoch: 7 batch 346 loss: 0.65398014 acc: 83.59375\n",
      "Epoch: 7 batch 347 loss: 0.61713624 acc: 75.78125\n",
      "Epoch: 7 batch 348 loss: 0.6033912 acc: 78.90625\n",
      "Epoch: 7 batch 349 loss: 0.73086804 acc: 71.09375\n",
      "Epoch: 7 batch 350 loss: 0.71901613 acc: 75.0\n",
      "Epoch: 7 batch 351 loss: 0.5927708 acc: 78.90625\n",
      "Epoch: 7 batch 352 loss: 0.56369907 acc: 81.25\n",
      "Epoch: 7 batch 353 loss: 0.6459508 acc: 79.6875\n",
      "Epoch: 7 batch 354 loss: 0.4864027 acc: 82.03125\n",
      "Epoch: 7 batch 355 loss: 0.6839234 acc: 75.78125\n",
      "Epoch: 7 batch 356 loss: 0.686059 acc: 74.21875\n",
      "Epoch: 7 batch 357 loss: 0.51852036 acc: 82.03125\n",
      "Epoch: 7 batch 358 loss: 0.6483629 acc: 78.125\n",
      "Epoch: 7 batch 359 loss: 0.6448926 acc: 83.59375\n",
      "Epoch: 7 batch 360 loss: 0.6295658 acc: 76.5625\n",
      "Epoch: 7 batch 361 loss: 0.8197774 acc: 73.4375\n",
      "Epoch: 7 batch 362 loss: 0.6447093 acc: 76.5625\n",
      "Epoch: 7 batch 363 loss: 0.9224225 acc: 66.40625\n",
      "Epoch: 7 batch 364 loss: 0.61050975 acc: 76.5625\n",
      "Epoch: 7 batch 365 loss: 0.5638178 acc: 77.34375\n",
      "Epoch: 7 batch 366 loss: 0.4810056 acc: 85.9375\n",
      "Epoch: 7 batch 367 loss: 0.78974426 acc: 70.3125\n",
      "Epoch: 7 batch 368 loss: 0.63621414 acc: 75.0\n",
      "Epoch: 7 batch 369 loss: 0.51917624 acc: 79.6875\n",
      "Epoch: 7 batch 370 loss: 0.5271485 acc: 81.25\n",
      "Epoch: 7 batch 371 loss: 0.53128767 acc: 80.46875\n",
      "Epoch: 7 batch 372 loss: 0.57343763 acc: 80.46875\n",
      "Epoch: 7 batch 373 loss: 0.6902808 acc: 74.21875\n",
      "Epoch: 7 batch 374 loss: 0.6512815 acc: 75.78125\n",
      "Epoch: 7 batch 375 loss: 0.7593416 acc: 73.4375\n",
      "Epoch: 7 batch 376 loss: 0.6503647 acc: 85.15625\n",
      "Epoch: 7 batch 377 loss: 0.4941325 acc: 85.15625\n",
      "Epoch: 7 batch 378 loss: 0.59591186 acc: 81.25\n",
      "Epoch: 7 batch 379 loss: 0.7601636 acc: 73.4375\n",
      "Epoch: 7 batch 380 loss: 0.65982527 acc: 71.875\n",
      "Epoch: 7 batch 381 loss: 0.6332209 acc: 75.78125\n",
      "Epoch: 7 batch 382 loss: 0.778114 acc: 73.4375\n",
      "Epoch: 7 batch 383 loss: 0.76364845 acc: 71.875\n",
      "Epoch: 7 batch 384 loss: 0.48767316 acc: 88.28125\n",
      "Epoch: 7 batch 385 loss: 0.7645079 acc: 71.09375\n",
      "Epoch: 7 batch 386 loss: 0.59481025 acc: 82.03125\n",
      "Epoch: 7 batch 387 loss: 0.63210756 acc: 78.90625\n",
      "Epoch: 7 batch 388 loss: 0.70891154 acc: 72.65625\n",
      "Epoch: 7 batch 389 loss: 0.52914673 acc: 79.6875\n",
      "After epoch 7 test loss 0.8408783671855926 test accuracy 0.7134000062942505\n",
      "Epoch: 8 batch 0 loss: 0.60979766 acc: 71.41587734222412\n",
      "Epoch: 8 batch 1 loss: 0.6625214 acc: 76.5625\n",
      "Epoch: 8 batch 2 loss: 0.7064384 acc: 72.65625\n",
      "Epoch: 8 batch 3 loss: 0.6106059 acc: 78.125\n",
      "Epoch: 8 batch 4 loss: 0.54582274 acc: 81.25\n",
      "Epoch: 8 batch 5 loss: 0.8103968 acc: 72.65625\n",
      "Epoch: 8 batch 6 loss: 0.79051745 acc: 69.53125\n",
      "Epoch: 8 batch 7 loss: 0.6450249 acc: 80.46875\n",
      "Epoch: 8 batch 8 loss: 0.7113435 acc: 75.0\n",
      "Epoch: 8 batch 9 loss: 0.5632838 acc: 80.46875\n",
      "Epoch: 8 batch 10 loss: 0.6972923 acc: 78.125\n",
      "Epoch: 8 batch 11 loss: 0.6547097 acc: 81.25\n",
      "Epoch: 8 batch 12 loss: 0.650645 acc: 74.21875\n",
      "Epoch: 8 batch 13 loss: 0.5604842 acc: 82.8125\n",
      "Epoch: 8 batch 14 loss: 0.53990257 acc: 81.25\n",
      "Epoch: 8 batch 15 loss: 0.7428681 acc: 73.4375\n",
      "Epoch: 8 batch 16 loss: 0.70260155 acc: 77.34375\n",
      "Epoch: 8 batch 17 loss: 0.7267615 acc: 76.5625\n",
      "Epoch: 8 batch 18 loss: 0.56731373 acc: 80.46875\n",
      "Epoch: 8 batch 19 loss: 0.5606055 acc: 82.8125\n",
      "Epoch: 8 batch 20 loss: 0.46798524 acc: 82.03125\n",
      "Epoch: 8 batch 21 loss: 0.6543436 acc: 77.34375\n",
      "Epoch: 8 batch 22 loss: 0.60692835 acc: 77.34375\n",
      "Epoch: 8 batch 23 loss: 0.7531257 acc: 74.21875\n",
      "Epoch: 8 batch 24 loss: 0.6604656 acc: 78.125\n",
      "Epoch: 8 batch 25 loss: 0.6675128 acc: 70.3125\n",
      "Epoch: 8 batch 26 loss: 0.70679635 acc: 82.8125\n",
      "Epoch: 8 batch 27 loss: 0.57677466 acc: 81.25\n",
      "Epoch: 8 batch 28 loss: 0.54691374 acc: 79.6875\n",
      "Epoch: 8 batch 29 loss: 0.6049029 acc: 75.78125\n",
      "Epoch: 8 batch 30 loss: 0.6740358 acc: 74.21875\n",
      "Epoch: 8 batch 31 loss: 0.6257432 acc: 78.125\n",
      "Epoch: 8 batch 32 loss: 0.6743929 acc: 76.5625\n",
      "Epoch: 8 batch 33 loss: 0.5634191 acc: 78.90625\n",
      "Epoch: 8 batch 34 loss: 0.6195713 acc: 78.90625\n",
      "Epoch: 8 batch 35 loss: 0.6900367 acc: 78.125\n",
      "Epoch: 8 batch 36 loss: 0.569893 acc: 81.25\n",
      "Epoch: 8 batch 37 loss: 0.6798392 acc: 74.21875\n",
      "Epoch: 8 batch 38 loss: 0.78512585 acc: 77.34375\n",
      "Epoch: 8 batch 39 loss: 0.7420273 acc: 73.4375\n",
      "Epoch: 8 batch 40 loss: 0.59447145 acc: 78.90625\n",
      "Epoch: 8 batch 41 loss: 0.69993055 acc: 78.125\n",
      "Epoch: 8 batch 42 loss: 0.7393895 acc: 74.21875\n",
      "Epoch: 8 batch 43 loss: 0.6860206 acc: 77.34375\n",
      "Epoch: 8 batch 44 loss: 0.67327726 acc: 77.34375\n",
      "Epoch: 8 batch 45 loss: 0.5849631 acc: 78.125\n",
      "Epoch: 8 batch 46 loss: 0.5346742 acc: 78.125\n",
      "Epoch: 8 batch 47 loss: 0.61406577 acc: 75.78125\n",
      "Epoch: 8 batch 48 loss: 0.55946505 acc: 79.6875\n",
      "Epoch: 8 batch 49 loss: 0.487401 acc: 83.59375\n",
      "Epoch: 8 batch 50 loss: 0.6236303 acc: 75.0\n",
      "Epoch: 8 batch 51 loss: 0.6005105 acc: 78.90625\n",
      "Epoch: 8 batch 52 loss: 0.65706635 acc: 77.34375\n",
      "Epoch: 8 batch 53 loss: 0.88801867 acc: 78.125\n",
      "Epoch: 8 batch 54 loss: 0.59759843 acc: 75.78125\n",
      "Epoch: 8 batch 55 loss: 0.6427028 acc: 75.78125\n",
      "Epoch: 8 batch 56 loss: 0.5839895 acc: 79.6875\n",
      "Epoch: 8 batch 57 loss: 0.5676336 acc: 81.25\n",
      "Epoch: 8 batch 58 loss: 0.69180524 acc: 80.46875\n",
      "Epoch: 8 batch 59 loss: 0.77291644 acc: 68.75\n",
      "Epoch: 8 batch 60 loss: 0.59105635 acc: 78.90625\n",
      "Epoch: 8 batch 61 loss: 0.6812296 acc: 75.78125\n",
      "Epoch: 8 batch 62 loss: 0.64130485 acc: 77.34375\n",
      "Epoch: 8 batch 63 loss: 0.50585896 acc: 84.375\n",
      "Epoch: 8 batch 64 loss: 0.62829244 acc: 77.34375\n",
      "Epoch: 8 batch 65 loss: 0.61613303 acc: 75.0\n",
      "Epoch: 8 batch 66 loss: 0.6097511 acc: 78.125\n",
      "Epoch: 8 batch 67 loss: 0.62539864 acc: 76.5625\n",
      "Epoch: 8 batch 68 loss: 0.7523072 acc: 70.3125\n",
      "Epoch: 8 batch 69 loss: 0.8476317 acc: 75.0\n",
      "Epoch: 8 batch 70 loss: 0.8259491 acc: 69.53125\n",
      "Epoch: 8 batch 71 loss: 0.62545264 acc: 80.46875\n",
      "Epoch: 8 batch 72 loss: 0.7180549 acc: 74.21875\n",
      "Epoch: 8 batch 73 loss: 0.65862787 acc: 76.5625\n",
      "Epoch: 8 batch 74 loss: 0.52265 acc: 80.46875\n",
      "Epoch: 8 batch 75 loss: 0.66784173 acc: 77.34375\n",
      "Epoch: 8 batch 76 loss: 0.67589873 acc: 75.78125\n",
      "Epoch: 8 batch 77 loss: 0.5764144 acc: 78.125\n",
      "Epoch: 8 batch 78 loss: 0.64248496 acc: 78.125\n",
      "Epoch: 8 batch 79 loss: 0.6756471 acc: 76.5625\n",
      "Epoch: 8 batch 80 loss: 0.55425155 acc: 81.25\n",
      "Epoch: 8 batch 81 loss: 0.9170679 acc: 74.21875\n",
      "Epoch: 8 batch 82 loss: 0.54424536 acc: 78.90625\n",
      "Epoch: 8 batch 83 loss: 0.67987794 acc: 77.34375\n",
      "Epoch: 8 batch 84 loss: 0.65404546 acc: 77.34375\n",
      "Epoch: 8 batch 85 loss: 0.6933718 acc: 73.4375\n",
      "Epoch: 8 batch 86 loss: 0.82171607 acc: 66.40625\n",
      "Epoch: 8 batch 87 loss: 0.47204503 acc: 82.8125\n",
      "Epoch: 8 batch 88 loss: 0.6122841 acc: 79.6875\n",
      "Epoch: 8 batch 89 loss: 0.6109637 acc: 79.6875\n",
      "Epoch: 8 batch 90 loss: 0.6188687 acc: 75.78125\n",
      "Epoch: 8 batch 91 loss: 0.6406485 acc: 75.0\n",
      "Epoch: 8 batch 92 loss: 0.7242042 acc: 76.5625\n",
      "Epoch: 8 batch 93 loss: 0.4459644 acc: 83.59375\n",
      "Epoch: 8 batch 94 loss: 0.76729167 acc: 70.3125\n",
      "Epoch: 8 batch 95 loss: 0.54099274 acc: 82.03125\n",
      "Epoch: 8 batch 96 loss: 0.6155148 acc: 81.25\n",
      "Epoch: 8 batch 97 loss: 0.66537964 acc: 76.5625\n",
      "Epoch: 8 batch 98 loss: 0.6258344 acc: 79.6875\n",
      "Epoch: 8 batch 99 loss: 0.62302405 acc: 81.25\n",
      "Epoch: 8 batch 100 loss: 0.5872768 acc: 79.6875\n",
      "Epoch: 8 batch 101 loss: 0.6576464 acc: 75.0\n",
      "Epoch: 8 batch 102 loss: 0.56264937 acc: 78.90625\n",
      "Epoch: 8 batch 103 loss: 0.725633 acc: 77.34375\n",
      "Epoch: 8 batch 104 loss: 0.6678982 acc: 70.3125\n",
      "Epoch: 8 batch 105 loss: 0.51927733 acc: 78.125\n",
      "Epoch: 8 batch 106 loss: 0.6624042 acc: 77.34375\n",
      "Epoch: 8 batch 107 loss: 0.7445195 acc: 72.65625\n",
      "Epoch: 8 batch 108 loss: 0.6658515 acc: 76.5625\n",
      "Epoch: 8 batch 109 loss: 0.6185862 acc: 80.46875\n",
      "Epoch: 8 batch 110 loss: 0.62472224 acc: 73.4375\n",
      "Epoch: 8 batch 111 loss: 0.740495 acc: 74.21875\n",
      "Epoch: 8 batch 112 loss: 0.64963543 acc: 78.90625\n",
      "Epoch: 8 batch 113 loss: 0.5071187 acc: 82.03125\n",
      "Epoch: 8 batch 114 loss: 0.81414413 acc: 71.875\n",
      "Epoch: 8 batch 115 loss: 0.5128537 acc: 82.03125\n",
      "Epoch: 8 batch 116 loss: 0.6926144 acc: 71.875\n",
      "Epoch: 8 batch 117 loss: 0.59594053 acc: 81.25\n",
      "Epoch: 8 batch 118 loss: 0.66648793 acc: 76.5625\n",
      "Epoch: 8 batch 119 loss: 0.5808703 acc: 78.125\n",
      "Epoch: 8 batch 120 loss: 0.61994326 acc: 78.90625\n",
      "Epoch: 8 batch 121 loss: 0.7286736 acc: 73.4375\n",
      "Epoch: 8 batch 122 loss: 0.59783316 acc: 76.5625\n",
      "Epoch: 8 batch 123 loss: 0.7069689 acc: 74.21875\n",
      "Epoch: 8 batch 124 loss: 0.6094575 acc: 76.5625\n",
      "Epoch: 8 batch 125 loss: 0.5427529 acc: 80.46875\n",
      "Epoch: 8 batch 126 loss: 0.68184114 acc: 78.90625\n",
      "Epoch: 8 batch 127 loss: 0.5747281 acc: 80.46875\n",
      "Epoch: 8 batch 128 loss: 0.49485764 acc: 82.03125\n",
      "Epoch: 8 batch 129 loss: 0.63216543 acc: 79.6875\n",
      "Epoch: 8 batch 130 loss: 0.6046022 acc: 79.6875\n",
      "Epoch: 8 batch 131 loss: 0.59380525 acc: 83.59375\n",
      "Epoch: 8 batch 132 loss: 0.49381077 acc: 80.46875\n",
      "Epoch: 8 batch 133 loss: 0.63744146 acc: 81.25\n",
      "Epoch: 8 batch 134 loss: 0.48196483 acc: 85.15625\n",
      "Epoch: 8 batch 135 loss: 0.67273754 acc: 76.5625\n",
      "Epoch: 8 batch 136 loss: 0.6428263 acc: 77.34375\n",
      "Epoch: 8 batch 137 loss: 0.5510549 acc: 83.59375\n",
      "Epoch: 8 batch 138 loss: 0.886225 acc: 67.96875\n",
      "Epoch: 8 batch 139 loss: 0.59932005 acc: 79.6875\n",
      "Epoch: 8 batch 140 loss: 0.68923783 acc: 75.0\n",
      "Epoch: 8 batch 141 loss: 0.58775145 acc: 79.6875\n",
      "Epoch: 8 batch 142 loss: 0.68002576 acc: 82.03125\n",
      "Epoch: 8 batch 143 loss: 0.5129445 acc: 82.8125\n",
      "Epoch: 8 batch 144 loss: 0.77964324 acc: 71.09375\n",
      "Epoch: 8 batch 145 loss: 0.69326234 acc: 77.34375\n",
      "Epoch: 8 batch 146 loss: 0.6646903 acc: 78.125\n",
      "Epoch: 8 batch 147 loss: 0.5479114 acc: 83.59375\n",
      "Epoch: 8 batch 148 loss: 0.70858055 acc: 74.21875\n",
      "Epoch: 8 batch 149 loss: 0.6862297 acc: 74.21875\n",
      "Epoch: 8 batch 150 loss: 0.6188952 acc: 80.46875\n",
      "Epoch: 8 batch 151 loss: 0.54373354 acc: 80.46875\n",
      "Epoch: 8 batch 152 loss: 0.6896883 acc: 79.6875\n",
      "Epoch: 8 batch 153 loss: 0.8135096 acc: 75.0\n",
      "Epoch: 8 batch 154 loss: 0.7199297 acc: 76.5625\n",
      "Epoch: 8 batch 155 loss: 0.6271032 acc: 81.25\n",
      "Epoch: 8 batch 156 loss: 0.6647352 acc: 78.90625\n",
      "Epoch: 8 batch 157 loss: 0.6777332 acc: 75.0\n",
      "Epoch: 8 batch 158 loss: 0.73412204 acc: 73.4375\n",
      "Epoch: 8 batch 159 loss: 0.61074054 acc: 79.6875\n",
      "Epoch: 8 batch 160 loss: 0.6224546 acc: 78.90625\n",
      "Epoch: 8 batch 161 loss: 0.6512555 acc: 77.34375\n",
      "Epoch: 8 batch 162 loss: 0.6464871 acc: 79.6875\n",
      "Epoch: 8 batch 163 loss: 0.5104083 acc: 81.25\n",
      "Epoch: 8 batch 164 loss: 0.60366565 acc: 80.46875\n",
      "Epoch: 8 batch 165 loss: 0.7159633 acc: 72.65625\n",
      "Epoch: 8 batch 166 loss: 0.53830135 acc: 80.46875\n",
      "Epoch: 8 batch 167 loss: 0.60040414 acc: 80.46875\n",
      "Epoch: 8 batch 168 loss: 0.5740837 acc: 79.6875\n",
      "Epoch: 8 batch 169 loss: 0.75204706 acc: 74.21875\n",
      "Epoch: 8 batch 170 loss: 0.60384774 acc: 82.03125\n",
      "Epoch: 8 batch 171 loss: 0.72935957 acc: 76.5625\n",
      "Epoch: 8 batch 172 loss: 0.585703 acc: 77.34375\n",
      "Epoch: 8 batch 173 loss: 0.6139531 acc: 75.0\n",
      "Epoch: 8 batch 174 loss: 0.7055098 acc: 72.65625\n",
      "Epoch: 8 batch 175 loss: 0.7178013 acc: 72.65625\n",
      "Epoch: 8 batch 176 loss: 0.5250698 acc: 79.6875\n",
      "Epoch: 8 batch 177 loss: 0.57992923 acc: 80.46875\n",
      "Epoch: 8 batch 178 loss: 0.73471093 acc: 76.5625\n",
      "Epoch: 8 batch 179 loss: 0.70945525 acc: 74.21875\n",
      "Epoch: 8 batch 180 loss: 0.5621093 acc: 82.03125\n",
      "Epoch: 8 batch 181 loss: 0.73093784 acc: 73.4375\n",
      "Epoch: 8 batch 182 loss: 0.5343432 acc: 85.15625\n",
      "Epoch: 8 batch 183 loss: 0.7468546 acc: 77.34375\n",
      "Epoch: 8 batch 184 loss: 0.5177957 acc: 78.90625\n",
      "Epoch: 8 batch 185 loss: 0.48142803 acc: 84.375\n",
      "Epoch: 8 batch 186 loss: 0.60135174 acc: 81.25\n",
      "Epoch: 8 batch 187 loss: 0.42659724 acc: 81.25\n",
      "Epoch: 8 batch 188 loss: 0.57488227 acc: 78.125\n",
      "Epoch: 8 batch 189 loss: 0.50448227 acc: 84.375\n",
      "Epoch: 8 batch 190 loss: 0.74924755 acc: 78.90625\n",
      "Epoch: 8 batch 191 loss: 0.6071201 acc: 78.90625\n",
      "Epoch: 8 batch 192 loss: 0.492422 acc: 81.25\n",
      "Epoch: 8 batch 193 loss: 0.659896 acc: 76.5625\n",
      "Epoch: 8 batch 194 loss: 0.5170736 acc: 83.59375\n",
      "Epoch: 8 batch 195 loss: 0.44667232 acc: 85.15625\n",
      "Epoch: 8 batch 196 loss: 0.5549128 acc: 81.25\n",
      "Epoch: 8 batch 197 loss: 0.76147664 acc: 73.4375\n",
      "Epoch: 8 batch 198 loss: 0.52252907 acc: 83.59375\n",
      "Epoch: 8 batch 199 loss: 0.6325005 acc: 81.25\n",
      "Epoch: 8 batch 200 loss: 0.6363611 acc: 78.125\n",
      "Epoch: 8 batch 201 loss: 0.48198986 acc: 81.25\n",
      "Epoch: 8 batch 202 loss: 0.59191215 acc: 79.6875\n",
      "Epoch: 8 batch 203 loss: 0.5475811 acc: 76.5625\n",
      "Epoch: 8 batch 204 loss: 0.55241096 acc: 79.6875\n",
      "Epoch: 8 batch 205 loss: 0.65616935 acc: 76.5625\n",
      "Epoch: 8 batch 206 loss: 0.5973444 acc: 76.5625\n",
      "Epoch: 8 batch 207 loss: 0.57533723 acc: 80.46875\n",
      "Epoch: 8 batch 208 loss: 0.6710347 acc: 77.34375\n",
      "Epoch: 8 batch 209 loss: 0.6438522 acc: 78.125\n",
      "Epoch: 8 batch 210 loss: 0.6101635 acc: 81.25\n",
      "Epoch: 8 batch 211 loss: 0.750803 acc: 75.0\n",
      "Epoch: 8 batch 212 loss: 0.67645174 acc: 80.46875\n",
      "Epoch: 8 batch 213 loss: 0.55423427 acc: 81.25\n",
      "Epoch: 8 batch 214 loss: 0.66742694 acc: 76.5625\n",
      "Epoch: 8 batch 215 loss: 0.63322943 acc: 75.0\n",
      "Epoch: 8 batch 216 loss: 0.50976515 acc: 83.59375\n",
      "Epoch: 8 batch 217 loss: 0.60130364 acc: 78.125\n",
      "Epoch: 8 batch 218 loss: 0.49362233 acc: 83.59375\n",
      "Epoch: 8 batch 219 loss: 0.5394798 acc: 79.6875\n",
      "Epoch: 8 batch 220 loss: 0.55105865 acc: 81.25\n",
      "Epoch: 8 batch 221 loss: 0.66417336 acc: 79.6875\n",
      "Epoch: 8 batch 222 loss: 0.6417248 acc: 76.5625\n",
      "Epoch: 8 batch 223 loss: 0.71331465 acc: 75.78125\n",
      "Epoch: 8 batch 224 loss: 0.64425176 acc: 74.21875\n",
      "Epoch: 8 batch 225 loss: 0.55248255 acc: 78.125\n",
      "Epoch: 8 batch 226 loss: 0.5759674 acc: 78.90625\n",
      "Epoch: 8 batch 227 loss: 0.651273 acc: 75.0\n",
      "Epoch: 8 batch 228 loss: 0.66531 acc: 77.34375\n",
      "Epoch: 8 batch 229 loss: 0.3978215 acc: 88.28125\n",
      "Epoch: 8 batch 230 loss: 0.4783868 acc: 81.25\n",
      "Epoch: 8 batch 231 loss: 0.62719834 acc: 80.46875\n",
      "Epoch: 8 batch 232 loss: 0.46155748 acc: 87.5\n",
      "Epoch: 8 batch 233 loss: 0.48864883 acc: 82.03125\n",
      "Epoch: 8 batch 234 loss: 0.5012946 acc: 82.03125\n",
      "Epoch: 8 batch 235 loss: 0.66965497 acc: 71.09375\n",
      "Epoch: 8 batch 236 loss: 0.5722687 acc: 81.25\n",
      "Epoch: 8 batch 237 loss: 0.6261108 acc: 80.46875\n",
      "Epoch: 8 batch 238 loss: 0.69260436 acc: 78.125\n",
      "Epoch: 8 batch 239 loss: 0.611902 acc: 78.90625\n",
      "Epoch: 8 batch 240 loss: 0.6975914 acc: 77.34375\n",
      "Epoch: 8 batch 241 loss: 0.61323607 acc: 84.375\n",
      "Epoch: 8 batch 242 loss: 0.48310226 acc: 83.59375\n",
      "Epoch: 8 batch 243 loss: 0.56369054 acc: 78.90625\n",
      "Epoch: 8 batch 244 loss: 0.532701 acc: 80.46875\n",
      "Epoch: 8 batch 245 loss: 0.6213305 acc: 81.25\n",
      "Epoch: 8 batch 246 loss: 0.6516991 acc: 76.5625\n",
      "Epoch: 8 batch 247 loss: 0.5875719 acc: 78.90625\n",
      "Epoch: 8 batch 248 loss: 0.5320901 acc: 81.25\n",
      "Epoch: 8 batch 249 loss: 0.6460974 acc: 79.6875\n",
      "Epoch: 8 batch 250 loss: 0.7837889 acc: 75.0\n",
      "Epoch: 8 batch 251 loss: 0.68834996 acc: 78.125\n",
      "Epoch: 8 batch 252 loss: 0.60318446 acc: 78.125\n",
      "Epoch: 8 batch 253 loss: 0.7194943 acc: 74.21875\n",
      "Epoch: 8 batch 254 loss: 0.77598035 acc: 75.78125\n",
      "Epoch: 8 batch 255 loss: 0.670481 acc: 75.78125\n",
      "Epoch: 8 batch 256 loss: 0.6584929 acc: 78.90625\n",
      "Epoch: 8 batch 257 loss: 0.9330501 acc: 66.40625\n",
      "Epoch: 8 batch 258 loss: 0.6552379 acc: 75.0\n",
      "Epoch: 8 batch 259 loss: 0.6586733 acc: 75.78125\n",
      "Epoch: 8 batch 260 loss: 0.66244304 acc: 71.875\n",
      "Epoch: 8 batch 261 loss: 0.71246135 acc: 78.125\n",
      "Epoch: 8 batch 262 loss: 0.813699 acc: 74.21875\n",
      "Epoch: 8 batch 263 loss: 0.5452096 acc: 81.25\n",
      "Epoch: 8 batch 264 loss: 0.6065532 acc: 78.125\n",
      "Epoch: 8 batch 265 loss: 0.686574 acc: 73.4375\n",
      "Epoch: 8 batch 266 loss: 0.6288197 acc: 78.125\n",
      "Epoch: 8 batch 267 loss: 0.58036935 acc: 77.34375\n",
      "Epoch: 8 batch 268 loss: 0.7039932 acc: 75.78125\n",
      "Epoch: 8 batch 269 loss: 0.6136384 acc: 79.6875\n",
      "Epoch: 8 batch 270 loss: 0.64348894 acc: 77.34375\n",
      "Epoch: 8 batch 271 loss: 0.6744225 acc: 78.125\n",
      "Epoch: 8 batch 272 loss: 0.51575327 acc: 82.03125\n",
      "Epoch: 8 batch 273 loss: 0.6031482 acc: 77.34375\n",
      "Epoch: 8 batch 274 loss: 0.6289406 acc: 78.90625\n",
      "Epoch: 8 batch 275 loss: 0.63545865 acc: 76.5625\n",
      "Epoch: 8 batch 276 loss: 0.53612083 acc: 78.90625\n",
      "Epoch: 8 batch 277 loss: 0.5399679 acc: 79.6875\n",
      "Epoch: 8 batch 278 loss: 0.5805382 acc: 80.46875\n",
      "Epoch: 8 batch 279 loss: 0.6834821 acc: 71.09375\n",
      "Epoch: 8 batch 280 loss: 0.49168074 acc: 83.59375\n",
      "Epoch: 8 batch 281 loss: 0.5183636 acc: 82.8125\n",
      "Epoch: 8 batch 282 loss: 0.6515731 acc: 75.0\n",
      "Epoch: 8 batch 283 loss: 0.5597348 acc: 82.8125\n",
      "Epoch: 8 batch 284 loss: 0.6055244 acc: 81.25\n",
      "Epoch: 8 batch 285 loss: 0.53363496 acc: 82.8125\n",
      "Epoch: 8 batch 286 loss: 0.6708822 acc: 74.21875\n",
      "Epoch: 8 batch 287 loss: 0.62543523 acc: 79.6875\n",
      "Epoch: 8 batch 288 loss: 0.80090094 acc: 68.75\n",
      "Epoch: 8 batch 289 loss: 0.6873398 acc: 78.90625\n",
      "Epoch: 8 batch 290 loss: 0.7260246 acc: 75.78125\n",
      "Epoch: 8 batch 291 loss: 0.73493063 acc: 72.65625\n",
      "Epoch: 8 batch 292 loss: 0.62314194 acc: 79.6875\n",
      "Epoch: 8 batch 293 loss: 0.66200626 acc: 79.6875\n",
      "Epoch: 8 batch 294 loss: 0.64111525 acc: 75.78125\n",
      "Epoch: 8 batch 295 loss: 0.586937 acc: 82.03125\n",
      "Epoch: 8 batch 296 loss: 0.78990054 acc: 70.3125\n",
      "Epoch: 8 batch 297 loss: 0.64216757 acc: 75.78125\n",
      "Epoch: 8 batch 298 loss: 0.5366015 acc: 82.8125\n",
      "Epoch: 8 batch 299 loss: 0.7024157 acc: 74.21875\n",
      "Epoch: 8 batch 300 loss: 0.6893872 acc: 78.90625\n",
      "Epoch: 8 batch 301 loss: 0.70037043 acc: 75.0\n",
      "Epoch: 8 batch 302 loss: 0.5453298 acc: 81.25\n",
      "Epoch: 8 batch 303 loss: 0.51870406 acc: 78.90625\n",
      "Epoch: 8 batch 304 loss: 0.6131156 acc: 77.34375\n",
      "Epoch: 8 batch 305 loss: 0.5395378 acc: 78.125\n",
      "Epoch: 8 batch 306 loss: 0.68526006 acc: 80.46875\n",
      "Epoch: 8 batch 307 loss: 0.79281926 acc: 75.0\n",
      "Epoch: 8 batch 308 loss: 0.53764045 acc: 85.15625\n",
      "Epoch: 8 batch 309 loss: 0.6999322 acc: 74.21875\n",
      "Epoch: 8 batch 310 loss: 0.84740746 acc: 72.65625\n",
      "Epoch: 8 batch 311 loss: 0.58621794 acc: 78.90625\n",
      "Epoch: 8 batch 312 loss: 0.5815234 acc: 83.59375\n",
      "Epoch: 8 batch 313 loss: 0.5321553 acc: 78.125\n",
      "Epoch: 8 batch 314 loss: 0.52080345 acc: 75.78125\n",
      "Epoch: 8 batch 315 loss: 0.67516243 acc: 67.96875\n",
      "Epoch: 8 batch 316 loss: 0.65816444 acc: 75.78125\n",
      "Epoch: 8 batch 317 loss: 0.6270479 acc: 78.125\n",
      "Epoch: 8 batch 318 loss: 0.69480586 acc: 76.5625\n",
      "Epoch: 8 batch 319 loss: 0.7117592 acc: 76.5625\n",
      "Epoch: 8 batch 320 loss: 0.6327049 acc: 77.34375\n",
      "Epoch: 8 batch 321 loss: 0.5525957 acc: 81.25\n",
      "Epoch: 8 batch 322 loss: 0.5604532 acc: 79.6875\n",
      "Epoch: 8 batch 323 loss: 0.580268 acc: 75.0\n",
      "Epoch: 8 batch 324 loss: 0.71022767 acc: 73.4375\n",
      "Epoch: 8 batch 325 loss: 0.5729861 acc: 77.34375\n",
      "Epoch: 8 batch 326 loss: 0.5340191 acc: 77.34375\n",
      "Epoch: 8 batch 327 loss: 0.5928691 acc: 79.6875\n",
      "Epoch: 8 batch 328 loss: 0.6874307 acc: 75.0\n",
      "Epoch: 8 batch 329 loss: 0.657222 acc: 79.6875\n",
      "Epoch: 8 batch 330 loss: 0.4577911 acc: 85.15625\n",
      "Epoch: 8 batch 331 loss: 0.49421683 acc: 78.90625\n",
      "Epoch: 8 batch 332 loss: 0.73114824 acc: 73.4375\n",
      "Epoch: 8 batch 333 loss: 0.60380864 acc: 81.25\n",
      "Epoch: 8 batch 334 loss: 0.527102 acc: 82.8125\n",
      "Epoch: 8 batch 335 loss: 0.6755748 acc: 77.34375\n",
      "Epoch: 8 batch 336 loss: 0.5968591 acc: 78.90625\n",
      "Epoch: 8 batch 337 loss: 0.6297163 acc: 78.90625\n",
      "Epoch: 8 batch 338 loss: 0.6389946 acc: 75.78125\n",
      "Epoch: 8 batch 339 loss: 0.5689385 acc: 78.90625\n",
      "Epoch: 8 batch 340 loss: 0.59757066 acc: 81.25\n",
      "Epoch: 8 batch 341 loss: 0.7112168 acc: 82.03125\n",
      "Epoch: 8 batch 342 loss: 0.6517588 acc: 77.34375\n",
      "Epoch: 8 batch 343 loss: 0.52324563 acc: 82.03125\n",
      "Epoch: 8 batch 344 loss: 0.62178844 acc: 81.25\n",
      "Epoch: 8 batch 345 loss: 0.70237803 acc: 79.6875\n",
      "Epoch: 8 batch 346 loss: 0.5332783 acc: 78.90625\n",
      "Epoch: 8 batch 347 loss: 0.62134635 acc: 77.34375\n",
      "Epoch: 8 batch 348 loss: 0.5466541 acc: 79.6875\n",
      "Epoch: 8 batch 349 loss: 0.62007785 acc: 75.0\n",
      "Epoch: 8 batch 350 loss: 0.7160039 acc: 78.125\n",
      "Epoch: 8 batch 351 loss: 0.59177756 acc: 79.6875\n",
      "Epoch: 8 batch 352 loss: 0.45851213 acc: 85.15625\n",
      "Epoch: 8 batch 353 loss: 0.60199666 acc: 82.03125\n",
      "Epoch: 8 batch 354 loss: 0.49495506 acc: 81.25\n",
      "Epoch: 8 batch 355 loss: 0.6063899 acc: 75.0\n",
      "Epoch: 8 batch 356 loss: 0.6981404 acc: 73.4375\n",
      "Epoch: 8 batch 357 loss: 0.51306844 acc: 82.03125\n",
      "Epoch: 8 batch 358 loss: 0.635981 acc: 79.6875\n",
      "Epoch: 8 batch 359 loss: 0.57898104 acc: 82.8125\n",
      "Epoch: 8 batch 360 loss: 0.50386786 acc: 85.9375\n",
      "Epoch: 8 batch 361 loss: 0.690114 acc: 74.21875\n",
      "Epoch: 8 batch 362 loss: 0.6652408 acc: 78.90625\n",
      "Epoch: 8 batch 363 loss: 0.7936339 acc: 74.21875\n",
      "Epoch: 8 batch 364 loss: 0.5096363 acc: 82.03125\n",
      "Epoch: 8 batch 365 loss: 0.447429 acc: 82.8125\n",
      "Epoch: 8 batch 366 loss: 0.57808495 acc: 79.6875\n",
      "Epoch: 8 batch 367 loss: 0.7439792 acc: 69.53125\n",
      "Epoch: 8 batch 368 loss: 0.52780235 acc: 78.125\n",
      "Epoch: 8 batch 369 loss: 0.6533712 acc: 75.78125\n",
      "Epoch: 8 batch 370 loss: 0.6014726 acc: 79.6875\n",
      "Epoch: 8 batch 371 loss: 0.5967219 acc: 75.78125\n",
      "Epoch: 8 batch 372 loss: 0.54755855 acc: 85.9375\n",
      "Epoch: 8 batch 373 loss: 0.7012405 acc: 73.4375\n",
      "Epoch: 8 batch 374 loss: 0.7125423 acc: 75.0\n",
      "Epoch: 8 batch 375 loss: 0.747307 acc: 75.78125\n",
      "Epoch: 8 batch 376 loss: 0.5528238 acc: 82.8125\n",
      "Epoch: 8 batch 377 loss: 0.577048 acc: 81.25\n",
      "Epoch: 8 batch 378 loss: 0.5148246 acc: 82.8125\n",
      "Epoch: 8 batch 379 loss: 0.6758449 acc: 75.0\n",
      "Epoch: 8 batch 380 loss: 0.54577863 acc: 82.03125\n",
      "Epoch: 8 batch 381 loss: 0.6075351 acc: 78.125\n",
      "Epoch: 8 batch 382 loss: 0.6655685 acc: 79.6875\n",
      "Epoch: 8 batch 383 loss: 0.63525426 acc: 73.4375\n",
      "Epoch: 8 batch 384 loss: 0.5004133 acc: 85.15625\n",
      "Epoch: 8 batch 385 loss: 0.6344031 acc: 78.90625\n",
      "Epoch: 8 batch 386 loss: 0.53856623 acc: 81.25\n",
      "Epoch: 8 batch 387 loss: 0.6048359 acc: 79.6875\n",
      "Epoch: 8 batch 388 loss: 0.61943 acc: 77.34375\n",
      "Epoch: 8 batch 389 loss: 0.5498177 acc: 77.34375\n",
      "After epoch 8 test loss 0.8472417715072632 test accuracy 0.7164999842643738\n",
      "Epoch: 9 batch 0 loss: 0.47918472 acc: 71.83057069778442\n",
      "Epoch: 9 batch 1 loss: 0.51886535 acc: 80.46875\n",
      "Epoch: 9 batch 2 loss: 0.61851454 acc: 76.5625\n",
      "Epoch: 9 batch 3 loss: 0.59704506 acc: 78.125\n",
      "Epoch: 9 batch 4 loss: 0.651792 acc: 75.0\n",
      "Epoch: 9 batch 5 loss: 0.6214259 acc: 80.46875\n",
      "Epoch: 9 batch 6 loss: 0.68010306 acc: 75.78125\n",
      "Epoch: 9 batch 7 loss: 0.403157 acc: 89.0625\n",
      "Epoch: 9 batch 8 loss: 0.71711135 acc: 76.5625\n",
      "Epoch: 9 batch 9 loss: 0.42813915 acc: 86.71875\n",
      "Epoch: 9 batch 10 loss: 0.64586365 acc: 78.90625\n",
      "Epoch: 9 batch 11 loss: 0.53525376 acc: 80.46875\n",
      "Epoch: 9 batch 12 loss: 0.5759928 acc: 82.8125\n",
      "Epoch: 9 batch 13 loss: 0.6008026 acc: 82.8125\n",
      "Epoch: 9 batch 14 loss: 0.5276487 acc: 82.03125\n",
      "Epoch: 9 batch 15 loss: 0.71886325 acc: 71.875\n",
      "Epoch: 9 batch 16 loss: 0.55938786 acc: 79.6875\n",
      "Epoch: 9 batch 17 loss: 0.6465087 acc: 74.21875\n",
      "Epoch: 9 batch 18 loss: 0.621115 acc: 77.34375\n",
      "Epoch: 9 batch 19 loss: 0.61755955 acc: 79.6875\n",
      "Epoch: 9 batch 20 loss: 0.44609573 acc: 82.8125\n",
      "Epoch: 9 batch 21 loss: 0.64162314 acc: 76.5625\n",
      "Epoch: 9 batch 22 loss: 0.44167286 acc: 84.375\n",
      "Epoch: 9 batch 23 loss: 0.6415985 acc: 74.21875\n",
      "Epoch: 9 batch 24 loss: 0.65731853 acc: 77.34375\n",
      "Epoch: 9 batch 25 loss: 0.5461807 acc: 81.25\n",
      "Epoch: 9 batch 26 loss: 0.63780034 acc: 76.5625\n",
      "Epoch: 9 batch 27 loss: 0.44300872 acc: 83.59375\n",
      "Epoch: 9 batch 28 loss: 0.50877523 acc: 81.25\n",
      "Epoch: 9 batch 29 loss: 0.59815645 acc: 76.5625\n",
      "Epoch: 9 batch 30 loss: 0.65201676 acc: 77.34375\n",
      "Epoch: 9 batch 31 loss: 0.63504183 acc: 74.21875\n",
      "Epoch: 9 batch 32 loss: 0.66458297 acc: 78.125\n",
      "Epoch: 9 batch 33 loss: 0.5003183 acc: 80.46875\n",
      "Epoch: 9 batch 34 loss: 0.47383785 acc: 85.9375\n",
      "Epoch: 9 batch 35 loss: 0.55421555 acc: 81.25\n",
      "Epoch: 9 batch 36 loss: 0.41973302 acc: 85.9375\n",
      "Epoch: 9 batch 37 loss: 0.6807885 acc: 78.125\n",
      "Epoch: 9 batch 38 loss: 0.740073 acc: 77.34375\n",
      "Epoch: 9 batch 39 loss: 0.7570088 acc: 67.1875\n",
      "Epoch: 9 batch 40 loss: 0.60025644 acc: 78.90625\n",
      "Epoch: 9 batch 41 loss: 0.6671525 acc: 78.125\n",
      "Epoch: 9 batch 42 loss: 0.7062594 acc: 71.875\n",
      "Epoch: 9 batch 43 loss: 0.606469 acc: 77.34375\n",
      "Epoch: 9 batch 44 loss: 0.7069044 acc: 75.0\n",
      "Epoch: 9 batch 45 loss: 0.60249245 acc: 74.21875\n",
      "Epoch: 9 batch 46 loss: 0.6485598 acc: 76.5625\n",
      "Epoch: 9 batch 47 loss: 0.5026409 acc: 82.8125\n",
      "Epoch: 9 batch 48 loss: 0.4894925 acc: 82.03125\n",
      "Epoch: 9 batch 49 loss: 0.49307758 acc: 80.46875\n",
      "Epoch: 9 batch 50 loss: 0.55765337 acc: 78.90625\n",
      "Epoch: 9 batch 51 loss: 0.51212907 acc: 85.15625\n",
      "Epoch: 9 batch 52 loss: 0.609612 acc: 81.25\n",
      "Epoch: 9 batch 53 loss: 0.6534389 acc: 77.34375\n",
      "Epoch: 9 batch 54 loss: 0.53150725 acc: 83.59375\n",
      "Epoch: 9 batch 55 loss: 0.7228354 acc: 75.0\n",
      "Epoch: 9 batch 56 loss: 0.5460532 acc: 79.6875\n",
      "Epoch: 9 batch 57 loss: 0.5481169 acc: 82.03125\n",
      "Epoch: 9 batch 58 loss: 0.48776504 acc: 82.8125\n",
      "Epoch: 9 batch 59 loss: 0.5694426 acc: 78.125\n",
      "Epoch: 9 batch 60 loss: 0.48055995 acc: 79.6875\n",
      "Epoch: 9 batch 61 loss: 0.6196698 acc: 79.6875\n",
      "Epoch: 9 batch 62 loss: 0.5633545 acc: 82.8125\n",
      "Epoch: 9 batch 63 loss: 0.48883036 acc: 78.125\n",
      "Epoch: 9 batch 64 loss: 0.64539635 acc: 76.5625\n",
      "Epoch: 9 batch 65 loss: 0.5737901 acc: 79.6875\n",
      "Epoch: 9 batch 66 loss: 0.65286785 acc: 77.34375\n",
      "Epoch: 9 batch 67 loss: 0.5669149 acc: 82.8125\n",
      "Epoch: 9 batch 68 loss: 0.60736763 acc: 78.90625\n",
      "Epoch: 9 batch 69 loss: 0.6524476 acc: 78.90625\n",
      "Epoch: 9 batch 70 loss: 0.6689374 acc: 75.0\n",
      "Epoch: 9 batch 71 loss: 0.6094928 acc: 77.34375\n",
      "Epoch: 9 batch 72 loss: 0.61332643 acc: 78.90625\n",
      "Epoch: 9 batch 73 loss: 0.6087235 acc: 76.5625\n",
      "Epoch: 9 batch 74 loss: 0.35316062 acc: 89.84375\n",
      "Epoch: 9 batch 75 loss: 0.49411234 acc: 82.03125\n",
      "Epoch: 9 batch 76 loss: 0.6524781 acc: 80.46875\n",
      "Epoch: 9 batch 77 loss: 0.5112872 acc: 83.59375\n",
      "Epoch: 9 batch 78 loss: 0.54354835 acc: 82.03125\n",
      "Epoch: 9 batch 79 loss: 0.5342445 acc: 78.125\n",
      "Epoch: 9 batch 80 loss: 0.54699403 acc: 82.03125\n",
      "Epoch: 9 batch 81 loss: 0.91678977 acc: 71.09375\n",
      "Epoch: 9 batch 82 loss: 0.61048746 acc: 77.34375\n",
      "Epoch: 9 batch 83 loss: 0.653985 acc: 74.21875\n",
      "Epoch: 9 batch 84 loss: 0.5691279 acc: 82.03125\n",
      "Epoch: 9 batch 85 loss: 0.6556454 acc: 80.46875\n",
      "Epoch: 9 batch 86 loss: 0.63813114 acc: 74.21875\n",
      "Epoch: 9 batch 87 loss: 0.4706226 acc: 82.03125\n",
      "Epoch: 9 batch 88 loss: 0.57604617 acc: 82.03125\n",
      "Epoch: 9 batch 89 loss: 0.5247091 acc: 78.125\n",
      "Epoch: 9 batch 90 loss: 0.3944006 acc: 85.9375\n",
      "Epoch: 9 batch 91 loss: 0.52144015 acc: 82.03125\n",
      "Epoch: 9 batch 92 loss: 0.7187045 acc: 75.78125\n",
      "Epoch: 9 batch 93 loss: 0.46005177 acc: 83.59375\n",
      "Epoch: 9 batch 94 loss: 0.788066 acc: 72.65625\n",
      "Epoch: 9 batch 95 loss: 0.58719796 acc: 75.78125\n",
      "Epoch: 9 batch 96 loss: 0.6033036 acc: 76.5625\n",
      "Epoch: 9 batch 97 loss: 0.7551315 acc: 73.4375\n",
      "Epoch: 9 batch 98 loss: 0.692116 acc: 78.90625\n",
      "Epoch: 9 batch 99 loss: 0.6622536 acc: 73.4375\n",
      "Epoch: 9 batch 100 loss: 0.5384892 acc: 82.03125\n",
      "Epoch: 9 batch 101 loss: 0.62233466 acc: 72.65625\n",
      "Epoch: 9 batch 102 loss: 0.45042306 acc: 82.8125\n",
      "Epoch: 9 batch 103 loss: 0.84047914 acc: 77.34375\n",
      "Epoch: 9 batch 104 loss: 0.6810297 acc: 75.0\n",
      "Epoch: 9 batch 105 loss: 0.49618095 acc: 82.8125\n",
      "Epoch: 9 batch 106 loss: 0.5668085 acc: 78.125\n",
      "Epoch: 9 batch 107 loss: 0.79344976 acc: 69.53125\n",
      "Epoch: 9 batch 108 loss: 0.5814605 acc: 78.90625\n",
      "Epoch: 9 batch 109 loss: 0.49005753 acc: 80.46875\n",
      "Epoch: 9 batch 110 loss: 0.57173896 acc: 79.6875\n",
      "Epoch: 9 batch 111 loss: 0.731057 acc: 70.3125\n",
      "Epoch: 9 batch 112 loss: 0.69006824 acc: 80.46875\n",
      "Epoch: 9 batch 113 loss: 0.51614845 acc: 79.6875\n",
      "Epoch: 9 batch 114 loss: 0.6785177 acc: 78.125\n",
      "Epoch: 9 batch 115 loss: 0.5405348 acc: 84.375\n",
      "Epoch: 9 batch 116 loss: 0.6658247 acc: 82.03125\n",
      "Epoch: 9 batch 117 loss: 0.5792635 acc: 78.125\n",
      "Epoch: 9 batch 118 loss: 0.67279667 acc: 75.78125\n",
      "Epoch: 9 batch 119 loss: 0.52383614 acc: 78.90625\n",
      "Epoch: 9 batch 120 loss: 0.5962558 acc: 78.90625\n",
      "Epoch: 9 batch 121 loss: 0.6916001 acc: 76.5625\n",
      "Epoch: 9 batch 122 loss: 0.58674157 acc: 81.25\n",
      "Epoch: 9 batch 123 loss: 0.6290236 acc: 80.46875\n",
      "Epoch: 9 batch 124 loss: 0.571785 acc: 76.5625\n",
      "Epoch: 9 batch 125 loss: 0.5573826 acc: 82.03125\n",
      "Epoch: 9 batch 126 loss: 0.5439943 acc: 78.125\n",
      "Epoch: 9 batch 127 loss: 0.48308036 acc: 84.375\n",
      "Epoch: 9 batch 128 loss: 0.5893228 acc: 80.46875\n",
      "Epoch: 9 batch 129 loss: 0.61278826 acc: 76.5625\n",
      "Epoch: 9 batch 130 loss: 0.53375804 acc: 81.25\n",
      "Epoch: 9 batch 131 loss: 0.5896741 acc: 81.25\n",
      "Epoch: 9 batch 132 loss: 0.5441013 acc: 82.8125\n",
      "Epoch: 9 batch 133 loss: 0.58176875 acc: 76.5625\n",
      "Epoch: 9 batch 134 loss: 0.5610628 acc: 81.25\n",
      "Epoch: 9 batch 135 loss: 0.6527096 acc: 78.90625\n",
      "Epoch: 9 batch 136 loss: 0.5264597 acc: 81.25\n",
      "Epoch: 9 batch 137 loss: 0.4147959 acc: 83.59375\n",
      "Epoch: 9 batch 138 loss: 0.78778625 acc: 74.21875\n",
      "Epoch: 9 batch 139 loss: 0.57669175 acc: 80.46875\n",
      "Epoch: 9 batch 140 loss: 0.6029952 acc: 77.34375\n",
      "Epoch: 9 batch 141 loss: 0.46658212 acc: 85.9375\n",
      "Epoch: 9 batch 142 loss: 0.6699118 acc: 76.5625\n",
      "Epoch: 9 batch 143 loss: 0.7277281 acc: 75.78125\n",
      "Epoch: 9 batch 144 loss: 0.73981416 acc: 69.53125\n",
      "Epoch: 9 batch 145 loss: 0.73691124 acc: 73.4375\n",
      "Epoch: 9 batch 146 loss: 0.62097156 acc: 81.25\n",
      "Epoch: 9 batch 147 loss: 0.5392219 acc: 82.03125\n",
      "Epoch: 9 batch 148 loss: 0.5568876 acc: 78.125\n",
      "Epoch: 9 batch 149 loss: 0.5221139 acc: 79.6875\n",
      "Epoch: 9 batch 150 loss: 0.59712625 acc: 77.34375\n",
      "Epoch: 9 batch 151 loss: 0.4721637 acc: 85.15625\n",
      "Epoch: 9 batch 152 loss: 0.535041 acc: 77.34375\n",
      "Epoch: 9 batch 153 loss: 0.60890675 acc: 79.6875\n",
      "Epoch: 9 batch 154 loss: 0.7785407 acc: 71.875\n",
      "Epoch: 9 batch 155 loss: 0.53658897 acc: 80.46875\n",
      "Epoch: 9 batch 156 loss: 0.5112371 acc: 82.03125\n",
      "Epoch: 9 batch 157 loss: 0.5972816 acc: 78.90625\n",
      "Epoch: 9 batch 158 loss: 0.47018242 acc: 81.25\n",
      "Epoch: 9 batch 159 loss: 0.56181264 acc: 75.0\n",
      "Epoch: 9 batch 160 loss: 0.55933803 acc: 80.46875\n",
      "Epoch: 9 batch 161 loss: 0.61426294 acc: 77.34375\n",
      "Epoch: 9 batch 162 loss: 0.6076212 acc: 83.59375\n",
      "Epoch: 9 batch 163 loss: 0.49489334 acc: 78.90625\n",
      "Epoch: 9 batch 164 loss: 0.45882127 acc: 80.46875\n",
      "Epoch: 9 batch 165 loss: 0.6067049 acc: 81.25\n",
      "Epoch: 9 batch 166 loss: 0.581755 acc: 77.34375\n",
      "Epoch: 9 batch 167 loss: 0.56062627 acc: 78.125\n",
      "Epoch: 9 batch 168 loss: 0.74205375 acc: 71.09375\n",
      "Epoch: 9 batch 169 loss: 0.6922996 acc: 73.4375\n",
      "Epoch: 9 batch 170 loss: 0.5572974 acc: 82.8125\n",
      "Epoch: 9 batch 171 loss: 0.60779965 acc: 78.125\n",
      "Epoch: 9 batch 172 loss: 0.52717334 acc: 81.25\n",
      "Epoch: 9 batch 173 loss: 0.59425044 acc: 76.5625\n",
      "Epoch: 9 batch 174 loss: 0.76160175 acc: 75.78125\n",
      "Epoch: 9 batch 175 loss: 0.71034205 acc: 68.75\n",
      "Epoch: 9 batch 176 loss: 0.60751355 acc: 80.46875\n",
      "Epoch: 9 batch 177 loss: 0.4871388 acc: 83.59375\n",
      "Epoch: 9 batch 178 loss: 0.57367724 acc: 82.03125\n",
      "Epoch: 9 batch 179 loss: 0.6347292 acc: 78.90625\n",
      "Epoch: 9 batch 180 loss: 0.45904648 acc: 85.15625\n",
      "Epoch: 9 batch 181 loss: 0.6280339 acc: 77.34375\n",
      "Epoch: 9 batch 182 loss: 0.44506148 acc: 88.28125\n",
      "Epoch: 9 batch 183 loss: 0.55982035 acc: 81.25\n",
      "Epoch: 9 batch 184 loss: 0.59297985 acc: 79.6875\n",
      "Epoch: 9 batch 185 loss: 0.4789497 acc: 84.375\n",
      "Epoch: 9 batch 186 loss: 0.42886958 acc: 84.375\n",
      "Epoch: 9 batch 187 loss: 0.36857793 acc: 85.15625\n",
      "Epoch: 9 batch 188 loss: 0.49519426 acc: 83.59375\n",
      "Epoch: 9 batch 189 loss: 0.5779911 acc: 83.59375\n",
      "Epoch: 9 batch 190 loss: 0.5624045 acc: 83.59375\n",
      "Epoch: 9 batch 191 loss: 0.53218526 acc: 80.46875\n",
      "Epoch: 9 batch 192 loss: 0.5333338 acc: 81.25\n",
      "Epoch: 9 batch 193 loss: 0.700403 acc: 78.125\n",
      "Epoch: 9 batch 194 loss: 0.46508205 acc: 83.59375\n",
      "Epoch: 9 batch 195 loss: 0.5181742 acc: 82.8125\n",
      "Epoch: 9 batch 196 loss: 0.5296957 acc: 79.6875\n",
      "Epoch: 9 batch 197 loss: 0.60957146 acc: 78.125\n",
      "Epoch: 9 batch 198 loss: 0.4410495 acc: 88.28125\n",
      "Epoch: 9 batch 199 loss: 0.7284074 acc: 81.25\n",
      "Epoch: 9 batch 200 loss: 0.6576725 acc: 76.5625\n",
      "Epoch: 9 batch 201 loss: 0.53515106 acc: 79.6875\n",
      "Epoch: 9 batch 202 loss: 0.6497439 acc: 75.78125\n",
      "Epoch: 9 batch 203 loss: 0.48698235 acc: 78.90625\n",
      "Epoch: 9 batch 204 loss: 0.51547337 acc: 80.46875\n",
      "Epoch: 9 batch 205 loss: 0.5343012 acc: 84.375\n",
      "Epoch: 9 batch 206 loss: 0.6216356 acc: 76.5625\n",
      "Epoch: 9 batch 207 loss: 0.47786522 acc: 86.71875\n",
      "Epoch: 9 batch 208 loss: 0.6101993 acc: 77.34375\n",
      "Epoch: 9 batch 209 loss: 0.65005475 acc: 74.21875\n",
      "Epoch: 9 batch 210 loss: 0.5430474 acc: 79.6875\n",
      "Epoch: 9 batch 211 loss: 0.8330798 acc: 71.09375\n",
      "Epoch: 9 batch 212 loss: 0.5927216 acc: 75.0\n",
      "Epoch: 9 batch 213 loss: 0.5395843 acc: 78.90625\n",
      "Epoch: 9 batch 214 loss: 0.6370187 acc: 77.34375\n",
      "Epoch: 9 batch 215 loss: 0.48790535 acc: 83.59375\n",
      "Epoch: 9 batch 216 loss: 0.51126707 acc: 80.46875\n",
      "Epoch: 9 batch 217 loss: 0.4698134 acc: 81.25\n",
      "Epoch: 9 batch 218 loss: 0.44955394 acc: 82.8125\n",
      "Epoch: 9 batch 219 loss: 0.5490694 acc: 80.46875\n",
      "Epoch: 9 batch 220 loss: 0.48496938 acc: 85.15625\n",
      "Epoch: 9 batch 221 loss: 0.7081774 acc: 75.0\n",
      "Epoch: 9 batch 222 loss: 0.66811514 acc: 71.09375\n",
      "Epoch: 9 batch 223 loss: 0.54199886 acc: 82.03125\n",
      "Epoch: 9 batch 224 loss: 0.6711406 acc: 73.4375\n",
      "Epoch: 9 batch 225 loss: 0.4713926 acc: 85.9375\n",
      "Epoch: 9 batch 226 loss: 0.6073259 acc: 76.5625\n",
      "Epoch: 9 batch 227 loss: 0.7416033 acc: 79.6875\n",
      "Epoch: 9 batch 228 loss: 0.5312755 acc: 80.46875\n",
      "Epoch: 9 batch 229 loss: 0.3928172 acc: 84.375\n",
      "Epoch: 9 batch 230 loss: 0.4989682 acc: 81.25\n",
      "Epoch: 9 batch 231 loss: 0.57440746 acc: 80.46875\n",
      "Epoch: 9 batch 232 loss: 0.4079226 acc: 87.5\n",
      "Epoch: 9 batch 233 loss: 0.46900582 acc: 83.59375\n",
      "Epoch: 9 batch 234 loss: 0.42643052 acc: 85.15625\n",
      "Epoch: 9 batch 235 loss: 0.5503214 acc: 78.125\n",
      "Epoch: 9 batch 236 loss: 0.4537508 acc: 85.9375\n",
      "Epoch: 9 batch 237 loss: 0.62519306 acc: 78.125\n",
      "Epoch: 9 batch 238 loss: 0.4995232 acc: 85.15625\n",
      "Epoch: 9 batch 239 loss: 0.6278802 acc: 78.125\n",
      "Epoch: 9 batch 240 loss: 0.7068151 acc: 78.125\n",
      "Epoch: 9 batch 241 loss: 0.6633077 acc: 78.90625\n",
      "Epoch: 9 batch 242 loss: 0.46173123 acc: 82.8125\n",
      "Epoch: 9 batch 243 loss: 0.5142323 acc: 80.46875\n",
      "Epoch: 9 batch 244 loss: 0.60072774 acc: 73.4375\n",
      "Epoch: 9 batch 245 loss: 0.60433865 acc: 78.90625\n",
      "Epoch: 9 batch 246 loss: 0.5388588 acc: 83.59375\n",
      "Epoch: 9 batch 247 loss: 0.6055013 acc: 79.6875\n",
      "Epoch: 9 batch 248 loss: 0.60009056 acc: 81.25\n",
      "Epoch: 9 batch 249 loss: 0.6077856 acc: 82.03125\n",
      "Epoch: 9 batch 250 loss: 0.53274024 acc: 82.03125\n",
      "Epoch: 9 batch 251 loss: 0.60463595 acc: 75.0\n",
      "Epoch: 9 batch 252 loss: 0.57624567 acc: 81.25\n",
      "Epoch: 9 batch 253 loss: 0.602213 acc: 79.6875\n",
      "Epoch: 9 batch 254 loss: 0.5305149 acc: 81.25\n",
      "Epoch: 9 batch 255 loss: 0.53165936 acc: 82.03125\n",
      "Epoch: 9 batch 256 loss: 0.5506945 acc: 80.46875\n",
      "Epoch: 9 batch 257 loss: 0.59760046 acc: 74.21875\n",
      "Epoch: 9 batch 258 loss: 0.6299025 acc: 78.125\n",
      "Epoch: 9 batch 259 loss: 0.73386467 acc: 75.0\n",
      "Epoch: 9 batch 260 loss: 0.61696786 acc: 72.65625\n",
      "Epoch: 9 batch 261 loss: 0.6474545 acc: 78.90625\n",
      "Epoch: 9 batch 262 loss: 0.6802161 acc: 72.65625\n",
      "Epoch: 9 batch 263 loss: 0.523309 acc: 84.375\n",
      "Epoch: 9 batch 264 loss: 0.5987907 acc: 75.78125\n",
      "Epoch: 9 batch 265 loss: 0.6285986 acc: 77.34375\n",
      "Epoch: 9 batch 266 loss: 0.5244267 acc: 82.03125\n",
      "Epoch: 9 batch 267 loss: 0.5976388 acc: 79.6875\n",
      "Epoch: 9 batch 268 loss: 0.5825956 acc: 77.34375\n",
      "Epoch: 9 batch 269 loss: 0.5506185 acc: 80.46875\n",
      "Epoch: 9 batch 270 loss: 0.6454635 acc: 75.78125\n",
      "Epoch: 9 batch 271 loss: 0.5675731 acc: 80.46875\n",
      "Epoch: 9 batch 272 loss: 0.43187773 acc: 82.8125\n",
      "Epoch: 9 batch 273 loss: 0.5452305 acc: 77.34375\n",
      "Epoch: 9 batch 274 loss: 0.558897 acc: 79.6875\n",
      "Epoch: 9 batch 275 loss: 0.5532696 acc: 77.34375\n",
      "Epoch: 9 batch 276 loss: 0.6999892 acc: 72.65625\n",
      "Epoch: 9 batch 277 loss: 0.5796599 acc: 74.21875\n",
      "Epoch: 9 batch 278 loss: 0.5372191 acc: 81.25\n",
      "Epoch: 9 batch 279 loss: 0.6370664 acc: 75.78125\n",
      "Epoch: 9 batch 280 loss: 0.43564475 acc: 83.59375\n",
      "Epoch: 9 batch 281 loss: 0.5093128 acc: 81.25\n",
      "Epoch: 9 batch 282 loss: 0.6085361 acc: 76.5625\n",
      "Epoch: 9 batch 283 loss: 0.6584065 acc: 75.78125\n",
      "Epoch: 9 batch 284 loss: 0.5357237 acc: 82.03125\n",
      "Epoch: 9 batch 285 loss: 0.5218101 acc: 80.46875\n",
      "Epoch: 9 batch 286 loss: 0.6296126 acc: 77.34375\n",
      "Epoch: 9 batch 287 loss: 0.5999575 acc: 82.8125\n",
      "Epoch: 9 batch 288 loss: 0.6893572 acc: 75.78125\n",
      "Epoch: 9 batch 289 loss: 0.47792807 acc: 82.8125\n",
      "Epoch: 9 batch 290 loss: 0.66424984 acc: 75.78125\n",
      "Epoch: 9 batch 291 loss: 0.72193384 acc: 75.78125\n",
      "Epoch: 9 batch 292 loss: 0.5091452 acc: 82.8125\n",
      "Epoch: 9 batch 293 loss: 0.5215378 acc: 82.03125\n",
      "Epoch: 9 batch 294 loss: 0.6465 acc: 76.5625\n",
      "Epoch: 9 batch 295 loss: 0.57697564 acc: 78.90625\n",
      "Epoch: 9 batch 296 loss: 0.73308057 acc: 75.78125\n",
      "Epoch: 9 batch 297 loss: 0.6172556 acc: 75.0\n",
      "Epoch: 9 batch 298 loss: 0.5601658 acc: 78.90625\n",
      "Epoch: 9 batch 299 loss: 0.563074 acc: 78.90625\n",
      "Epoch: 9 batch 300 loss: 0.52623457 acc: 79.6875\n",
      "Epoch: 9 batch 301 loss: 0.54176044 acc: 76.5625\n",
      "Epoch: 9 batch 302 loss: 0.5881943 acc: 80.46875\n",
      "Epoch: 9 batch 303 loss: 0.43477228 acc: 85.9375\n",
      "Epoch: 9 batch 304 loss: 0.58908 acc: 77.34375\n",
      "Epoch: 9 batch 305 loss: 0.5794118 acc: 76.5625\n",
      "Epoch: 9 batch 306 loss: 0.4843973 acc: 85.15625\n",
      "Epoch: 9 batch 307 loss: 0.54189324 acc: 78.125\n",
      "Epoch: 9 batch 308 loss: 0.42473626 acc: 85.9375\n",
      "Epoch: 9 batch 309 loss: 0.77111447 acc: 78.125\n",
      "Epoch: 9 batch 310 loss: 0.6789968 acc: 76.5625\n",
      "Epoch: 9 batch 311 loss: 0.5317792 acc: 82.03125\n",
      "Epoch: 9 batch 312 loss: 0.5760008 acc: 80.46875\n",
      "Epoch: 9 batch 313 loss: 0.5398685 acc: 75.78125\n",
      "Epoch: 9 batch 314 loss: 0.5025431 acc: 78.90625\n",
      "Epoch: 9 batch 315 loss: 0.5708367 acc: 78.90625\n",
      "Epoch: 9 batch 316 loss: 0.6217507 acc: 75.78125\n",
      "Epoch: 9 batch 317 loss: 0.71905816 acc: 79.6875\n",
      "Epoch: 9 batch 318 loss: 0.5386975 acc: 82.03125\n",
      "Epoch: 9 batch 319 loss: 0.63544816 acc: 75.0\n",
      "Epoch: 9 batch 320 loss: 0.55759895 acc: 76.5625\n",
      "Epoch: 9 batch 321 loss: 0.48489463 acc: 85.9375\n",
      "Epoch: 9 batch 322 loss: 0.7597184 acc: 76.5625\n",
      "Epoch: 9 batch 323 loss: 0.56459695 acc: 77.34375\n",
      "Epoch: 9 batch 324 loss: 0.5198045 acc: 81.25\n",
      "Epoch: 9 batch 325 loss: 0.56569827 acc: 78.90625\n",
      "Epoch: 9 batch 326 loss: 0.49005878 acc: 85.9375\n",
      "Epoch: 9 batch 327 loss: 0.6179464 acc: 82.8125\n",
      "Epoch: 9 batch 328 loss: 0.665903 acc: 78.90625\n",
      "Epoch: 9 batch 329 loss: 0.6007265 acc: 83.59375\n",
      "Epoch: 9 batch 330 loss: 0.43239984 acc: 84.375\n",
      "Epoch: 9 batch 331 loss: 0.53623843 acc: 82.03125\n",
      "Epoch: 9 batch 332 loss: 0.6853456 acc: 75.0\n",
      "Epoch: 9 batch 333 loss: 0.55701196 acc: 78.90625\n",
      "Epoch: 9 batch 334 loss: 0.5245216 acc: 80.46875\n",
      "Epoch: 9 batch 335 loss: 0.70610905 acc: 76.5625\n",
      "Epoch: 9 batch 336 loss: 0.4692313 acc: 83.59375\n",
      "Epoch: 9 batch 337 loss: 0.59683657 acc: 74.21875\n",
      "Epoch: 9 batch 338 loss: 0.55569625 acc: 80.46875\n",
      "Epoch: 9 batch 339 loss: 0.532663 acc: 79.6875\n",
      "Epoch: 9 batch 340 loss: 0.539013 acc: 79.6875\n",
      "Epoch: 9 batch 341 loss: 0.5119616 acc: 79.6875\n",
      "Epoch: 9 batch 342 loss: 0.7147402 acc: 74.21875\n",
      "Epoch: 9 batch 343 loss: 0.48411936 acc: 83.59375\n",
      "Epoch: 9 batch 344 loss: 0.57267725 acc: 78.90625\n",
      "Epoch: 9 batch 345 loss: 0.6007141 acc: 82.03125\n",
      "Epoch: 9 batch 346 loss: 0.5342772 acc: 78.125\n",
      "Epoch: 9 batch 347 loss: 0.49215734 acc: 84.375\n",
      "Epoch: 9 batch 348 loss: 0.47177187 acc: 82.03125\n",
      "Epoch: 9 batch 349 loss: 0.5570754 acc: 81.25\n",
      "Epoch: 9 batch 350 loss: 0.6797173 acc: 78.125\n",
      "Epoch: 9 batch 351 loss: 0.5339201 acc: 84.375\n",
      "Epoch: 9 batch 352 loss: 0.5117241 acc: 81.25\n",
      "Epoch: 9 batch 353 loss: 0.6241705 acc: 81.25\n",
      "Epoch: 9 batch 354 loss: 0.47824317 acc: 80.46875\n",
      "Epoch: 9 batch 355 loss: 0.710465 acc: 70.3125\n",
      "Epoch: 9 batch 356 loss: 0.6475408 acc: 77.34375\n",
      "Epoch: 9 batch 357 loss: 0.5346911 acc: 82.03125\n",
      "Epoch: 9 batch 358 loss: 0.72317237 acc: 78.125\n",
      "Epoch: 9 batch 359 loss: 0.6379422 acc: 80.46875\n",
      "Epoch: 9 batch 360 loss: 0.45567653 acc: 84.375\n",
      "Epoch: 9 batch 361 loss: 0.50285625 acc: 79.6875\n",
      "Epoch: 9 batch 362 loss: 0.5439656 acc: 75.78125\n",
      "Epoch: 9 batch 363 loss: 0.6856369 acc: 71.09375\n",
      "Epoch: 9 batch 364 loss: 0.5457995 acc: 80.46875\n",
      "Epoch: 9 batch 365 loss: 0.41951668 acc: 85.9375\n",
      "Epoch: 9 batch 366 loss: 0.52955073 acc: 78.90625\n",
      "Epoch: 9 batch 367 loss: 0.6859609 acc: 73.4375\n",
      "Epoch: 9 batch 368 loss: 0.44780853 acc: 84.375\n",
      "Epoch: 9 batch 369 loss: 0.511436 acc: 81.25\n",
      "Epoch: 9 batch 370 loss: 0.43967646 acc: 84.375\n",
      "Epoch: 9 batch 371 loss: 0.55095875 acc: 78.90625\n",
      "Epoch: 9 batch 372 loss: 0.6294606 acc: 79.6875\n",
      "Epoch: 9 batch 373 loss: 0.56759113 acc: 75.0\n",
      "Epoch: 9 batch 374 loss: 0.63235646 acc: 80.46875\n",
      "Epoch: 9 batch 375 loss: 0.8312717 acc: 69.53125\n",
      "Epoch: 9 batch 376 loss: 0.7217886 acc: 82.8125\n",
      "Epoch: 9 batch 377 loss: 0.4789643 acc: 84.375\n",
      "Epoch: 9 batch 378 loss: 0.49461088 acc: 83.59375\n",
      "Epoch: 9 batch 379 loss: 0.5520525 acc: 82.03125\n",
      "Epoch: 9 batch 380 loss: 0.58687365 acc: 82.8125\n",
      "Epoch: 9 batch 381 loss: 0.49775937 acc: 82.03125\n",
      "Epoch: 9 batch 382 loss: 0.5546246 acc: 78.90625\n",
      "Epoch: 9 batch 383 loss: 0.4521519 acc: 81.25\n",
      "Epoch: 9 batch 384 loss: 0.4972022 acc: 85.15625\n",
      "Epoch: 9 batch 385 loss: 0.631526 acc: 75.0\n",
      "Epoch: 9 batch 386 loss: 0.56140393 acc: 84.375\n",
      "Epoch: 9 batch 387 loss: 0.5521278 acc: 81.25\n",
      "Epoch: 9 batch 388 loss: 0.6187486 acc: 78.90625\n",
      "Epoch: 9 batch 389 loss: 0.56872255 acc: 80.46875\n",
      "After epoch 9 test loss 0.8471360444068908 test accuracy 0.713699996471405\n",
      "Epoch: 10 batch 0 loss: 0.49277884 acc: 71.51461243629456\n",
      "Epoch: 10 batch 1 loss: 0.5233136 acc: 75.78125\n",
      "Epoch: 10 batch 2 loss: 0.49390227 acc: 85.9375\n",
      "Epoch: 10 batch 3 loss: 0.5460638 acc: 85.15625\n",
      "Epoch: 10 batch 4 loss: 0.6007316 acc: 79.6875\n",
      "Epoch: 10 batch 5 loss: 0.6563458 acc: 78.90625\n",
      "Epoch: 10 batch 6 loss: 0.6413398 acc: 78.125\n",
      "Epoch: 10 batch 7 loss: 0.4688465 acc: 83.59375\n",
      "Epoch: 10 batch 8 loss: 0.642544 acc: 80.46875\n",
      "Epoch: 10 batch 9 loss: 0.45966586 acc: 89.0625\n",
      "Epoch: 10 batch 10 loss: 0.59672475 acc: 80.46875\n",
      "Epoch: 10 batch 11 loss: 0.5767587 acc: 79.6875\n",
      "Epoch: 10 batch 12 loss: 0.6270386 acc: 80.46875\n",
      "Epoch: 10 batch 13 loss: 0.48079592 acc: 82.8125\n",
      "Epoch: 10 batch 14 loss: 0.49135202 acc: 83.59375\n",
      "Epoch: 10 batch 15 loss: 0.62701464 acc: 78.125\n",
      "Epoch: 10 batch 16 loss: 0.43882146 acc: 85.15625\n",
      "Epoch: 10 batch 17 loss: 0.48240823 acc: 84.375\n",
      "Epoch: 10 batch 18 loss: 0.38713452 acc: 85.9375\n",
      "Epoch: 10 batch 19 loss: 0.5152574 acc: 81.25\n",
      "Epoch: 10 batch 20 loss: 0.4363029 acc: 85.9375\n",
      "Epoch: 10 batch 21 loss: 0.6174195 acc: 80.46875\n",
      "Epoch: 10 batch 22 loss: 0.61576504 acc: 78.90625\n",
      "Epoch: 10 batch 23 loss: 0.66987896 acc: 77.34375\n",
      "Epoch: 10 batch 24 loss: 0.581569 acc: 79.6875\n",
      "Epoch: 10 batch 25 loss: 0.699675 acc: 75.78125\n",
      "Epoch: 10 batch 26 loss: 0.75949836 acc: 78.125\n",
      "Epoch: 10 batch 27 loss: 0.5162974 acc: 82.8125\n",
      "Epoch: 10 batch 28 loss: 0.51922023 acc: 82.8125\n",
      "Epoch: 10 batch 29 loss: 0.4983607 acc: 84.375\n",
      "Epoch: 10 batch 30 loss: 0.5325112 acc: 82.8125\n",
      "Epoch: 10 batch 31 loss: 0.46359837 acc: 82.03125\n",
      "Epoch: 10 batch 32 loss: 0.6273663 acc: 80.46875\n",
      "Epoch: 10 batch 33 loss: 0.54818135 acc: 83.59375\n",
      "Epoch: 10 batch 34 loss: 0.54837126 acc: 82.8125\n",
      "Epoch: 10 batch 35 loss: 0.62572604 acc: 81.25\n",
      "Epoch: 10 batch 36 loss: 0.4724149 acc: 84.375\n",
      "Epoch: 10 batch 37 loss: 0.58114654 acc: 80.46875\n",
      "Epoch: 10 batch 38 loss: 0.6569135 acc: 80.46875\n",
      "Epoch: 10 batch 39 loss: 0.6096903 acc: 81.25\n",
      "Epoch: 10 batch 40 loss: 0.6247718 acc: 76.5625\n",
      "Epoch: 10 batch 41 loss: 0.5117361 acc: 81.25\n",
      "Epoch: 10 batch 42 loss: 0.63380224 acc: 80.46875\n",
      "Epoch: 10 batch 43 loss: 0.7069131 acc: 75.78125\n",
      "Epoch: 10 batch 44 loss: 0.5409767 acc: 78.125\n",
      "Epoch: 10 batch 45 loss: 0.63869727 acc: 75.0\n",
      "Epoch: 10 batch 46 loss: 0.5027064 acc: 84.375\n",
      "Epoch: 10 batch 47 loss: 0.42776012 acc: 85.9375\n",
      "Epoch: 10 batch 48 loss: 0.5065898 acc: 81.25\n",
      "Epoch: 10 batch 49 loss: 0.48982066 acc: 82.03125\n",
      "Epoch: 10 batch 50 loss: 0.5410648 acc: 81.25\n",
      "Epoch: 10 batch 51 loss: 0.58146846 acc: 80.46875\n",
      "Epoch: 10 batch 52 loss: 0.6519638 acc: 75.0\n",
      "Epoch: 10 batch 53 loss: 0.6097638 acc: 76.5625\n",
      "Epoch: 10 batch 54 loss: 0.42197645 acc: 84.375\n",
      "Epoch: 10 batch 55 loss: 0.5514662 acc: 76.5625\n",
      "Epoch: 10 batch 56 loss: 0.49686772 acc: 82.03125\n",
      "Epoch: 10 batch 57 loss: 0.46252537 acc: 82.8125\n",
      "Epoch: 10 batch 58 loss: 0.41552714 acc: 85.15625\n",
      "Epoch: 10 batch 59 loss: 0.6018256 acc: 80.46875\n",
      "Epoch: 10 batch 60 loss: 0.49307665 acc: 85.15625\n",
      "Epoch: 10 batch 61 loss: 0.6014293 acc: 78.90625\n",
      "Epoch: 10 batch 62 loss: 0.6184777 acc: 75.78125\n",
      "Epoch: 10 batch 63 loss: 0.47940275 acc: 85.15625\n",
      "Epoch: 10 batch 64 loss: 0.56191915 acc: 83.59375\n",
      "Epoch: 10 batch 65 loss: 0.47814465 acc: 78.125\n",
      "Epoch: 10 batch 66 loss: 0.6142641 acc: 76.5625\n",
      "Epoch: 10 batch 67 loss: 0.46761477 acc: 85.15625\n",
      "Epoch: 10 batch 68 loss: 0.5712639 acc: 76.5625\n",
      "Epoch: 10 batch 69 loss: 0.5909624 acc: 84.375\n",
      "Epoch: 10 batch 70 loss: 0.6927762 acc: 75.0\n",
      "Epoch: 10 batch 71 loss: 0.5783758 acc: 82.03125\n",
      "Epoch: 10 batch 72 loss: 0.5336261 acc: 81.25\n",
      "Epoch: 10 batch 73 loss: 0.53840095 acc: 80.46875\n",
      "Epoch: 10 batch 74 loss: 0.5208173 acc: 81.25\n",
      "Epoch: 10 batch 75 loss: 0.52352196 acc: 81.25\n",
      "Epoch: 10 batch 76 loss: 0.4945383 acc: 85.15625\n",
      "Epoch: 10 batch 77 loss: 0.3973763 acc: 85.15625\n",
      "Epoch: 10 batch 78 loss: 0.45601496 acc: 87.5\n",
      "Epoch: 10 batch 79 loss: 0.5477594 acc: 79.6875\n",
      "Epoch: 10 batch 80 loss: 0.51165855 acc: 82.03125\n",
      "Epoch: 10 batch 81 loss: 0.7863022 acc: 73.4375\n",
      "Epoch: 10 batch 82 loss: 0.5137981 acc: 77.34375\n",
      "Epoch: 10 batch 83 loss: 0.52384305 acc: 83.59375\n",
      "Epoch: 10 batch 84 loss: 0.6744874 acc: 78.90625\n",
      "Epoch: 10 batch 85 loss: 0.59731185 acc: 80.46875\n",
      "Epoch: 10 batch 86 loss: 0.6459827 acc: 77.34375\n",
      "Epoch: 10 batch 87 loss: 0.50160146 acc: 82.8125\n",
      "Epoch: 10 batch 88 loss: 0.5333391 acc: 81.25\n",
      "Epoch: 10 batch 89 loss: 0.4365549 acc: 85.15625\n",
      "Epoch: 10 batch 90 loss: 0.47780997 acc: 82.8125\n",
      "Epoch: 10 batch 91 loss: 0.5542204 acc: 82.03125\n",
      "Epoch: 10 batch 92 loss: 0.5290891 acc: 82.03125\n",
      "Epoch: 10 batch 93 loss: 0.4608674 acc: 84.375\n",
      "Epoch: 10 batch 94 loss: 0.6461257 acc: 73.4375\n",
      "Epoch: 10 batch 95 loss: 0.5036122 acc: 80.46875\n",
      "Epoch: 10 batch 96 loss: 0.57416934 acc: 78.125\n",
      "Epoch: 10 batch 97 loss: 0.58334816 acc: 79.6875\n",
      "Epoch: 10 batch 98 loss: 0.5199435 acc: 83.59375\n",
      "Epoch: 10 batch 99 loss: 0.632708 acc: 81.25\n",
      "Epoch: 10 batch 100 loss: 0.47728357 acc: 83.59375\n",
      "Epoch: 10 batch 101 loss: 0.6542842 acc: 75.78125\n",
      "Epoch: 10 batch 102 loss: 0.5071023 acc: 82.03125\n",
      "Epoch: 10 batch 103 loss: 0.57909834 acc: 81.25\n",
      "Epoch: 10 batch 104 loss: 0.6193963 acc: 79.6875\n",
      "Epoch: 10 batch 105 loss: 0.39987984 acc: 84.375\n",
      "Epoch: 10 batch 106 loss: 0.67854893 acc: 76.5625\n",
      "Epoch: 10 batch 107 loss: 0.65253603 acc: 79.6875\n",
      "Epoch: 10 batch 108 loss: 0.5349458 acc: 84.375\n",
      "Epoch: 10 batch 109 loss: 0.523258 acc: 78.90625\n",
      "Epoch: 10 batch 110 loss: 0.6047431 acc: 80.46875\n",
      "Epoch: 10 batch 111 loss: 0.5011511 acc: 82.8125\n",
      "Epoch: 10 batch 112 loss: 0.56083846 acc: 76.5625\n",
      "Epoch: 10 batch 113 loss: 0.556213 acc: 76.5625\n",
      "Epoch: 10 batch 114 loss: 0.5714519 acc: 78.90625\n",
      "Epoch: 10 batch 115 loss: 0.42530522 acc: 86.71875\n",
      "Epoch: 10 batch 116 loss: 0.5360204 acc: 81.25\n",
      "Epoch: 10 batch 117 loss: 0.4804069 acc: 84.375\n",
      "Epoch: 10 batch 118 loss: 0.6202151 acc: 76.5625\n",
      "Epoch: 10 batch 119 loss: 0.5540391 acc: 76.5625\n",
      "Epoch: 10 batch 120 loss: 0.4751351 acc: 86.71875\n",
      "Epoch: 10 batch 121 loss: 0.7096918 acc: 74.21875\n",
      "Epoch: 10 batch 122 loss: 0.65048957 acc: 74.21875\n",
      "Epoch: 10 batch 123 loss: 0.5279522 acc: 80.46875\n",
      "Epoch: 10 batch 124 loss: 0.5844463 acc: 80.46875\n",
      "Epoch: 10 batch 125 loss: 0.5182613 acc: 84.375\n",
      "Epoch: 10 batch 126 loss: 0.44346207 acc: 81.25\n",
      "Epoch: 10 batch 127 loss: 0.47934705 acc: 85.15625\n",
      "Epoch: 10 batch 128 loss: 0.5006196 acc: 81.25\n",
      "Epoch: 10 batch 129 loss: 0.56376207 acc: 80.46875\n",
      "Epoch: 10 batch 130 loss: 0.4884787 acc: 80.46875\n",
      "Epoch: 10 batch 131 loss: 0.59741944 acc: 80.46875\n",
      "Epoch: 10 batch 132 loss: 0.3737194 acc: 85.9375\n",
      "Epoch: 10 batch 133 loss: 0.48326465 acc: 82.8125\n",
      "Epoch: 10 batch 134 loss: 0.46510398 acc: 85.9375\n",
      "Epoch: 10 batch 135 loss: 0.6555315 acc: 81.25\n",
      "Epoch: 10 batch 136 loss: 0.50543416 acc: 82.03125\n",
      "Epoch: 10 batch 137 loss: 0.49870306 acc: 82.8125\n",
      "Epoch: 10 batch 138 loss: 0.6830669 acc: 77.34375\n",
      "Epoch: 10 batch 139 loss: 0.5106486 acc: 81.25\n",
      "Epoch: 10 batch 140 loss: 0.45870173 acc: 85.15625\n",
      "Epoch: 10 batch 141 loss: 0.5509687 acc: 81.25\n",
      "Epoch: 10 batch 142 loss: 0.60416234 acc: 76.5625\n",
      "Epoch: 10 batch 143 loss: 0.5569779 acc: 79.6875\n",
      "Epoch: 10 batch 144 loss: 0.5453193 acc: 84.375\n",
      "Epoch: 10 batch 145 loss: 0.5962262 acc: 82.03125\n",
      "Epoch: 10 batch 146 loss: 0.5248743 acc: 79.6875\n",
      "Epoch: 10 batch 147 loss: 0.46135455 acc: 82.8125\n",
      "Epoch: 10 batch 148 loss: 0.57518995 acc: 78.90625\n",
      "Epoch: 10 batch 149 loss: 0.4893005 acc: 80.46875\n",
      "Epoch: 10 batch 150 loss: 0.6161219 acc: 77.34375\n",
      "Epoch: 10 batch 151 loss: 0.5336171 acc: 81.25\n",
      "Epoch: 10 batch 152 loss: 0.6445466 acc: 73.4375\n",
      "Epoch: 10 batch 153 loss: 0.5680835 acc: 82.03125\n",
      "Epoch: 10 batch 154 loss: 0.6358267 acc: 77.34375\n",
      "Epoch: 10 batch 155 loss: 0.5926215 acc: 79.6875\n",
      "Epoch: 10 batch 156 loss: 0.48623055 acc: 79.6875\n",
      "Epoch: 10 batch 157 loss: 0.46708447 acc: 85.15625\n",
      "Epoch: 10 batch 158 loss: 0.521995 acc: 80.46875\n",
      "Epoch: 10 batch 159 loss: 0.44507605 acc: 87.5\n",
      "Epoch: 10 batch 160 loss: 0.48120865 acc: 81.25\n",
      "Epoch: 10 batch 161 loss: 0.6060883 acc: 80.46875\n",
      "Epoch: 10 batch 162 loss: 0.52813613 acc: 83.59375\n",
      "Epoch: 10 batch 163 loss: 0.47340062 acc: 80.46875\n",
      "Epoch: 10 batch 164 loss: 0.4370398 acc: 85.15625\n",
      "Epoch: 10 batch 165 loss: 0.5392181 acc: 82.8125\n",
      "Epoch: 10 batch 166 loss: 0.58826286 acc: 80.46875\n",
      "Epoch: 10 batch 167 loss: 0.72600055 acc: 73.4375\n",
      "Epoch: 10 batch 168 loss: 0.6806247 acc: 75.78125\n",
      "Epoch: 10 batch 169 loss: 0.53595006 acc: 80.46875\n",
      "Epoch: 10 batch 170 loss: 0.5667838 acc: 81.25\n",
      "Epoch: 10 batch 171 loss: 0.66383106 acc: 75.78125\n",
      "Epoch: 10 batch 172 loss: 0.5200615 acc: 77.34375\n",
      "Epoch: 10 batch 173 loss: 0.49483812 acc: 78.90625\n",
      "Epoch: 10 batch 174 loss: 0.48846963 acc: 82.8125\n",
      "Epoch: 10 batch 175 loss: 0.6924941 acc: 75.0\n",
      "Epoch: 10 batch 176 loss: 0.448553 acc: 85.9375\n",
      "Epoch: 10 batch 177 loss: 0.56627333 acc: 85.15625\n",
      "Epoch: 10 batch 178 loss: 0.7546963 acc: 70.3125\n",
      "Epoch: 10 batch 179 loss: 0.49445462 acc: 80.46875\n",
      "Epoch: 10 batch 180 loss: 0.50851715 acc: 79.6875\n",
      "Epoch: 10 batch 181 loss: 0.56903505 acc: 77.34375\n",
      "Epoch: 10 batch 182 loss: 0.3498993 acc: 89.0625\n",
      "Epoch: 10 batch 183 loss: 0.70546937 acc: 78.125\n",
      "Epoch: 10 batch 184 loss: 0.4736364 acc: 81.25\n",
      "Epoch: 10 batch 185 loss: 0.46652672 acc: 82.8125\n",
      "Epoch: 10 batch 186 loss: 0.45619723 acc: 86.71875\n",
      "Epoch: 10 batch 187 loss: 0.38491496 acc: 89.84375\n",
      "Epoch: 10 batch 188 loss: 0.47426188 acc: 81.25\n",
      "Epoch: 10 batch 189 loss: 0.46580195 acc: 82.03125\n",
      "Epoch: 10 batch 190 loss: 0.6671186 acc: 78.125\n",
      "Epoch: 10 batch 191 loss: 0.5192511 acc: 82.8125\n",
      "Epoch: 10 batch 192 loss: 0.49286586 acc: 82.8125\n",
      "Epoch: 10 batch 193 loss: 0.67243123 acc: 80.46875\n",
      "Epoch: 10 batch 194 loss: 0.5833337 acc: 79.6875\n",
      "Epoch: 10 batch 195 loss: 0.41816244 acc: 85.9375\n",
      "Epoch: 10 batch 196 loss: 0.45866117 acc: 83.59375\n",
      "Epoch: 10 batch 197 loss: 0.49851227 acc: 82.8125\n",
      "Epoch: 10 batch 198 loss: 0.51844734 acc: 82.03125\n",
      "Epoch: 10 batch 199 loss: 0.57711685 acc: 83.59375\n",
      "Epoch: 10 batch 200 loss: 0.7386941 acc: 72.65625\n",
      "Epoch: 10 batch 201 loss: 0.47031176 acc: 83.59375\n",
      "Epoch: 10 batch 202 loss: 0.49520868 acc: 85.9375\n",
      "Epoch: 10 batch 203 loss: 0.3685736 acc: 87.5\n",
      "Epoch: 10 batch 204 loss: 0.49232215 acc: 78.90625\n",
      "Epoch: 10 batch 205 loss: 0.6275519 acc: 79.6875\n",
      "Epoch: 10 batch 206 loss: 0.5444961 acc: 78.125\n",
      "Epoch: 10 batch 207 loss: 0.40118167 acc: 83.59375\n",
      "Epoch: 10 batch 208 loss: 0.5413468 acc: 82.8125\n",
      "Epoch: 10 batch 209 loss: 0.5742238 acc: 78.125\n",
      "Epoch: 10 batch 210 loss: 0.47918993 acc: 83.59375\n",
      "Epoch: 10 batch 211 loss: 0.5041467 acc: 80.46875\n",
      "Epoch: 10 batch 212 loss: 0.5884142 acc: 81.25\n",
      "Epoch: 10 batch 213 loss: 0.43795317 acc: 89.84375\n",
      "Epoch: 10 batch 214 loss: 0.6671166 acc: 75.78125\n",
      "Epoch: 10 batch 215 loss: 0.5787715 acc: 78.90625\n",
      "Epoch: 10 batch 216 loss: 0.4476987 acc: 87.5\n",
      "Epoch: 10 batch 217 loss: 0.51625174 acc: 78.90625\n",
      "Epoch: 10 batch 218 loss: 0.42483515 acc: 83.59375\n",
      "Epoch: 10 batch 219 loss: 0.49948406 acc: 82.8125\n",
      "Epoch: 10 batch 220 loss: 0.4393345 acc: 81.25\n",
      "Epoch: 10 batch 221 loss: 0.50517094 acc: 79.6875\n",
      "Epoch: 10 batch 222 loss: 0.5060184 acc: 82.03125\n",
      "Epoch: 10 batch 223 loss: 0.5520281 acc: 80.46875\n",
      "Epoch: 10 batch 224 loss: 0.49363422 acc: 82.8125\n",
      "Epoch: 10 batch 225 loss: 0.45782986 acc: 80.46875\n",
      "Epoch: 10 batch 226 loss: 0.44201618 acc: 84.375\n",
      "Epoch: 10 batch 227 loss: 0.60930383 acc: 77.34375\n",
      "Epoch: 10 batch 228 loss: 0.49199104 acc: 82.03125\n",
      "Epoch: 10 batch 229 loss: 0.35779166 acc: 87.5\n",
      "Epoch: 10 batch 230 loss: 0.44983047 acc: 81.25\n",
      "Epoch: 10 batch 231 loss: 0.493377 acc: 82.8125\n",
      "Epoch: 10 batch 232 loss: 0.4442441 acc: 85.9375\n",
      "Epoch: 10 batch 233 loss: 0.42005938 acc: 84.375\n",
      "Epoch: 10 batch 234 loss: 0.47277588 acc: 85.15625\n",
      "Epoch: 10 batch 235 loss: 0.56292427 acc: 78.90625\n",
      "Epoch: 10 batch 236 loss: 0.4874897 acc: 81.25\n",
      "Epoch: 10 batch 237 loss: 0.52790964 acc: 78.90625\n",
      "Epoch: 10 batch 238 loss: 0.488513 acc: 85.15625\n",
      "Epoch: 10 batch 239 loss: 0.46652883 acc: 85.15625\n",
      "Epoch: 10 batch 240 loss: 0.6873169 acc: 75.78125\n",
      "Epoch: 10 batch 241 loss: 0.63615584 acc: 80.46875\n",
      "Epoch: 10 batch 242 loss: 0.47115466 acc: 85.15625\n",
      "Epoch: 10 batch 243 loss: 0.5073209 acc: 81.25\n",
      "Epoch: 10 batch 244 loss: 0.5812935 acc: 81.25\n",
      "Epoch: 10 batch 245 loss: 0.5062938 acc: 80.46875\n",
      "Epoch: 10 batch 246 loss: 0.48195985 acc: 83.59375\n",
      "Epoch: 10 batch 247 loss: 0.4338772 acc: 85.9375\n",
      "Epoch: 10 batch 248 loss: 0.6055349 acc: 78.90625\n",
      "Epoch: 10 batch 249 loss: 0.57961863 acc: 82.03125\n",
      "Epoch: 10 batch 250 loss: 0.5619919 acc: 80.46875\n",
      "Epoch: 10 batch 251 loss: 0.552897 acc: 79.6875\n",
      "Epoch: 10 batch 252 loss: 0.56823397 acc: 79.6875\n",
      "Epoch: 10 batch 253 loss: 0.5291544 acc: 82.03125\n",
      "Epoch: 10 batch 254 loss: 0.6041378 acc: 76.5625\n",
      "Epoch: 10 batch 255 loss: 0.5979158 acc: 78.90625\n",
      "Epoch: 10 batch 256 loss: 0.6323127 acc: 76.5625\n",
      "Epoch: 10 batch 257 loss: 0.56520593 acc: 76.5625\n",
      "Epoch: 10 batch 258 loss: 0.55153096 acc: 82.8125\n",
      "Epoch: 10 batch 259 loss: 0.6167166 acc: 80.46875\n",
      "Epoch: 10 batch 260 loss: 0.55969423 acc: 78.125\n",
      "Epoch: 10 batch 261 loss: 0.6191735 acc: 80.46875\n",
      "Epoch: 10 batch 262 loss: 0.60444254 acc: 75.0\n",
      "Epoch: 10 batch 263 loss: 0.43124688 acc: 85.15625\n",
      "Epoch: 10 batch 264 loss: 0.46301943 acc: 86.71875\n",
      "Epoch: 10 batch 265 loss: 0.5798216 acc: 78.125\n",
      "Epoch: 10 batch 266 loss: 0.6445756 acc: 78.125\n",
      "Epoch: 10 batch 267 loss: 0.565769 acc: 78.90625\n",
      "Epoch: 10 batch 268 loss: 0.69238985 acc: 75.78125\n",
      "Epoch: 10 batch 269 loss: 0.5012696 acc: 84.375\n",
      "Epoch: 10 batch 270 loss: 0.5277848 acc: 78.125\n",
      "Epoch: 10 batch 271 loss: 0.5648311 acc: 79.6875\n",
      "Epoch: 10 batch 272 loss: 0.4243783 acc: 84.375\n",
      "Epoch: 10 batch 273 loss: 0.6152013 acc: 78.125\n",
      "Epoch: 10 batch 274 loss: 0.5604702 acc: 80.46875\n",
      "Epoch: 10 batch 275 loss: 0.5779584 acc: 79.6875\n",
      "Epoch: 10 batch 276 loss: 0.46889526 acc: 82.03125\n",
      "Epoch: 10 batch 277 loss: 0.53734523 acc: 82.03125\n",
      "Epoch: 10 batch 278 loss: 0.60258555 acc: 77.34375\n",
      "Epoch: 10 batch 279 loss: 0.5948153 acc: 84.375\n",
      "Epoch: 10 batch 280 loss: 0.4352214 acc: 83.59375\n",
      "Epoch: 10 batch 281 loss: 0.45719016 acc: 84.375\n",
      "Epoch: 10 batch 282 loss: 0.48165303 acc: 79.6875\n",
      "Epoch: 10 batch 283 loss: 0.47146273 acc: 84.375\n",
      "Epoch: 10 batch 284 loss: 0.6070998 acc: 80.46875\n",
      "Epoch: 10 batch 285 loss: 0.46998194 acc: 83.59375\n",
      "Epoch: 10 batch 286 loss: 0.62276363 acc: 79.6875\n",
      "Epoch: 10 batch 287 loss: 0.43989497 acc: 85.15625\n",
      "Epoch: 10 batch 288 loss: 0.67770344 acc: 76.5625\n",
      "Epoch: 10 batch 289 loss: 0.5266448 acc: 79.6875\n",
      "Epoch: 10 batch 290 loss: 0.58471084 acc: 77.34375\n",
      "Epoch: 10 batch 291 loss: 0.7469049 acc: 78.125\n",
      "Epoch: 10 batch 292 loss: 0.6298701 acc: 78.125\n",
      "Epoch: 10 batch 293 loss: 0.54124606 acc: 76.5625\n",
      "Epoch: 10 batch 294 loss: 0.6409179 acc: 79.6875\n",
      "Epoch: 10 batch 295 loss: 0.5484481 acc: 83.59375\n",
      "Epoch: 10 batch 296 loss: 0.7092941 acc: 71.09375\n",
      "Epoch: 10 batch 297 loss: 0.51489264 acc: 82.03125\n",
      "Epoch: 10 batch 298 loss: 0.60742474 acc: 79.6875\n",
      "Epoch: 10 batch 299 loss: 0.5558119 acc: 82.03125\n",
      "Epoch: 10 batch 300 loss: 0.44281837 acc: 86.71875\n",
      "Epoch: 10 batch 301 loss: 0.47726774 acc: 83.59375\n",
      "Epoch: 10 batch 302 loss: 0.39682004 acc: 85.9375\n",
      "Epoch: 10 batch 303 loss: 0.33214054 acc: 86.71875\n",
      "Epoch: 10 batch 304 loss: 0.5038053 acc: 78.125\n",
      "Epoch: 10 batch 305 loss: 0.44800293 acc: 81.25\n",
      "Epoch: 10 batch 306 loss: 0.4645395 acc: 80.46875\n",
      "Epoch: 10 batch 307 loss: 0.5916474 acc: 79.6875\n",
      "Epoch: 10 batch 308 loss: 0.59295285 acc: 80.46875\n",
      "Epoch: 10 batch 309 loss: 0.628044 acc: 75.78125\n",
      "Epoch: 10 batch 310 loss: 0.71055275 acc: 75.0\n",
      "Epoch: 10 batch 311 loss: 0.4606186 acc: 82.8125\n",
      "Epoch: 10 batch 312 loss: 0.47364262 acc: 85.9375\n",
      "Epoch: 10 batch 313 loss: 0.51304156 acc: 80.46875\n",
      "Epoch: 10 batch 314 loss: 0.5450486 acc: 80.46875\n",
      "Epoch: 10 batch 315 loss: 0.65651417 acc: 77.34375\n",
      "Epoch: 10 batch 316 loss: 0.5267141 acc: 85.15625\n",
      "Epoch: 10 batch 317 loss: 0.60082644 acc: 75.78125\n",
      "Epoch: 10 batch 318 loss: 0.5250116 acc: 81.25\n",
      "Epoch: 10 batch 319 loss: 0.6567692 acc: 78.90625\n",
      "Epoch: 10 batch 320 loss: 0.61173594 acc: 81.25\n",
      "Epoch: 10 batch 321 loss: 0.45385277 acc: 85.9375\n",
      "Epoch: 10 batch 322 loss: 0.4805346 acc: 83.59375\n",
      "Epoch: 10 batch 323 loss: 0.6206632 acc: 78.125\n",
      "Epoch: 10 batch 324 loss: 0.692635 acc: 74.21875\n",
      "Epoch: 10 batch 325 loss: 0.4782731 acc: 84.375\n",
      "Epoch: 10 batch 326 loss: 0.42534855 acc: 85.9375\n",
      "Epoch: 10 batch 327 loss: 0.63473815 acc: 80.46875\n",
      "Epoch: 10 batch 328 loss: 0.53545654 acc: 84.375\n",
      "Epoch: 10 batch 329 loss: 0.5940035 acc: 82.8125\n",
      "Epoch: 10 batch 330 loss: 0.4200821 acc: 85.9375\n",
      "Epoch: 10 batch 331 loss: 0.50300103 acc: 78.90625\n",
      "Epoch: 10 batch 332 loss: 0.6357622 acc: 77.34375\n",
      "Epoch: 10 batch 333 loss: 0.46384746 acc: 85.15625\n",
      "Epoch: 10 batch 334 loss: 0.44130158 acc: 84.375\n",
      "Epoch: 10 batch 335 loss: 0.65660495 acc: 77.34375\n",
      "Epoch: 10 batch 336 loss: 0.49855602 acc: 80.46875\n",
      "Epoch: 10 batch 337 loss: 0.6611326 acc: 76.5625\n",
      "Epoch: 10 batch 338 loss: 0.6769892 acc: 78.125\n",
      "Epoch: 10 batch 339 loss: 0.66261125 acc: 74.21875\n",
      "Epoch: 10 batch 340 loss: 0.49815667 acc: 82.03125\n",
      "Epoch: 10 batch 341 loss: 0.4779157 acc: 85.9375\n",
      "Epoch: 10 batch 342 loss: 0.6062163 acc: 70.3125\n",
      "Epoch: 10 batch 343 loss: 0.48467433 acc: 82.8125\n",
      "Epoch: 10 batch 344 loss: 0.53924 acc: 82.8125\n",
      "Epoch: 10 batch 345 loss: 0.6367403 acc: 75.78125\n",
      "Epoch: 10 batch 346 loss: 0.574541 acc: 73.4375\n",
      "Epoch: 10 batch 347 loss: 0.52338916 acc: 81.25\n",
      "Epoch: 10 batch 348 loss: 0.44546354 acc: 87.5\n",
      "Epoch: 10 batch 349 loss: 0.5599381 acc: 82.8125\n",
      "Epoch: 10 batch 350 loss: 0.591678 acc: 75.78125\n",
      "Epoch: 10 batch 351 loss: 0.44603878 acc: 78.90625\n",
      "Epoch: 10 batch 352 loss: 0.3959854 acc: 85.9375\n",
      "Epoch: 10 batch 353 loss: 0.48471197 acc: 85.15625\n",
      "Epoch: 10 batch 354 loss: 0.37301478 acc: 87.5\n",
      "Epoch: 10 batch 355 loss: 0.55130136 acc: 82.03125\n",
      "Epoch: 10 batch 356 loss: 0.62600267 acc: 75.0\n",
      "Epoch: 10 batch 357 loss: 0.32737643 acc: 89.84375\n",
      "Epoch: 10 batch 358 loss: 0.5962875 acc: 78.125\n",
      "Epoch: 10 batch 359 loss: 0.44658923 acc: 86.71875\n",
      "Epoch: 10 batch 360 loss: 0.4512173 acc: 82.03125\n",
      "Epoch: 10 batch 361 loss: 0.5977324 acc: 76.5625\n",
      "Epoch: 10 batch 362 loss: 0.4711833 acc: 80.46875\n",
      "Epoch: 10 batch 363 loss: 0.62922335 acc: 75.0\n",
      "Epoch: 10 batch 364 loss: 0.44662064 acc: 88.28125\n",
      "Epoch: 10 batch 365 loss: 0.45634288 acc: 86.71875\n",
      "Epoch: 10 batch 366 loss: 0.5110433 acc: 81.25\n",
      "Epoch: 10 batch 367 loss: 0.73584247 acc: 78.125\n",
      "Epoch: 10 batch 368 loss: 0.43870318 acc: 85.9375\n",
      "Epoch: 10 batch 369 loss: 0.50871193 acc: 83.59375\n",
      "Epoch: 10 batch 370 loss: 0.539783 acc: 79.6875\n",
      "Epoch: 10 batch 371 loss: 0.546389 acc: 80.46875\n",
      "Epoch: 10 batch 372 loss: 0.48305944 acc: 84.375\n",
      "Epoch: 10 batch 373 loss: 0.580445 acc: 82.03125\n",
      "Epoch: 10 batch 374 loss: 0.5230954 acc: 82.03125\n",
      "Epoch: 10 batch 375 loss: 0.6334976 acc: 77.34375\n",
      "Epoch: 10 batch 376 loss: 0.5148373 acc: 86.71875\n",
      "Epoch: 10 batch 377 loss: 0.47089404 acc: 85.15625\n",
      "Epoch: 10 batch 378 loss: 0.4709432 acc: 82.03125\n",
      "Epoch: 10 batch 379 loss: 0.5750059 acc: 78.125\n",
      "Epoch: 10 batch 380 loss: 0.489716 acc: 81.25\n",
      "Epoch: 10 batch 381 loss: 0.48995915 acc: 83.59375\n",
      "Epoch: 10 batch 382 loss: 0.5721549 acc: 78.125\n",
      "Epoch: 10 batch 383 loss: 0.5312669 acc: 79.6875\n",
      "Epoch: 10 batch 384 loss: 0.3892794 acc: 87.5\n",
      "Epoch: 10 batch 385 loss: 0.54920894 acc: 78.125\n",
      "Epoch: 10 batch 386 loss: 0.5080883 acc: 85.9375\n",
      "Epoch: 10 batch 387 loss: 0.46519938 acc: 84.375\n",
      "Epoch: 10 batch 388 loss: 0.41884613 acc: 82.8125\n",
      "Epoch: 10 batch 389 loss: 0.55074024 acc: 80.46875\n",
      "After epoch 10 test loss 0.8768823460578918 test accuracy 0.7210000157356262\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import keras\n",
    "from keract import get_activations\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "'''\n",
    "Function that returns the trainand test data of the CIFAR10 already preprocessed\n",
    "'''\n",
    "def getCIFAR10():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 32, 32\n",
    "    num_classes = 10\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # format of the tensor\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "        input_shape = (3, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "    # convert in to float the images\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # new normalization with z-score\n",
    "    mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "    std = np.std(x_train,axis=(0,1,2,3))\n",
    "    x_train = (x_train-mean)/(std+1e-7)\n",
    "    x_test = (x_test-mean)/(std+1e-7)\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    print('CIFAR10 loaded')\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "'''\n",
    "Small function that returns the shape of the CIFAR10 images\n",
    "'''\n",
    "def getCIFAR10InputShape():\n",
    "    img_rows, img_cols = 32, 32\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "        \n",
    "    return input_shape\n",
    "    \n",
    "'''\n",
    "Function that returns a simple student done by 2 convolutions, a maxpool and a final two fully connected layers\n",
    "'''\n",
    "def getSimpleStudent(input_shape):\n",
    "    num_classes = 10\n",
    "    #model definition\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    print('Simple student loaded')\n",
    "    return model\n",
    "    \n",
    "'''\n",
    "Function to try to train the simple sutdent in order to unerstand its capabilites\n",
    "'''\n",
    "def trainSimpleStudent(epochs):\n",
    "    \n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    \n",
    "    input_shape = getCIFAR10InputShape()\n",
    "    \n",
    "    model = getSimpleStudent(input_shape)\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    batch_size = 128\n",
    "    n_batches = math.floor( x_train.shape[0] / batch_size)\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        for i in range(0,n_batches):\n",
    "            imgs = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            loss = model.train_on_batch(imgs,labels)\n",
    "            print(\"Epoch: \" + str(e+1) + \" batch \" + str(i) + \" loss: \" + str(loss[0]) + \" acc: \" + str( 100*loss[1]))\n",
    "            \n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('After epoch ' + str(e+1) + ' test loss ' + str(score[0]) + ' test accuracy ' + str(score[1]))\n",
    "\n",
    "trainSimpleStudent(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pseudocodice per il paper\n",
    "\n",
    "# for i in range(batches):\n",
    "    # z = noise(100)\n",
    "    # generated_images = Generator(z)\n",
    "    # (output, teacher_activations) = teacher(generated_images)\n",
    "    ''' Teacher is the pre-trained network that outputs its activations and the result \n",
    "    or it can outputs only the result and we can get the activations with K.function '''\n",
    "    # combined.train_on_batch(generated_images, (outputs,teacher_activations)) \n",
    "    '''Combined is a network that has at the start the generator and then the freezed student.\n",
    "    The labels it gets are the results and the activations of the teacher\n",
    "    The loss is to increase the distance between the labels and the output of itself\n",
    "    therefore this network has to output its intermediate activations'''\n",
    "    \n",
    "    # for j in range(ns):\n",
    "        # student.train_on_batch(generated_images, (outputs,teacher_activations))\n",
    "        ''' Student network that outputs its results and its intermediate activations, \n",
    "        and its loss is to match the output and activations of the teacher'''\n",
    "\n",
    "    ''' if i don't find a way to output intermediate activations from a model we could always\n",
    "        get the activations with K.fuction and the data, and then input them to the loss as the label.\n",
    "        This method does not seems to increase too much the training time...\n",
    "    '''\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
