{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Wide-Resnet 16x2\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "            Conv2d-3           [-1, 32, 32, 32]           4,608\n",
      "           Dropout-4           [-1, 32, 32, 32]               0\n",
      "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
      "            Conv2d-6           [-1, 32, 32, 32]           9,216\n",
      "            Conv2d-7           [-1, 32, 32, 32]             512\n",
      "        wide_basic-8           [-1, 32, 32, 32]               0\n",
      "       BatchNorm2d-9           [-1, 32, 32, 32]              64\n",
      "           Conv2d-10           [-1, 32, 32, 32]           9,216\n",
      "          Dropout-11           [-1, 32, 32, 32]               0\n",
      "      BatchNorm2d-12           [-1, 32, 32, 32]              64\n",
      "           Conv2d-13           [-1, 32, 32, 32]           9,216\n",
      "       wide_basic-14           [-1, 32, 32, 32]               0\n",
      "      BatchNorm2d-15           [-1, 32, 32, 32]              64\n",
      "           Conv2d-16           [-1, 64, 16, 16]          18,432\n",
      "          Dropout-17           [-1, 64, 16, 16]               0\n",
      "      BatchNorm2d-18           [-1, 64, 16, 16]             128\n",
      "           Conv2d-19           [-1, 64, 16, 16]          36,864\n",
      "           Conv2d-20           [-1, 64, 16, 16]           2,048\n",
      "       wide_basic-21           [-1, 64, 16, 16]               0\n",
      "      BatchNorm2d-22           [-1, 64, 16, 16]             128\n",
      "           Conv2d-23           [-1, 64, 16, 16]          36,864\n",
      "          Dropout-24           [-1, 64, 16, 16]               0\n",
      "      BatchNorm2d-25           [-1, 64, 16, 16]             128\n",
      "           Conv2d-26           [-1, 64, 16, 16]          36,864\n",
      "       wide_basic-27           [-1, 64, 16, 16]               0\n",
      "      BatchNorm2d-28           [-1, 64, 16, 16]             128\n",
      "           Conv2d-29            [-1, 128, 8, 8]          73,728\n",
      "          Dropout-30            [-1, 128, 8, 8]               0\n",
      "      BatchNorm2d-31            [-1, 128, 8, 8]             256\n",
      "           Conv2d-32            [-1, 128, 8, 8]         147,456\n",
      "           Conv2d-33            [-1, 128, 8, 8]           8,192\n",
      "       wide_basic-34            [-1, 128, 8, 8]               0\n",
      "      BatchNorm2d-35            [-1, 128, 8, 8]             256\n",
      "           Conv2d-36            [-1, 128, 8, 8]         147,456\n",
      "          Dropout-37            [-1, 128, 8, 8]               0\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "           Conv2d-39            [-1, 128, 8, 8]         147,456\n",
      "       wide_basic-40            [-1, 128, 8, 8]               0\n",
      "      BatchNorm2d-41            [-1, 128, 8, 8]             256\n",
      "           Linear-42                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 691,674\n",
      "Trainable params: 691,674\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.94\n",
      "Params size (MB): 2.64\n",
      "Estimated Total Size (MB): 8.59\n",
      "----------------------------------------------------------------\n",
      "| Wide-Resnet 16x2\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 32, 32, 32])\n",
      "torch.Size([1, 64, 16, 16])\n",
      "torch.Size([1, 128, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# net = Wide_ResNet(16, 2, 0, 10) #(depth, widen_factor, dropout_rate, num_classes)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n",
    "        init.constant(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.constant(m.weight, 1)\n",
    "        init.constant(m.bias, 0)\n",
    "\n",
    "class wide_basic(nn.Module):\n",
    "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
    "        super(wide_basic, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += self.shortcut(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Wide_ResNet(nn.Module):\n",
    "    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
    "        super(Wide_ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        assert ((depth-4)%6 ==0), 'Wide-resnet depth should be 6n+4'\n",
    "        n = (depth-4)/6\n",
    "        k = widen_factor\n",
    "\n",
    "        print('| Wide-Resnet %dx%d' %(depth, widen_factor))\n",
    "        nStages = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._wide_layer(wide_basic, nStages[1], n, dropout_rate, stride=1)\n",
    "        self.layer2 = self._wide_layer(wide_basic, nStages[2], n, dropout_rate, stride=2)\n",
    "        self.layer3 = self._wide_layer(wide_basic, nStages[3], n, dropout_rate, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n",
    "        self.linear = nn.Linear(nStages[3], num_classes)\n",
    "\n",
    "    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
    "        #print(type(num_blocks))\n",
    "        strides = [stride] + [1]*(int(num_blocks)-1)\n",
    "        layers = []\n",
    "\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
    "            self.in_planes = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        act1 = out\n",
    "        out = self.layer2(out)\n",
    "        act2 = out\n",
    "        out = self.layer3(out)\n",
    "        act3 = out\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, act1, act2, act3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    \n",
    "    model = Wide_ResNet(depth=16, widen_factor=2, dropout_rate=0.0, num_classes=10).to(device)\n",
    "    \n",
    "    summary(model, input_size=(3, 32, 32))\n",
    "    \n",
    "        \n",
    "    \n",
    "    net=Wide_ResNet(16, 2, 0, 10)\n",
    "    \n",
    "    y = net(Variable(torch.randn(1,3,32,32)))[0]\n",
    "    a1 = net(Variable(torch.randn(1,3,32,32)))[1]\n",
    "    a2 = net(Variable(torch.randn(1,3,32,32)))[2]\n",
    "    a3 = net(Variable(torch.randn(1,3,32,32)))[3]\n",
    "\n",
    "    print(y.size()) #size of output layer\n",
    "    print(a1.size()) #size of activation1 layer\n",
    "    print(a2.size()) #size of activation2 layer\n",
    "    print(a3.size()) #size of activation3 layer\n",
    "    \n",
    "#net=Wide_ResNet(16, 2, 0, 10) #(depth, widen_factor, dropout_rate, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
