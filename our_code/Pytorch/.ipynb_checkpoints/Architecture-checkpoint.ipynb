{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "| Wide-Resnet 16x2\n",
      "| Wide-Resnet 16x1\n",
      "Teacher net test:\n",
      "\t Test loss: \t 0.004332, \t Test accuracy \t 81.45\n",
      "Student net test:\n",
      "\t Test loss: \t 0.018027, \t Test accuracy \t 10.10\n",
      "Batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/torch/nn/functional.py:1946: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.19704370200634003\n",
      "Stud loss :0.4913065046072006\n",
      "Student net test:\n",
      "\t Test loss: \t 0.029392, \t Test accuracy \t 10.02\n",
      "Batch 1\n",
      "Gen loss :-0.11669502407312393\n",
      "Stud loss :0.2732813894748688\n",
      "Batch 2\n",
      "Gen loss :-0.05323195457458496\n",
      "Stud loss :0.18428010120987892\n",
      "Batch 3\n",
      "Gen loss :-0.02305629849433899\n",
      "Stud loss :0.14837367609143257\n",
      "Batch 4\n",
      "Gen loss :-0.011543425731360912\n",
      "Stud loss :0.13176514357328414\n",
      "Batch 5\n",
      "Gen loss :-0.006782103329896927\n",
      "Stud loss :0.1135058969259262\n",
      "Batch 6\n",
      "Gen loss :-0.004704089369624853\n",
      "Stud loss :0.10592681169509888\n",
      "Batch 7\n",
      "Gen loss :-0.0035686076153069735\n",
      "Stud loss :0.10211708024144173\n",
      "Batch 8\n",
      "Gen loss :-0.0029252253007143736\n",
      "Stud loss :0.09694579690694809\n",
      "Batch 9\n",
      "Gen loss :-0.0025428382214158773\n",
      "Stud loss :0.09644677266478538\n",
      "Batch 10\n",
      "Gen loss :-0.0021936490666121244\n",
      "Stud loss :0.08962583690881729\n",
      "Batch 11\n",
      "Gen loss :-0.001921864808537066\n",
      "Stud loss :0.08321783281862735\n",
      "Batch 12\n",
      "Gen loss :-0.0016996437916532159\n",
      "Stud loss :0.07874535173177719\n",
      "Batch 13\n",
      "Gen loss :-0.0015296427300199866\n",
      "Stud loss :0.07713660076260567\n",
      "Batch 14\n",
      "Gen loss :-0.0013809636002406478\n",
      "Stud loss :0.07621356099843979\n",
      "Batch 15\n",
      "Gen loss :-0.0012529207160696387\n",
      "Stud loss :0.07050047144293785\n",
      "Batch 16\n",
      "Gen loss :-0.0011491529876366258\n",
      "Stud loss :0.06971687786281108\n",
      "Batch 17\n",
      "Gen loss :-0.0010498417541384697\n",
      "Stud loss :0.06892849504947662\n",
      "Batch 18\n",
      "Gen loss :-0.0009295949712395668\n",
      "Stud loss :0.0611331220716238\n",
      "Batch 19\n",
      "Gen loss :-0.0008641674066893756\n",
      "Stud loss :0.0653889887034893\n",
      "Batch 20\n",
      "Gen loss :-0.0007927535916678607\n",
      "Stud loss :0.05997880883514881\n",
      "Student net test:\n",
      "\t Test loss: \t 0.067474, \t Test accuracy \t 10.02\n",
      "Batch 21\n",
      "Gen loss :-0.0007619020761922002\n",
      "Stud loss :0.0641845002770424\n",
      "Batch 22\n",
      "Gen loss :-0.0007159019005484879\n",
      "Stud loss :0.06197035796940327\n",
      "Batch 23\n",
      "Gen loss :-0.0006792577914893627\n",
      "Stud loss :0.06196182556450367\n",
      "Batch 24\n",
      "Gen loss :-0.0006302170222625136\n",
      "Stud loss :0.062330343574285504\n",
      "Batch 25\n",
      "Gen loss :-0.0005953345098532736\n",
      "Stud loss :0.05953325591981411\n",
      "Batch 26\n",
      "Gen loss :-0.0005630660452879965\n",
      "Stud loss :0.055778943374753\n",
      "Batch 27\n",
      "Gen loss :-0.0005320796044543386\n",
      "Stud loss :0.05784219615161419\n",
      "Batch 28\n",
      "Gen loss :-0.0004948010318912566\n",
      "Stud loss :0.05431303083896637\n",
      "Batch 29\n",
      "Gen loss :-0.00047461019130423665\n",
      "Stud loss :0.055451718345284465\n",
      "Batch 30\n",
      "Gen loss :-0.00044943063403479755\n",
      "Stud loss :0.05469398982822895\n",
      "Batch 31\n",
      "Gen loss :-0.0004306787741370499\n",
      "Stud loss :0.06098725236952305\n",
      "Batch 32\n",
      "Gen loss :-0.0010512876324355602\n",
      "Stud loss :0.06483995877206325\n",
      "Batch 33\n",
      "Gen loss :-0.00044317255378700793\n",
      "Stud loss :0.05465952903032303\n",
      "Batch 34\n",
      "Gen loss :-0.0005020186654292047\n",
      "Stud loss :0.05651026293635368\n",
      "Batch 35\n",
      "Gen loss :-0.0010586815187707543\n",
      "Stud loss :0.05967385657131672\n",
      "Batch 36\n",
      "Gen loss :-0.0008434794726781547\n",
      "Stud loss :0.05673252753913403\n",
      "Batch 37\n",
      "Gen loss :-0.0009971632389351726\n",
      "Stud loss :0.05132534466683865\n",
      "Batch 38\n",
      "Gen loss :-0.0038741433527320623\n",
      "Stud loss :0.06271008215844631\n",
      "Batch 39\n",
      "Gen loss :-0.005008047446608543\n",
      "Stud loss :0.06998698525130749\n",
      "Batch 40\n",
      "Gen loss :-0.004995730705559254\n",
      "Stud loss :0.08368350639939308\n",
      "Student net test:\n",
      "\t Test loss: \t 0.092709, \t Test accuracy \t 10.02\n",
      "Batch 41\n",
      "Gen loss :-0.011099527589976788\n",
      "Stud loss :0.16706368178129197\n",
      "Batch 42\n",
      "Gen loss :-0.03333103656768799\n",
      "Stud loss :0.22952739149332047\n",
      "Batch 43\n",
      "Gen loss :-0.048396799713373184\n",
      "Stud loss :0.12272736206650733\n",
      "Batch 44\n",
      "Gen loss :-0.02574044279754162\n",
      "Stud loss :0.05844747498631477\n",
      "Batch 45\n",
      "Gen loss :-0.016582144424319267\n",
      "Stud loss :0.04362682029604912\n",
      "Batch 46\n",
      "Gen loss :-0.006099191959947348\n",
      "Stud loss :0.029186336882412435\n",
      "Batch 47\n",
      "Gen loss :-0.005593099631369114\n",
      "Stud loss :0.026658305525779726\n",
      "Batch 48\n",
      "Gen loss :-0.003968045115470886\n",
      "Stud loss :0.022529421374201775\n",
      "Batch 49\n",
      "Gen loss :-0.0029109616298228502\n",
      "Stud loss :0.02007566820830107\n",
      "Batch 50\n",
      "Gen loss :-0.002660792088136077\n",
      "Stud loss :0.017899368703365327\n",
      "Batch 51\n",
      "Gen loss :-0.00263580190949142\n",
      "Stud loss :0.017675087414681913\n",
      "Batch 52\n",
      "Gen loss :-0.002295307582244277\n",
      "Stud loss :0.016046441812068223\n",
      "Batch 53\n",
      "Gen loss :-0.0021226515527814627\n",
      "Stud loss :0.015484978072345256\n",
      "Batch 54\n",
      "Gen loss :-0.0019905369263142347\n",
      "Stud loss :0.01494685309007764\n",
      "Batch 55\n",
      "Gen loss :-0.0019441702170297503\n",
      "Stud loss :0.014244174771010875\n",
      "Batch 56\n",
      "Gen loss :-0.0018831525230780244\n",
      "Stud loss :0.013994676899164915\n",
      "Batch 57\n",
      "Gen loss :-0.0017717283917590976\n",
      "Stud loss :0.012849362939596176\n",
      "Batch 58\n",
      "Gen loss :-0.001976151717826724\n",
      "Stud loss :0.013003428839147091\n",
      "Batch 59\n",
      "Gen loss :-0.0016426962101832032\n",
      "Stud loss :0.01265320386737585\n",
      "Batch 60\n",
      "Gen loss :-0.0018552031833678484\n",
      "Stud loss :0.012574701197445392\n",
      "Student net test:\n",
      "\t Test loss: \t 0.077115, \t Test accuracy \t 10.01\n",
      "Batch 61\n",
      "Gen loss :-0.0015153865097090602\n",
      "Stud loss :0.011891755275428295\n",
      "Batch 62\n",
      "Gen loss :-0.00161044264677912\n",
      "Stud loss :0.011800232902169227\n",
      "Batch 63\n",
      "Gen loss :-0.0014487769221886992\n",
      "Stud loss :0.011694462783634663\n",
      "Batch 64\n",
      "Gen loss :-0.0015145372599363327\n",
      "Stud loss :0.011495866533368825\n",
      "Batch 65\n",
      "Gen loss :-0.0013554253382608294\n",
      "Stud loss :0.010776631347835065\n",
      "Batch 66\n",
      "Gen loss :-0.0012936069397255778\n",
      "Stud loss :0.010578942392021418\n",
      "Batch 67\n",
      "Gen loss :-0.0012804203433915973\n",
      "Stud loss :0.010364392679184676\n",
      "Batch 68\n",
      "Gen loss :-0.0012114531127735972\n",
      "Stud loss :0.009933100081980228\n",
      "Batch 69\n",
      "Gen loss :-0.0014583022566512227\n",
      "Stud loss :0.01046057315543294\n",
      "Batch 70\n",
      "Gen loss :-0.001121775945648551\n",
      "Stud loss :0.010086193867027759\n",
      "Batch 71\n",
      "Gen loss :-0.0011124012526124716\n",
      "Stud loss :0.010043518245220184\n",
      "Batch 72\n",
      "Gen loss :-0.0011443339753895998\n",
      "Stud loss :0.009895413089543581\n",
      "Batch 73\n",
      "Gen loss :-0.001073086867108941\n",
      "Stud loss :0.00977632887661457\n",
      "Batch 74\n",
      "Gen loss :-0.0011776626342907548\n",
      "Stud loss :0.009546697698533535\n",
      "Batch 75\n",
      "Gen loss :-0.0011155992979183793\n",
      "Stud loss :0.009232840500772\n",
      "Batch 76\n",
      "Gen loss :-0.0010056480532512069\n",
      "Stud loss :0.009297570772469043\n",
      "Batch 77\n",
      "Gen loss :-0.0012156799202784896\n",
      "Stud loss :0.009314856491982937\n",
      "Batch 78\n",
      "Gen loss :-0.00129159155767411\n",
      "Stud loss :0.009600049443542957\n",
      "Batch 79\n",
      "Gen loss :-0.001047445461153984\n",
      "Stud loss :0.009309446252882481\n",
      "Batch 80\n",
      "Gen loss :-0.0009094082051888108\n",
      "Stud loss :0.009001938439905644\n",
      "Student net test:\n",
      "\t Test loss: \t 0.075053, \t Test accuracy \t 10.02\n",
      "Batch 81\n",
      "Gen loss :-0.0013089675921946764\n",
      "Stud loss :0.009558959119021892\n",
      "Batch 82\n",
      "Gen loss :-0.0012058154679834843\n",
      "Stud loss :0.009087692573666572\n",
      "Batch 83\n",
      "Gen loss :-0.0010334065882489085\n",
      "Stud loss :0.009078282676637173\n",
      "Batch 84\n",
      "Gen loss :-0.0010780555894598365\n",
      "Stud loss :0.00912132766097784\n",
      "Batch 85\n",
      "Gen loss :-0.001032564789056778\n",
      "Stud loss :0.009040707722306252\n",
      "Batch 86\n",
      "Gen loss :-0.0010118626523762941\n",
      "Stud loss :0.0091783725656569\n",
      "Batch 87\n",
      "Gen loss :-0.0010003151837736368\n",
      "Stud loss :0.008419775404036045\n",
      "Batch 88\n",
      "Gen loss :-0.0012242109514772892\n",
      "Stud loss :0.009172735270112753\n",
      "Batch 89\n",
      "Gen loss :-0.0013776567066088319\n",
      "Stud loss :0.008450407767668366\n",
      "Batch 90\n",
      "Gen loss :-0.0010791337117552757\n",
      "Stud loss :0.00873745046555996\n",
      "Batch 91\n",
      "Gen loss :-0.0008823465905152261\n",
      "Stud loss :0.008537155203521251\n",
      "Batch 92\n",
      "Gen loss :-0.0010265648597851396\n",
      "Stud loss :0.00883101997897029\n",
      "Batch 93\n",
      "Gen loss :-0.001005296828225255\n",
      "Stud loss :0.008569021802395582\n",
      "Batch 94\n",
      "Gen loss :-0.0008807151461951435\n",
      "Stud loss :0.008376688184216618\n",
      "Batch 95\n",
      "Gen loss :-0.00090090895537287\n",
      "Stud loss :0.008242236403748392\n",
      "Batch 96\n",
      "Gen loss :-0.0009058233699761331\n",
      "Stud loss :0.008297474635764957\n",
      "Batch 97\n",
      "Gen loss :-0.0009249750874005258\n",
      "Stud loss :0.00850656209513545\n",
      "Batch 98\n",
      "Gen loss :-0.001006793463602662\n",
      "Stud loss :0.008364569256082177\n",
      "Batch 99\n",
      "Gen loss :-0.0011228583753108978\n",
      "Stud loss :0.008524242648854851\n",
      "Batch 100\n",
      "Gen loss :-0.0008609702927060425\n",
      "Stud loss :0.00797235439531505\n",
      "Student net test:\n",
      "\t Test loss: \t 0.076548, \t Test accuracy \t 10.01\n",
      "Batch 101\n",
      "Gen loss :-0.0008358860504813492\n",
      "Stud loss :0.008410911494866013\n",
      "Batch 102\n",
      "Gen loss :-0.000986350467428565\n",
      "Stud loss :0.008157008280977606\n",
      "Batch 103\n",
      "Gen loss :-0.0008798258495517075\n",
      "Stud loss :0.008097218442708254\n",
      "Batch 104\n",
      "Gen loss :-0.000819329172372818\n",
      "Stud loss :0.008559725619852543\n",
      "Batch 105\n",
      "Gen loss :-0.0007715122192166746\n",
      "Stud loss :0.008479918679222464\n",
      "Batch 106\n",
      "Gen loss :-0.0008451691828668118\n",
      "Stud loss :0.008343068603426217\n",
      "Batch 107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.000896961078979075\n",
      "Stud loss :0.008262773789465428\n",
      "Batch 108\n",
      "Gen loss :-0.0008375655161216855\n",
      "Stud loss :0.008369325753301382\n",
      "Batch 109\n",
      "Gen loss :-0.0010189767926931381\n",
      "Stud loss :0.008486206363886594\n",
      "Batch 110\n",
      "Gen loss :-0.0011325024534016848\n",
      "Stud loss :0.008512733737006783\n",
      "Batch 111\n",
      "Gen loss :-0.000875613302923739\n",
      "Stud loss :0.008723379112780094\n",
      "Batch 112\n",
      "Gen loss :-0.0010550027946010232\n",
      "Stud loss :0.008357910485938192\n",
      "Batch 113\n",
      "Gen loss :-0.000863665365613997\n",
      "Stud loss :0.008813877310603856\n",
      "Batch 114\n",
      "Gen loss :-0.0007741716108284891\n",
      "Stud loss :0.008682869747281075\n",
      "Batch 115\n",
      "Gen loss :-0.0008438844233751297\n",
      "Stud loss :0.008472861815243959\n",
      "Batch 116\n",
      "Gen loss :-0.0007698810077272356\n",
      "Stud loss :0.00844981879927218\n",
      "Batch 117\n",
      "Gen loss :-0.0010637718951329589\n",
      "Stud loss :0.009032769221812486\n",
      "Batch 118\n",
      "Gen loss :-0.0011014320189133286\n",
      "Stud loss :0.00891452506184578\n",
      "Batch 119\n",
      "Gen loss :-0.000991043052636087\n",
      "Stud loss :0.009128301497548818\n",
      "Batch 120\n",
      "Gen loss :-0.000964379112701863\n",
      "Stud loss :0.009157225955277681\n",
      "Student net test:\n",
      "\t Test loss: \t 0.082691, \t Test accuracy \t 10.04\n",
      "Batch 121\n",
      "Gen loss :-0.000986801809631288\n",
      "Stud loss :0.009126183390617371\n",
      "Batch 122\n",
      "Gen loss :-0.0009352429769933224\n",
      "Stud loss :0.008680814504623413\n",
      "Batch 123\n",
      "Gen loss :-0.0009402716532349586\n",
      "Stud loss :0.008866130374372005\n",
      "Batch 124\n",
      "Gen loss :-0.0008449790184386075\n",
      "Stud loss :0.0087450104765594\n",
      "Batch 125\n",
      "Gen loss :-0.0007720565772615373\n",
      "Stud loss :0.008592903800308704\n",
      "Batch 126\n",
      "Gen loss :-0.0008450992754660547\n",
      "Stud loss :0.008777109906077385\n",
      "Batch 127\n",
      "Gen loss :-0.0009368403116241097\n",
      "Stud loss :0.009658512007445098\n",
      "Batch 128\n",
      "Gen loss :-0.0009186784154735506\n",
      "Stud loss :0.009045681264251471\n",
      "Batch 129\n",
      "Gen loss :-0.0013479188783094287\n",
      "Stud loss :0.009752775728702544\n",
      "Batch 130\n",
      "Gen loss :-0.0014374660095199943\n",
      "Stud loss :0.009716878551989794\n",
      "Batch 131\n",
      "Gen loss :-0.0011201826855540276\n",
      "Stud loss :0.009647948760539293\n",
      "Batch 132\n",
      "Gen loss :-0.0012059705331921577\n",
      "Stud loss :0.009890897106379271\n",
      "Batch 133\n",
      "Gen loss :-0.0012404819717630744\n",
      "Stud loss :0.00910524995997548\n",
      "Batch 134\n",
      "Gen loss :-0.0010577300563454628\n",
      "Stud loss :0.010129508189857006\n",
      "Batch 135\n",
      "Gen loss :-0.0009809479815885425\n",
      "Stud loss :0.009286930784583092\n",
      "Batch 136\n",
      "Gen loss :-0.0008397837518714368\n",
      "Stud loss :0.009692787006497384\n",
      "Batch 137\n",
      "Gen loss :-0.0009669910068623722\n",
      "Stud loss :0.010019619297236205\n",
      "Batch 138\n",
      "Gen loss :-0.0008657023427076638\n",
      "Stud loss :0.009763182420283557\n",
      "Batch 139\n",
      "Gen loss :-0.00096489442512393\n",
      "Stud loss :0.009681630413979292\n",
      "Batch 140\n",
      "Gen loss :-0.001314808032475412\n",
      "Stud loss :0.010396685730665923\n",
      "Student net test:\n",
      "\t Test loss: \t 0.080343, \t Test accuracy \t 10.01\n",
      "Batch 141\n",
      "Gen loss :-0.0012310518650338054\n",
      "Stud loss :0.010503072943538427\n",
      "Batch 142\n",
      "Gen loss :-0.001303897355683148\n",
      "Stud loss :0.011410325858741998\n",
      "Batch 143\n",
      "Gen loss :-0.0015461720759049058\n",
      "Stud loss :0.012257839553058147\n",
      "Batch 144\n",
      "Gen loss :-0.0013596079079434276\n",
      "Stud loss :0.011692130006849766\n",
      "Batch 145\n",
      "Gen loss :-0.0013231814373284578\n",
      "Stud loss :0.011494874954223633\n",
      "Batch 146\n",
      "Gen loss :-0.001430366886779666\n",
      "Stud loss :0.012292817700654268\n",
      "Batch 147\n",
      "Gen loss :-0.0013023935025557876\n",
      "Stud loss :0.012186905276030302\n",
      "Batch 148\n",
      "Gen loss :-0.001400840119458735\n",
      "Stud loss :0.01336828600615263\n",
      "Batch 149\n",
      "Gen loss :-0.0016249293694272637\n",
      "Stud loss :0.014310938771814108\n",
      "Batch 150\n",
      "Gen loss :-0.0016332966042682528\n",
      "Stud loss :0.013810550421476364\n",
      "Batch 151\n",
      "Gen loss :-0.0014000395312905312\n",
      "Stud loss :0.0129547786898911\n",
      "Batch 152\n",
      "Gen loss :-0.001542786369100213\n",
      "Stud loss :0.014991526398807764\n",
      "Batch 153\n",
      "Gen loss :-0.0019323284504935145\n",
      "Stud loss :0.017455876525491475\n",
      "Batch 154\n",
      "Gen loss :-0.0022799076978117228\n",
      "Stud loss :0.017665295861661434\n",
      "Batch 155\n",
      "Gen loss :-0.0016315054381266236\n",
      "Stud loss :0.018018197175115346\n",
      "Batch 156\n",
      "Gen loss :-0.002516659675166011\n",
      "Stud loss :0.02084101103246212\n",
      "Batch 157\n",
      "Gen loss :-0.0035387035459280014\n",
      "Stud loss :0.03851535804569721\n",
      "Batch 158\n",
      "Gen loss :-0.007801544852554798\n",
      "Stud loss :0.03759342692792415\n",
      "Batch 159\n",
      "Gen loss :-0.012498336844146252\n",
      "Stud loss :0.08774130418896675\n",
      "Batch 160\n",
      "Gen loss :-0.014716751873493195\n",
      "Stud loss :0.10366030037403107\n",
      "Student net test:\n",
      "\t Test loss: \t 0.022483, \t Test accuracy \t 15.29\n",
      "Batch 161\n",
      "Gen loss :-0.027629155665636063\n",
      "Stud loss :0.1851634219288826\n",
      "Batch 162\n",
      "Gen loss :-0.03639936074614525\n",
      "Stud loss :0.1737050749361515\n",
      "Batch 163\n",
      "Gen loss :-0.0325591154396534\n",
      "Stud loss :0.1989170044660568\n",
      "Batch 164\n",
      "Gen loss :-0.03791678696870804\n",
      "Stud loss :0.2769807606935501\n",
      "Batch 165\n",
      "Gen loss :-0.05645532160997391\n",
      "Stud loss :0.24917518943548203\n",
      "Batch 166\n",
      "Gen loss :-0.037772037088871\n",
      "Stud loss :0.15359998047351836\n",
      "Batch 167\n",
      "Gen loss :-0.027041763067245483\n",
      "Stud loss :0.11666313633322715\n",
      "Batch 168\n",
      "Gen loss :-0.0198938250541687\n",
      "Stud loss :0.0980265662074089\n",
      "Batch 169\n",
      "Gen loss :-0.021458400413393974\n",
      "Stud loss :0.09924393519759178\n",
      "Batch 170\n",
      "Gen loss :-0.022228119894862175\n",
      "Stud loss :0.08828603327274323\n",
      "Batch 171\n",
      "Gen loss :-0.017803456634283066\n",
      "Stud loss :0.08287717029452324\n",
      "Batch 172\n",
      "Gen loss :-0.01778467558324337\n",
      "Stud loss :0.07897403240203857\n",
      "Batch 173\n",
      "Gen loss :-0.016558025032281876\n",
      "Stud loss :0.07815391570329666\n",
      "Batch 174\n",
      "Gen loss :-0.016519740223884583\n",
      "Stud loss :0.07350359931588173\n",
      "Batch 175\n",
      "Gen loss :-0.01669478975236416\n",
      "Stud loss :0.07209888771176338\n",
      "Batch 176\n",
      "Gen loss :-0.017082517966628075\n",
      "Stud loss :0.07021807506680489\n",
      "Batch 177\n",
      "Gen loss :-0.016344688832759857\n",
      "Stud loss :0.06736711338162422\n",
      "Batch 178\n",
      "Gen loss :-0.014202493242919445\n",
      "Stud loss :0.06046926975250244\n",
      "Batch 179\n",
      "Gen loss :-0.01291732955724001\n",
      "Stud loss :0.06463251113891602\n",
      "Batch 180\n",
      "Gen loss :-0.012984789907932281\n",
      "Stud loss :0.06269876137375832\n",
      "Student net test:\n",
      "\t Test loss: \t 0.040423, \t Test accuracy \t 12.74\n",
      "Batch 181\n",
      "Gen loss :-0.01249635498970747\n",
      "Stud loss :0.061678368225693704\n",
      "Batch 182\n",
      "Gen loss :-0.018210524693131447\n",
      "Stud loss :0.0664342675358057\n",
      "Batch 183\n",
      "Gen loss :-0.01565469242632389\n",
      "Stud loss :0.05983178429305554\n",
      "Batch 184\n",
      "Gen loss :-0.01160228718072176\n",
      "Stud loss :0.05696212910115719\n",
      "Batch 185\n",
      "Gen loss :-0.014696273021399975\n",
      "Stud loss :0.059492813795804976\n",
      "Batch 186\n",
      "Gen loss :-0.012719026766717434\n",
      "Stud loss :0.05556510165333748\n",
      "Batch 187\n",
      "Gen loss :-0.016722997650504112\n",
      "Stud loss :0.06125976890325546\n",
      "Batch 188\n",
      "Gen loss :-0.013262656517326832\n",
      "Stud loss :0.05681207776069641\n",
      "Batch 189\n",
      "Gen loss :-0.01200750470161438\n",
      "Stud loss :0.05782574266195297\n",
      "Batch 190\n",
      "Gen loss :-0.013341331854462624\n",
      "Stud loss :0.05423800051212311\n",
      "Batch 191\n",
      "Gen loss :-0.013626215048134327\n",
      "Stud loss :0.05788914747536182\n",
      "Batch 192\n",
      "Gen loss :-0.013385913334786892\n",
      "Stud loss :0.05776660405099392\n",
      "Batch 193\n",
      "Gen loss :-0.012979447841644287\n",
      "Stud loss :0.05662524811923504\n",
      "Batch 194\n",
      "Gen loss :-0.011480631306767464\n",
      "Stud loss :0.05496284104883671\n",
      "Batch 195\n",
      "Gen loss :-0.012740284204483032\n",
      "Stud loss :0.05728733502328396\n",
      "Batch 196\n",
      "Gen loss :-0.014858757145702839\n",
      "Stud loss :0.05989731140434742\n",
      "Batch 197\n",
      "Gen loss :-0.011663811281323433\n",
      "Stud loss :0.05986987389624119\n",
      "Batch 198\n",
      "Gen loss :-0.013554118573665619\n",
      "Stud loss :0.05787569619715214\n",
      "Batch 199\n",
      "Gen loss :-0.011182731948792934\n",
      "Stud loss :0.052998046204447744\n",
      "Batch 200\n",
      "Gen loss :-0.01224676426500082\n",
      "Stud loss :0.057336023077368736\n",
      "Student net test:\n",
      "\t Test loss: \t 0.035927, \t Test accuracy \t 16.58\n",
      "Batch 201\n",
      "Gen loss :-0.014412364922463894\n",
      "Stud loss :0.0579328540712595\n",
      "Batch 202\n",
      "Gen loss :-0.012673094868659973\n",
      "Stud loss :0.05490368232131004\n",
      "Batch 203\n",
      "Gen loss :-0.013283860869705677\n",
      "Stud loss :0.06478442177176476\n",
      "Batch 204\n",
      "Gen loss :-0.013284647837281227\n",
      "Stud loss :0.05710057280957699\n",
      "Batch 205\n",
      "Gen loss :-0.012721478939056396\n",
      "Stud loss :0.0602207038551569\n",
      "Batch 206\n",
      "Gen loss :-0.013336724601686\n",
      "Stud loss :0.06089712716639042\n",
      "Batch 207\n",
      "Gen loss :-0.015206473879516125\n",
      "Stud loss :0.06627137809991837\n",
      "Batch 208\n",
      "Gen loss :-0.016322800889611244\n",
      "Stud loss :0.06332699060440064\n",
      "Batch 209\n",
      "Gen loss :-0.01283970195800066\n",
      "Stud loss :0.05676983706653118\n",
      "Batch 210\n",
      "Gen loss :-0.02006307803094387\n",
      "Stud loss :0.07107215821743011\n",
      "Batch 211\n",
      "Gen loss :-0.013735865242779255\n",
      "Stud loss :0.06306480839848519\n",
      "Batch 212\n",
      "Gen loss :-0.017421232536435127\n",
      "Stud loss :0.06568200141191483\n",
      "Batch 213\n",
      "Gen loss :-0.01710527017712593\n",
      "Stud loss :0.06946120671927929\n",
      "Batch 214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.013296491466462612\n",
      "Stud loss :0.06292060576379299\n",
      "Batch 215\n",
      "Gen loss :-0.01769843138754368\n",
      "Stud loss :0.06683884486556053\n",
      "Batch 216\n",
      "Gen loss :-0.012832269072532654\n",
      "Stud loss :0.06103239506483078\n",
      "Batch 217\n",
      "Gen loss :-0.014526220969855785\n",
      "Stud loss :0.06726521886885166\n",
      "Batch 218\n",
      "Gen loss :-0.014619332738220692\n",
      "Stud loss :0.06932308673858642\n",
      "Batch 219\n",
      "Gen loss :-0.015457344241440296\n",
      "Stud loss :0.07093526870012283\n",
      "Batch 220\n",
      "Gen loss :-0.01695954240858555\n",
      "Stud loss :0.07640839889645576\n",
      "Student net test:\n",
      "\t Test loss: \t 0.032012, \t Test accuracy \t 20.54\n",
      "Batch 221\n",
      "Gen loss :-0.017638621851801872\n",
      "Stud loss :0.08135442323982715\n",
      "Batch 222\n",
      "Gen loss :-0.020806578919291496\n",
      "Stud loss :0.09403396397829056\n",
      "Batch 223\n",
      "Gen loss :-0.017850151285529137\n",
      "Stud loss :0.08773765712976456\n",
      "Batch 224\n",
      "Gen loss :-0.017965255305171013\n",
      "Stud loss :0.097135541588068\n",
      "Batch 225\n",
      "Gen loss :-0.02894476056098938\n",
      "Stud loss :0.11751035377383232\n",
      "Batch 226\n",
      "Gen loss :-0.03665146231651306\n",
      "Stud loss :0.14653602987527847\n",
      "Batch 227\n",
      "Gen loss :-0.05078408867120743\n",
      "Stud loss :0.17630284354090692\n",
      "Batch 228\n",
      "Gen loss :-0.07015674561262131\n",
      "Stud loss :0.2190652832388878\n",
      "Batch 229\n",
      "Gen loss :-0.047418493777513504\n",
      "Stud loss :0.12728123515844345\n",
      "Batch 230\n",
      "Gen loss :-0.013581803999841213\n",
      "Stud loss :0.08559043928980828\n",
      "Batch 231\n",
      "Gen loss :-0.015024472959339619\n",
      "Stud loss :0.08937222436070442\n",
      "Batch 232\n",
      "Gen loss :-0.009320124983787537\n",
      "Stud loss :0.06946492567658424\n",
      "Batch 233\n",
      "Gen loss :-0.008997483178973198\n",
      "Stud loss :0.07100545577704906\n",
      "Batch 234\n",
      "Gen loss :-0.005882858764380217\n",
      "Stud loss :0.05836123265326023\n",
      "Batch 235\n",
      "Gen loss :-0.005719439592212439\n",
      "Stud loss :0.059156067296862604\n",
      "Batch 236\n",
      "Gen loss :-0.008606488816440105\n",
      "Stud loss :0.0653060670942068\n",
      "Batch 237\n",
      "Gen loss :-0.009169590659439564\n",
      "Stud loss :0.06394769810140133\n",
      "Batch 238\n",
      "Gen loss :-0.009283849969506264\n",
      "Stud loss :0.05372955538332462\n",
      "Batch 239\n",
      "Gen loss :-0.013305822387337685\n",
      "Stud loss :0.06830423213541507\n",
      "Batch 240\n",
      "Gen loss :-0.010042806155979633\n",
      "Stud loss :0.0523204430937767\n",
      "Student net test:\n",
      "\t Test loss: \t 0.027523, \t Test accuracy \t 17.48\n",
      "Batch 241\n",
      "Gen loss :-0.009133589453995228\n",
      "Stud loss :0.05820971541106701\n",
      "Batch 242\n",
      "Gen loss :-0.012282568030059338\n",
      "Stud loss :0.06043485663831234\n",
      "Batch 243\n",
      "Gen loss :-0.013659032993018627\n",
      "Stud loss :0.05552021376788616\n",
      "Batch 244\n",
      "Gen loss :-0.013341397047042847\n",
      "Stud loss :0.05999440588057041\n",
      "Batch 245\n",
      "Gen loss :-0.013229402713477612\n",
      "Stud loss :0.06244725026190281\n",
      "Batch 246\n",
      "Gen loss :-0.016808679327368736\n",
      "Stud loss :0.0763875737786293\n",
      "Batch 247\n",
      "Gen loss :-0.019387876614928246\n",
      "Stud loss :0.0600469496101141\n",
      "Batch 248\n",
      "Gen loss :-0.01911056600511074\n",
      "Stud loss :0.06899662278592586\n",
      "Batch 249\n",
      "Gen loss :-0.02037177048623562\n",
      "Stud loss :0.06701985485851765\n",
      "Batch 250\n",
      "Gen loss :-0.02025510184466839\n",
      "Stud loss :0.05874304696917534\n",
      "Batch 251\n",
      "Gen loss :-0.018500132486224174\n",
      "Stud loss :0.060252212733030316\n",
      "Batch 252\n",
      "Gen loss :-0.01593933440744877\n",
      "Stud loss :0.05201249495148659\n",
      "Batch 253\n",
      "Gen loss :-0.015069684945046902\n",
      "Stud loss :0.04880143329501152\n",
      "Batch 254\n",
      "Gen loss :-0.012547320686280727\n",
      "Stud loss :0.045200787484645844\n",
      "Batch 255\n",
      "Gen loss :-0.014197969809174538\n",
      "Stud loss :0.0449648167937994\n",
      "Batch 256\n",
      "Gen loss :-0.011840681545436382\n",
      "Stud loss :0.03982991985976696\n",
      "Batch 257\n",
      "Gen loss :-0.010248586535453796\n",
      "Stud loss :0.03989957831799984\n",
      "Batch 258\n",
      "Gen loss :-0.009897680021822453\n",
      "Stud loss :0.03961801007390022\n",
      "Batch 259\n",
      "Gen loss :-0.009142222814261913\n",
      "Stud loss :0.039200200140476225\n",
      "Batch 260\n",
      "Gen loss :-0.008148329332470894\n",
      "Stud loss :0.03525517489761114\n",
      "Student net test:\n",
      "\t Test loss: \t 0.032676, \t Test accuracy \t 14.05\n",
      "Batch 261\n",
      "Gen loss :-0.007548538502305746\n",
      "Stud loss :0.035200755670666696\n",
      "Batch 262\n",
      "Gen loss :-0.007669007871299982\n",
      "Stud loss :0.03401625715196133\n",
      "Batch 263\n",
      "Gen loss :-0.00844286847859621\n",
      "Stud loss :0.03323202133178711\n",
      "Batch 264\n",
      "Gen loss :-0.007102711591869593\n",
      "Stud loss :0.032900469936430456\n",
      "Batch 265\n",
      "Gen loss :-0.006857965141534805\n",
      "Stud loss :0.03300745077431202\n",
      "Batch 266\n",
      "Gen loss :-0.008276458829641342\n",
      "Stud loss :0.030646247044205665\n",
      "Batch 267\n",
      "Gen loss :-0.007862200029194355\n",
      "Stud loss :0.031043018959462643\n",
      "Batch 268\n",
      "Gen loss :-0.007671285420656204\n",
      "Stud loss :0.03121205363422632\n",
      "Batch 269\n",
      "Gen loss :-0.006130436901003122\n",
      "Stud loss :0.031785789504647256\n",
      "Batch 270\n",
      "Gen loss :-0.005914637353271246\n",
      "Stud loss :0.031398572213947776\n",
      "Batch 271\n",
      "Gen loss :-0.006512422114610672\n",
      "Stud loss :0.03131761085242033\n",
      "Batch 272\n",
      "Gen loss :-0.0060465456917881966\n",
      "Stud loss :0.029979910887777805\n",
      "Batch 273\n",
      "Gen loss :-0.007189749274402857\n",
      "Stud loss :0.029041795805096627\n",
      "Batch 274\n",
      "Gen loss :-0.007619486190378666\n",
      "Stud loss :0.030002332478761672\n",
      "Batch 275\n",
      "Gen loss :-0.005988501477986574\n",
      "Stud loss :0.030412165075540544\n",
      "Batch 276\n",
      "Gen loss :-0.0061393664218485355\n",
      "Stud loss :0.028385016694664957\n",
      "Batch 277\n",
      "Gen loss :-0.005796622019261122\n",
      "Stud loss :0.02728889901190996\n",
      "Batch 278\n",
      "Gen loss :-0.005647321231663227\n",
      "Stud loss :0.027786431089043617\n",
      "Batch 279\n",
      "Gen loss :-0.00515827676281333\n",
      "Stud loss :0.02767703849822283\n",
      "Batch 280\n",
      "Gen loss :-0.00655828882008791\n",
      "Stud loss :0.02763962559401989\n",
      "Student net test:\n",
      "\t Test loss: \t 0.035960, \t Test accuracy \t 16.44\n",
      "Batch 281\n",
      "Gen loss :-0.006483785342425108\n",
      "Stud loss :0.029538751766085624\n",
      "Batch 282\n",
      "Gen loss :-0.0047782305628061295\n",
      "Stud loss :0.02515275403857231\n",
      "Batch 283\n",
      "Gen loss :-0.006631861906498671\n",
      "Stud loss :0.026507694646716116\n",
      "Batch 284\n",
      "Gen loss :-0.005426035262644291\n",
      "Stud loss :0.02851989809423685\n",
      "Batch 285\n",
      "Gen loss :-0.005502939689904451\n",
      "Stud loss :0.02858203202486038\n",
      "Batch 286\n",
      "Gen loss :-0.007320346776396036\n",
      "Stud loss :0.02813304029405117\n",
      "Batch 287\n",
      "Gen loss :-0.006494087632745504\n",
      "Stud loss :0.027788920514285564\n",
      "Batch 288\n",
      "Gen loss :-0.0061936997808516026\n",
      "Stud loss :0.02794927656650543\n",
      "Batch 289\n",
      "Gen loss :-0.006941580213606358\n",
      "Stud loss :0.027669202722609042\n",
      "Batch 290\n",
      "Gen loss :-0.006019247230142355\n",
      "Stud loss :0.026437368430197238\n",
      "Batch 291\n",
      "Gen loss :-0.005246496293693781\n",
      "Stud loss :0.025630515627563\n",
      "Batch 292\n",
      "Gen loss :-0.005134203936904669\n",
      "Stud loss :0.026519106701016426\n",
      "Batch 293\n",
      "Gen loss :-0.004766256548464298\n",
      "Stud loss :0.024888174049556254\n",
      "Batch 294\n",
      "Gen loss :-0.006074749398976564\n",
      "Stud loss :0.02681575845927\n",
      "Batch 295\n",
      "Gen loss :-0.005575168412178755\n",
      "Stud loss :0.027684582397341728\n",
      "Batch 296\n",
      "Gen loss :-0.005972582846879959\n",
      "Stud loss :0.026041839644312857\n",
      "Batch 297\n",
      "Gen loss :-0.004947963170707226\n",
      "Stud loss :0.025525399856269358\n",
      "Batch 298\n",
      "Gen loss :-0.0052266800776124\n",
      "Stud loss :0.02373013589531183\n",
      "Batch 299\n",
      "Gen loss :-0.0052780634723603725\n",
      "Stud loss :0.02630346976220608\n",
      "Batch 300\n",
      "Gen loss :-0.005697132553905249\n",
      "Stud loss :0.026167365722358227\n",
      "Student net test:\n",
      "\t Test loss: \t 0.033513, \t Test accuracy \t 18.59\n",
      "Batch 301\n",
      "Gen loss :-0.004674852825701237\n",
      "Stud loss :0.02481247428804636\n",
      "Batch 302\n",
      "Gen loss :-0.004536912310868502\n",
      "Stud loss :0.023909405805170536\n",
      "Batch 303\n",
      "Gen loss :-0.005243933293968439\n",
      "Stud loss :0.02389696128666401\n",
      "Batch 304\n",
      "Gen loss :-0.004516828339546919\n",
      "Stud loss :0.024106239899992943\n",
      "Batch 305\n",
      "Gen loss :-0.004888250958174467\n",
      "Stud loss :0.02691344041377306\n",
      "Batch 306\n",
      "Gen loss :-0.005447385832667351\n",
      "Stud loss :0.026705091819167137\n",
      "Batch 307\n",
      "Gen loss :-0.005893021356314421\n",
      "Stud loss :0.025790896639227866\n",
      "Batch 308\n",
      "Gen loss :-0.0060248891822993755\n",
      "Stud loss :0.02538368571549654\n",
      "Batch 309\n",
      "Gen loss :-0.00468762731179595\n",
      "Stud loss :0.024654165282845496\n",
      "Batch 310\n",
      "Gen loss :-0.006625381298363209\n",
      "Stud loss :0.02823051866143942\n",
      "Batch 311\n",
      "Gen loss :-0.004868007730692625\n",
      "Stud loss :0.025107510946691038\n",
      "Batch 312\n",
      "Gen loss :-0.004977123346179724\n",
      "Stud loss :0.025198893435299397\n",
      "Batch 313\n",
      "Gen loss :-0.004744540899991989\n",
      "Stud loss :0.02507241554558277\n",
      "Batch 314\n",
      "Gen loss :-0.004994878079742193\n",
      "Stud loss :0.02476363517343998\n",
      "Batch 315\n",
      "Gen loss :-0.004882465582340956\n",
      "Stud loss :0.025489444844424725\n",
      "Batch 316\n",
      "Gen loss :-0.005574223585426807\n",
      "Stud loss :0.025664840638637543\n",
      "Batch 317\n",
      "Gen loss :-0.006013619247823954\n",
      "Stud loss :0.025712712854146957\n",
      "Batch 318\n",
      "Gen loss :-0.005733468569815159\n",
      "Stud loss :0.024394137412309648\n",
      "Batch 319\n",
      "Gen loss :-0.005515700206160545\n",
      "Stud loss :0.024745951779186724\n",
      "Batch 320\n",
      "Gen loss :-0.004926788154989481\n",
      "Stud loss :0.024638628400862218\n",
      "Student net test:\n",
      "\t Test loss: \t 0.035334, \t Test accuracy \t 20.12\n",
      "Batch 321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.005674341227859259\n",
      "Stud loss :0.02499695047736168\n",
      "Batch 322\n",
      "Gen loss :-0.006280349101871252\n",
      "Stud loss :0.026569523103535175\n",
      "Batch 323\n",
      "Gen loss :-0.005274009890854359\n",
      "Stud loss :0.02457724343985319\n",
      "Batch 324\n",
      "Gen loss :-0.0050397636368870735\n",
      "Stud loss :0.02521888930350542\n",
      "Batch 325\n",
      "Gen loss :-0.005437081679701805\n",
      "Stud loss :0.02426519487053156\n",
      "Batch 326\n",
      "Gen loss :-0.005512375850230455\n",
      "Stud loss :0.02617575563490391\n",
      "Batch 327\n",
      "Gen loss :-0.004894312471151352\n",
      "Stud loss :0.025589629635214807\n",
      "Batch 328\n",
      "Gen loss :-0.004598430823534727\n",
      "Stud loss :0.024301376193761826\n",
      "Batch 329\n",
      "Gen loss :-0.005401795729994774\n",
      "Stud loss :0.02346032653003931\n",
      "Batch 330\n",
      "Gen loss :-0.005309609696269035\n",
      "Stud loss :0.02348516136407852\n",
      "Batch 331\n",
      "Gen loss :-0.005885121878236532\n",
      "Stud loss :0.02485437449067831\n",
      "Batch 332\n",
      "Gen loss :-0.005510570947080851\n",
      "Stud loss :0.025778743624687194\n",
      "Batch 333\n",
      "Gen loss :-0.005005206447094679\n",
      "Stud loss :0.024989986792206764\n",
      "Batch 334\n",
      "Gen loss :-0.005162648856639862\n",
      "Stud loss :0.024367128126323225\n",
      "Batch 335\n",
      "Gen loss :-0.006423159036785364\n",
      "Stud loss :0.02660634499043226\n",
      "Batch 336\n",
      "Gen loss :-0.004614826291799545\n",
      "Stud loss :0.02441775444895029\n",
      "Batch 337\n",
      "Gen loss :-0.004972048103809357\n",
      "Stud loss :0.023556775599718093\n",
      "Batch 338\n",
      "Gen loss :-0.005437375046312809\n",
      "Stud loss :0.02683583404868841\n",
      "Batch 339\n",
      "Gen loss :-0.004563427530229092\n",
      "Stud loss :0.023726020567119123\n",
      "Batch 340\n",
      "Gen loss :-0.004479825031012297\n",
      "Stud loss :0.02394351866096258\n",
      "Student net test:\n",
      "\t Test loss: \t 0.035665, \t Test accuracy \t 19.37\n",
      "Batch 341\n",
      "Gen loss :-0.005130492150783539\n",
      "Stud loss :0.023516359739005566\n",
      "Batch 342\n",
      "Gen loss :-0.005125831812620163\n",
      "Stud loss :0.023524234630167486\n",
      "Batch 343\n",
      "Gen loss :-0.005458258092403412\n",
      "Stud loss :0.024095399677753447\n",
      "Batch 344\n",
      "Gen loss :-0.005458419676870108\n",
      "Stud loss :0.025143525563180447\n",
      "Batch 345\n",
      "Gen loss :-0.005111390259116888\n",
      "Stud loss :0.024400504119694234\n",
      "Batch 346\n",
      "Gen loss :-0.005123856943100691\n",
      "Stud loss :0.023691953904926778\n",
      "Batch 347\n",
      "Gen loss :-0.005609551910310984\n",
      "Stud loss :0.026290003396570682\n",
      "Batch 348\n",
      "Gen loss :-0.00575605034828186\n",
      "Stud loss :0.026774457655847073\n",
      "Batch 349\n",
      "Gen loss :-0.0056439475156366825\n",
      "Stud loss :0.026095020584762096\n",
      "Batch 350\n",
      "Gen loss :-0.005239787977188826\n",
      "Stud loss :0.02556398753076792\n",
      "Batch 351\n",
      "Gen loss :-0.006258114706724882\n",
      "Stud loss :0.026403151638805866\n",
      "Batch 352\n",
      "Gen loss :-0.005122286267578602\n",
      "Stud loss :0.026255971379578115\n",
      "Batch 353\n",
      "Gen loss :-0.006691148038953543\n",
      "Stud loss :0.02709536477923393\n",
      "Batch 354\n",
      "Gen loss :-0.004639858845621347\n",
      "Stud loss :0.024668240547180177\n",
      "Batch 355\n",
      "Gen loss :-0.006152951158583164\n",
      "Stud loss :0.026346058025956152\n",
      "Batch 356\n",
      "Gen loss :-0.0075904326513409615\n",
      "Stud loss :0.028830306231975557\n",
      "Batch 357\n",
      "Gen loss :-0.005500485189259052\n",
      "Stud loss :0.026959889382123948\n",
      "Batch 358\n",
      "Gen loss :-0.005408341996371746\n",
      "Stud loss :0.026161557249724864\n",
      "Batch 359\n",
      "Gen loss :-0.006979634054005146\n",
      "Stud loss :0.027770005539059638\n",
      "Batch 360\n",
      "Gen loss :-0.00626455107703805\n",
      "Stud loss :0.02618562839925289\n",
      "Student net test:\n",
      "\t Test loss: \t 0.037544, \t Test accuracy \t 20.80\n",
      "Batch 361\n",
      "Gen loss :-0.005596288479864597\n",
      "Stud loss :0.02804935947060585\n",
      "Batch 362\n",
      "Gen loss :-0.006876236293464899\n",
      "Stud loss :0.029168277606368066\n",
      "Batch 363\n",
      "Gen loss :-0.006665352266281843\n",
      "Stud loss :0.028599069640040397\n",
      "Batch 364\n",
      "Gen loss :-0.006335555110126734\n",
      "Stud loss :0.026943400502204895\n",
      "Batch 365\n",
      "Gen loss :-0.005161232315003872\n",
      "Stud loss :0.02641616780310869\n",
      "Batch 366\n",
      "Gen loss :-0.005446065217256546\n",
      "Stud loss :0.02614961639046669\n",
      "Batch 367\n",
      "Gen loss :-0.005195070523768663\n",
      "Stud loss :0.027489817515015603\n",
      "Batch 368\n",
      "Gen loss :-0.006691116373986006\n",
      "Stud loss :0.02851927001029253\n",
      "Batch 369\n",
      "Gen loss :-0.006236938294023275\n",
      "Stud loss :0.027913063392043115\n",
      "Batch 370\n",
      "Gen loss :-0.0054652076214551926\n",
      "Stud loss :0.028447883576154707\n",
      "Batch 371\n",
      "Gen loss :-0.006161375902593136\n",
      "Stud loss :0.02919795550405979\n",
      "Batch 372\n",
      "Gen loss :-0.005542592611163855\n",
      "Stud loss :0.028518589958548547\n",
      "Batch 373\n",
      "Gen loss :-0.007615352515131235\n",
      "Stud loss :0.03130896780639887\n",
      "Batch 374\n",
      "Gen loss :-0.006539798807352781\n",
      "Stud loss :0.030041952803730964\n",
      "Batch 375\n",
      "Gen loss :-0.006384508218616247\n",
      "Stud loss :0.03157142549753189\n",
      "Batch 376\n",
      "Gen loss :-0.006767270155251026\n",
      "Stud loss :0.031295930966734886\n",
      "Batch 377\n",
      "Gen loss :-0.007566903717815876\n",
      "Stud loss :0.03166427686810493\n",
      "Batch 378\n",
      "Gen loss :-0.007952786050736904\n",
      "Stud loss :0.03136058356612921\n",
      "Batch 379\n",
      "Gen loss :-0.00791111309081316\n",
      "Stud loss :0.03163222000002861\n",
      "Batch 380\n",
      "Gen loss :-0.008173622190952301\n",
      "Stud loss :0.03162605613470078\n",
      "Student net test:\n",
      "\t Test loss: \t 0.036112, \t Test accuracy \t 20.56\n",
      "Batch 381\n",
      "Gen loss :-0.010252616368234158\n",
      "Stud loss :0.036330387368798255\n",
      "Batch 382\n",
      "Gen loss :-0.009125620126724243\n",
      "Stud loss :0.036138249933719634\n",
      "Batch 383\n",
      "Gen loss :-0.0082081975415349\n",
      "Stud loss :0.036390472948551175\n",
      "Batch 384\n",
      "Gen loss :-0.008498306386172771\n",
      "Stud loss :0.037631769105792044\n",
      "Batch 385\n",
      "Gen loss :-0.009197177365422249\n",
      "Stud loss :0.036969991587102416\n",
      "Batch 386\n",
      "Gen loss :-0.007762794848531485\n",
      "Stud loss :0.03670428711920977\n",
      "Batch 387\n",
      "Gen loss :-0.008756428025662899\n",
      "Stud loss :0.03792946133762598\n",
      "Batch 388\n",
      "Gen loss :-0.011072773486375809\n",
      "Stud loss :0.04184754341840744\n",
      "Batch 389\n",
      "Gen loss :-0.010659972205758095\n",
      "Stud loss :0.0415128692984581\n",
      "Batch 390\n",
      "Gen loss :-0.011000282131135464\n",
      "Stud loss :0.04028310310095549\n",
      "Batch 391\n",
      "Gen loss :-0.01063062995672226\n",
      "Stud loss :0.044381044805049896\n",
      "Batch 392\n",
      "Gen loss :-0.011395140551030636\n",
      "Stud loss :0.04576965272426605\n",
      "Batch 393\n",
      "Gen loss :-0.010508046485483646\n",
      "Stud loss :0.04324076771736145\n",
      "Batch 394\n",
      "Gen loss :-0.011661057360470295\n",
      "Stud loss :0.047527674585580826\n",
      "Batch 395\n",
      "Gen loss :-0.012144960463047028\n",
      "Stud loss :0.045221523568034175\n",
      "Batch 396\n",
      "Gen loss :-0.01403557974845171\n",
      "Stud loss :0.04935009479522705\n",
      "Batch 397\n",
      "Gen loss :-0.012585180811583996\n",
      "Stud loss :0.050527263805270194\n",
      "Batch 398\n",
      "Gen loss :-0.013063830323517323\n",
      "Stud loss :0.04968516379594803\n",
      "Batch 399\n",
      "Gen loss :-0.01515290793031454\n",
      "Stud loss :0.05465076267719269\n",
      "Batch 400\n",
      "Gen loss :-0.011836640536785126\n",
      "Stud loss :0.04907904341816902\n",
      "Student net test:\n",
      "\t Test loss: \t 0.021241, \t Test accuracy \t 32.64\n",
      "Batch 401\n",
      "Gen loss :-0.011535604484379292\n",
      "Stud loss :0.05077953264117241\n",
      "Batch 402\n",
      "Gen loss :-0.011612649075686932\n",
      "Stud loss :0.051327970996499064\n",
      "Batch 403\n",
      "Gen loss :-0.014502659440040588\n",
      "Stud loss :0.05496804490685463\n",
      "Batch 404\n",
      "Gen loss :-0.012503618374466896\n",
      "Stud loss :0.05125603936612606\n",
      "Batch 405\n",
      "Gen loss :-0.010699811391532421\n",
      "Stud loss :0.05026211403310299\n",
      "Batch 406\n",
      "Gen loss :-0.012689474038779736\n",
      "Stud loss :0.05067877992987633\n",
      "Batch 407\n",
      "Gen loss :-0.012162821367383003\n",
      "Stud loss :0.050127113983035085\n",
      "Batch 408\n",
      "Gen loss :-0.011796933598816395\n",
      "Stud loss :0.04903975240886212\n",
      "Batch 409\n",
      "Gen loss :-0.011944175697863102\n",
      "Stud loss :0.050115305557847024\n",
      "Batch 410\n",
      "Gen loss :-0.016141032800078392\n",
      "Stud loss :0.054909367486834525\n",
      "Batch 411\n",
      "Gen loss :-0.011907506734132767\n",
      "Stud loss :0.05204240046441555\n",
      "Batch 412\n",
      "Gen loss :-0.013950389809906483\n",
      "Stud loss :0.05229856520891189\n",
      "Batch 413\n",
      "Gen loss :-0.013339914381504059\n",
      "Stud loss :0.052562978118658066\n",
      "Batch 414\n",
      "Gen loss :-0.011802438646554947\n",
      "Stud loss :0.05181165374815464\n",
      "Batch 415\n",
      "Gen loss :-0.014352715574204922\n",
      "Stud loss :0.0569200687110424\n",
      "Batch 416\n",
      "Gen loss :-0.012826214544475079\n",
      "Stud loss :0.05400577373802662\n",
      "Batch 417\n",
      "Gen loss :-0.013061905279755592\n",
      "Stud loss :0.05427577048540115\n",
      "Batch 418\n",
      "Gen loss :-0.015187312848865986\n",
      "Stud loss :0.05496458373963833\n",
      "Batch 419\n",
      "Gen loss :-0.012779890559613705\n",
      "Stud loss :0.052844376489520076\n",
      "Batch 420\n",
      "Gen loss :-0.015594745054841042\n",
      "Stud loss :0.056485838070511815\n",
      "Student net test:\n",
      "\t Test loss: \t 0.022443, \t Test accuracy \t 29.78\n",
      "Batch 421\n",
      "Gen loss :-0.01431536115705967\n",
      "Stud loss :0.060146006569266316\n",
      "Batch 422\n",
      "Gen loss :-0.012696419842541218\n",
      "Stud loss :0.059502700716257094\n",
      "Batch 423\n",
      "Gen loss :-0.020288357511162758\n",
      "Stud loss :0.06896957978606225\n",
      "Batch 424\n",
      "Gen loss :-0.021320104598999023\n",
      "Stud loss :0.0656940683722496\n",
      "Batch 425\n",
      "Gen loss :-0.015623646788299084\n",
      "Stud loss :0.06043635196983814\n",
      "Batch 426\n",
      "Gen loss :-0.020118121057748795\n",
      "Stud loss :0.06683502979576587\n",
      "Batch 427\n",
      "Gen loss :-0.0166094359010458\n",
      "Stud loss :0.0626300934702158\n",
      "Batch 428\n",
      "Gen loss :-0.013792085461318493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stud loss :0.061992093548178674\n",
      "Batch 429\n",
      "Gen loss :-0.017310766503214836\n",
      "Stud loss :0.06058026514947414\n",
      "Batch 430\n",
      "Gen loss :-0.0160395335406065\n",
      "Stud loss :0.06265334747731685\n",
      "Batch 431\n",
      "Gen loss :-0.015707775950431824\n",
      "Stud loss :0.06468667685985566\n",
      "Batch 432\n",
      "Gen loss :-0.01755817048251629\n",
      "Stud loss :0.06796149872243404\n",
      "Batch 433\n",
      "Gen loss :-0.01925632916390896\n",
      "Stud loss :0.06750903874635697\n",
      "Batch 434\n",
      "Gen loss :-0.017589304596185684\n",
      "Stud loss :0.0655752331018448\n",
      "Batch 435\n",
      "Gen loss :-0.020715558901429176\n",
      "Stud loss :0.07155461311340332\n",
      "Batch 436\n",
      "Gen loss :-0.01896822825074196\n",
      "Stud loss :0.06966598182916642\n",
      "Batch 437\n",
      "Gen loss :-0.01764996163547039\n",
      "Stud loss :0.0722113836556673\n",
      "Batch 438\n",
      "Gen loss :-0.018457448109984398\n",
      "Stud loss :0.07161606587469578\n",
      "Batch 439\n",
      "Gen loss :-0.017858406528830528\n",
      "Stud loss :0.07044531740248203\n",
      "Batch 440\n",
      "Gen loss :-0.01894241012632847\n",
      "Stud loss :0.0673159345984459\n",
      "Student net test:\n",
      "\t Test loss: \t 0.024832, \t Test accuracy \t 27.25\n",
      "Batch 441\n",
      "Gen loss :-0.019005078822374344\n",
      "Stud loss :0.06765837147831917\n",
      "Batch 442\n",
      "Gen loss :-0.018389873206615448\n",
      "Stud loss :0.07293800003826618\n",
      "Batch 443\n",
      "Gen loss :-0.018626101315021515\n",
      "Stud loss :0.0703409768640995\n",
      "Batch 444\n",
      "Gen loss :-0.016759280115365982\n",
      "Stud loss :0.07094480656087399\n",
      "Batch 445\n",
      "Gen loss :-0.01387091912329197\n",
      "Stud loss :0.0646916177123785\n",
      "Batch 446\n",
      "Gen loss :-0.017295368015766144\n",
      "Stud loss :0.07029445804655551\n",
      "Batch 447\n",
      "Gen loss :-0.016255060210824013\n",
      "Stud loss :0.06831770092248916\n",
      "Batch 448\n",
      "Gen loss :-0.017390761524438858\n",
      "Stud loss :0.06916277967393399\n",
      "Batch 449\n",
      "Gen loss :-0.016816867515444756\n",
      "Stud loss :0.06800838671624661\n",
      "Batch 450\n",
      "Gen loss :-0.017743131145834923\n",
      "Stud loss :0.07107411958277225\n",
      "Batch 451\n",
      "Gen loss :-0.016450505703687668\n",
      "Stud loss :0.0688972145318985\n",
      "Batch 452\n",
      "Gen loss :-0.019109714776277542\n",
      "Stud loss :0.0764482319355011\n",
      "Batch 453\n",
      "Gen loss :-0.02052556350827217\n",
      "Stud loss :0.07947580218315124\n",
      "Batch 454\n",
      "Gen loss :-0.020072439685463905\n",
      "Stud loss :0.07592973187565803\n",
      "Batch 455\n",
      "Gen loss :-0.02177148126065731\n",
      "Stud loss :0.07557754814624787\n",
      "Batch 456\n",
      "Gen loss :-0.017850587144494057\n",
      "Stud loss :0.07312522418797016\n",
      "Batch 457\n",
      "Gen loss :-0.01948586478829384\n",
      "Stud loss :0.07533737868070603\n",
      "Batch 458\n",
      "Gen loss :-0.018571678549051285\n",
      "Stud loss :0.07845804281532764\n",
      "Batch 459\n",
      "Gen loss :-0.023338358849287033\n",
      "Stud loss :0.08150489665567875\n",
      "Batch 460\n",
      "Gen loss :-0.019339656457304955\n",
      "Stud loss :0.08059176467359067\n",
      "Student net test:\n",
      "\t Test loss: \t 0.018483, \t Test accuracy \t 34.88\n",
      "Batch 461\n",
      "Gen loss :-0.0174104031175375\n",
      "Stud loss :0.0798062015324831\n",
      "Batch 462\n",
      "Gen loss :-0.018538378179073334\n",
      "Stud loss :0.07909863404929637\n",
      "Batch 463\n",
      "Gen loss :-0.019616808742284775\n",
      "Stud loss :0.08078336790204048\n",
      "Batch 464\n",
      "Gen loss :-0.019051222130656242\n",
      "Stud loss :0.08081592060625553\n",
      "Batch 465\n",
      "Gen loss :-0.018248645588755608\n",
      "Stud loss :0.08175141960382462\n",
      "Batch 466\n",
      "Gen loss :-0.020763978362083435\n",
      "Stud loss :0.08205574341118335\n",
      "Batch 467\n",
      "Gen loss :-0.02314111590385437\n",
      "Stud loss :0.08791171461343765\n",
      "Batch 468\n",
      "Gen loss :-0.020811904221773148\n",
      "Stud loss :0.08609946370124817\n",
      "Batch 469\n",
      "Gen loss :-0.027493000030517578\n",
      "Stud loss :0.09928162544965743\n",
      "Batch 470\n",
      "Gen loss :-0.022976204752922058\n",
      "Stud loss :0.09101521074771882\n",
      "Batch 471\n",
      "Gen loss :-0.021119333803653717\n",
      "Stud loss :0.08302437476813793\n",
      "Batch 472\n",
      "Gen loss :-0.021451834589242935\n",
      "Stud loss :0.08585338555276394\n",
      "Batch 473\n",
      "Gen loss :-0.02308451570570469\n",
      "Stud loss :0.08559088818728924\n",
      "Batch 474\n",
      "Gen loss :-0.023467743769288063\n",
      "Stud loss :0.09465872943401336\n",
      "Batch 475\n",
      "Gen loss :-0.029322702437639236\n",
      "Stud loss :0.10393491238355637\n",
      "Batch 476\n",
      "Gen loss :-0.023177847266197205\n",
      "Stud loss :0.09180191680788993\n",
      "Batch 477\n",
      "Gen loss :-0.02497638203203678\n",
      "Stud loss :0.0936754509806633\n",
      "Batch 478\n",
      "Gen loss :-0.02151729166507721\n",
      "Stud loss :0.08924279212951661\n",
      "Batch 479\n",
      "Gen loss :-0.023751571774482727\n",
      "Stud loss :0.09728513509035111\n",
      "Batch 480\n",
      "Gen loss :-0.02072153612971306\n",
      "Stud loss :0.0896821640431881\n",
      "Student net test:\n",
      "\t Test loss: \t 0.019380, \t Test accuracy \t 33.57\n",
      "Batch 481\n",
      "Gen loss :-0.023152796551585197\n",
      "Stud loss :0.08930504620075226\n",
      "Batch 482\n",
      "Gen loss :-0.020751966163516045\n",
      "Stud loss :0.08644494637846947\n",
      "Batch 483\n",
      "Gen loss :-0.02284129522740841\n",
      "Stud loss :0.08966410309076309\n",
      "Batch 484\n",
      "Gen loss :-0.02485361509025097\n",
      "Stud loss :0.0866419818252325\n",
      "Batch 485\n",
      "Gen loss :-0.024320082738995552\n",
      "Stud loss :0.09012800306081772\n",
      "Batch 486\n",
      "Gen loss :-0.025947904214262962\n",
      "Stud loss :0.09643510207533837\n",
      "Batch 487\n",
      "Gen loss :-0.02190607599914074\n",
      "Stud loss :0.08370676636695862\n",
      "Batch 488\n",
      "Gen loss :-0.018899468705058098\n",
      "Stud loss :0.07983553931117057\n",
      "Batch 489\n",
      "Gen loss :-0.01666933484375477\n",
      "Stud loss :0.07788633555173874\n",
      "Batch 490\n",
      "Gen loss :-0.020915156230330467\n",
      "Stud loss :0.08256444558501244\n",
      "Batch 491\n",
      "Gen loss :-0.02155335061252117\n",
      "Stud loss :0.08342061303555966\n",
      "Batch 492\n",
      "Gen loss :-0.0188156571239233\n",
      "Stud loss :0.0815812062472105\n",
      "Batch 493\n",
      "Gen loss :-0.018795613199472427\n",
      "Stud loss :0.07770340479910373\n",
      "Batch 494\n",
      "Gen loss :-0.02116023562848568\n",
      "Stud loss :0.0833696499466896\n",
      "Batch 495\n",
      "Gen loss :-0.021211175248026848\n",
      "Stud loss :0.08367014899849892\n",
      "Batch 496\n",
      "Gen loss :-0.01734636165201664\n",
      "Stud loss :0.08050334677100182\n",
      "Batch 497\n",
      "Gen loss :-0.02184034325182438\n",
      "Stud loss :0.08947515040636063\n",
      "Batch 498\n",
      "Gen loss :-0.018311256542801857\n",
      "Stud loss :0.08139085993170739\n",
      "Batch 499\n",
      "Gen loss :-0.024862421676516533\n",
      "Stud loss :0.08872828260064125\n",
      "Batch 500\n",
      "Gen loss :-0.023837147280573845\n",
      "Stud loss :0.08667303249239922\n",
      "Student net test:\n",
      "\t Test loss: \t 0.016551, \t Test accuracy \t 38.62\n",
      "Batch 501\n",
      "Gen loss :-0.023345811292529106\n",
      "Stud loss :0.10481196865439416\n",
      "Batch 502\n",
      "Gen loss :-0.02362576313316822\n",
      "Stud loss :0.09450033158063889\n",
      "Batch 503\n",
      "Gen loss :-0.028472617268562317\n",
      "Stud loss :0.0969419501721859\n",
      "Batch 504\n",
      "Gen loss :-0.02179609052836895\n",
      "Stud loss :0.10099775046110153\n",
      "Batch 505\n",
      "Gen loss :-0.028662804514169693\n",
      "Stud loss :0.09817376956343651\n",
      "Batch 506\n",
      "Gen loss :-0.027859700843691826\n",
      "Stud loss :0.09440862685441971\n",
      "Batch 507\n",
      "Gen loss :-0.020406125113368034\n",
      "Stud loss :0.08816141560673714\n",
      "Batch 508\n",
      "Gen loss :-0.019845155999064445\n",
      "Stud loss :0.08154619969427586\n",
      "Batch 509\n",
      "Gen loss :-0.019636323675513268\n",
      "Stud loss :0.08200394175946712\n",
      "Batch 510\n",
      "Gen loss :-0.018910154700279236\n",
      "Stud loss :0.07864617742598057\n",
      "Batch 511\n",
      "Gen loss :-0.020897768437862396\n",
      "Stud loss :0.08095037937164307\n",
      "Batch 512\n",
      "Gen loss :-0.019307589158415794\n",
      "Stud loss :0.08106211647391319\n",
      "Batch 513\n",
      "Gen loss :-0.018641812726855278\n",
      "Stud loss :0.07812544889748096\n",
      "Batch 514\n",
      "Gen loss :-0.018481066450476646\n",
      "Stud loss :0.07960903719067573\n",
      "Batch 515\n",
      "Gen loss :-0.017406431958079338\n",
      "Stud loss :0.07509642243385314\n",
      "Batch 516\n",
      "Gen loss :-0.014995803125202656\n",
      "Stud loss :0.07642560228705406\n",
      "Batch 517\n",
      "Gen loss :-0.019652295857667923\n",
      "Stud loss :0.08648192472755908\n",
      "Batch 518\n",
      "Gen loss :-0.020405298098921776\n",
      "Stud loss :0.08940216898918152\n",
      "Batch 519\n",
      "Gen loss :-0.02122541330754757\n",
      "Stud loss :0.09076623320579529\n",
      "Batch 520\n",
      "Gen loss :-0.019365618005394936\n",
      "Stud loss :0.08545634225010872\n",
      "Student net test:\n",
      "\t Test loss: \t 0.014119, \t Test accuracy \t 43.58\n",
      "Batch 521\n",
      "Gen loss :-0.018257109448313713\n",
      "Stud loss :0.09212238118052482\n",
      "Batch 522\n",
      "Gen loss :-0.018116852268576622\n",
      "Stud loss :0.08295590654015542\n",
      "Batch 523\n",
      "Gen loss :-0.01831848919391632\n",
      "Stud loss :0.0826135165989399\n",
      "Batch 524\n",
      "Gen loss :-0.015272200107574463\n",
      "Stud loss :0.07677557840943336\n",
      "Batch 525\n",
      "Gen loss :-0.016567453742027283\n",
      "Stud loss :0.077657176181674\n",
      "Batch 526\n",
      "Gen loss :-0.015676595270633698\n",
      "Stud loss :0.07504268474876881\n",
      "Batch 527\n",
      "Gen loss :-0.013008161447942257\n",
      "Stud loss :0.07251696996390819\n",
      "Batch 528\n",
      "Gen loss :-0.015753230080008507\n",
      "Stud loss :0.06999929957091808\n",
      "Batch 529\n",
      "Gen loss :-0.01777246966958046\n",
      "Stud loss :0.07653001435101033\n",
      "Batch 530\n",
      "Gen loss :-0.01495720911771059\n",
      "Stud loss :0.07093854583799838\n",
      "Batch 531\n",
      "Gen loss :-0.014054670929908752\n",
      "Stud loss :0.06913919895887374\n",
      "Batch 532\n",
      "Gen loss :-0.014162393286824226\n",
      "Stud loss :0.06864016242325306\n",
      "Batch 533\n",
      "Gen loss :-0.013179323635995388\n",
      "Stud loss :0.0663735181093216\n",
      "Batch 534\n",
      "Gen loss :-0.013899113051593304\n",
      "Stud loss :0.06643665358424186\n",
      "Batch 535\n",
      "Gen loss :-0.015843799337744713\n",
      "Stud loss :0.06903080008924008\n",
      "Batch 536\n",
      "Gen loss :-0.017589977011084557\n",
      "Stud loss :0.07553685382008553\n",
      "Batch 537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.012724495492875576\n",
      "Stud loss :0.0667992316186428\n",
      "Batch 538\n",
      "Gen loss :-0.01578715816140175\n",
      "Stud loss :0.06747915260493756\n",
      "Batch 539\n",
      "Gen loss :-0.013189414516091347\n",
      "Stud loss :0.06488748826086521\n",
      "Batch 540\n",
      "Gen loss :-0.014302290976047516\n",
      "Stud loss :0.06768500693142414\n",
      "Student net test:\n",
      "\t Test loss: \t 0.023061, \t Test accuracy \t 31.19\n",
      "Batch 541\n",
      "Gen loss :-0.013648184947669506\n",
      "Stud loss :0.06790213249623775\n",
      "Batch 542\n",
      "Gen loss :-0.015007617883384228\n",
      "Stud loss :0.06888451017439365\n",
      "Batch 543\n",
      "Gen loss :-0.013899916782975197\n",
      "Stud loss :0.07193171456456185\n",
      "Batch 544\n",
      "Gen loss :-0.015490707941353321\n",
      "Stud loss :0.06886624731123447\n",
      "Batch 545\n",
      "Gen loss :-0.01291600801050663\n",
      "Stud loss :0.06423595771193505\n",
      "Batch 546\n",
      "Gen loss :-0.013468257151544094\n",
      "Stud loss :0.06615377403795719\n",
      "Batch 547\n",
      "Gen loss :-0.013246064074337482\n",
      "Stud loss :0.06796102784574032\n",
      "Batch 548\n",
      "Gen loss :-0.017564764246344566\n",
      "Stud loss :0.07312839776277542\n",
      "Batch 549\n",
      "Gen loss :-0.014015751890838146\n",
      "Stud loss :0.0673868328332901\n",
      "Batch 550\n",
      "Gen loss :-0.01263561099767685\n",
      "Stud loss :0.06459445804357529\n",
      "Batch 551\n",
      "Gen loss :-0.012068848125636578\n",
      "Stud loss :0.07196948938071727\n",
      "Batch 552\n",
      "Gen loss :-0.014966425485908985\n",
      "Stud loss :0.07447084225714207\n",
      "Batch 553\n",
      "Gen loss :-0.011766905896365643\n",
      "Stud loss :0.06929792128503323\n",
      "Batch 554\n",
      "Gen loss :-0.015772085636854172\n",
      "Stud loss :0.07237568572163582\n",
      "Batch 555\n",
      "Gen loss :-0.017344357445836067\n",
      "Stud loss :0.07966183312237263\n",
      "Batch 556\n",
      "Gen loss :-0.014507579617202282\n",
      "Stud loss :0.07327247709035874\n",
      "Batch 557\n",
      "Gen loss :-0.015912940725684166\n",
      "Stud loss :0.07673916891217232\n",
      "Batch 558\n",
      "Gen loss :-0.01490782666951418\n",
      "Stud loss :0.07758283913135529\n",
      "Batch 559\n",
      "Gen loss :-0.01538221538066864\n",
      "Stud loss :0.08143970631062984\n",
      "Batch 560\n",
      "Gen loss :-0.017135240137577057\n",
      "Stud loss :0.09014955833554268\n",
      "Student net test:\n",
      "\t Test loss: \t 0.013440, \t Test accuracy \t 49.24\n",
      "Batch 561\n",
      "Gen loss :-0.015956958755850792\n",
      "Stud loss :0.09436877220869064\n",
      "Batch 562\n",
      "Gen loss :-0.020309047773480415\n",
      "Stud loss :0.09444370567798614\n",
      "Batch 563\n",
      "Gen loss :-0.028428250923752785\n",
      "Stud loss :0.1192605547606945\n",
      "Batch 564\n",
      "Gen loss :-0.020491082221269608\n",
      "Stud loss :0.08933627530932427\n",
      "Batch 565\n",
      "Gen loss :-0.014022608287632465\n",
      "Stud loss :0.08279823884367943\n",
      "Batch 566\n",
      "Gen loss :-0.008977582678198814\n",
      "Stud loss :0.07049439400434494\n",
      "Batch 567\n",
      "Gen loss :-0.011317335069179535\n",
      "Stud loss :0.0673883881419897\n",
      "Batch 568\n",
      "Gen loss :-0.005886250641196966\n",
      "Stud loss :0.06259997710585594\n",
      "Batch 569\n",
      "Gen loss :-0.00712761003524065\n",
      "Stud loss :0.05981938466429711\n",
      "Batch 570\n",
      "Gen loss :-0.0051569947972893715\n",
      "Stud loss :0.05397837795317173\n",
      "Batch 571\n",
      "Gen loss :-0.0055747367441654205\n",
      "Stud loss :0.05122543126344681\n",
      "Batch 572\n",
      "Gen loss :-0.007687464356422424\n",
      "Stud loss :0.052378424629569056\n",
      "Batch 573\n",
      "Gen loss :-0.007849779911339283\n",
      "Stud loss :0.05082184039056301\n",
      "Batch 574\n",
      "Gen loss :-0.00645731994882226\n",
      "Stud loss :0.050035208091139795\n",
      "Batch 575\n",
      "Gen loss :-0.006223315838724375\n",
      "Stud loss :0.046409862115979195\n",
      "Batch 576\n",
      "Gen loss :-0.007389248348772526\n",
      "Stud loss :0.04806152395904064\n",
      "Batch 577\n",
      "Gen loss :-0.010453948751091957\n",
      "Stud loss :0.04829119108617306\n",
      "Batch 578\n",
      "Gen loss :-0.007146808784455061\n",
      "Stud loss :0.04433446004986763\n",
      "Batch 579\n",
      "Gen loss :-0.007565641310065985\n",
      "Stud loss :0.047258654981851576\n",
      "Batch 580\n",
      "Gen loss :-0.007416889071464539\n",
      "Stud loss :0.04041955284774303\n",
      "Student net test:\n",
      "\t Test loss: \t 0.017349, \t Test accuracy \t 34.44\n",
      "Batch 581\n",
      "Gen loss :-0.009521516971290112\n",
      "Stud loss :0.04224411472678184\n",
      "Batch 582\n",
      "Gen loss :-0.009636188857257366\n",
      "Stud loss :0.04446859359741211\n",
      "Batch 583\n",
      "Gen loss :-0.006269329693168402\n",
      "Stud loss :0.04221456125378609\n",
      "Batch 584\n",
      "Gen loss :-0.012501636520028114\n",
      "Stud loss :0.048331210017204286\n",
      "Batch 585\n",
      "Gen loss :-0.008908255957067013\n",
      "Stud loss :0.04679092988371849\n",
      "Batch 586\n",
      "Gen loss :-0.007916777394711971\n",
      "Stud loss :0.04318004362285137\n",
      "Batch 587\n",
      "Gen loss :-0.011814343743026257\n",
      "Stud loss :0.04639639221131801\n",
      "Batch 588\n",
      "Gen loss :-0.010235460475087166\n",
      "Stud loss :0.05226957201957703\n",
      "Batch 589\n",
      "Gen loss :-0.011300811544060707\n",
      "Stud loss :0.04943653792142868\n",
      "Batch 590\n",
      "Gen loss :-0.012297222390770912\n",
      "Stud loss :0.047258269786834714\n",
      "Batch 591\n",
      "Gen loss :-0.01588817499577999\n",
      "Stud loss :0.05578366182744503\n",
      "Batch 592\n",
      "Gen loss :-0.018436281010508537\n",
      "Stud loss :0.05639054477214813\n",
      "Batch 593\n",
      "Gen loss :-0.016041815280914307\n",
      "Stud loss :0.062137944623827934\n",
      "Batch 594\n",
      "Gen loss :-0.017550840973854065\n",
      "Stud loss :0.0634479895234108\n",
      "Batch 595\n",
      "Gen loss :-0.013726025819778442\n",
      "Stud loss :0.05944234766066074\n",
      "Batch 596\n",
      "Gen loss :-0.015376873314380646\n",
      "Stud loss :0.06417049206793309\n",
      "Batch 597\n",
      "Gen loss :-0.014932936988770962\n",
      "Stud loss :0.06112191118299961\n",
      "Batch 598\n",
      "Gen loss :-0.02017785608768463\n",
      "Stud loss :0.08578172847628593\n",
      "Batch 599\n",
      "Gen loss :-0.021453266963362694\n",
      "Stud loss :0.07479883432388305\n",
      "Batch 600\n",
      "Gen loss :-0.024533307179808617\n",
      "Stud loss :0.1074945606291294\n",
      "Student net test:\n",
      "\t Test loss: \t 0.013350, \t Test accuracy \t 47.40\n",
      "Batch 601\n",
      "Gen loss :-0.023540353402495384\n",
      "Stud loss :0.10677459836006165\n",
      "Batch 602\n",
      "Gen loss :-0.025287529453635216\n",
      "Stud loss :0.09893346577882767\n",
      "Batch 603\n",
      "Gen loss :-0.02375517599284649\n",
      "Stud loss :0.09558183923363686\n",
      "Batch 604\n",
      "Gen loss :-0.018624654039740562\n",
      "Stud loss :0.08003822714090347\n",
      "Batch 605\n",
      "Gen loss :-0.016205165535211563\n",
      "Stud loss :0.07678046114742756\n",
      "Batch 606\n",
      "Gen loss :-0.015344853512942791\n",
      "Stud loss :0.07005531378090382\n",
      "Batch 607\n",
      "Gen loss :-0.01344514824450016\n",
      "Stud loss :0.0659705389291048\n",
      "Batch 608\n",
      "Gen loss :-0.013179545290768147\n",
      "Stud loss :0.06149718724191189\n",
      "Batch 609\n",
      "Gen loss :-0.01281586941331625\n",
      "Stud loss :0.05624232329428196\n",
      "Batch 610\n",
      "Gen loss :-0.013641014695167542\n",
      "Stud loss :0.05663015767931938\n",
      "Batch 611\n",
      "Gen loss :-0.012511307373642921\n",
      "Stud loss :0.05718619637191295\n",
      "Batch 612\n",
      "Gen loss :-0.01122845709323883\n",
      "Stud loss :0.051656075194478034\n",
      "Batch 613\n",
      "Gen loss :-0.013024792075157166\n",
      "Stud loss :0.05058973282575607\n",
      "Batch 614\n",
      "Gen loss :-0.009900211356580257\n",
      "Stud loss :0.04716903939843178\n",
      "Batch 615\n",
      "Gen loss :-0.014217066578567028\n",
      "Stud loss :0.05098203755915165\n",
      "Batch 616\n",
      "Gen loss :-0.011566857807338238\n",
      "Stud loss :0.04686407707631588\n",
      "Batch 617\n",
      "Gen loss :-0.009376897476613522\n",
      "Stud loss :0.045568881556391716\n",
      "Batch 618\n",
      "Gen loss :-0.008255711756646633\n",
      "Stud loss :0.0423982284963131\n",
      "Batch 619\n",
      "Gen loss :-0.007664980832487345\n",
      "Stud loss :0.041658655554056165\n",
      "Batch 620\n",
      "Gen loss :-0.00894165225327015\n",
      "Stud loss :0.04371532537043095\n",
      "Student net test:\n",
      "\t Test loss: \t 0.013748, \t Test accuracy \t 47.97\n",
      "Batch 621\n",
      "Gen loss :-0.009866723790764809\n",
      "Stud loss :0.043587376549839976\n",
      "Batch 622\n",
      "Gen loss :-0.009298528544604778\n",
      "Stud loss :0.04241797924041748\n",
      "Batch 623\n",
      "Gen loss :-0.007902960292994976\n",
      "Stud loss :0.03946938812732696\n",
      "Batch 624\n",
      "Gen loss :-0.009705762378871441\n",
      "Stud loss :0.042978374287486076\n",
      "Batch 625\n",
      "Gen loss :-0.010532088577747345\n",
      "Stud loss :0.043243442103266715\n",
      "Batch 626\n",
      "Gen loss :-0.00923149660229683\n",
      "Stud loss :0.044082478061318396\n",
      "Batch 627\n",
      "Gen loss :-0.008217680267989635\n",
      "Stud loss :0.043442677706480026\n",
      "Batch 628\n",
      "Gen loss :-0.010138343088328838\n",
      "Stud loss :0.04687820635735988\n",
      "Batch 629\n",
      "Gen loss :-0.00978374108672142\n",
      "Stud loss :0.043200156465172765\n",
      "Batch 630\n",
      "Gen loss :-0.01106080412864685\n",
      "Stud loss :0.04620015360414982\n",
      "Batch 631\n",
      "Gen loss :-0.008539236150681973\n",
      "Stud loss :0.04332744553685188\n",
      "Batch 632\n",
      "Gen loss :-0.01312248595058918\n",
      "Stud loss :0.04682710692286492\n",
      "Batch 633\n",
      "Gen loss :-0.008298665285110474\n",
      "Stud loss :0.042450911551713946\n",
      "Batch 634\n",
      "Gen loss :-0.008742153644561768\n",
      "Stud loss :0.04146015495061874\n",
      "Batch 635\n",
      "Gen loss :-0.009182636626064777\n",
      "Stud loss :0.042605667188763616\n",
      "Batch 636\n",
      "Gen loss :-0.010525879450142384\n",
      "Stud loss :0.043955618515610695\n",
      "Batch 637\n",
      "Gen loss :-0.008867981843650341\n",
      "Stud loss :0.04026156067848206\n",
      "Batch 638\n",
      "Gen loss :-0.008954823017120361\n",
      "Stud loss :0.04164961837232113\n",
      "Batch 639\n",
      "Gen loss :-0.010655023157596588\n",
      "Stud loss :0.04344083331525326\n",
      "Batch 640\n",
      "Gen loss :-0.009293130598962307\n",
      "Stud loss :0.042027993872761724\n",
      "Student net test:\n",
      "\t Test loss: \t 0.014799, \t Test accuracy \t 43.77\n",
      "Batch 641\n",
      "Gen loss :-0.008970264345407486\n",
      "Stud loss :0.04424863122403622\n",
      "Batch 642\n",
      "Gen loss :-0.007835504598915577\n",
      "Stud loss :0.04116503931581974\n",
      "Batch 643\n",
      "Gen loss :-0.009058008901774883\n",
      "Stud loss :0.04127495028078556\n",
      "Batch 644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.009551974944770336\n",
      "Stud loss :0.04312584213912487\n",
      "Batch 645\n",
      "Gen loss :-0.008580345660448074\n",
      "Stud loss :0.042691930383443835\n",
      "Batch 646\n",
      "Gen loss :-0.010682939551770687\n",
      "Stud loss :0.044860322028398514\n",
      "Batch 647\n",
      "Gen loss :-0.008827977813780308\n",
      "Stud loss :0.04421420246362686\n",
      "Batch 648\n",
      "Gen loss :-0.01091244351118803\n",
      "Stud loss :0.04642755016684532\n",
      "Batch 649\n",
      "Gen loss :-0.012146525084972382\n",
      "Stud loss :0.04593916870653629\n",
      "Batch 650\n",
      "Gen loss :-0.009357147850096226\n",
      "Stud loss :0.044872748851776126\n",
      "Batch 651\n",
      "Gen loss :-0.010033870115876198\n",
      "Stud loss :0.045906081795692444\n",
      "Batch 652\n",
      "Gen loss :-0.010995668359100819\n",
      "Stud loss :0.0469194546341896\n",
      "Batch 653\n",
      "Gen loss :-0.01053842157125473\n",
      "Stud loss :0.04718719683587551\n",
      "Batch 654\n",
      "Gen loss :-0.008625208400189877\n",
      "Stud loss :0.045053808391094206\n",
      "Batch 655\n",
      "Gen loss :-0.012260599993169308\n",
      "Stud loss :0.04776026606559754\n",
      "Batch 656\n",
      "Gen loss :-0.010556647554039955\n",
      "Stud loss :0.04540090747177601\n",
      "Batch 657\n",
      "Gen loss :-0.0096736503764987\n",
      "Stud loss :0.04473953954875469\n",
      "Batch 658\n",
      "Gen loss :-0.008526453748345375\n",
      "Stud loss :0.045587117597460744\n",
      "Batch 659\n",
      "Gen loss :-0.011058885604143143\n",
      "Stud loss :0.04675539471209049\n",
      "Batch 660\n",
      "Gen loss :-0.009078273549675941\n",
      "Stud loss :0.045428108796477316\n",
      "Student net test:\n",
      "\t Test loss: \t 0.014645, \t Test accuracy \t 42.56\n",
      "Batch 661\n",
      "Gen loss :-0.010462815873324871\n",
      "Stud loss :0.04786357097327709\n",
      "Batch 662\n",
      "Gen loss :-0.008838444016873837\n",
      "Stud loss :0.04449745193123818\n",
      "Batch 663\n",
      "Gen loss :-0.010562269948422909\n",
      "Stud loss :0.04319821372628212\n",
      "Batch 664\n",
      "Gen loss :-0.011017540469765663\n",
      "Stud loss :0.04622278772294521\n",
      "Batch 665\n",
      "Gen loss :-0.011880479753017426\n",
      "Stud loss :0.047642548009753226\n",
      "Batch 666\n",
      "Gen loss :-0.012952029705047607\n",
      "Stud loss :0.04732298627495766\n",
      "Batch 667\n",
      "Gen loss :-0.009475159458816051\n",
      "Stud loss :0.04746340550482273\n",
      "Batch 668\n",
      "Gen loss :-0.010932544246315956\n",
      "Stud loss :0.04844909384846687\n",
      "Batch 669\n",
      "Gen loss :-0.010336746461689472\n",
      "Stud loss :0.04700128771364689\n",
      "Batch 670\n",
      "Gen loss :-0.010233663022518158\n",
      "Stud loss :0.04683732390403748\n",
      "Batch 671\n",
      "Gen loss :-0.010324514470994473\n",
      "Stud loss :0.04616701751947403\n",
      "Batch 672\n",
      "Gen loss :-0.008984177373349667\n",
      "Stud loss :0.04446162059903145\n",
      "Batch 673\n",
      "Gen loss :-0.008805260062217712\n",
      "Stud loss :0.0448607012629509\n",
      "Batch 674\n",
      "Gen loss :-0.008894076570868492\n",
      "Stud loss :0.04490205347537994\n",
      "Batch 675\n",
      "Gen loss :-0.010476181283593178\n",
      "Stud loss :0.0456990472972393\n",
      "Batch 676\n",
      "Gen loss :-0.01094540674239397\n",
      "Stud loss :0.04717356227338314\n",
      "Batch 677\n",
      "Gen loss :-0.010966573841869831\n",
      "Stud loss :0.04788915738463402\n",
      "Batch 678\n",
      "Gen loss :-0.011606534011662006\n",
      "Stud loss :0.0475039254873991\n",
      "Batch 679\n",
      "Gen loss :-0.00908123143017292\n",
      "Stud loss :0.044997555390000346\n",
      "Batch 680\n",
      "Gen loss :-0.009791902266442776\n",
      "Stud loss :0.045599165931344034\n",
      "Student net test:\n",
      "\t Test loss: \t 0.017482, \t Test accuracy \t 38.55\n",
      "Batch 681\n",
      "Gen loss :-0.011746055446565151\n",
      "Stud loss :0.047968998178839685\n",
      "Batch 682\n",
      "Gen loss :-0.009369725361466408\n",
      "Stud loss :0.046379412338137624\n",
      "Batch 683\n",
      "Gen loss :-0.009749536402523518\n",
      "Stud loss :0.04475628882646561\n",
      "Batch 684\n",
      "Gen loss :-0.009461731649935246\n",
      "Stud loss :0.04467162489891052\n",
      "Batch 685\n",
      "Gen loss :-0.010597829706966877\n",
      "Stud loss :0.04885560385882855\n",
      "Batch 686\n",
      "Gen loss :-0.009701813571155071\n",
      "Stud loss :0.04598025009036064\n",
      "Batch 687\n",
      "Gen loss :-0.00954500213265419\n",
      "Stud loss :0.045170999318361285\n",
      "Batch 688\n",
      "Gen loss :-0.011460991576313972\n",
      "Stud loss :0.048662062361836435\n",
      "Batch 689\n",
      "Gen loss :-0.010749374516308308\n",
      "Stud loss :0.046791504323482516\n",
      "Batch 690\n",
      "Gen loss :-0.010068452917039394\n",
      "Stud loss :0.04566435068845749\n",
      "Batch 691\n",
      "Gen loss :-0.010638873092830181\n",
      "Stud loss :0.04727076478302479\n",
      "Batch 692\n",
      "Gen loss :-0.01198967732489109\n",
      "Stud loss :0.04787461720407009\n",
      "Batch 693\n",
      "Gen loss :-0.009780480526387691\n",
      "Stud loss :0.04807398244738579\n",
      "Batch 694\n",
      "Gen loss :-0.009753274731338024\n",
      "Stud loss :0.04504179321229458\n",
      "Batch 695\n",
      "Gen loss :-0.011211843229830265\n",
      "Stud loss :0.04742808677256107\n",
      "Batch 696\n",
      "Gen loss :-0.013365230523049831\n",
      "Stud loss :0.04908753484487534\n",
      "Batch 697\n",
      "Gen loss :-0.012152222916483879\n",
      "Stud loss :0.04876544550061226\n",
      "Batch 698\n",
      "Gen loss :-0.012183855287730694\n",
      "Stud loss :0.04734611846506596\n",
      "Batch 699\n",
      "Gen loss :-0.010379834100604057\n",
      "Stud loss :0.04735161140561104\n",
      "Batch 700\n",
      "Gen loss :-0.010263015516102314\n",
      "Stud loss :0.04691222831606865\n",
      "Student net test:\n",
      "\t Test loss: \t 0.016980, \t Test accuracy \t 40.48\n",
      "Batch 701\n",
      "Gen loss :-0.009276134893298149\n",
      "Stud loss :0.0482002817094326\n",
      "Batch 702\n",
      "Gen loss :-0.008477832190692425\n",
      "Stud loss :0.046067729219794275\n",
      "Batch 703\n",
      "Gen loss :-0.007776313461363316\n",
      "Stud loss :0.04392089694738388\n",
      "Batch 704\n",
      "Gen loss :-0.008431901223957539\n",
      "Stud loss :0.04490537978708744\n",
      "Batch 705\n",
      "Gen loss :-0.009586066007614136\n",
      "Stud loss :0.04489125944674015\n",
      "Batch 706\n",
      "Gen loss :-0.00979603361338377\n",
      "Stud loss :0.04610918238759041\n",
      "Batch 707\n",
      "Gen loss :-0.012993620708584785\n",
      "Stud loss :0.047973867133259775\n",
      "Batch 708\n",
      "Gen loss :-0.01068996824324131\n",
      "Stud loss :0.047154683992266654\n",
      "Batch 709\n",
      "Gen loss :-0.010622138157486916\n",
      "Stud loss :0.05049445331096649\n",
      "Batch 710\n",
      "Gen loss :-0.01335016917437315\n",
      "Stud loss :0.04982217289507389\n",
      "Batch 711\n",
      "Gen loss :-0.01278760563582182\n",
      "Stud loss :0.05097640231251717\n",
      "Batch 712\n",
      "Gen loss :-0.01180112361907959\n",
      "Stud loss :0.051542621478438376\n",
      "Batch 713\n",
      "Gen loss :-0.011193260550498962\n",
      "Stud loss :0.05045218430459499\n",
      "Batch 714\n",
      "Gen loss :-0.011051434092223644\n",
      "Stud loss :0.04950585439801216\n",
      "Batch 715\n",
      "Gen loss :-0.010848157107830048\n",
      "Stud loss :0.04907074086368084\n",
      "Batch 716\n",
      "Gen loss :-0.010731386952102184\n",
      "Stud loss :0.049181966856122014\n",
      "Batch 717\n",
      "Gen loss :-0.010257934220135212\n",
      "Stud loss :0.049607472866773604\n",
      "Batch 718\n",
      "Gen loss :-0.010370406322181225\n",
      "Stud loss :0.04649250283837318\n",
      "Batch 719\n",
      "Gen loss :-0.01010614912956953\n",
      "Stud loss :0.049551306292414665\n",
      "Batch 720\n",
      "Gen loss :-0.009651112370193005\n",
      "Stud loss :0.04701121225953102\n",
      "Student net test:\n",
      "\t Test loss: \t 0.016177, \t Test accuracy \t 40.76\n",
      "Batch 721\n",
      "Gen loss :-0.00862819142639637\n",
      "Stud loss :0.04866752400994301\n",
      "Batch 722\n",
      "Gen loss :-0.009474633261561394\n",
      "Stud loss :0.05007114261388779\n",
      "Batch 723\n",
      "Gen loss :-0.009538446553051472\n",
      "Stud loss :0.049111321195960046\n",
      "Batch 724\n",
      "Gen loss :-0.011264946311712265\n",
      "Stud loss :0.05078415609896183\n",
      "Batch 725\n",
      "Gen loss :-0.010863028466701508\n",
      "Stud loss :0.05001828968524933\n",
      "Batch 726\n",
      "Gen loss :-0.009294009767472744\n",
      "Stud loss :0.048505325615406034\n",
      "Batch 727\n",
      "Gen loss :-0.009031066671013832\n",
      "Stud loss :0.04804060943424702\n",
      "Batch 728\n",
      "Gen loss :-0.009055917151272297\n",
      "Stud loss :0.04600389711558819\n",
      "Batch 729\n",
      "Gen loss :-0.010051381774246693\n",
      "Stud loss :0.04772172048687935\n",
      "Batch 730\n",
      "Gen loss :-0.010940794833004475\n",
      "Stud loss :0.04792374111711979\n",
      "Batch 731\n",
      "Gen loss :-0.011659177951514721\n",
      "Stud loss :0.04824122600257397\n",
      "Batch 732\n",
      "Gen loss :-0.011434133164584637\n",
      "Stud loss :0.04945579767227173\n",
      "Batch 733\n",
      "Gen loss :-0.011834269389510155\n",
      "Stud loss :0.049425842240452766\n",
      "Batch 734\n",
      "Gen loss :-0.010655517689883709\n",
      "Stud loss :0.0500303216278553\n",
      "Batch 735\n",
      "Gen loss :-0.013281193561851978\n",
      "Stud loss :0.05018740966916084\n",
      "Batch 736\n",
      "Gen loss :-0.011794530786573887\n",
      "Stud loss :0.05040213465690613\n",
      "Batch 737\n",
      "Gen loss :-0.011830207891762257\n",
      "Stud loss :0.049652667716145515\n",
      "Batch 738\n",
      "Gen loss :-0.011192956008017063\n",
      "Stud loss :0.04973300099372864\n",
      "Batch 739\n",
      "Gen loss :-0.01132559310644865\n",
      "Stud loss :0.05016694590449333\n",
      "Batch 740\n",
      "Gen loss :-0.010238837450742722\n",
      "Stud loss :0.049142385646700856\n",
      "Student net test:\n",
      "\t Test loss: \t 0.014915, \t Test accuracy \t 42.86\n",
      "Batch 741\n",
      "Gen loss :-0.009405630640685558\n",
      "Stud loss :0.04879422299563885\n",
      "Batch 742\n",
      "Gen loss :-0.012048857286572456\n",
      "Stud loss :0.051735852286219595\n",
      "Batch 743\n",
      "Gen loss :-0.010398194193840027\n",
      "Stud loss :0.05115360766649246\n",
      "Batch 744\n",
      "Gen loss :-0.011097514070570469\n",
      "Stud loss :0.05222420841455459\n",
      "Batch 745\n",
      "Gen loss :-0.009534134529531002\n",
      "Stud loss :0.04764768034219742\n",
      "Batch 746\n",
      "Gen loss :-0.012734618969261646\n",
      "Stud loss :0.05175165496766567\n",
      "Batch 747\n",
      "Gen loss :-0.010319259949028492\n",
      "Stud loss :0.050276719778776166\n",
      "Batch 748\n",
      "Gen loss :-0.009615099988877773\n",
      "Stud loss :0.04817666672170162\n",
      "Batch 749\n",
      "Gen loss :-0.010108204558491707\n",
      "Stud loss :0.04815310277044773\n",
      "Batch 750\n",
      "Gen loss :-0.00994147825986147\n",
      "Stud loss :0.05138172022998333\n",
      "Batch 751\n",
      "Gen loss :-0.009982016868889332\n",
      "Stud loss :0.04753159694373608\n",
      "Batch 752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.010812784545123577\n",
      "Stud loss :0.05181880816817284\n",
      "Batch 753\n",
      "Gen loss :-0.01050406601279974\n",
      "Stud loss :0.052983736619353294\n",
      "Batch 754\n",
      "Gen loss :-0.010474525392055511\n",
      "Stud loss :0.05050730630755425\n",
      "Batch 755\n",
      "Gen loss :-0.012393190525472164\n",
      "Stud loss :0.05236758813261986\n",
      "Batch 756\n",
      "Gen loss :-0.013643271289765835\n",
      "Stud loss :0.053251459077000615\n",
      "Batch 757\n",
      "Gen loss :-0.017322083935141563\n",
      "Stud loss :0.05518090277910233\n",
      "Batch 758\n",
      "Gen loss :-0.011673521250486374\n",
      "Stud loss :0.053853008151054385\n",
      "Batch 759\n",
      "Gen loss :-0.0105039207264781\n",
      "Stud loss :0.053537018969655036\n",
      "Batch 760\n",
      "Gen loss :-0.011148606427013874\n",
      "Stud loss :0.05095738507807255\n",
      "Student net test:\n",
      "\t Test loss: \t 0.015065, \t Test accuracy \t 42.21\n",
      "Batch 761\n",
      "Gen loss :-0.01106600183993578\n",
      "Stud loss :0.05279076285660267\n",
      "Batch 762\n",
      "Gen loss :-0.011599804274737835\n",
      "Stud loss :0.051365474611520766\n",
      "Batch 763\n",
      "Gen loss :-0.009379680268466473\n",
      "Stud loss :0.05079679265618324\n",
      "Batch 764\n",
      "Gen loss :-0.011346030049026012\n",
      "Stud loss :0.05252078138291836\n",
      "Batch 765\n",
      "Gen loss :-0.011952982284128666\n",
      "Stud loss :0.05246539562940598\n",
      "Batch 766\n",
      "Gen loss :-0.012076246552169323\n",
      "Stud loss :0.05394800491631031\n",
      "Batch 767\n",
      "Gen loss :-0.012838083319365978\n",
      "Stud loss :0.053780702874064445\n",
      "Batch 768\n",
      "Gen loss :-0.009873142465949059\n",
      "Stud loss :0.05383257269859314\n",
      "Batch 769\n",
      "Gen loss :-0.010371637530624866\n",
      "Stud loss :0.05025266669690609\n",
      "Batch 770\n",
      "Gen loss :-0.013737219385802746\n",
      "Stud loss :0.057054121419787406\n",
      "Batch 771\n",
      "Gen loss :-0.01087994035333395\n",
      "Stud loss :0.05531684495508671\n",
      "Batch 772\n",
      "Gen loss :-0.011232945136725903\n",
      "Stud loss :0.054812885075807574\n",
      "Batch 773\n",
      "Gen loss :-0.01265860814601183\n",
      "Stud loss :0.05787549801170826\n",
      "Batch 774\n",
      "Gen loss :-0.011768040247261524\n",
      "Stud loss :0.05449068173766136\n",
      "Batch 775\n",
      "Gen loss :-0.012140372768044472\n",
      "Stud loss :0.0560525443404913\n",
      "Batch 776\n",
      "Gen loss :-0.01090614777058363\n",
      "Stud loss :0.05520338080823421\n",
      "Batch 777\n",
      "Gen loss :-0.014679270796477795\n",
      "Stud loss :0.056817331537604335\n",
      "Batch 778\n",
      "Gen loss :-0.01158797275274992\n",
      "Stud loss :0.05747349299490452\n",
      "Batch 779\n",
      "Gen loss :-0.011711398139595985\n",
      "Stud loss :0.05716114416718483\n",
      "Batch 780\n",
      "Gen loss :-0.01227138563990593\n",
      "Stud loss :0.05634557195007801\n",
      "Student net test:\n",
      "\t Test loss: \t 0.012455, \t Test accuracy \t 50.32\n",
      "Batch 781\n",
      "Gen loss :-0.01158194150775671\n",
      "Stud loss :0.057070058584213254\n",
      "Batch 782\n",
      "Gen loss :-0.011371324770152569\n",
      "Stud loss :0.05617357827723026\n",
      "Batch 783\n",
      "Gen loss :-0.013581404462456703\n",
      "Stud loss :0.05829514190554619\n",
      "Batch 784\n",
      "Gen loss :-0.011310991831123829\n",
      "Stud loss :0.05830794796347618\n",
      "Batch 785\n",
      "Gen loss :-0.01475455891340971\n",
      "Stud loss :0.05981198698282242\n",
      "Batch 786\n",
      "Gen loss :-0.013522873632609844\n",
      "Stud loss :0.06060633733868599\n",
      "Batch 787\n",
      "Gen loss :-0.013731077313423157\n",
      "Stud loss :0.06042763069272041\n",
      "Batch 788\n",
      "Gen loss :-0.018323587253689766\n",
      "Stud loss :0.06662794649600982\n",
      "Batch 789\n",
      "Gen loss :-0.015028457157313824\n",
      "Stud loss :0.06539694480597973\n",
      "Batch 790\n",
      "Gen loss :-0.01956877112388611\n",
      "Stud loss :0.06701509132981301\n",
      "Batch 791\n",
      "Gen loss :-0.015153157524764538\n",
      "Stud loss :0.06845856681466103\n",
      "Batch 792\n",
      "Gen loss :-0.01350924652069807\n",
      "Stud loss :0.06571742109954357\n",
      "Batch 793\n",
      "Gen loss :-0.014607015065848827\n",
      "Stud loss :0.06329778619110585\n",
      "Batch 794\n",
      "Gen loss :-0.01363306399434805\n",
      "Stud loss :0.062270795181393626\n",
      "Batch 795\n",
      "Gen loss :-0.016607537865638733\n",
      "Stud loss :0.06929718628525734\n",
      "Batch 796\n",
      "Gen loss :-0.017327930778265\n",
      "Stud loss :0.06619540192186832\n",
      "Batch 797\n",
      "Gen loss :-0.015160431154072285\n",
      "Stud loss :0.06565122120082378\n",
      "Batch 798\n",
      "Gen loss :-0.018557947129011154\n",
      "Stud loss :0.06726230345666409\n",
      "Batch 799\n",
      "Gen loss :-0.015798866748809814\n",
      "Stud loss :0.06576322019100189\n",
      "Batch 800\n",
      "Gen loss :-0.016382895410060883\n",
      "Stud loss :0.06608532778918744\n",
      "Student net test:\n",
      "\t Test loss: \t 0.013906, \t Test accuracy \t 47.85\n",
      "Batch 801\n",
      "Gen loss :-0.016857076436281204\n",
      "Stud loss :0.0661238044500351\n",
      "Batch 802\n",
      "Gen loss :-0.01629151590168476\n",
      "Stud loss :0.0625241320580244\n",
      "Batch 803\n",
      "Gen loss :-0.01791859231889248\n",
      "Stud loss :0.06712105721235276\n",
      "Batch 804\n",
      "Gen loss :-0.020674599334597588\n",
      "Stud loss :0.06785310804843903\n",
      "Batch 805\n",
      "Gen loss :-0.017803018912672997\n",
      "Stud loss :0.06916693411767483\n",
      "Batch 806\n",
      "Gen loss :-0.01906379871070385\n",
      "Stud loss :0.0684636678546667\n",
      "Batch 807\n",
      "Gen loss :-0.01855110190808773\n",
      "Stud loss :0.071211788803339\n",
      "Batch 808\n",
      "Gen loss :-0.017602501437067986\n",
      "Stud loss :0.06806831322610378\n",
      "Batch 809\n",
      "Gen loss :-0.020654022693634033\n",
      "Stud loss :0.07324211075901985\n",
      "Batch 810\n",
      "Gen loss :-0.019993901252746582\n",
      "Stud loss :0.0729245264083147\n",
      "Batch 811\n",
      "Gen loss :-0.017159253358840942\n",
      "Stud loss :0.06555547453463077\n",
      "Batch 812\n",
      "Gen loss :-0.015202554874122143\n",
      "Stud loss :0.06675410345196724\n",
      "Batch 813\n",
      "Gen loss :-0.01459839940071106\n",
      "Stud loss :0.06317375302314758\n",
      "Batch 814\n",
      "Gen loss :-0.01873687468469143\n",
      "Stud loss :0.07068980820477008\n",
      "Batch 815\n",
      "Gen loss :-0.017017066478729248\n",
      "Stud loss :0.06425307430326939\n",
      "Batch 816\n",
      "Gen loss :-0.016290126368403435\n",
      "Stud loss :0.06336119547486305\n",
      "Batch 817\n",
      "Gen loss :-0.017070939764380455\n",
      "Stud loss :0.06523051857948303\n",
      "Batch 818\n",
      "Gen loss :-0.019133051857352257\n",
      "Stud loss :0.06798976249992847\n",
      "Batch 819\n",
      "Gen loss :-0.01525898277759552\n",
      "Stud loss :0.06673436090350152\n",
      "Batch 820\n",
      "Gen loss :-0.018543045967817307\n",
      "Stud loss :0.06753927804529666\n",
      "Student net test:\n",
      "\t Test loss: \t 0.016400, \t Test accuracy \t 43.21\n",
      "Batch 821\n",
      "Gen loss :-0.017585456371307373\n",
      "Stud loss :0.06726573705673218\n",
      "Batch 822\n",
      "Gen loss :-0.015125095844268799\n",
      "Stud loss :0.0640260972082615\n",
      "Batch 823\n",
      "Gen loss :-0.01777462475001812\n",
      "Stud loss :0.06875947117805481\n",
      "Batch 824\n",
      "Gen loss :-0.01704452745616436\n",
      "Stud loss :0.06449767276644706\n",
      "Batch 825\n",
      "Gen loss :-0.014969280920922756\n",
      "Stud loss :0.0618512574583292\n",
      "Batch 826\n",
      "Gen loss :-0.014945946633815765\n",
      "Stud loss :0.06485261470079422\n",
      "Batch 827\n",
      "Gen loss :-0.01489375252276659\n",
      "Stud loss :0.06750208847224712\n",
      "Batch 828\n",
      "Gen loss :-0.015720898285508156\n",
      "Stud loss :0.06669184863567353\n",
      "Batch 829\n",
      "Gen loss :-0.015691401436924934\n",
      "Stud loss :0.06530353538691998\n",
      "Batch 830\n",
      "Gen loss :-0.016093140468001366\n",
      "Stud loss :0.06424171961843968\n",
      "Batch 831\n",
      "Gen loss :-0.01641143672168255\n",
      "Stud loss :0.06507254391908646\n",
      "Batch 832\n",
      "Gen loss :-0.018011106178164482\n",
      "Stud loss :0.06744279079139233\n",
      "Batch 833\n",
      "Gen loss :-0.017772242426872253\n",
      "Stud loss :0.06943338960409165\n",
      "Batch 834\n",
      "Gen loss :-0.014968812465667725\n",
      "Stud loss :0.06499150544404983\n",
      "Batch 835\n",
      "Gen loss :-0.017731742933392525\n",
      "Stud loss :0.06480111144483089\n",
      "Batch 836\n",
      "Gen loss :-0.018566271290183067\n",
      "Stud loss :0.07351640276610852\n",
      "Batch 837\n",
      "Gen loss :-0.018089167773723602\n",
      "Stud loss :0.07172874957323075\n",
      "Batch 838\n",
      "Gen loss :-0.016032975167036057\n",
      "Stud loss :0.06904901340603828\n",
      "Batch 839\n",
      "Gen loss :-0.01780679263174534\n",
      "Stud loss :0.07050183713436127\n",
      "Batch 840\n",
      "Gen loss :-0.01900537870824337\n",
      "Stud loss :0.07106286585330963\n",
      "Student net test:\n",
      "\t Test loss: \t 0.011337, \t Test accuracy \t 53.16\n",
      "Batch 841\n",
      "Gen loss :-0.01676507294178009\n",
      "Stud loss :0.06783108711242676\n",
      "Batch 842\n",
      "Gen loss :-0.016675090417265892\n",
      "Stud loss :0.06521149389445782\n",
      "Batch 843\n",
      "Gen loss :-0.01744118705391884\n",
      "Stud loss :0.07123008519411086\n",
      "Batch 844\n",
      "Gen loss :-0.017695609480142593\n",
      "Stud loss :0.06980110704898834\n",
      "Batch 845\n",
      "Gen loss :-0.016685158014297485\n",
      "Stud loss :0.06988451331853866\n",
      "Batch 846\n",
      "Gen loss :-0.019220218062400818\n",
      "Stud loss :0.06886895149946212\n",
      "Batch 847\n",
      "Gen loss :-0.019593749195337296\n",
      "Stud loss :0.07000111155211926\n",
      "Batch 848\n",
      "Gen loss :-0.020831331610679626\n",
      "Stud loss :0.07340090908110142\n",
      "Batch 849\n",
      "Gen loss :-0.021231066435575485\n",
      "Stud loss :0.07148486599326134\n",
      "Batch 850\n",
      "Gen loss :-0.017861483618617058\n",
      "Stud loss :0.06977131366729736\n",
      "Batch 851\n",
      "Gen loss :-0.016590312123298645\n",
      "Stud loss :0.07265625298023223\n",
      "Batch 852\n",
      "Gen loss :-0.021213596686720848\n",
      "Stud loss :0.0717013705521822\n",
      "Batch 853\n",
      "Gen loss :-0.019716082140803337\n",
      "Stud loss :0.07319932170212269\n",
      "Batch 854\n",
      "Gen loss :-0.021039465442299843\n",
      "Stud loss :0.07216353714466095\n",
      "Batch 855\n",
      "Gen loss :-0.018099650740623474\n",
      "Stud loss :0.07137926928699016\n",
      "Batch 856\n",
      "Gen loss :-0.019515028223395348\n",
      "Stud loss :0.07107462584972382\n",
      "Batch 857\n",
      "Gen loss :-0.020381320267915726\n",
      "Stud loss :0.07457950115203857\n",
      "Batch 858\n",
      "Gen loss :-0.019475629553198814\n",
      "Stud loss :0.0720557615160942\n",
      "Batch 859\n",
      "Gen loss :-0.018524741753935814\n",
      "Stud loss :0.06946568638086319\n",
      "Batch 860\n",
      "Gen loss :-0.016939152032136917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stud loss :0.07013584598898888\n",
      "Student net test:\n",
      "\t Test loss: \t 0.013782, \t Test accuracy \t 49.23\n",
      "Batch 861\n",
      "Gen loss :-0.018406635150313377\n",
      "Stud loss :0.06980278566479683\n",
      "Batch 862\n",
      "Gen loss :-0.017843617126345634\n",
      "Stud loss :0.0703457809984684\n",
      "Batch 863\n",
      "Gen loss :-0.019205769523978233\n",
      "Stud loss :0.06897399611771107\n",
      "Batch 864\n",
      "Gen loss :-0.019426319748163223\n",
      "Stud loss :0.07875583320856094\n",
      "Batch 865\n",
      "Gen loss :-0.01612558774650097\n",
      "Stud loss :0.07071887515485287\n",
      "Batch 866\n",
      "Gen loss :-0.021018078550696373\n",
      "Stud loss :0.07007804363965989\n",
      "Batch 867\n",
      "Gen loss :-0.017764829099178314\n",
      "Stud loss :0.06875250935554504\n",
      "Batch 868\n",
      "Gen loss :-0.020840153098106384\n",
      "Stud loss :0.07320020087063313\n",
      "Batch 869\n",
      "Gen loss :-0.019828563556075096\n",
      "Stud loss :0.07702579647302628\n",
      "Batch 870\n",
      "Gen loss :-0.015815867111086845\n",
      "Stud loss :0.06776966378092766\n",
      "Batch 871\n",
      "Gen loss :-0.02031274512410164\n",
      "Stud loss :0.07147916778922081\n",
      "Batch 872\n",
      "Gen loss :-0.01810913160443306\n",
      "Stud loss :0.06772235073149205\n",
      "Batch 873\n",
      "Gen loss :-0.016147097572684288\n",
      "Stud loss :0.06831755228340626\n",
      "Batch 874\n",
      "Gen loss :-0.014702481217682362\n",
      "Stud loss :0.0675167616456747\n",
      "Batch 875\n",
      "Gen loss :-0.01782400906085968\n",
      "Stud loss :0.0658054631203413\n",
      "Batch 876\n",
      "Gen loss :-0.019456425681710243\n",
      "Stud loss :0.07438714392483234\n",
      "Batch 877\n",
      "Gen loss :-0.016938330605626106\n",
      "Stud loss :0.07090231999754906\n",
      "Batch 878\n",
      "Gen loss :-0.016414178535342216\n",
      "Stud loss :0.07103226147592068\n",
      "Batch 879\n",
      "Gen loss :-0.01849086955189705\n",
      "Stud loss :0.07512664161622525\n",
      "Batch 880\n",
      "Gen loss :-0.01936967484652996\n",
      "Stud loss :0.07307613901793956\n",
      "Student net test:\n",
      "\t Test loss: \t 0.013054, \t Test accuracy \t 51.70\n",
      "Batch 881\n",
      "Gen loss :-0.01910959556698799\n",
      "Stud loss :0.07134537398815155\n",
      "Batch 882\n",
      "Gen loss :-0.015952162444591522\n",
      "Stud loss :0.0708320140838623\n",
      "Batch 883\n",
      "Gen loss :-0.019027534872293472\n",
      "Stud loss :0.07389021143317223\n",
      "Batch 884\n",
      "Gen loss :-0.017811402678489685\n",
      "Stud loss :0.07555501461029053\n",
      "Batch 885\n",
      "Gen loss :-0.016398150473833084\n",
      "Stud loss :0.06831379309296608\n",
      "Batch 886\n",
      "Gen loss :-0.016484994441270828\n",
      "Stud loss :0.07388400249183177\n",
      "Batch 887\n",
      "Gen loss :-0.018712257966399193\n",
      "Stud loss :0.07294933758676052\n",
      "Batch 888\n",
      "Gen loss :-0.019032364711165428\n",
      "Stud loss :0.07695778012275696\n",
      "Batch 889\n",
      "Gen loss :-0.021254662424325943\n",
      "Stud loss :0.07595370225608349\n",
      "Batch 890\n",
      "Gen loss :-0.021091623231768608\n",
      "Stud loss :0.07668815478682518\n",
      "Batch 891\n",
      "Gen loss :-0.017572825774550438\n",
      "Stud loss :0.07157289236783981\n",
      "Batch 892\n",
      "Gen loss :-0.01843413896858692\n",
      "Stud loss :0.07254759259521962\n",
      "Batch 893\n",
      "Gen loss :-0.016331294551491737\n",
      "Stud loss :0.06977002769708633\n",
      "Batch 894\n",
      "Gen loss :-0.020376747474074364\n",
      "Stud loss :0.07641492001712322\n",
      "Batch 895\n",
      "Gen loss :-0.01605117879807949\n",
      "Stud loss :0.07239204309880734\n",
      "Batch 896\n",
      "Gen loss :-0.019466927275061607\n",
      "Stud loss :0.0768489245325327\n",
      "Batch 897\n",
      "Gen loss :-0.021577725186944008\n",
      "Stud loss :0.07986675836145878\n",
      "Batch 898\n",
      "Gen loss :-0.019258780404925346\n",
      "Stud loss :0.07554022409021854\n",
      "Batch 899\n",
      "Gen loss :-0.01825379952788353\n",
      "Stud loss :0.07119756527245044\n",
      "Batch 900\n",
      "Gen loss :-0.017306817695498466\n",
      "Stud loss :0.07241881377995014\n",
      "Student net test:\n",
      "\t Test loss: \t 0.012105, \t Test accuracy \t 52.79\n",
      "Batch 901\n",
      "Gen loss :-0.018392134457826614\n",
      "Stud loss :0.07190857864916325\n",
      "Batch 902\n",
      "Gen loss :-0.017631588503718376\n",
      "Stud loss :0.07085709571838379\n",
      "Batch 903\n",
      "Gen loss :-0.016262207180261612\n",
      "Stud loss :0.06999483853578567\n",
      "Batch 904\n",
      "Gen loss :-0.018596088513731956\n",
      "Stud loss :0.0728805273771286\n",
      "Batch 905\n",
      "Gen loss :-0.017639975994825363\n",
      "Stud loss :0.06960832439363003\n",
      "Batch 906\n",
      "Gen loss :-0.015708355233073235\n",
      "Stud loss :0.06917092055082322\n",
      "Batch 907\n",
      "Gen loss :-0.01474742591381073\n",
      "Stud loss :0.06518843360245227\n",
      "Batch 908\n",
      "Gen loss :-0.015644580125808716\n",
      "Stud loss :0.06962445676326752\n",
      "Batch 909\n",
      "Gen loss :-0.017457516863942146\n",
      "Stud loss :0.06839422658085823\n",
      "Batch 910\n",
      "Gen loss :-0.021320706233382225\n",
      "Stud loss :0.07177431918680668\n",
      "Batch 911\n",
      "Gen loss :-0.016018090769648552\n",
      "Stud loss :0.06698524504899979\n",
      "Batch 912\n",
      "Gen loss :-0.02037992514669895\n",
      "Stud loss :0.07252814620733261\n",
      "Batch 913\n",
      "Gen loss :-0.018013644963502884\n",
      "Stud loss :0.06901759468019009\n",
      "Batch 914\n",
      "Gen loss :-0.019989026710391045\n",
      "Stud loss :0.07441036365926265\n",
      "Batch 915\n",
      "Gen loss :-0.015627484768629074\n",
      "Stud loss :0.06999745443463326\n",
      "Batch 916\n",
      "Gen loss :-0.017053598538041115\n",
      "Stud loss :0.06860297359526157\n",
      "Batch 917\n",
      "Gen loss :-0.018141990527510643\n",
      "Stud loss :0.0717665445059538\n",
      "Batch 918\n",
      "Gen loss :-0.016492510214447975\n",
      "Stud loss :0.06884609796106815\n",
      "Batch 919\n",
      "Gen loss :-0.01921885460615158\n",
      "Stud loss :0.0687163732945919\n",
      "Batch 920\n",
      "Gen loss :-0.018275851383805275\n",
      "Stud loss :0.07415190115571021\n",
      "Student net test:\n",
      "\t Test loss: \t 0.011470, \t Test accuracy \t 55.37\n",
      "Batch 921\n",
      "Gen loss :-0.018379146233201027\n",
      "Stud loss :0.07320687845349312\n",
      "Batch 922\n",
      "Gen loss :-0.018031002953648567\n",
      "Stud loss :0.07163408063352109\n",
      "Batch 923\n",
      "Gen loss :-0.019161133095622063\n",
      "Stud loss :0.07534093074500561\n",
      "Batch 924\n",
      "Gen loss :-0.016299868002533913\n",
      "Stud loss :0.06969916187226773\n",
      "Batch 925\n",
      "Gen loss :-0.017479440197348595\n",
      "Stud loss :0.06728657558560372\n",
      "Batch 926\n",
      "Gen loss :-0.01741991750895977\n",
      "Stud loss :0.06784270443022251\n",
      "Batch 927\n",
      "Gen loss :-0.016506848856806755\n",
      "Stud loss :0.06682065911591054\n",
      "Batch 928\n",
      "Gen loss :-0.018193358555436134\n",
      "Stud loss :0.06833336427807808\n",
      "Batch 929\n",
      "Gen loss :-0.02001509815454483\n",
      "Stud loss :0.07286289371550084\n",
      "Batch 930\n",
      "Gen loss :-0.018176062032580376\n",
      "Stud loss :0.0691896378993988\n",
      "Batch 931\n",
      "Gen loss :-0.01831759698688984\n",
      "Stud loss :0.07235558517277241\n",
      "Batch 932\n",
      "Gen loss :-0.019511301070451736\n",
      "Stud loss :0.06992341466248035\n",
      "Batch 933\n",
      "Gen loss :-0.015501344576478004\n",
      "Stud loss :0.06694738939404488\n",
      "Batch 934\n",
      "Gen loss :-0.021693823859095573\n",
      "Stud loss :0.07619011066854\n",
      "Batch 935\n",
      "Gen loss :-0.020864296704530716\n",
      "Stud loss :0.07603000514209271\n",
      "Batch 936\n",
      "Gen loss :-0.016243012621998787\n",
      "Stud loss :0.07114133536815644\n",
      "Batch 937\n",
      "Gen loss :-0.0191484447568655\n",
      "Stud loss :0.06922891698777675\n",
      "Batch 938\n",
      "Gen loss :-0.016590502113103867\n",
      "Stud loss :0.06663752868771552\n",
      "Batch 939\n",
      "Gen loss :-0.018918681889772415\n",
      "Stud loss :0.07125195376574993\n",
      "Batch 940\n",
      "Gen loss :-0.016192665323615074\n",
      "Stud loss :0.06676212437450886\n",
      "Student net test:\n",
      "\t Test loss: \t 0.009928, \t Test accuracy \t 59.12\n",
      "Batch 941\n",
      "Gen loss :-0.018554141744971275\n",
      "Stud loss :0.06812070533633233\n",
      "Batch 942\n",
      "Gen loss :-0.02086293324828148\n",
      "Stud loss :0.07445040792226791\n",
      "Batch 943\n",
      "Gen loss :-0.0155295729637146\n",
      "Stud loss :0.06584420837461949\n",
      "Batch 944\n",
      "Gen loss :-0.01802554540336132\n",
      "Stud loss :0.06980616003274917\n",
      "Batch 945\n",
      "Gen loss :-0.018868379294872284\n",
      "Stud loss :0.07241275981068611\n",
      "Batch 946\n",
      "Gen loss :-0.020519059151411057\n",
      "Stud loss :0.07774228304624557\n",
      "Batch 947\n",
      "Gen loss :-0.017489036545157433\n",
      "Stud loss :0.07178287468850612\n",
      "Batch 948\n",
      "Gen loss :-0.01640377752482891\n",
      "Stud loss :0.06825723834335803\n",
      "Batch 949\n",
      "Gen loss :-0.0180511437356472\n",
      "Stud loss :0.06782132051885129\n",
      "Batch 950\n",
      "Gen loss :-0.016154084354639053\n",
      "Stud loss :0.06522456258535385\n",
      "Batch 951\n",
      "Gen loss :-0.01928611472249031\n",
      "Stud loss :0.07017860077321529\n",
      "Batch 952\n",
      "Gen loss :-0.015061519108712673\n",
      "Stud loss :0.06783333718776703\n",
      "Batch 953\n",
      "Gen loss :-0.01608765497803688\n",
      "Stud loss :0.06648003533482552\n",
      "Batch 954\n",
      "Gen loss :-0.020236456766724586\n",
      "Stud loss :0.06924222148954869\n",
      "Batch 955\n",
      "Gen loss :-0.018665267154574394\n",
      "Stud loss :0.07217022031545639\n",
      "Batch 956\n",
      "Gen loss :-0.015984196215867996\n",
      "Stud loss :0.06566714867949486\n",
      "Batch 957\n",
      "Gen loss :-0.01985696144402027\n",
      "Stud loss :0.06896532624959946\n",
      "Batch 958\n",
      "Gen loss :-0.01751919463276863\n",
      "Stud loss :0.072150444611907\n",
      "Batch 959\n",
      "Gen loss :-0.017670972272753716\n",
      "Stud loss :0.07124346606433392\n",
      "Batch 960\n",
      "Gen loss :-0.018810780718922615\n",
      "Stud loss :0.07176859229803086\n",
      "Student net test:\n",
      "\t Test loss: \t 0.010150, \t Test accuracy \t 58.46\n",
      "Batch 961\n",
      "Gen loss :-0.015152198262512684\n",
      "Stud loss :0.06500749066472053\n",
      "Batch 962\n",
      "Gen loss :-0.01777305267751217\n",
      "Stud loss :0.06848216950893402\n",
      "Batch 963\n",
      "Gen loss :-0.017917653545737267\n",
      "Stud loss :0.07041091471910477\n",
      "Batch 964\n",
      "Gen loss :-0.014706186018884182\n",
      "Stud loss :0.06713451854884625\n",
      "Batch 965\n",
      "Gen loss :-0.019332924857735634\n",
      "Stud loss :0.07746264338493347\n",
      "Batch 966\n",
      "Gen loss :-0.019249005243182182\n",
      "Stud loss :0.07228513918817044\n",
      "Batch 967\n",
      "Gen loss :-0.019359266385436058\n",
      "Stud loss :0.07038208097219467\n",
      "Batch 968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.01650509051978588\n",
      "Stud loss :0.06954846940934659\n",
      "Batch 969\n",
      "Gen loss :-0.018038397654891014\n",
      "Stud loss :0.07085665874183178\n",
      "Batch 970\n",
      "Gen loss :-0.016478247940540314\n",
      "Stud loss :0.06904644779860973\n",
      "Batch 971\n",
      "Gen loss :-0.014911608770489693\n",
      "Stud loss :0.07061453759670258\n",
      "Batch 972\n",
      "Gen loss :-0.016346966847777367\n",
      "Stud loss :0.06957094185054302\n",
      "Batch 973\n",
      "Gen loss :-0.01713079772889614\n",
      "Stud loss :0.07271518111228943\n",
      "Batch 974\n",
      "Gen loss :-0.01659664325416088\n",
      "Stud loss :0.07263014018535614\n",
      "Batch 975\n",
      "Gen loss :-0.018957043066620827\n",
      "Stud loss :0.0738187287002802\n",
      "Batch 976\n",
      "Gen loss :-0.01701853610575199\n",
      "Stud loss :0.06991328671574593\n",
      "Batch 977\n",
      "Gen loss :-0.01804967410862446\n",
      "Stud loss :0.07184542641043663\n",
      "Batch 978\n",
      "Gen loss :-0.017708001658320427\n",
      "Stud loss :0.07271864041686057\n",
      "Batch 979\n",
      "Gen loss :-0.015784909948706627\n",
      "Stud loss :0.06935142688453197\n",
      "Batch 980\n",
      "Gen loss :-0.01815096102654934\n",
      "Stud loss :0.0696093924343586\n",
      "Student net test:\n",
      "\t Test loss: \t 0.009494, \t Test accuracy \t 60.22\n",
      "Batch 981\n",
      "Gen loss :-0.016245562583208084\n",
      "Stud loss :0.07020554021000862\n",
      "Batch 982\n",
      "Gen loss :-0.017354387789964676\n",
      "Stud loss :0.07121088840067387\n",
      "Batch 983\n",
      "Gen loss :-0.016399985179305077\n",
      "Stud loss :0.06783087588846684\n",
      "Batch 984\n",
      "Gen loss :-0.017772028222680092\n",
      "Stud loss :0.07349932454526424\n",
      "Batch 985\n",
      "Gen loss :-0.017191974446177483\n",
      "Stud loss :0.07056668475270271\n",
      "Batch 986\n",
      "Gen loss :-0.01711016520857811\n",
      "Stud loss :0.06974793374538421\n",
      "Batch 987\n",
      "Gen loss :-0.019700108096003532\n",
      "Stud loss :0.07370002456009388\n",
      "Batch 988\n",
      "Gen loss :-0.021134091541171074\n",
      "Stud loss :0.07471351772546768\n",
      "Batch 989\n",
      "Gen loss :-0.01789291389286518\n",
      "Stud loss :0.0700150340795517\n",
      "Batch 990\n",
      "Gen loss :-0.020133299753069878\n",
      "Stud loss :0.07139981128275394\n",
      "Batch 991\n",
      "Gen loss :-0.01903376542031765\n",
      "Stud loss :0.07336018495261669\n",
      "Batch 992\n",
      "Gen loss :-0.018991967663168907\n",
      "Stud loss :0.07367888577282429\n",
      "Batch 993\n",
      "Gen loss :-0.0185854434967041\n",
      "Stud loss :0.07245832234621048\n",
      "Batch 994\n",
      "Gen loss :-0.02040136232972145\n",
      "Stud loss :0.07752343602478504\n",
      "Batch 995\n",
      "Gen loss :-0.01569300889968872\n",
      "Stud loss :0.06713508181273938\n",
      "Batch 996\n",
      "Gen loss :-0.017920104786753654\n",
      "Stud loss :0.06867075264453888\n",
      "Batch 997\n",
      "Gen loss :-0.020206069573760033\n",
      "Stud loss :0.07337073162198067\n",
      "Batch 998\n",
      "Gen loss :-0.01850743591785431\n",
      "Stud loss :0.07175292670726777\n",
      "Batch 999\n",
      "Gen loss :-0.016588004305958748\n",
      "Stud loss :0.0696532279253006\n",
      "Batch 1000\n",
      "Gen loss :-0.01778365485370159\n",
      "Stud loss :0.06979128085076809\n",
      "Student net test:\n",
      "\t Test loss: \t 0.008603, \t Test accuracy \t 62.34\n",
      "Batch 1001\n",
      "Gen loss :-0.01965704932808876\n",
      "Stud loss :0.07317994609475136\n",
      "Batch 1002\n",
      "Gen loss :-0.018166236579418182\n",
      "Stud loss :0.06963727548718453\n",
      "Batch 1003\n",
      "Gen loss :-0.01715383119881153\n",
      "Stud loss :0.07033430896699429\n",
      "Batch 1004\n",
      "Gen loss :-0.019776849076151848\n",
      "Stud loss :0.0721916452050209\n",
      "Batch 1005\n",
      "Gen loss :-0.018635017797350883\n",
      "Stud loss :0.07159781977534294\n",
      "Batch 1006\n",
      "Gen loss :-0.018952254205942154\n",
      "Stud loss :0.07216322273015977\n",
      "Batch 1007\n",
      "Gen loss :-0.019881660118699074\n",
      "Stud loss :0.07142258509993553\n",
      "Batch 1008\n",
      "Gen loss :-0.01696644350886345\n",
      "Stud loss :0.07064423486590385\n",
      "Batch 1009\n",
      "Gen loss :-0.016995443031191826\n",
      "Stud loss :0.07325673773884774\n",
      "Batch 1010\n",
      "Gen loss :-0.02111005410552025\n",
      "Stud loss :0.07605783455073833\n",
      "Batch 1011\n",
      "Gen loss :-0.017524665221571922\n",
      "Stud loss :0.07039819210767746\n",
      "Batch 1012\n",
      "Gen loss :-0.017659178003668785\n",
      "Stud loss :0.06863327994942665\n",
      "Batch 1013\n",
      "Gen loss :-0.017271241173148155\n",
      "Stud loss :0.0709676906466484\n",
      "Batch 1014\n",
      "Gen loss :-0.016074949875473976\n",
      "Stud loss :0.07183438017964364\n",
      "Batch 1015\n",
      "Gen loss :-0.018517466261982918\n",
      "Stud loss :0.07041234932839871\n",
      "Batch 1016\n",
      "Gen loss :-0.019274413585662842\n",
      "Stud loss :0.07359204813838005\n",
      "Batch 1017\n",
      "Gen loss :-0.018917111679911613\n",
      "Stud loss :0.07607600837945938\n",
      "Batch 1018\n",
      "Gen loss :-0.014941366389393806\n",
      "Stud loss :0.06985832415521145\n",
      "Batch 1019\n",
      "Gen loss :-0.017130723223090172\n",
      "Stud loss :0.06952114664018154\n",
      "Batch 1020\n",
      "Gen loss :-0.021474970504641533\n",
      "Stud loss :0.07631287723779678\n",
      "Student net test:\n",
      "\t Test loss: \t 0.008741, \t Test accuracy \t 62.42\n",
      "Batch 1021\n",
      "Gen loss :-0.020829087123274803\n",
      "Stud loss :0.07400687560439109\n",
      "Batch 1022\n",
      "Gen loss :-0.013255104422569275\n",
      "Stud loss :0.06727951020002365\n",
      "Batch 1023\n",
      "Gen loss :-0.02055511437356472\n",
      "Stud loss :0.0724663097411394\n",
      "Batch 1024\n",
      "Gen loss :-0.01783830299973488\n",
      "Stud loss :0.07459353730082512\n",
      "Batch 1025\n",
      "Gen loss :-0.020749248564243317\n",
      "Stud loss :0.07340865209698677\n",
      "Batch 1026\n",
      "Gen loss :-0.017244460061192513\n",
      "Stud loss :0.06986366957426071\n",
      "Batch 1027\n",
      "Gen loss :-0.017514759674668312\n",
      "Stud loss :0.0716545294970274\n",
      "Batch 1028\n",
      "Gen loss :-0.018368227407336235\n",
      "Stud loss :0.07344735488295555\n",
      "Batch 1029\n",
      "Gen loss :-0.01768418587744236\n",
      "Stud loss :0.07202863954007625\n",
      "Batch 1030\n",
      "Gen loss :-0.018211189657449722\n",
      "Stud loss :0.0706394400447607\n",
      "Batch 1031\n",
      "Gen loss :-0.017784669995307922\n",
      "Stud loss :0.07198152244091034\n",
      "Batch 1032\n",
      "Gen loss :-0.017295608296990395\n",
      "Stud loss :0.0704121332615614\n",
      "Batch 1033\n",
      "Gen loss :-0.017879504710435867\n",
      "Stud loss :0.07049490809440613\n",
      "Batch 1034\n",
      "Gen loss :-0.018261536955833435\n",
      "Stud loss :0.07430210784077644\n",
      "Batch 1035\n",
      "Gen loss :-0.019758695736527443\n",
      "Stud loss :0.07770398855209351\n",
      "Batch 1036\n",
      "Gen loss :-0.019121600314974785\n",
      "Stud loss :0.07939397916197777\n",
      "Batch 1037\n",
      "Gen loss :-0.01870286464691162\n",
      "Stud loss :0.07663092948496342\n",
      "Batch 1038\n",
      "Gen loss :-0.01797953061759472\n",
      "Stud loss :0.07287807650864124\n",
      "Batch 1039\n",
      "Gen loss :-0.01642725244164467\n",
      "Stud loss :0.07385785430669785\n",
      "Batch 1040\n",
      "Gen loss :-0.01586397923529148\n",
      "Stud loss :0.07036854103207588\n",
      "Student net test:\n",
      "\t Test loss: \t 0.009415, \t Test accuracy \t 61.79\n",
      "Batch 1041\n",
      "Gen loss :-0.02006104402244091\n",
      "Stud loss :0.07696892395615577\n",
      "Batch 1042\n",
      "Gen loss :-0.01723305694758892\n",
      "Stud loss :0.07566579058766365\n",
      "Batch 1043\n",
      "Gen loss :-0.018376437947154045\n",
      "Stud loss :0.07219033688306808\n",
      "Batch 1044\n",
      "Gen loss :-0.015632083639502525\n",
      "Stud loss :0.07271697595715523\n",
      "Batch 1045\n",
      "Gen loss :-0.02367009036242962\n",
      "Stud loss :0.08267109468579292\n",
      "Batch 1046\n",
      "Gen loss :-0.01960502564907074\n",
      "Stud loss :0.08235342726111412\n",
      "Batch 1047\n",
      "Gen loss :-0.019531680271029472\n",
      "Stud loss :0.07776690945029259\n",
      "Batch 1048\n",
      "Gen loss :-0.019805723801255226\n",
      "Stud loss :0.07584923692047596\n",
      "Batch 1049\n",
      "Gen loss :-0.016859376803040504\n",
      "Stud loss :0.06937647946178913\n",
      "Batch 1050\n",
      "Gen loss :-0.01547999121248722\n",
      "Stud loss :0.07106435559689998\n",
      "Batch 1051\n",
      "Gen loss :-0.01933363638818264\n",
      "Stud loss :0.0742437232285738\n",
      "Batch 1052\n",
      "Gen loss :-0.018008684739470482\n",
      "Stud loss :0.07689841985702514\n",
      "Batch 1053\n",
      "Gen loss :-0.01825624518096447\n",
      "Stud loss :0.07418661937117577\n",
      "Batch 1054\n",
      "Gen loss :-0.01659676991403103\n",
      "Stud loss :0.07226910293102265\n",
      "Batch 1055\n",
      "Gen loss :-0.016780022531747818\n",
      "Stud loss :0.07455971948802471\n",
      "Batch 1056\n",
      "Gen loss :-0.017007531598210335\n",
      "Stud loss :0.0734888806939125\n",
      "Batch 1057\n",
      "Gen loss :-0.017235245555639267\n",
      "Stud loss :0.07326499335467815\n",
      "Batch 1058\n",
      "Gen loss :-0.01937132142484188\n",
      "Stud loss :0.07528856843709945\n",
      "Batch 1059\n",
      "Gen loss :-0.018987122923135757\n",
      "Stud loss :0.07645145431160927\n",
      "Batch 1060\n",
      "Gen loss :-0.018898213282227516\n",
      "Stud loss :0.07677463665604592\n",
      "Student net test:\n",
      "\t Test loss: \t 0.010470, \t Test accuracy \t 57.79\n",
      "Batch 1061\n",
      "Gen loss :-0.017887352034449577\n",
      "Stud loss :0.07474972642958164\n",
      "Batch 1062\n",
      "Gen loss :-0.01808210089802742\n",
      "Stud loss :0.07562381140887738\n",
      "Batch 1063\n",
      "Gen loss :-0.018416820093989372\n",
      "Stud loss :0.0743320494890213\n",
      "Batch 1064\n",
      "Gen loss :-0.0178089402616024\n",
      "Stud loss :0.07353044264018535\n",
      "Batch 1065\n",
      "Gen loss :-0.016269324347376823\n",
      "Stud loss :0.07003877311944962\n",
      "Batch 1066\n",
      "Gen loss :-0.014765850268304348\n",
      "Stud loss :0.07289046198129653\n",
      "Batch 1067\n",
      "Gen loss :-0.020036442205309868\n",
      "Stud loss :0.07198237106204033\n",
      "Batch 1068\n",
      "Gen loss :-0.020109456032514572\n",
      "Stud loss :0.08000482171773911\n",
      "Batch 1069\n",
      "Gen loss :-0.016426030546426773\n",
      "Stud loss :0.07450389564037323\n",
      "Batch 1070\n",
      "Gen loss :-0.016018498688936234\n",
      "Stud loss :0.07167876288294792\n",
      "Batch 1071\n",
      "Gen loss :-0.016943564638495445\n",
      "Stud loss :0.07126353718340397\n",
      "Batch 1072\n",
      "Gen loss :-0.01761607639491558\n",
      "Stud loss :0.07006854414939881\n",
      "Batch 1073\n",
      "Gen loss :-0.020205417647957802\n",
      "Stud loss :0.07525472342967987\n",
      "Batch 1074\n",
      "Gen loss :-0.01746002957224846\n",
      "Stud loss :0.07336110509932041\n",
      "Batch 1075\n",
      "Gen loss :-0.02092883177101612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stud loss :0.0750728689134121\n",
      "Batch 1076\n",
      "Gen loss :-0.014989945106208324\n",
      "Stud loss :0.07316587269306182\n",
      "Batch 1077\n",
      "Gen loss :-0.016561217606067657\n",
      "Stud loss :0.071696712449193\n",
      "Batch 1078\n",
      "Gen loss :-0.016859715804457664\n",
      "Stud loss :0.06959822438657284\n",
      "Batch 1079\n",
      "Gen loss :-0.018478747457265854\n",
      "Stud loss :0.07356437109410763\n",
      "Batch 1080\n",
      "Gen loss :-0.01756061054766178\n",
      "Stud loss :0.07138077579438687\n",
      "Student net test:\n",
      "\t Test loss: \t 0.008893, \t Test accuracy \t 62.17\n",
      "Batch 1081\n",
      "Gen loss :-0.020658573135733604\n",
      "Stud loss :0.07341547906398774\n",
      "Batch 1082\n",
      "Gen loss :-0.01764518767595291\n",
      "Stud loss :0.07252166792750359\n",
      "Batch 1083\n",
      "Gen loss :-0.017481118440628052\n",
      "Stud loss :0.07111995816230773\n",
      "Batch 1084\n",
      "Gen loss :-0.01766776107251644\n",
      "Stud loss :0.0728283528238535\n",
      "Batch 1085\n",
      "Gen loss :-0.016149157658219337\n",
      "Stud loss :0.06911525838077068\n",
      "Batch 1086\n",
      "Gen loss :-0.01830573007464409\n",
      "Stud loss :0.07532687000930309\n",
      "Batch 1087\n",
      "Gen loss :-0.01817811094224453\n",
      "Stud loss :0.07491683848202228\n",
      "Batch 1088\n",
      "Gen loss :-0.019782651215791702\n",
      "Stud loss :0.07572738714516163\n",
      "Batch 1089\n",
      "Gen loss :-0.016353214159607887\n",
      "Stud loss :0.07067391388118267\n",
      "Batch 1090\n",
      "Gen loss :-0.01929885521531105\n",
      "Stud loss :0.07465134039521218\n",
      "Batch 1091\n",
      "Gen loss :-0.016430621966719627\n",
      "Stud loss :0.07126456573605537\n",
      "Batch 1092\n",
      "Gen loss :-0.020146597176790237\n",
      "Stud loss :0.07554080486297607\n",
      "Batch 1093\n",
      "Gen loss :-0.015345430932939053\n",
      "Stud loss :0.07178177386522293\n",
      "Batch 1094\n",
      "Gen loss :-0.020921099931001663\n",
      "Stud loss :0.07616805881261826\n",
      "Batch 1095\n",
      "Gen loss :-0.017471764236688614\n",
      "Stud loss :0.07379696667194366\n",
      "Batch 1096\n",
      "Gen loss :-0.01784479431807995\n",
      "Stud loss :0.07409272603690624\n",
      "Batch 1097\n",
      "Gen loss :-0.017272094264626503\n",
      "Stud loss :0.07299798615276813\n",
      "Batch 1098\n",
      "Gen loss :-0.021611129865050316\n",
      "Stud loss :0.07984822094440461\n",
      "Batch 1099\n",
      "Gen loss :-0.0201684832572937\n",
      "Stud loss :0.07905838117003441\n",
      "Batch 1100\n",
      "Gen loss :-0.018390661105513573\n",
      "Stud loss :0.0745315995067358\n",
      "Student net test:\n",
      "\t Test loss: \t 0.010066, \t Test accuracy \t 59.29\n",
      "Batch 1101\n",
      "Gen loss :-0.0167098306119442\n",
      "Stud loss :0.06946219243109227\n",
      "Batch 1102\n",
      "Gen loss :-0.016016213223338127\n",
      "Stud loss :0.0737380962818861\n",
      "Batch 1103\n",
      "Gen loss :-0.015674753114581108\n",
      "Stud loss :0.06774938628077506\n",
      "Batch 1104\n",
      "Gen loss :-0.01824006997048855\n",
      "Stud loss :0.07548187300562859\n",
      "Batch 1105\n",
      "Gen loss :-0.01594436727464199\n",
      "Stud loss :0.06949262730777264\n",
      "Batch 1106\n",
      "Gen loss :-0.016485964879393578\n",
      "Stud loss :0.0714831780642271\n",
      "Batch 1107\n",
      "Gen loss :-0.017774565145373344\n",
      "Stud loss :0.07190891057252884\n",
      "Batch 1108\n",
      "Gen loss :-0.01785607635974884\n",
      "Stud loss :0.07408889122307301\n",
      "Batch 1109\n",
      "Gen loss :-0.01804858259856701\n",
      "Stud loss :0.07137497365474701\n",
      "Batch 1110\n",
      "Gen loss :-0.01832161657512188\n",
      "Stud loss :0.074067759886384\n",
      "Batch 1111\n",
      "Gen loss :-0.020105868577957153\n",
      "Stud loss :0.07711425945162773\n",
      "Batch 1112\n",
      "Gen loss :-0.01942180097103119\n",
      "Stud loss :0.07626464255154133\n",
      "Batch 1113\n",
      "Gen loss :-0.017710519954562187\n",
      "Stud loss :0.07390059344470501\n",
      "Batch 1114\n",
      "Gen loss :-0.01843908801674843\n",
      "Stud loss :0.07245604321360588\n",
      "Batch 1115\n",
      "Gen loss :-0.015861371532082558\n",
      "Stud loss :0.06833745390176774\n",
      "Batch 1116\n",
      "Gen loss :-0.01761665754020214\n",
      "Stud loss :0.06894835717976093\n",
      "Batch 1117\n",
      "Gen loss :-0.01703411340713501\n",
      "Stud loss :0.0699768990278244\n",
      "Batch 1118\n",
      "Gen loss :-0.01723637990653515\n",
      "Stud loss :0.07149186283349991\n",
      "Batch 1119\n",
      "Gen loss :-0.016799015924334526\n",
      "Stud loss :0.07023922614753246\n",
      "Batch 1120\n",
      "Gen loss :-0.020806947723031044\n",
      "Stud loss :0.07653197236359119\n",
      "Student net test:\n",
      "\t Test loss: \t 0.008614, \t Test accuracy \t 64.64\n",
      "Batch 1121\n",
      "Gen loss :-0.020346790552139282\n",
      "Stud loss :0.0760348878800869\n",
      "Batch 1122\n",
      "Gen loss :-0.021425379440188408\n",
      "Stud loss :0.08015419021248818\n",
      "Batch 1123\n",
      "Gen loss :-0.02244972251355648\n",
      "Stud loss :0.08006383925676346\n",
      "Batch 1124\n",
      "Gen loss :-0.018897533416748047\n",
      "Stud loss :0.0786289107054472\n",
      "Batch 1125\n",
      "Gen loss :-0.021134672686457634\n",
      "Stud loss :0.07504433840513229\n",
      "Batch 1126\n",
      "Gen loss :-0.02181149832904339\n",
      "Stud loss :0.07810691483318806\n",
      "Batch 1127\n",
      "Gen loss :-0.017552722245454788\n",
      "Stud loss :0.07460437528789043\n",
      "Batch 1128\n",
      "Gen loss :-0.017813613638281822\n",
      "Stud loss :0.0757847536355257\n",
      "Batch 1129\n",
      "Gen loss :-0.02027263678610325\n",
      "Stud loss :0.0756315965205431\n",
      "Batch 1130\n",
      "Gen loss :-0.01880098693072796\n",
      "Stud loss :0.07208359837532044\n",
      "Batch 1131\n",
      "Gen loss :-0.017493361607193947\n",
      "Stud loss :0.07194905877113342\n",
      "Batch 1132\n",
      "Gen loss :-0.022691115736961365\n",
      "Stud loss :0.0752636268734932\n",
      "Batch 1133\n",
      "Gen loss :-0.015312564559280872\n",
      "Stud loss :0.07440126091241836\n",
      "Batch 1134\n",
      "Gen loss :-0.017070060595870018\n",
      "Stud loss :0.06770277991890908\n",
      "Batch 1135\n",
      "Gen loss :-0.015902167186141014\n",
      "Stud loss :0.06752513237297535\n",
      "Batch 1136\n",
      "Gen loss :-0.02188716270029545\n",
      "Stud loss :0.0744027055799961\n",
      "Batch 1137\n",
      "Gen loss :-0.017921358346939087\n",
      "Stud loss :0.07283449508249759\n",
      "Batch 1138\n",
      "Gen loss :-0.01698688231408596\n",
      "Stud loss :0.0716333493590355\n",
      "Batch 1139\n",
      "Gen loss :-0.015265307389199734\n",
      "Stud loss :0.07111865021288395\n",
      "Batch 1140\n",
      "Gen loss :-0.014724037609994411\n",
      "Stud loss :0.07026682235300541\n",
      "Student net test:\n",
      "\t Test loss: \t 0.007666, \t Test accuracy \t 66.98\n",
      "Batch 1141\n",
      "Gen loss :-0.01770479790866375\n",
      "Stud loss :0.07184820026159286\n",
      "Batch 1142\n",
      "Gen loss :-0.018657181411981583\n",
      "Stud loss :0.07246618568897248\n",
      "Batch 1143\n",
      "Gen loss :-0.01807880587875843\n",
      "Stud loss :0.0724003154784441\n",
      "Batch 1144\n",
      "Gen loss :-0.01786099746823311\n",
      "Stud loss :0.07630042880773544\n",
      "Batch 1145\n",
      "Gen loss :-0.01703248731791973\n",
      "Stud loss :0.07397490814328193\n",
      "Batch 1146\n",
      "Gen loss :-0.01810179464519024\n",
      "Stud loss :0.07196603715419769\n",
      "Batch 1147\n",
      "Gen loss :-0.017710600048303604\n",
      "Stud loss :0.0723897635936737\n",
      "Batch 1148\n",
      "Gen loss :-0.01414628978818655\n",
      "Stud loss :0.06976030766963959\n",
      "Batch 1149\n",
      "Gen loss :-0.017742320895195007\n",
      "Stud loss :0.07148277238011361\n",
      "Batch 1150\n",
      "Gen loss :-0.02176884189248085\n",
      "Stud loss :0.07571491003036498\n",
      "Batch 1151\n",
      "Gen loss :-0.019824335351586342\n",
      "Stud loss :0.07603022158145904\n",
      "Batch 1152\n",
      "Gen loss :-0.017269626259803772\n",
      "Stud loss :0.07054688893258572\n",
      "Batch 1153\n",
      "Gen loss :-0.016355982050299644\n",
      "Stud loss :0.07021456211805344\n",
      "Batch 1154\n",
      "Gen loss :-0.019142428413033485\n",
      "Stud loss :0.0722583532333374\n",
      "Batch 1155\n",
      "Gen loss :-0.01858680509030819\n",
      "Stud loss :0.07123215571045875\n",
      "Batch 1156\n",
      "Gen loss :-0.015830909833312035\n",
      "Stud loss :0.06947420910000801\n",
      "Batch 1157\n",
      "Gen loss :-0.01658613048493862\n",
      "Stud loss :0.06799210533499718\n",
      "Batch 1158\n",
      "Gen loss :-0.017887085676193237\n",
      "Stud loss :0.074209925532341\n",
      "Batch 1159\n",
      "Gen loss :-0.01481444388628006\n",
      "Stud loss :0.06951445825397969\n",
      "Batch 1160\n",
      "Gen loss :-0.01646287739276886\n",
      "Stud loss :0.07744066379964351\n",
      "Student net test:\n",
      "\t Test loss: \t 0.009035, \t Test accuracy \t 62.11\n",
      "Batch 1161\n",
      "Gen loss :-0.01671547256410122\n",
      "Stud loss :0.06999896578490734\n",
      "Batch 1162\n",
      "Gen loss :-0.018521064892411232\n",
      "Stud loss :0.06958856098353863\n",
      "Batch 1163\n",
      "Gen loss :-0.018480554223060608\n",
      "Stud loss :0.06921507082879544\n",
      "Batch 1164\n",
      "Gen loss :-0.02043657936155796\n",
      "Stud loss :0.07440075427293777\n",
      "Batch 1165\n",
      "Gen loss :-0.01828180067241192\n",
      "Stud loss :0.0745601449161768\n",
      "Batch 1166\n",
      "Gen loss :-0.016747446730732918\n",
      "Stud loss :0.06871516816318035\n",
      "Batch 1167\n",
      "Gen loss :-0.014926674775779247\n",
      "Stud loss :0.06838201507925987\n",
      "Batch 1168\n",
      "Gen loss :-0.017045287415385246\n",
      "Stud loss :0.07151426337659358\n",
      "Batch 1169\n",
      "Gen loss :-0.024856695905327797\n",
      "Stud loss :0.07984506823122502\n",
      "Batch 1170\n",
      "Gen loss :-0.017651597037911415\n",
      "Stud loss :0.07178836315870285\n",
      "Batch 1171\n",
      "Gen loss :-0.02166377566754818\n",
      "Stud loss :0.07727530300617218\n",
      "Batch 1172\n",
      "Gen loss :-0.017045289278030396\n",
      "Stud loss :0.07242784164845943\n",
      "Batch 1173\n",
      "Gen loss :-0.019849780946969986\n",
      "Stud loss :0.07499970495700836\n",
      "Batch 1174\n",
      "Gen loss :-0.019283205270767212\n",
      "Stud loss :0.07271626368165016\n",
      "Batch 1175\n",
      "Gen loss :-0.018743468448519707\n",
      "Stud loss :0.07082328386604786\n",
      "Batch 1176\n",
      "Gen loss :-0.019062573090195656\n",
      "Stud loss :0.07342708595097065\n",
      "Batch 1177\n",
      "Gen loss :-0.020707232877612114\n",
      "Stud loss :0.07325715385377407\n",
      "Batch 1178\n",
      "Gen loss :-0.019863588735461235\n",
      "Stud loss :0.07593210451304913\n",
      "Batch 1179\n",
      "Gen loss :-0.021670779213309288\n",
      "Stud loss :0.07901573404669762\n",
      "Batch 1180\n",
      "Gen loss :-0.017902955412864685\n",
      "Stud loss :0.07618836015462875\n",
      "Student net test:\n",
      "\t Test loss: \t 0.010335, \t Test accuracy \t 59.55\n",
      "Batch 1181\n",
      "Gen loss :-0.019142139703035355\n",
      "Stud loss :0.0731622539460659\n",
      "Batch 1182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.01841883361339569\n",
      "Stud loss :0.0741082090884447\n",
      "Batch 1183\n",
      "Gen loss :-0.01892874203622341\n",
      "Stud loss :0.07557560727000237\n",
      "Batch 1184\n",
      "Gen loss :-0.01710687205195427\n",
      "Stud loss :0.07236929684877395\n",
      "Batch 1185\n",
      "Gen loss :-0.017530953511595726\n",
      "Stud loss :0.06939261928200721\n",
      "Batch 1186\n",
      "Gen loss :-0.019083520397543907\n",
      "Stud loss :0.06972380094230175\n",
      "Batch 1187\n",
      "Gen loss :-0.01826414093375206\n",
      "Stud loss :0.07175561077892781\n",
      "Batch 1188\n",
      "Gen loss :-0.019740935415029526\n",
      "Stud loss :0.07231424935162067\n",
      "Batch 1189\n",
      "Gen loss :-0.016104508191347122\n",
      "Stud loss :0.06593220196664333\n",
      "Batch 1190\n",
      "Gen loss :-0.01835625246167183\n",
      "Stud loss :0.07011389695107936\n",
      "Batch 1191\n",
      "Gen loss :-0.017530998215079308\n",
      "Stud loss :0.06835129708051682\n",
      "Batch 1192\n",
      "Gen loss :-0.017855441197752953\n",
      "Stud loss :0.07284406200051308\n",
      "Batch 1193\n",
      "Gen loss :-0.019434845075011253\n",
      "Stud loss :0.07032561711966992\n",
      "Batch 1194\n",
      "Gen loss :-0.015129977837204933\n",
      "Stud loss :0.07022701650857925\n",
      "Batch 1195\n",
      "Gen loss :-0.01703673042356968\n",
      "Stud loss :0.06653131656348706\n",
      "Batch 1196\n",
      "Gen loss :-0.01796942949295044\n",
      "Stud loss :0.07320241071283817\n",
      "Batch 1197\n",
      "Gen loss :-0.016484247520565987\n",
      "Stud loss :0.07045253440737724\n",
      "Batch 1198\n",
      "Gen loss :-0.016963964328169823\n",
      "Stud loss :0.07005798630416393\n",
      "Batch 1199\n",
      "Gen loss :-0.020165298134088516\n",
      "Stud loss :0.07365679070353508\n",
      "Batch 1200\n",
      "Gen loss :-0.016299566254019737\n",
      "Stud loss :0.06914872974157334\n",
      "Student net test:\n",
      "\t Test loss: \t 0.009093, \t Test accuracy \t 62.01\n",
      "Batch 1201\n",
      "Gen loss :-0.01760684885084629\n",
      "Stud loss :0.06797967478632927\n",
      "Batch 1202\n",
      "Gen loss :-0.01628524623811245\n",
      "Stud loss :0.07255061753094197\n",
      "Batch 1203\n",
      "Gen loss :-0.019872412085533142\n",
      "Stud loss :0.07142489366233348\n",
      "Batch 1204\n",
      "Gen loss :-0.018474366515874863\n",
      "Stud loss :0.07228334471583367\n",
      "Batch 1205\n",
      "Gen loss :-0.015585960820317268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-20beb48266ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_stud\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_input_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-20beb48266ce>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(n_batches, lr_gen, lr_stud, batch_size, test_batch_size, g_input_dim, ng, ns, test_freq, beta)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mstudent_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mstud_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mstudent_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from wideresnet import Wide_ResNet\n",
    "from generator import Generator\n",
    "from cifar10utils import getData, test\n",
    "\n",
    "'''\n",
    "generator loss:\n",
    "@output : logits of the student\n",
    "@output : logits of the teacher\n",
    "\n",
    "for the KL div as said here https://discuss.pytorch.org/t/kl-divergence-produces-negative-values/16791/4\n",
    "and here https://discuss.pytorch.org/t/kullback-leibler-divergence-loss-function-giving-negative-values/763/2\n",
    "the inputs should be logprobs for the output(student) and probabilities for the targets(teacher)\n",
    "\n",
    "this was very difficult to understand \n",
    "\n",
    "'''\n",
    "def attention(x):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/szagoruyko/attention-transfer\n",
    "    :param x = activations\n",
    "    \"\"\"\n",
    "    return F.normalize(x.pow(2).mean(1).view(x.size(0), -1))\n",
    "\n",
    "\n",
    "def attention_diff(x, y):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/szagoruyko/attention-transfer\n",
    "    :param x = activations\n",
    "    :param y = activations\n",
    "    \"\"\"\n",
    "    return (attention(x) - attention(y)).pow(2).mean()\n",
    "\n",
    "\n",
    "def divergence(student_logits, teacher_logits):\n",
    "    divergence = F.kl_div(F.log_softmax(student_logits, dim=1), F.softmax(teacher_logits, dim=1))\n",
    "\n",
    "    return divergence\n",
    "\n",
    "\n",
    "def KT_loss_generator(student_logits, teacher_logits):\n",
    "\n",
    "    divergence_loss = divergence(student_logits, teacher_logits)\n",
    "    total_loss = - divergence_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def KT_loss_student(student_logits, teacher_logits, student_activations, teacher_activations,beta):\n",
    "\n",
    "    divergence_loss = divergence(student_logits, teacher_logits)\n",
    "    if beta > 0:\n",
    "        at_loss = 0\n",
    "        for i in range(len(student_activations)):\n",
    "            at_loss = at_loss + beta * attention_diff(student_activations[i], teacher_activations[i])\n",
    "    else:\n",
    "        at_loss = 0\n",
    "\n",
    "    total_loss = divergence_loss + at_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "def main(n_batches,lr_gen,lr_stud,batch_size,test_batch_size,g_input_dim,ng,ns,test_freq,beta):\n",
    "    \n",
    "    device = 'cuda:0'\n",
    "    \n",
    "    # Get the data\n",
    "    train_loader, val_loader, test_loader = getData(batch_size,test_batch_size,0.1)\n",
    "    \n",
    "    #test_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    teacher = Wide_ResNet(16,2,0,10)\n",
    "    teacher = teacher.to(device)\n",
    "    teacher.load_state_dict(torch.load('./pretrained_models/cifar_net_test.pth'))\n",
    "    \n",
    "    generator = Generator(z_dim=g_input_dim)\n",
    "    generator = generator.to(device)\n",
    "    generator.train()\n",
    "    \n",
    "    student = Wide_ResNet(16,1,0,10)\n",
    "    student = student.to(device)\n",
    "    \n",
    "    generator_optim = torch.optim.Adam(generator.parameters(), lr=lr_gen)\n",
    "    gen_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(generator_optim, n_batches)\n",
    "    \n",
    "    student_optim = torch.optim.Adam(student.parameters(), lr=lr_stud)\n",
    "    stud_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(generator_optim, n_batches)\n",
    "    \n",
    "    print('Teacher net test:')\n",
    "    test_loss, test_accuracy = test(test_loader,teacher,device)\n",
    "    teacher.eval()\n",
    "    print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "    \n",
    "    print('Student net test:')\n",
    "    test_loss, test_accuracy = test(test_loader,student,device)\n",
    "    print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        print('Batch ' + str(i))\n",
    "        noise = torch.randn(batch_size,g_input_dim)\n",
    "        noise = noise.to(device)\n",
    "        \n",
    "        gen_loss_print = 0\n",
    "        \n",
    "        for j in range(ng):\n",
    "            gen_imgs = generator(noise)\n",
    "            gen_imgs = gen_imgs.to(device)\n",
    "\n",
    "            teacher_pred, *teacher_activations = teacher(gen_imgs)\n",
    "            student_pred, *student_activations = student(gen_imgs)\n",
    "\n",
    "            gen_loss = KT_loss_generator(student_pred,teacher_pred)\n",
    "            generator_optim.zero_grad()\n",
    "            gen_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), 5)\n",
    "\n",
    "            generator_optim.step()\n",
    "            \n",
    "            gen_loss_print += gen_loss.item()\n",
    "        \n",
    "        print('Gen loss :' + str(gen_loss_print/ng) )\n",
    "        \n",
    "        stud_loss_print = 0\n",
    "        for j in range(ns):\n",
    "            student.train()\n",
    "            gen_imgs = generator(noise)\n",
    "            teacher_pred, *teacher_activations = teacher(gen_imgs)\n",
    "            student_pred, *student_activations = student(gen_imgs)\n",
    "            \n",
    "            stud_loss = KT_loss_student(student_pred,teacher_pred, student_activations,teacher_activations, beta )\n",
    "            student_optim.zero_grad()\n",
    "            stud_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), 5)\n",
    "            student_optim.step()\n",
    "            \n",
    "            stud_loss_print += stud_loss.item()\n",
    "        \n",
    "        print('Stud loss :' + str(stud_loss_print/ns) )\n",
    "            \n",
    "        stud_scheduler.step()\n",
    "        gen_scheduler.step()\n",
    "        \n",
    "        if(i % test_freq) == 0:\n",
    "            print('Student net test:')\n",
    "            test_loss, test_accuracy = test(test_loader,student,device)\n",
    "            print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "    \n",
    "n_batches = 2001\n",
    "lr_gen = 1e-3\n",
    "lr_stud = 2e-3\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "g_input_dim = 100\n",
    "ng = 1\n",
    "ns = 10\n",
    "test_freq = 20\n",
    "beta = 250\n",
    "    \n",
    "main(n_batches,lr_gen,lr_stud,batch_size,test_batch_size,g_input_dim,ng,ns,test_freq,beta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
