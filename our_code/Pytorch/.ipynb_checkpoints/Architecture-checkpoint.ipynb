{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "| Wide-Resnet 16x2\n",
      "| Wide-Resnet 16x1\n",
      "Teacher net test:\n",
      "\t Test loss: \t 0.004356, \t Test accuracy \t 81.47\n",
      "Student net test:\n",
      "\t Test loss: \t 0.004356, \t Test accuracy \t 81.47\n",
      "Batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/ipykernel_launcher.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/ipykernel_launcher.py:93: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/ipykernel_launcher.py:102: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/ipykernel_launcher.py:103: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.08278881758451462\n",
      "Stud loss :0.09982872754335403\n",
      "Student net test:\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t Test loss: \t 0.011429, \t Test accuracy \t 59.36\n",
      "Batch 1\n",
      "Gen loss :-0.0761445164680481\n",
      "Stud loss :0.08729197159409523\n",
      "Batch 2\n",
      "Gen loss :-0.08127801865339279\n",
      "Stud loss :0.10628908537328244\n",
      "Batch 3\n",
      "Gen loss :-0.0946321040391922\n",
      "Stud loss :0.11840420700609684\n",
      "Batch 4\n",
      "Gen loss :-0.08500321209430695\n",
      "Stud loss :0.10360203720629216\n",
      "Batch 5\n",
      "Gen loss :-0.08993116766214371\n",
      "Stud loss :0.11170846745371818\n",
      "Student net test:\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t Test loss: \t 0.029878, \t Test accuracy \t 30.74\n",
      "Batch 6\n",
      "Gen loss :-0.08656004816293716\n",
      "Stud loss :0.10214017070829869\n",
      "Batch 7\n",
      "Gen loss :-0.09681618213653564\n",
      "Stud loss :0.11374375633895398\n",
      "Batch 8\n",
      "Gen loss :-0.08284517377614975\n",
      "Stud loss :0.09625374525785446\n",
      "Batch 9\n",
      "Gen loss :-0.08573966473340988\n",
      "Stud loss :0.09663736969232559\n",
      "Batch 10\n",
      "Gen loss :-0.08730758726596832\n",
      "Stud loss :0.09876527301967145\n",
      "Student net test:\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t Test loss: \t 0.030673, \t Test accuracy \t 30.75\n",
      "Batch 11\n",
      "Gen loss :-0.09408050030469894\n",
      "Stud loss :0.10975061096251011\n",
      "Batch 12\n",
      "Gen loss :-0.08762779831886292\n",
      "Stud loss :0.10079505071043968\n",
      "Batch 13\n",
      "Gen loss :-0.08337139338254929\n",
      "Stud loss :0.09976681433618069\n",
      "Batch 14\n",
      "Gen loss :-0.08194587379693985\n",
      "Stud loss :0.1072439856827259\n",
      "Batch 15\n",
      "Gen loss :-0.08708750456571579\n",
      "Stud loss :0.10084116309881211\n",
      "Student net test:\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t Test loss: \t 0.029213, \t Test accuracy \t 31.75\n",
      "Batch 16\n",
      "Gen loss :-0.0796966403722763\n",
      "Stud loss :0.09730449467897415\n",
      "Batch 17\n",
      "Gen loss :-0.07969474047422409\n",
      "Stud loss :0.10306428521871566\n",
      "Batch 18\n",
      "Gen loss :-0.0819842740893364\n",
      "Stud loss :0.09656880162656307\n",
      "Batch 19\n",
      "Gen loss :-0.0732567235827446\n",
      "Stud loss :0.09754685908555985\n",
      "Batch 20\n",
      "Gen loss :-0.09067791700363159\n",
      "Stud loss :0.10553731992840767\n",
      "Student net test:\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t Test loss: \t 0.029110, \t Test accuracy \t 31.11\n",
      "Batch 21\n",
      "Gen loss :-0.0808611735701561\n",
      "Stud loss :0.10065771788358688\n",
      "Batch 22\n",
      "Gen loss :-0.08393404632806778\n",
      "Stud loss :0.10055228099226951\n",
      "Batch 23\n",
      "Gen loss :-0.08193717151880264\n",
      "Stud loss :0.10029164627194405\n",
      "Batch 24\n",
      "Gen loss :-0.08556199818849564\n",
      "Stud loss :0.10289718806743622\n",
      "Batch 25\n",
      "Gen loss :-0.07903946191072464\n",
      "Stud loss :0.09831269085407257\n",
      "Student net test:\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t Test loss: \t 0.027128, \t Test accuracy \t 31.38\n",
      "Batch 26\n",
      "Gen loss :-0.07874725013971329\n",
      "Stud loss :0.10233402326703071\n",
      "Batch 27\n",
      "Gen loss :-0.0818881094455719\n",
      "Stud loss :0.10211210623383522\n",
      "Batch 28\n",
      "Gen loss :-0.08326048403978348\n",
      "Stud loss :0.1037970207631588\n",
      "Batch 29\n",
      "Gen loss :-0.08513625711202621\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc2d206d93ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mtest_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_stud\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_input_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-bc2d206d93ff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(n_batches, lr_gen, lr_stud, batch_size, test_batch_size, g_input_dim, ng, ns, test_freq)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mstud_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudentLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mteacher_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mstudent_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mstud_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mstudent_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from wideresnet import Wide_ResNet\n",
    "from generator import Generator\n",
    "\n",
    "'''\n",
    "Function that loads the dataset and returns the data-loaders\n",
    "'''\n",
    "def getData(batch_size,test_batch_size,val_percentage):\n",
    "    # Normalize the training set with data augmentation\n",
    "    transform_train = transforms.Compose([ \n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomRotation(20),\n",
    "        torchvision.transforms.ColorJitter(brightness=0.03, contrast=0.03, saturation=0.03, hue=0.03),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    # Normalize the test set same as training set without augmentation\n",
    "    transform_test = transforms.Compose([ \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Download/Load data\n",
    "    full_training_data = torchvision.datasets.CIFAR10('./data',train = True,transform=transform_train,download=True)  \n",
    "    test_data = torchvision.datasets.CIFAR10('./data',train = False,transform=transform_test,download=True)  \n",
    "\n",
    "    # Create train and validation splits\n",
    "    num_samples = len(full_training_data)\n",
    "    training_samples = int((1-val_percentage)*num_samples+1)\n",
    "    validation_samples = num_samples - training_samples\n",
    "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
    "\n",
    "    # Initialize dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(training_data,batch_size=batch_size,shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data,batch_size=batch_size,shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=test_batch_size,shuffle=False,drop_last=False,num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "'''\n",
    "Function to test that returns the loss per sample and the total accuracy\n",
    "'''\n",
    "def test(data_loader,net,cost_fun,device):\n",
    "    net.eval()\n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "    cumulative_accuracy = 0.\n",
    "    \n",
    "    loss_funct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (inputs,targets) in enumerate(data_loader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = net(inputs)[0]\n",
    "        \n",
    "        loss = loss_funct(outputs,targets)\n",
    "\n",
    "        # Metrics computation\n",
    "        samples+=inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
    "'''\n",
    "generator loss:\n",
    "@output : logits of the student\n",
    "@output : logits of the teacher\n",
    "\n",
    "for the KL div as said here https://discuss.pytorch.org/t/kl-divergence-produces-negative-values/16791/4\n",
    "and here https://discuss.pytorch.org/t/kullback-leibler-divergence-loss-function-giving-negative-values/763/2\n",
    "the inputs should be logprobs for the output(student) and probabilities for the targets(teacher)\n",
    "\n",
    "this was very difficult to undertand \n",
    "\n",
    "'''\n",
    "def genLoss(output, target):\n",
    "    student_pred = F.log_softmax(output)\n",
    "    teacher_pred = F.softmax(target)\n",
    "    \n",
    "    loss = F.kl_div(student_pred,teacher_pred)\n",
    "    minus_loss = -loss\n",
    "    \n",
    "    return minus_loss\n",
    "\n",
    "def studentLoss(output,target):\n",
    "    \n",
    "    student_pred = F.log_softmax(output)\n",
    "    teacher_pred = F.softmax(target)\n",
    "    \n",
    "    loss = F.kl_div(student_pred,teacher_pred)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(n_batches,lr_gen,lr_stud,batch_size,test_batch_size,g_input_dim,ng,ns,test_freq):\n",
    "    \n",
    "    device = 'cuda:0'\n",
    "    \n",
    "    # Get the data\n",
    "    train_loader, val_loader, test_loader = getData(batch_size,test_batch_size,0.1)\n",
    "    \n",
    "    test_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    teacher = Wide_ResNet(16,2,0,10)\n",
    "    teacher = teacher.to(device)\n",
    "    teacher.load_state_dict(torch.load('./pretrained_models/cifar_net_test.pth'))\n",
    "    \n",
    "    generator = Generator(z_dim=g_input_dim)\n",
    "    generator = generator.to(device)\n",
    "    generator.train()\n",
    "    \n",
    "    student = Wide_ResNet(16,1,0,10)\n",
    "    student = student.to(device)\n",
    "    \n",
    "    generator_optim = torch.optim.Adam(generator.parameters(), lr=lr_gen)\n",
    "    gen_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(generator_optim, n_batches)\n",
    "    \n",
    "    student_optim = torch.optim.Adam(student.parameters(), lr=lr_stud)\n",
    "    stud_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(generator_optim, n_batches)\n",
    "    \n",
    "    print('Teacher net test:')\n",
    "    test_loss, test_accuracy = test(test_loader,teacher,test_loss,device)\n",
    "    print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "    \n",
    "    print('Student net test:')\n",
    "    test_loss, test_accuracy = test(test_loader,teacher,test_loss,device)\n",
    "    print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        print('Batch ' + str(i))\n",
    "        noise = torch.randn(batch_size,g_input_dim)\n",
    "        noise = noise.to(device)\n",
    "        \n",
    "        gen_loss_print = 0\n",
    "        \n",
    "        for j in range(ng):\n",
    "            gen_imgs = generator(noise)\n",
    "            gen_imgs = gen_imgs.to(device)\n",
    "\n",
    "            teacher_pred, *teacher_activations = teacher(gen_imgs)\n",
    "            student_pred, *student_activations = student(gen_imgs)\n",
    "\n",
    "            gen_loss = genLoss(student_pred,teacher_pred)\n",
    "            generator_optim.zero_grad()\n",
    "            gen_loss.backward()\n",
    "\n",
    "            generator_optim.step()\n",
    "            \n",
    "            gen_loss_print += gen_loss.item()\n",
    "        \n",
    "        print('Gen loss :' + str(gen_loss_print/ng) )\n",
    "        \n",
    "        stud_loss_print = 0\n",
    "        for j in range(ns):\n",
    "            student.train()\n",
    "            gen_imgs = generator(noise)\n",
    "            teacher_pred, *teacher_activations = teacher(gen_imgs)\n",
    "            student_pred, *student_activations = student(gen_imgs)\n",
    "            \n",
    "            stud_loss = studentLoss(student_pred,teacher_pred)\n",
    "            student_optim.zero_grad()\n",
    "            stud_loss.backward()\n",
    "            student_optim.step()\n",
    "            \n",
    "            stud_loss_print += stud_loss.item()\n",
    "        \n",
    "        print('Stud loss :' + str(stud_loss_print/ns) )\n",
    "            \n",
    "        stud_scheduler.step()\n",
    "        gen_scheduler.step()\n",
    "        \n",
    "        if(i % test_freq) == 0:\n",
    "            print('Student net test:')\n",
    "            test_loss, test_accuracy = test(test_loader,teacher,test_loss,device)\n",
    "            print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "    \n",
    "n_batches = 100\n",
    "lr_gen = 2e-3\n",
    "lr_stud = 2e-3\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "g_input_dim = 100\n",
    "ng = 1\n",
    "ns = 10\n",
    "test_freq = 5\n",
    "    \n",
    "main(n_batches,lr_gen,lr_stud,batch_size,test_batch_size,g_input_dim,ng,ns,test_freq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
