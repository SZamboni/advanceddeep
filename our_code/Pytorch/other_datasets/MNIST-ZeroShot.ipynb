{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture teacher : WRN-40-2 , from file ./MNIST-teacher-40-2.pth\n",
      "Architecture student : WRN-16-1 , to file ./MNIST-t-40-2-student-16-1.pth\n",
      "Batches: 80001 , batch_size 128 , test_batch_size 128\n",
      "Student lr: 0.001 , Generator lr: 0.001 , Beta: 250\n",
      "Ng: 1 , Ns: 10\n",
      "Test_freq: 100 , g_input_dim100\n",
      "Device: cuda:0\n",
      "| Wide-Resnet 40x2\n",
      "| Wide-Resnet 16x1\n",
      "Teacher net test:\n",
      "\t Test loss: \t 0.000180, \t Test accuracy \t 99.20\n",
      "Student net test:\n",
      "\t Test loss: \t 0.018199, \t Test accuracy \t 9.58\n",
      "Starting training\n",
      "Batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/torch/nn/functional.py:1932: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss :-0.15508654713630676\n",
      "Stud loss :0.5823781847953796\n",
      "Student net test:\n",
      "\t Test loss: \t 0.021252, \t Test accuracy \t 9.82\n",
      "Saving\n",
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-122fd32dbe0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m main(n_batches,lr_gen,lr_stud,batch_size,test_batch_size,g_input_dim,ng,ns,test_freq,beta, t_depth, t_width,\n\u001b[0;32m--> 261\u001b[0;31m         s_depth, s_width, teacher_file, student_file, device, teach_device)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-122fd32dbe0f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(n_batches, lr_gen, lr_stud, batch_size, test_batch_size, g_input_dim, ng, ns, test_freq, beta, t_depth, t_width, s_depth, s_width, teacher_file, student_file, device, teach_device)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mstudent_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mstud_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mstudent_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "from channel_wide_res_net import Channel_Wide_ResNet\n",
    "\n",
    "from generator_mnist import Generator\n",
    "\n",
    "'''\n",
    "Function that loads the dataset and returns the data-loaders\n",
    "'''\n",
    "def getData(batch_size,test_batch_size,val_percentage):\n",
    "    # Normalize the training set with data augmentation\n",
    "    transform_train = transforms.Compose([ \n",
    "        torchvision.transforms.Resize(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "    \n",
    "    # Normalize the test set same as training set without augmentation\n",
    "    transform_test = transforms.Compose([ \n",
    "        torchvision.transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "\n",
    "    # Download/Load data\n",
    "    full_training_data = torchvision.datasets.MNIST('/home/test/data',train = True,transform=transform_train,download=True)  \n",
    "    test_data = torchvision.datasets.MNIST('/home/test/data',train = False,transform=transform_test,download=True)  \n",
    "\n",
    "    # Create train and validation splits\n",
    "    num_samples = len(full_training_data)\n",
    "    training_samples = int((1-val_percentage)*num_samples+1)\n",
    "    validation_samples = num_samples - training_samples\n",
    "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
    "\n",
    "    # Initialize dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(training_data,batch_size=batch_size,shuffle=True, drop_last=True, num_workers = 4)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data,batch_size=batch_size,shuffle=False, drop_last=False, num_workers = 2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,batch_size=test_batch_size,shuffle=False, drop_last=False, num_workers = 2)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "'''\n",
    "Function to test that returns the loss per sample and the total accuracy\n",
    "'''\n",
    "def test(data_loader,net,device):\n",
    "    net.eval()\n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "    cumulative_accuracy = 0.\n",
    "    \n",
    "    loss_funct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (inputs,targets) in enumerate(data_loader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = net(inputs)[0]\n",
    "        \n",
    "        loss = loss_funct(outputs,targets)\n",
    "\n",
    "        # Metrics computation\n",
    "        samples+=inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
    "\n",
    "'''\n",
    "generator loss:\n",
    "@output : logits of the student\n",
    "@output : logits of the teacher\n",
    "\n",
    "for the KL div as said here https://discuss.pytorch.org/t/kl-divergence-produces-negative-values/16791/4\n",
    "and here https://discuss.pytorch.org/t/kullback-leibler-divergence-loss-function-giving-negative-values/763/2\n",
    "the inputs should be logprobs for the output(student) and probabilities for the targets(teacher)\n",
    "\n",
    "this was very difficult to understand \n",
    "\n",
    "'''\n",
    "def attention(x):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/szagoruyko/attention-transfer\n",
    "    :param x = activations\n",
    "    \"\"\"\n",
    "    return F.normalize(x.pow(2).mean(1).view(x.size(0), -1))\n",
    "\n",
    "\n",
    "def attention_diff(x, y):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/szagoruyko/attention-transfer\n",
    "    :param x = activations\n",
    "    :param y = activations\n",
    "    \"\"\"\n",
    "    return (attention(x) - attention(y)).pow(2).mean()\n",
    "\n",
    "\n",
    "def divergence(student_logits, teacher_logits):\n",
    "    divergence = F.kl_div(F.log_softmax(student_logits, dim=1), F.softmax(teacher_logits, dim=1))\n",
    "\n",
    "    return divergence\n",
    "\n",
    "\n",
    "def KT_loss_generator(student_logits, teacher_logits):\n",
    "\n",
    "    divergence_loss = divergence(student_logits, teacher_logits)\n",
    "    total_loss = - divergence_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def KT_loss_student(student_logits, teacher_logits, student_activations, teacher_activations,beta):\n",
    "\n",
    "    divergence_loss = divergence(student_logits, teacher_logits)\n",
    "    if beta > 0:\n",
    "        at_loss = 0\n",
    "        for i in range(len(student_activations)):\n",
    "            at_loss = at_loss + beta * attention_diff(student_activations[i], teacher_activations[i])\n",
    "    else:\n",
    "        at_loss = 0\n",
    "\n",
    "    total_loss = divergence_loss + at_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "def main(n_batches,lr_gen,lr_stud,batch_size,test_batch_size,g_input_dim,ng,ns,test_freq,beta, t_depth, t_width,\n",
    "        s_depth, s_width, teacher_file, student_file, device,teach_device):\n",
    "    \n",
    "    # Print info experiment\n",
    "    print('Architecture teacher : WRN-' + str(t_depth) + '-' + str(t_width) + ' , from file ' + str(teacher_file))\n",
    "    print('Architecture student : WRN-' + str(s_depth) + '-' + str(s_width)+ ' , to file ' + str(student_file))\n",
    "    print('Batches: ' + str(n_batches) + ' , batch_size ' + str(batch_size) + ' , test_batch_size ' + str(test_batch_size))\n",
    "    print('Student lr: ' + str(lr_gen) + ' , Generator lr: ' + str(lr_gen) + ' , Beta: ' + str(beta))\n",
    "    print('Ng: ' + str(ng) + ' , Ns: ' + str(ns))\n",
    "    print('Test_freq: ' + str(test_freq) + ' , g_input_dim' + str(g_input_dim))\n",
    "    print('Device: ' + str(device))\n",
    "    \n",
    "    # Get the data\n",
    "    train_loader, val_loader, test_loader = getData(batch_size,test_batch_size,0.1)\n",
    "    \n",
    "    # Get the teacher\n",
    "    teacher = Channel_Wide_ResNet(1,t_depth,t_width,0,10)\n",
    "    teacher.load_state_dict(torch.load(teacher_file, map_location={teach_device: device}))\n",
    "    teacher = teacher.to(device)\n",
    "    \n",
    "    # Create the generator\n",
    "    generator = Generator(z_dim=g_input_dim)\n",
    "    generator = generator.to(device)\n",
    "    generator.train()\n",
    "    \n",
    "    # Create the student\n",
    "    student = Channel_Wide_ResNet(1,s_depth,s_width,0,10)\n",
    "    student = student.to(device)\n",
    "    student.train()\n",
    "    \n",
    "    # Create optimizers (Adam) and LRschedulers(CosineAnnealing) for the generator and the student\n",
    "    generator_optim = torch.optim.Adam(generator.parameters(), lr=lr_gen)\n",
    "    gen_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(generator_optim, n_batches)\n",
    "    \n",
    "    student_optim = torch.optim.Adam(student.parameters(), lr=lr_stud)\n",
    "    stud_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(student_optim, n_batches)\n",
    "    \n",
    "    print('Teacher net test:')\n",
    "    test_loss, test_accuracy = test(test_loader,teacher,device)\n",
    "    teacher.eval()\n",
    "    print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "    \n",
    "    print('Student net test:')\n",
    "    test_loss, test_accuracy = test(test_loader,student,device)\n",
    "    print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "    \n",
    "    print('Starting training')\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        noise = torch.randn(batch_size,g_input_dim)\n",
    "        noise = noise.to(device)\n",
    "        \n",
    "        gen_loss_print = 0\n",
    "        \n",
    "        for j in range(ng):\n",
    "            gen_imgs = generator(noise)\n",
    "            gen_imgs = gen_imgs.to(device)\n",
    "\n",
    "            teacher_pred, *teacher_activations = teacher(gen_imgs)\n",
    "            student_pred, *student_activations = student(gen_imgs)\n",
    "\n",
    "            gen_loss = KT_loss_generator(student_pred,teacher_pred)\n",
    "            generator_optim.zero_grad()\n",
    "            gen_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), 5)\n",
    "\n",
    "            generator_optim.step()\n",
    "            \n",
    "            gen_loss_print += gen_loss.item()\n",
    "        \n",
    "        stud_loss_print = 0\n",
    "        for j in range(ns):    \n",
    "            with torch.no_grad(): # no_grad speeds up computation\n",
    "                teacher_pred, *teacher_activations = teacher(gen_imgs)\n",
    "                gen_imgs = generator(noise)\n",
    "                \n",
    "            student_pred, *student_activations = student(gen_imgs)\n",
    "            \n",
    "            stud_loss = KT_loss_student(student_pred,teacher_pred, student_activations,teacher_activations, beta )\n",
    "            student_optim.zero_grad()\n",
    "            stud_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), 5)\n",
    "            student_optim.step()\n",
    "            \n",
    "            stud_loss_print += stud_loss.item()\n",
    "            \n",
    "        stud_scheduler.step()\n",
    "        gen_scheduler.step()\n",
    "        \n",
    "        if(i % test_freq) == 0:\n",
    "            print('Gen loss :' + str(gen_loss_print/ng) )\n",
    "            print('Stud loss :' + str(stud_loss_print/ns) )\n",
    "            print('Student net test:')\n",
    "            test_loss, test_accuracy = test(test_loader,student,device)\n",
    "            print('\\t Test loss: \\t {:.6f}, \\t Test accuracy \\t {:.2f}'.format(test_loss, test_accuracy))\n",
    "            print('Saving')\n",
    "            torch.save(student.state_dict(),student_file)\n",
    "            student.train()\n",
    "            \n",
    "    print('Finished and saving')\n",
    "    torch.save(student.state_dict(),student_file)\n",
    "            \n",
    "    \n",
    "n_batches = 10000\n",
    "lr_gen = 1e-3\n",
    "lr_stud = 2e-3\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "g_input_dim = 100\n",
    "ng = 1\n",
    "ns = 10\n",
    "test_freq = 100\n",
    "beta = 250\n",
    "t_depth = 40\n",
    "t_width = 2\n",
    "s_depth = 16\n",
    "s_width = 1\n",
    "teacher_file = './MNIST-teacher-'+str(t_depth) + '-' + str(t_width) + '.pth'\n",
    "student_file = './MNIST-t-' + str(t_depth) + '-' + str(t_width) + '-student-' + str(s_depth) + '-' + str(s_width) + '.pth'\n",
    "device = 'cuda:0'\n",
    "teach_device= 'cuda:0'\n",
    "    \n",
    "main(n_batches,lr_gen,lr_stud,batch_size,test_batch_size,g_input_dim,ng,ns,test_freq,beta, t_depth, t_width,\n",
    "        s_depth, s_width, teacher_file, student_file, device, teach_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
