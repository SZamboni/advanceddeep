{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-df5701e0052b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#from wrnet_model import create_wide_residual_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprova_wrn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_wide_residual_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m '''\n\u001b[1;32m     33\u001b[0m \u001b[0mFunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtrainand\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCIFAR10\u001b[0m \u001b[0malready\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/advanceddeep/our_code/older failed tests/prova_wrn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.losses import KLDivergence\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#from wrnet_model import create_wide_residual_network\n",
    "from prova_wrn import create_wide_residual_network\n",
    "'''\n",
    "Function that returns the trainand test data of the CIFAR10 already preprocessed\n",
    "'''\n",
    "def getCIFAR10():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 32, 32\n",
    "    num_classes = 10\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # format of the tensor\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "        input_shape = (3, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "    # convert in to float the images\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # new normalization with z-score\n",
    "    mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "    std = np.std(x_train,axis=(0,1,2,3))\n",
    "    x_train = (x_train-mean)/(std+1e-7)\n",
    "    x_test = (x_test-mean)/(std+1e-7)\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    print('CIFAR10 loaded')\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "'''\n",
    "Small function that returns the shape of the CIFAR10 images\n",
    "'''\n",
    "def getCIFAR10InputShape():\n",
    "    img_rows, img_cols = 32, 32\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 3)\n",
    "        \n",
    "    return input_shape\n",
    "\n",
    "'''\n",
    "Function that loads from a file the teacher\n",
    "'''\n",
    "def getTeacher(file_name):\n",
    "    with open(file_name + '.json', 'r') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(file_name + '.h5')\n",
    "    \n",
    "    with open(file_name + '_layer1.json', 'r') as f:\n",
    "        m1 = model_from_json(f.read())\n",
    "    m1.load_weights(file_name + '_layer1.h5')\n",
    "    \n",
    "    with open(file_name + '_layer2.json', 'r') as f:\n",
    "        m2 = model_from_json(f.read())\n",
    "    m2.load_weights(file_name + '_layer2.h5')\n",
    "    \n",
    "    with open(file_name + '_layer3.json', 'r') as f:\n",
    "        m3 = model_from_json(f.read())\n",
    "    m3.load_weights(file_name + '_layer3.h5')    \n",
    "\n",
    "\"\"\"def getTeacher(file_name):\n",
    "    # Model reconstruction from JSON file\n",
    "    with open(file_name + '.json', 'r') as f:\n",
    "        model = model_from_json(f.read())\n",
    "\n",
    "    # Load weights into the new model\n",
    "    model.load_weights(file_name + '.h5')\n",
    "    \n",
    "    print('Teacher loaded from ' + file_name + '.h5')\n",
    "    return model\"\"\"\n",
    "    \n",
    "'''\n",
    "Function that loads from a file the teacher and test it on the CIRAF10 dataset\n",
    "'''\n",
    "def testTeacher(file_name):\n",
    "    \n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    \n",
    "    model = getTeacher(file_name)\n",
    "    \n",
    "    # define optimizer\n",
    "    opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=opt_rms,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # final evaluation on test\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Teacher test loss:', score[0])\n",
    "    print('Teacher test accuracy:', score[1])\n",
    "\n",
    "    \n",
    "'''\n",
    "Function that returns a simple student done by 2 convolutions, a maxpool and a final two fully connected layers\n",
    "'''\n",
    "def getStudent(input_shape):\n",
    "    num_classes = 10\n",
    "    model1,model2=create_wide_residual_network_student(input_shape, num_classes, N=2, k=1, dropout=0.)\n",
    "    \n",
    "    print('Student loaded')\n",
    "    return model1,model2\n",
    "    \n",
    "'''\n",
    "Function to try to train the simple sutdent in order to unerstand its capabilites\n",
    "'''\n",
    "def trainStudent(epochs):\n",
    "    \n",
    "    x_train,y_train,x_test,y_test = getCIFAR10()\n",
    "    \n",
    "    input_shape = getCIFAR10InputShape()\n",
    "    \n",
    "    model = getStudent(input_shape)\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    batch_size = 128\n",
    "    n_batches = math.floor( x_train.shape[0] / batch_size)\n",
    "\n",
    "    for e in range(epochs):\n",
    "    \n",
    "        for i in range(0,n_batches):\n",
    "            imgs = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            loss = model.train_on_batch(imgs,labels)\n",
    "            print(\"Epoch: \" + str(e+1) + \" batch \" + str(i) + \" loss: \" + str(loss[0]) + \" acc: \" + str( 100*loss[1]))\n",
    "            \n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('After epoch ' + str(e+1) + ' test loss ' + str(score[0]) + ' test accuracy ' + str(score[1]))\n",
    "\n",
    "'''\n",
    "Function that returns a simple generator\n",
    "'''\n",
    "def getGenerator():\n",
    "\n",
    "        noise_shape = (100,)\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        img_shape = getCIFAR10InputShape()\n",
    "\n",
    "        model.add(Dense(128*8**2, input_shape=noise_shape))\n",
    "        model.add(Reshape((8, 8, 128)))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2)) \n",
    "                  \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(3, kernel_size=(3,3), strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization())   \n",
    "        \n",
    "        #model.summary()\n",
    "        print('Generator loaded')\n",
    "        return model\n",
    "\n",
    "def mypositiveloss(y_true,y_pred):\n",
    "    \n",
    "    #y_true = K.log(y_true)\n",
    "    #y_pred = K.log(y_pred)\n",
    "    \n",
    "    #loss=  keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    loss= tf.keras.losses.KLD(y_true, y_pred)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def mynegativeloss(y_true,y_pred):\n",
    "    \n",
    "    #y_true = K.log(y_true)\n",
    "    #y_pred = K.log(y_pred)\n",
    "    \n",
    "    #loss= keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    loss= tf.keras.losses.KLD(y_true, y_pred)\n",
    "    gen_loss = -loss\n",
    "    \n",
    "    return gen_loss\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = getCIFAR10()\n",
    "    \n",
    "    input_shape = getCIFAR10InputShape()\n",
    "    \n",
    "    teacher, t_layer1, t_layer2,t_layer3 = getTeacher('./pretrained_models/wrn_16_2')\n",
    "    teacher.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    t_layer1.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    t_layer2.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    t_layer3.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "\n",
    "    optim_stud = Adam(lr=2e-3, clipnorm=5.0)\n",
    "    optim_gen = Adam(lr=1e-3, clipnorm=5.0)\n",
    "\n",
    "    \n",
    "    student_train, student_test = getStudent(input_shape)\n",
    "    \n",
    "    student_test.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    student_train.compile(loss=keras.losses.kullback_leibler_divergence, optimizer=optim_stud)\n",
    "    \n",
    "    generator = getGenerator()\n",
    "    \n",
    "\"\"\"     \n",
    "    print('TEACHER SUMMARY:')\n",
    "    teacher.summary()\n",
    "    print('TEACHER L1 SUMMARY:')\n",
    "    t_layer1.summary()\n",
    "    print('TEACHER L2 SUMMARY:')\n",
    "    t_layer2.summary()\n",
    "    print('TEACHER L3 SUMMARY:')\n",
    "    t_layer3.summary()\n",
    "    \n",
    "    print('STUDENT SUMMARY:')\n",
    "    student_train.summary()\n",
    "\"\"\"    \n",
    "    generator.compile(loss=mynegativeloss, optimizer=optim_gen)\n",
    "    \n",
    "    student_train.trainable=False\n",
    "    #teacher.trainable = False\n",
    "\n",
    "    z = Input(shape=(100,))\n",
    "\n",
    "    #Generator makes a prediction\n",
    "    fake_imgs = generator(z)\n",
    "\n",
    "    #Discriminator attempts to categorise prediction\n",
    "    y = student_train(fake_imgs)\n",
    "\n",
    "    #gan = Model(z, y)\n",
    "    #gan.compile(loss=mynegativeloss,optimizer=optim_gen)\n",
    "    \n",
    "    t = teacher(fake_imgs)\n",
    "    #gan2 = Model(z,t)\n",
    "    #gan2.compile(loss=mypositiveswappedloss,optimizer=optim_gen)\n",
    "    \"\"\"\n",
    "    print('Student Summary:')\n",
    "    student_train.summary()\n",
    "    print('Generator Summary:')\n",
    "    generator.summary()\n",
    "    \"\"\"\n",
    "    \n",
    "    n_batches = 1000\n",
    "    batch_size = 128\n",
    "    log_freq = 10\n",
    "    ns = 10\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        \n",
    "        gen_imgs = generator.predict(noise)\n",
    "        \n",
    "        \n",
    "        #this is the cycle for generator (for i=0..ng)\n",
    "        t_predictions = teacher.predict(gen_imgs)\n",
    "        \n",
    "        s_predictions = student_train.predict(gen_imgs)\n",
    "        \n",
    "        #generator_loss= mynegativeloss(t_predictions,s_predictions)\n",
    "        \n",
    "        gen_loss = generator.train_on_batch(noise,s_predictions)\n",
    "        \n",
    "        \n",
    "        s_loss = 0\n",
    "        for j in range(ns):\n",
    "            \n",
    "            t_predictions = teacher.predict(gen_imgs)\n",
    "        \n",
    "            s_predictions sa1, sa2, sa3 = student_train.predict(gen_imgs)\n",
    "            \n",
    "            s_loss += student_train.train_on_batch(gen_imgs,s_predictions)\n",
    "        \n",
    "        print('batch ' + str(i) + '/' + str(n_batches) + ' G loss: ' + str(g_loss) + ' S loss: ' + str(s_loss/ns))\n",
    "        \n",
    "        if (i % log_freq) == 0:\n",
    "            score = student_test.evaluate(x_test, y_test, verbose=0)\n",
    "            print('Student test loss: '  + str(score))\n",
    "            \n",
    "            model_json = student_test.to_json()\n",
    "            with open('tmp-model' + str(i) + '.json','w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            student_test.save_weights('tmp-model' + str(i) + '.h5')\n",
    "            print('saved model ' + str(i))\n",
    "   \n",
    "        \n",
    "        \n",
    "    score = student_test.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Final student test loss: '  + str(score))\n",
    "\n",
    "    '''\n",
    "    for i in range(0,n_batches):\n",
    "        print('batch ' + str(i))\n",
    "        imgs = x_train[i*batch_size:(i+1)*batch_size]\n",
    "        labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "        t_predictions = teacher.predict(imgs)\n",
    "        s_predictions = student.predict(imgs)\n",
    "        print('teacher predictions: ')\n",
    "        print(t_predictions)\n",
    "        print('student predictions: ')\n",
    "        print(s_predictions)\n",
    "        \n",
    "        kl_div = tf.keras.losses.KLDivergence()\n",
    "        \n",
    "        \n",
    "        # to print the KL divergence\n",
    "        for j in range(batch_size):\n",
    "            loss1 = kl_div(t_predictions[j],s_predictions[j])\n",
    "            loss2 = tf.keras.losses.categorical_crossentropy(t_predictions[j],s_predictions[j])\n",
    "            with tf.Session() as sess:\n",
    "                init = tf.global_variables_initializer()\n",
    "                sess.run(init)\n",
    "                print('sample ' + str(j) + ' kl div: ' + str(loss1.eval()) + ' cat loss: ' + str(loss2.eval()))\n",
    "        '''\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
